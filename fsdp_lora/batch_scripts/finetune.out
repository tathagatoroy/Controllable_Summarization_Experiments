==========================================
SLURM_JOB_ID = 1198362
SLURM_NODELIST = gnode071
SLURM_JOB_GPUS = 0,1,2,3
==========================================
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
World size: 4
{'world_size': 4, 'train_type': 'qlora', 'llama_pro_path': None, 'batch_size': 1, 'context_length': 2048, 'gradient_accumulation_steps': 8, 'num_epochs': 1, 'dataset': 'macsum', 'macsum_path': '/home2/tathagato/summarization/MACSUM/dataset/macdoc/train_dataset.json', 'dataset_samples': -1, 'sharding_strategy': 'full_shard', 'use_gradient_checkpointing': 1, 'reentrant_checkpointing': 1, 'use_cpu_offload': 1, 'use_activation_cpu_offload': False, 'low_memory': True, 'no_sync': False, 'precision': 'bf16', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'save_model': 1, 'output_dir': '/scratch/tathagato/fsdp_qlora_experiments_30_August_mistral/extractiveness', 'lora_rank': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'lora_target_modules': 'all', 'verbose': 1, 'lr': 5e-05, 'apply_gradient_clipping': 1, 'grad_norm': 1.0, 'wd': 0.1, 'profile_memory': False, 'optimizer': 'adamw', 'lr_scheduler': 'cosine', 'loading_workers': -1, 'log_to': 'wandb', 'master_addr': 'localhost', 'master_port': '12355', 'seed': 42, 'project_name': 'fsdp_qlora', 'name': None, 'group': None, 'entity': None, 'n_bits': 4, 'profile': False, 'profiling_output': 'profiles', 'overwrite_profiling_output': True, 'with_stack': False, 'with_shapes': False, 'export_trace': True, 'export_memory_timeline': False, 'wait_steps': 0, 'warmup_steps': 1, 'active_steps': 2, 'repeat': 0, 'profiling_frequency': 10, 'max_steps': -1, 'attribute': 'extractiveness'}
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
Creating model 3
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Creating model 2
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Creating model 1
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSUM/fsdp_lora/wandb/run-20240830_052152-fdtxv9xb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sun-67
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/fdtxv9xb
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
dataset chosen is :  macsum
Example input_ids shape:  torch.Size([1, 682])
Example labels shape:  torch.Size([1, 682])
example input 
<s>[INST] Write a summary of the source text. The summary should be normal in extractiveness. Extractiveness is defined by the degree of exact copying from the source text. The source text is given below.  (CNN)Fans of the late actor Paul Walker knew that watching him in "Furious 7" would be bittersweet. Even so, many moviegoers said the final scenes of the new film, which earned a record $146 million over the weekend, still packed an emotional wallop. "Not gonna lie, I shed a few tears at the end of Furious 7. The tribute to Paul Walker was very well done," one woman said Monday on Twitter. Hers was just one of a flood of messages on social media from people who said they got choked up during scenes featuring Walker, who died at 40 in a car crash in November 2013, before filming on "Furious 7" was completed. To finish Walker's scenes, the makers of the movie used body doubles, computer-generated images and even the actor's brothers. But it was the ending that really got to moviegoers. In finishing "Furious 7," the film's producers sought to retire Walker's character, Brian, while paying homage to his role in the blockbuster "Furious" action franchise. But they felt that killing him off might appear exploitative. "If they had gone down the other path, I think I would have refused to finish making this movie," director James Wan told BuzzFeed. Instead, the movie's makers chose to "retire Paul's character in the most sincere and elegant way (they) could," Wan said. Their idea was to have Brian retire from his dangerous, high-octane lifestyle out of a sense of responsibility to his growing family with girlfriend Mia, who is pregnant with their second child. A scene late in the movie shows him and Mia playing on a beach with their son while the crew looks on -- essentially saying goodbye. Then his longtime buddy Dom reminisces about their years together, leading to a montage of Walker scenes from the first six movies. The song that plays over the montage is  "See You Again," a collaboration between Wiz Khalifa and Charlie Puth. Co-star Vin Diesel shared the video for the song late Sunday on his Facebook page, where it has more than 1.5 million likes. Fans on Twitter and Facebook mostly praised the movie's ending as a fitting tribute -- and an emotionally wrenching one. "Man I don't care how tough u are or how gangsta u claim to be . ...the last five minutes had me choked up in the movie theater ... I saw it 3 times in one day ... ...the ending is the deepest ending I've ever seen," one man wrote on the movie's Facebook page. [/INST]" The tribute to Paul Walker was very well done," said a fan after watching the Furious 7. The actor was on break from filming "Furious 7" at the time of the fiery accident which also claimed the life of the car's driver, Roger Rodas.</s>
tensor([[    1,     3, 12786,  1032, 14828,  1070,  1040,  3600,  3013, 29491,
          1183, 14828,  1791,  1115,  4891,  1065,  9899, 10760, 29491,  2297,
          2875, 10760,  1117,  4825,  1254,  1040,  6921,  1070,  4227,  3850,
          1056,  1245,  1040,  3600,  3013, 29491,  1183,  3600,  3013,  1117,
          2846,  4392, 29491, 29473,  1093, 29511, 12116, 29499, 29533,  1277,
          1070,  1040,  4677, 11732,  4688, 18502,  3348,  1137,  7033,  1481,
          1065,  1113, 29533, 26863, 29473, 29555, 29507,  1450,  1115,  3054,
          2300, 13291, 29491,  4895,  1347, 29493,  2055,  6762,  2412,  1172,
          1541,  1040,  2248, 15148,  1070,  1040,  1401,  3734, 29493,  1458,
         13607,  1032,  3163,  1197, 29508, 29549, 29552,  4609,  1522,  1040,
          9839, 29493,  2077, 15471,  1164, 11294,  4268,  1178, 29491,  1113,
          3369, 10343,  5620, 29493,  1083, 20619,  1032,  2432, 11793,  1206,
          1040,  1716,  1070,  8947,  1693, 29473, 29555, 29491,  1183,  1029,
          2751,  1066,  4688, 18502,  1171,  1983,  1930,  2971,  1630,  1392,
          3739,  1541, 10030,  1124, 12458, 29491,  1150,  1172,  1171,  1544,
          1392,  1070,  1032, 18036,  1070,  9338,  1124,  3577,  4845,  1245,
          1673,  1461,  1541,  1358,  2201,  1252,  9409,  1350,  2706, 15148,
         17080, 18502, 29493,  1461,  5615,  1206, 29473, 29549, 29502,  1065,
          1032,  2021, 15224,  1065,  5117, 29473, 29518, 29502, 29508, 29538,
         29493,  1927,  3734,  1056,  1124,  1113, 29533, 26863, 29473, 29555,
         29507,  1171,  8136, 29491,  2559,  8214, 18502, 29510, 29481, 15148,
         29493,  1040,  1058,  9542,  1070,  1040,  6762,  2075,  2955,  5027,
         10111, 29493,  6842, 29501, 12937,  6971,  1072,  1787,  1040, 11732,
         29510, 29481, 13414, 29491,  1860,  1146,  1171,  1040, 13594,  1137,
          2296,  2201,  1066,  6762,  2412,  1172, 29491,  1328, 20332,  1113,
         29533, 26863, 29473, 29555,  1630,  1040,  3734, 29510, 29481, 23630,
         13801,  1066, 26801, 18502, 29510, 29481,  4001, 29493, 13337, 29493,
          2080, 10728,  3921,  1233,  1066,  1284,  4673,  1065,  1040,  3492,
         29494,  4631,  1113, 29533, 26863, 29507,  3760, 21782, 29491,  1860,
          1358,  3538,  1137, 11703,  1481,  1573,  2427,  5073, 13341, 17230,
         29491,  1113,  4149,  1358,  1321,  4982,  1828,  1040,  1567,  3207,
         29493,  1083,  1841,  1083,  1450,  1274, 12158,  1066,  8214,  3260,
          1224,  6762,  1630,  7627,  5565,  1162,  1044,  3008,  1133,  9814,
         21735, 29491,  8930, 29493,  1040,  6762, 29510, 29481,  1058,  9542,
         10776,  1066,  1113,  2209,  1304,  4688, 29510, 29481,  4001,  1065,
          1040,  1848, 19793,  1165,  1072, 21351,  1837,  1093, 14028, 29499,
          1597,  1630,  1162,  1044,  1541, 29491,  7491,  3796,  1171,  1066,
          1274, 13337, 26801,  1245,  1284, 10027, 29493,  2254, 29501, 22776,
          2332, 16986,  1343,  1070,  1032,  4135,  1070, 10448,  1066,  1284,
          7253,  2773,  1163, 17159,  1119,  1283, 29493,  1461,  1117, 16214,
          1163,  1420,  2444,  2270, 29491,  1098,  7105,  4677,  1065,  1040,
          6762,  5138,  1481,  1072,  1119,  1283,  5311,  1124,  1032, 11073,
          1163,  1420,  2734,  2080,  1040, 10273,  5442,  1124,  2707, 14083,
          4445,  1947, 18432, 29491,  3247,  1284,  1811,  2304, 26891,  7479,
          1771, 25247,  2145,  1452,  1420,  2035,  3321, 29493,  6142,  1066,
          1032, 12663,  1233,  1070, 18502, 15148,  1245,  1040,  1675,  4290,
         11383, 29491,  1183,  4802,  1137,  9696,  1522,  1040, 12663,  1233,
          1117, 29473,  1113,  8760,  1763, 10474,  1630,  1032, 16377,  2212,
          1162,  1231,  1292,  6787, 26390,  1072, 13963,  1135,  3425, 29491,
          3860, 29501,  8658, 13438, 12171,  1069,  7199,  1040,  4566,  1122,
          1040,  4802,  4677,  8497,  1124,  1284,  9256,  3652, 29493,  1738,
          1146,  1427,  1448,  1589, 29473, 29508, 29491, 29550,  4609, 13440,
         29491,  1169,  1277,  1124, 12458,  1072,  9256,  8212, 29061,  1040,
          6762, 29510, 29481, 13594,  1158,  1032, 20836,  1029,  2751,  2707,
          1072,  1164, 27126,  1043,  5052,  1056,  1392, 29491,  1113,  3124,
          1083,  1717, 29510, 29475,  2424,  1678, 11110,  1100,  1228,  1210,
          1678, 13839,  7282,  1100,  4220,  1066,  1115,  1610,  4618,  2005,
          2200,  4127,  4254,  1321,  1296,  1252,  9409,  1350,  1065,  1040,
          6762, 18131,  4618,  1083,  3440,  1146, 29473, 29538,  3189,  1065,
          1392,  2138,  4618,  4618,  2005, 13594,  1117,  1040,  4302,  1142,
         13594,  1083, 29510,  1101,  3038,  3366,  1630,  1392,  1444,  5445,
          1124,  1040,  6762, 29510, 29481,  9256,  3652, 29491, 29473,     4,
         29507,  1183,  1029,  2751,  1066,  4688, 18502,  1171,  1983,  1930,
          2971,  1630,  1541,  1032,  8422,  1792,  7033,  1040,  8947,  1693,
         29473, 29555, 29491,  1183, 11732,  1171,  1124,  2489,  1245,  3734,
          1056,  1113, 29533, 26863, 29473, 29555, 29507,  1206,  1040,  1495,
          1070,  1040, 17075, 29492,  9086,  1458,  1603, 11103,  1040,  2179,
          1070,  1040,  2021, 29510, 29481,  7071, 29493, 14883,  9583,  1061,
         29491,     2]])
Creating model 0
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|███▎      | 1/3 [01:31<03:02, 91.39s/it]Downloading shards:  33%|███▎      | 1/3 [01:31<03:02, 91.41s/it]Downloading shards:  33%|███▎      | 1/3 [01:20<02:41, 80.65s/it]Downloading shards:  33%|███▎      | 1/3 [01:31<03:02, 91.40s/it]Downloading shards:  67%|██████▋   | 2/3 [03:04<01:32, 92.31s/it]Downloading shards:  67%|██████▋   | 2/3 [02:53<01:27, 87.88s/it]Downloading shards:  67%|██████▋   | 2/3 [03:04<01:32, 92.33s/it]Downloading shards:  67%|██████▋   | 2/3 [03:04<01:32, 92.32s/it]Downloading shards: 100%|██████████| 3/3 [04:20<00:00, 87.42s/it]Downloading shards: 100%|██████████| 3/3 [04:20<00:00, 86.82s/it]
Loading model 0
Total model params: 7248023552
Using n_workers: 10 for loading
Loading & Quantizing Model Shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [04:31<00:00, 89.83s/it]Downloading shards: 100%|██████████| 3/3 [04:31<00:00, 90.41s/it]
Downloading shards: 100%|██████████| 3/3 [04:31<00:00, 89.85s/it]Downloading shards: 100%|██████████| 3/3 [04:31<00:00, 90.43s/it]
Downloading shards: 100%|██████████| 3/3 [04:31<00:00, 89.85s/it]Downloading shards: 100%|██████████| 3/3 [04:31<00:00, 90.43s/it]
Loading model 1
Loading model 2
Loading model 3
Loading & Quantizing Model Shards:  33%|███▎      | 1/3 [00:10<00:21, 10.50s/it]Loading & Quantizing Model Shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.16s/it]Loading & Quantizing Model Shards: 100%|██████████| 3/3 [00:29<00:00,  9.77s/it]Loading & Quantizing Model Shards: 100%|██████████| 3/3 [00:29<00:00,  9.91s/it]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Loaded model weights in 29.729 seconds
Rank 0: Model created: 0.105 GiB
trainable params: 75,497,472 || all params: 7,323,521,024 || trainable%: 1.0308903566001424
Wrapping model w/ FSDP 0
Rank 0: Wrapped model: 1.822 GiB
Applying activation checkpointing 0
Config:
MistralConfig {
  "_name_or_path": "mistralai/Mistral-7B-Instruct-v0.3",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": false,
  "vocab_size": 32768
}

Model:
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): MistralForCausalLM(
        (model): MistralModel(
          (embed_tokens): Embedding(32768, 4096)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): MistralDecoderLayer(
                  (self_attn): MistralSdpaAttention(
                    (q_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (k_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (v_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): MistralRotaryEmbedding()
                  )
                  (mlp): MistralMLP(
                    (gate_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (up_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (down_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=14336, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (act_fn): SiLU()
                  )
                  (input_layernorm): MistralRMSNorm()
                  (post_attention_layernorm): MistralRMSNorm()
                )
              )
            )
          )
          (norm): MistralRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32768, bias=False)
      )
    )
  )
)
Starting training
Optimizer params:
Shape: torch.Size([67109888]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Total Training Steps: 133
  0%|          | 0/133 [00:00<?, ?it/s]/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Epoch 0, Loss 0.000:   0%|          | 0/133 [00:00<?, ?it/s]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Epoch 0, Loss 0.000:   1%|          | 1/133 [01:39<3:38:30, 99.32s/it]Epoch 0, Loss 2.259, LR 3.85e-06:   1%|          | 1/133 [01:39<3:38:30, 99.32s/it]Epoch 0, Loss 2.259, LR 3.85e-06:   2%|▏         | 2/133 [03:39<4:03:10, 111.38s/it]Epoch 0, Loss 2.281, LR 7.69e-06:   2%|▏         | 2/133 [03:39<4:03:10, 111.38s/it]Epoch 0, Loss 2.281, LR 7.69e-06:   2%|▏         | 3/133 [05:40<4:11:03, 115.87s/it]Epoch 0, Loss 2.219, LR 1.15e-05:   2%|▏         | 3/133 [05:40<4:11:03, 115.87s/it]Epoch 0, Loss 2.219, LR 1.15e-05:   3%|▎         | 4/133 [07:31<4:04:51, 113.89s/it]Epoch 0, Loss 2.263, LR 1.54e-05:   3%|▎         | 4/133 [07:31<4:04:51, 113.89s/it]Epoch 0, Loss 2.263, LR 1.54e-05:   4%|▍         | 5/133 [09:20<3:59:08, 112.10s/it]Epoch 0, Loss 2.225, LR 1.92e-05:   4%|▍         | 5/133 [09:20<3:59:08, 112.10s/it]Epoch 0, Loss 2.225, LR 1.92e-05:   5%|▍         | 6/133 [11:16<4:00:35, 113.67s/it]Epoch 0, Loss 2.174, LR 2.31e-05:   5%|▍         | 6/133 [11:16<4:00:35, 113.67s/it]Epoch 0, Loss 2.174, LR 2.31e-05:   5%|▌         | 7/133 [13:13<4:00:59, 114.76s/it]Epoch 0, Loss 2.163, LR 2.69e-05:   5%|▌         | 7/133 [13:13<4:00:59, 114.76s/it]Epoch 0, Loss 2.163, LR 2.69e-05:   6%|▌         | 8/133 [15:01<3:54:17, 112.46s/it]Epoch 0, Loss 2.284, LR 3.08e-05:   6%|▌         | 8/133 [15:01<3:54:17, 112.46s/it]Epoch 0, Loss 2.284, LR 3.08e-05:   7%|▋         | 9/133 [16:51<3:50:53, 111.73s/it]Epoch 0, Loss 2.138, LR 3.46e-05:   7%|▋         | 9/133 [16:51<3:50:53, 111.73s/it]Epoch 0, Loss 2.138, LR 3.46e-05:   8%|▊         | 10/133 [18:44<3:49:48, 112.10s/it]Epoch 0, Loss 2.086, LR 3.85e-05:   8%|▊         | 10/133 [18:44<3:49:48, 112.10s/it]Epoch 0, Loss 2.086, LR 3.85e-05:   8%|▊         | 11/133 [20:33<3:45:45, 111.03s/it]Epoch 0, Loss 2.103, LR 4.23e-05:   8%|▊         | 11/133 [20:33<3:45:45, 111.03s/it]Epoch 0, Loss 2.103, LR 4.23e-05:   9%|▉         | 12/133 [22:17<3:39:51, 109.02s/it]Epoch 0, Loss 2.053, LR 4.62e-05:   9%|▉         | 12/133 [22:17<3:39:51, 109.02s/it]Epoch 0, Loss 2.053, LR 4.62e-05:  10%|▉         | 13/133 [24:13<3:42:25, 111.21s/it]Epoch 0, Loss 1.986, LR 5.00e-05:  10%|▉         | 13/133 [24:13<3:42:25, 111.21s/it]Epoch 0, Loss 1.986, LR 5.00e-05:  11%|█         | 14/133 [26:14<3:46:31, 114.21s/it]Epoch 0, Loss 1.902, LR 5.00e-05:  11%|█         | 14/133 [26:14<3:46:31, 114.21s/it]Epoch 0, Loss 1.902, LR 5.00e-05:  11%|█▏        | 15/133 [28:04<3:42:02, 112.91s/it]Epoch 0, Loss 1.954, LR 5.00e-05:  11%|█▏        | 15/133 [28:04<3:42:02, 112.91s/it]Epoch 0, Loss 1.954, LR 5.00e-05:  12%|█▏        | 16/133 [29:56<3:39:25, 112.53s/it]Epoch 0, Loss 1.921, LR 4.99e-05:  12%|█▏        | 16/133 [29:56<3:39:25, 112.53s/it]Epoch 0, Loss 1.921, LR 4.99e-05:  13%|█▎        | 17/133 [31:42<3:33:39, 110.52s/it]Epoch 0, Loss 1.949, LR 4.99e-05:  13%|█▎        | 17/133 [31:42<3:33:39, 110.52s/it]Epoch 0, Loss 1.949, LR 4.99e-05:  14%|█▎        | 18/133 [33:37<3:34:23, 111.85s/it]Epoch 0, Loss 1.867, LR 4.98e-05:  14%|█▎        | 18/133 [33:37<3:34:23, 111.85s/it]Epoch 0, Loss 1.867, LR 4.98e-05:  14%|█▍        | 19/133 [35:23<3:29:10, 110.09s/it]Epoch 0, Loss 1.958, LR 4.97e-05:  14%|█▍        | 19/133 [35:23<3:29:10, 110.09s/it]Epoch 0, Loss 1.958, LR 4.97e-05:  15%|█▌        | 20/133 [37:09<3:24:57, 108.83s/it]Epoch 0, Loss 1.929, LR 4.96e-05:  15%|█▌        | 20/133 [37:09<3:24:57, 108.83s/it]Epoch 0, Loss 1.929, LR 4.96e-05:  16%|█▌        | 21/133 [39:03<3:26:26, 110.60s/it]Epoch 0, Loss 1.871, LR 4.95e-05:  16%|█▌        | 21/133 [39:03<3:26:26, 110.60s/it]Epoch 0, Loss 1.871, LR 4.95e-05:  17%|█▋        | 22/133 [41:03<3:29:46, 113.39s/it]Epoch 0, Loss 1.780, LR 4.94e-05:  17%|█▋        | 22/133 [41:03<3:29:46, 113.39s/it]Epoch 0, Loss 1.780, LR 4.94e-05:  17%|█▋        | 23/133 [43:09<3:34:35, 117.05s/it]Epoch 0, Loss 1.821, LR 4.92e-05:  17%|█▋        | 23/133 [43:09<3:34:35, 117.05s/it]Epoch 0, Loss 1.821, LR 4.92e-05:  18%|█▊        | 24/133 [45:03<3:31:15, 116.29s/it]Epoch 0, Loss 1.872, LR 4.91e-05:  18%|█▊        | 24/133 [45:03<3:31:15, 116.29s/it]Epoch 0, Loss 1.872, LR 4.91e-05:  19%|█▉        | 25/133 [47:01<3:30:04, 116.71s/it]Epoch 0, Loss 1.807, LR 4.89e-05:  19%|█▉        | 25/133 [47:01<3:30:04, 116.71s/it]Epoch 0, Loss 1.807, LR 4.89e-05:  20%|█▉        | 26/133 [49:00<3:29:11, 117.30s/it]Epoch 0, Loss 1.773, LR 4.87e-05:  20%|█▉        | 26/133 [49:00<3:29:11, 117.30s/it]Epoch 0, Loss 1.773, LR 4.87e-05:  20%|██        | 27/133 [51:00<3:28:40, 118.12s/it]Epoch 0, Loss 1.760, LR 4.85e-05:  20%|██        | 27/133 [51:00<3:28:40, 118.12s/it]Epoch 0, Loss 1.760, LR 4.85e-05:  21%|██        | 28/133 [52:51<3:23:12, 116.12s/it]Epoch 0, Loss 1.795, LR 4.83e-05:  21%|██        | 28/133 [52:51<3:23:12, 116.12s/it]Epoch 0, Loss 1.795, LR 4.83e-05:  22%|██▏       | 29/133 [54:35<3:14:56, 112.46s/it]Epoch 0, Loss 1.785, LR 4.81e-05:  22%|██▏       | 29/133 [54:35<3:14:56, 112.46s/it]Epoch 0, Loss 1.785, LR 4.81e-05:  23%|██▎       | 30/133 [56:22<3:10:10, 110.78s/it]Epoch 0, Loss 1.711, LR 4.78e-05:  23%|██▎       | 30/133 [56:22<3:10:10, 110.78s/it]Epoch 0, Loss 1.711, LR 4.78e-05:  23%|██▎       | 31/133 [58:26<3:15:09, 114.80s/it]Epoch 0, Loss 1.747, LR 4.75e-05:  23%|██▎       | 31/133 [58:26<3:15:09, 114.80s/it]Epoch 0, Loss 1.747, LR 4.75e-05:  24%|██▍       | 32/133 [1:00:01<3:03:25, 108.96s/it]Epoch 0, Loss 1.837, LR 4.73e-05:  24%|██▍       | 32/133 [1:00:02<3:03:25, 108.96s/it]Epoch 0, Loss 1.837, LR 4.73e-05:  25%|██▍       | 33/133 [1:01:58<3:05:20, 111.20s/it]Epoch 0, Loss 1.790, LR 4.70e-05:  25%|██▍       | 33/133 [1:01:58<3:05:20, 111.20s/it]Epoch 0, Loss 1.790, LR 4.70e-05:  26%|██▌       | 34/133 [1:03:54<3:05:48, 112.61s/it]Epoch 0, Loss 1.746, LR 4.67e-05:  26%|██▌       | 34/133 [1:03:54<3:05:48, 112.61s/it]Epoch 0, Loss 1.746, LR 4.67e-05:  26%|██▋       | 35/133 [1:05:48<3:04:38, 113.04s/it]Epoch 0, Loss 1.827, LR 4.64e-05:  26%|██▋       | 35/133 [1:05:48<3:04:38, 113.04s/it]Epoch 0, Loss 1.827, LR 4.64e-05:  27%|██▋       | 36/133 [1:07:41<3:02:47, 113.06s/it]Epoch 0, Loss 1.711, LR 4.60e-05:  27%|██▋       | 36/133 [1:07:41<3:02:47, 113.06s/it]Epoch 0, Loss 1.711, LR 4.60e-05:  28%|██▊       | 37/133 [1:09:30<2:59:06, 111.94s/it]Epoch 0, Loss 1.772, LR 4.57e-05:  28%|██▊       | 37/133 [1:09:30<2:59:06, 111.94s/it]Epoch 0, Loss 1.772, LR 4.57e-05:  29%|██▊       | 38/133 [1:11:30<3:01:05, 114.37s/it]Epoch 0, Loss 1.723, LR 4.54e-05:  29%|██▊       | 38/133 [1:11:30<3:01:05, 114.37s/it]Epoch 0, Loss 1.723, LR 4.54e-05:  29%|██▉       | 39/133 [1:13:24<2:58:39, 114.03s/it]Epoch 0, Loss 1.838, LR 4.50e-05:  29%|██▉       | 39/133 [1:13:24<2:58:39, 114.03s/it]Epoch 0, Loss 1.838, LR 4.50e-05:  30%|███       | 40/133 [1:15:10<2:53:26, 111.90s/it]Epoch 0, Loss 1.821, LR 4.46e-05:  30%|███       | 40/133 [1:15:11<2:53:26, 111.90s/it]Epoch 0, Loss 1.821, LR 4.46e-05:  31%|███       | 41/133 [1:17:04<2:52:25, 112.45s/it]Epoch 0, Loss 1.816, LR 4.42e-05:  31%|███       | 41/133 [1:17:04<2:52:25, 112.45s/it]Epoch 0, Loss 1.816, LR 4.42e-05:  32%|███▏      | 42/133 [1:19:07<2:55:20, 115.61s/it]Epoch 0, Loss 1.777, LR 4.38e-05:  32%|███▏      | 42/133 [1:19:07<2:55:20, 115.61s/it]Epoch 0, Loss 1.777, LR 4.38e-05:  32%|███▏      | 43/133 [1:21:02<2:52:59, 115.33s/it]Epoch 0, Loss 1.728, LR 4.34e-05:  32%|███▏      | 43/133 [1:21:02<2:52:59, 115.33s/it]Epoch 0, Loss 1.728, LR 4.34e-05:  33%|███▎      | 44/133 [1:22:48<2:46:56, 112.55s/it]Epoch 0, Loss 1.794, LR 4.30e-05:  33%|███▎      | 44/133 [1:22:48<2:46:56, 112.55s/it]Epoch 0, Loss 1.794, LR 4.30e-05:  34%|███▍      | 45/133 [1:24:41<2:45:06, 112.57s/it]Epoch 0, Loss 1.796, LR 4.26e-05:  34%|███▍      | 45/133 [1:24:41<2:45:06, 112.57s/it]Epoch 0, Loss 1.796, LR 4.26e-05:  35%|███▍      | 46/133 [1:26:44<2:48:08, 115.96s/it]Epoch 0, Loss 1.767, LR 4.21e-05:  35%|███▍      | 46/133 [1:26:45<2:48:08, 115.96s/it]Epoch 0, Loss 1.767, LR 4.21e-05:  35%|███▌      | 47/133 [1:28:26<2:40:11, 111.76s/it]Epoch 0, Loss 1.760, LR 4.17e-05:  35%|███▌      | 47/133 [1:28:26<2:40:11, 111.76s/it]Epoch 0, Loss 1.760, LR 4.17e-05:  36%|███▌      | 48/133 [1:30:14<2:36:33, 110.51s/it]Epoch 0, Loss 1.714, LR 4.12e-05:  36%|███▌      | 48/133 [1:30:14<2:36:33, 110.51s/it]Epoch 0, Loss 1.714, LR 4.12e-05:  37%|███▋      | 49/133 [1:32:26<2:43:50, 117.03s/it]Epoch 0, Loss 1.812, LR 4.07e-05:  37%|███▋      | 49/133 [1:32:26<2:43:50, 117.03s/it]Epoch 0, Loss 1.812, LR 4.07e-05:  38%|███▊      | 50/133 [1:34:16<2:38:53, 114.86s/it]Epoch 0, Loss 1.739, LR 4.02e-05:  38%|███▊      | 50/133 [1:34:16<2:38:53, 114.86s/it]Epoch 0, Loss 1.739, LR 4.02e-05:  38%|███▊      | 51/133 [1:36:09<2:36:02, 114.18s/it]Epoch 0, Loss 1.756, LR 3.98e-05:  38%|███▊      | 51/133 [1:36:09<2:36:02, 114.18s/it]Epoch 0, Loss 1.756, LR 3.98e-05:  39%|███▉      | 52/133 [1:38:06<2:35:21, 115.07s/it]Epoch 0, Loss 1.759, LR 3.93e-05:  39%|███▉      | 52/133 [1:38:06<2:35:21, 115.07s/it]Epoch 0, Loss 1.759, LR 3.93e-05:  40%|███▉      | 53/133 [1:39:53<2:30:23, 112.79s/it]Epoch 0, Loss 1.799, LR 3.87e-05:  40%|███▉      | 53/133 [1:39:53<2:30:23, 112.79s/it]Epoch 0, Loss 1.799, LR 3.87e-05:  41%|████      | 54/133 [1:41:46<2:28:41, 112.93s/it]Epoch 0, Loss 1.705, LR 3.82e-05:  41%|████      | 54/133 [1:41:47<2:28:41, 112.93s/it]Epoch 0, Loss 1.705, LR 3.82e-05:  41%|████▏     | 55/133 [1:43:37<2:25:49, 112.17s/it]Epoch 0, Loss 1.743, LR 3.77e-05:  41%|████▏     | 55/133 [1:43:37<2:25:49, 112.17s/it]Epoch 0, Loss 1.743, LR 3.77e-05:  42%|████▏     | 56/133 [1:45:22<2:21:24, 110.19s/it]Epoch 0, Loss 1.754, LR 3.72e-05:  42%|████▏     | 56/133 [1:45:23<2:21:24, 110.19s/it]Epoch 0, Loss 1.754, LR 3.72e-05:  43%|████▎     | 57/133 [1:47:18<2:21:29, 111.70s/it]Epoch 0, Loss 1.780, LR 3.67e-05:  43%|████▎     | 57/133 [1:47:18<2:21:29, 111.70s/it]Epoch 0, Loss 1.780, LR 3.67e-05:  44%|████▎     | 58/133 [1:49:13<2:20:47, 112.64s/it]Epoch 0, Loss 1.724, LR 3.61e-05:  44%|████▎     | 58/133 [1:49:13<2:20:47, 112.64s/it]Epoch 0, Loss 1.724, LR 3.61e-05:  44%|████▍     | 59/133 [1:51:12<2:21:37, 114.83s/it]Epoch 0, Loss 1.720, LR 3.56e-05:  44%|████▍     | 59/133 [1:51:12<2:21:37, 114.83s/it]Epoch 0, Loss 1.720, LR 3.56e-05:  45%|████▌     | 60/133 [1:53:04<2:18:23, 113.75s/it]Epoch 0, Loss 1.707, LR 3.50e-05:  45%|████▌     | 60/133 [1:53:04<2:18:23, 113.75s/it]Batch idx 0
Batch idx 1
Batch idx 2
Batch idx 3
Batch idx 4
Batch idx 5
Batch idx 6
Gradient norm: 1.328125
Batch idx 7
Batch idx 8
Batch idx 9
Batch idx 10
Batch idx 11
Batch idx 12
Batch idx 13
Batch idx 14
Gradient norm: 1.1171875
Batch idx 15
Batch idx 16
Batch idx 17
Batch idx 18
Batch idx 19
Batch idx 20
Batch idx 21
Batch idx 22
Gradient norm: 1.09375
Batch idx 23
Batch idx 24
Batch idx 25
Batch idx 26
Batch idx 27
Batch idx 28
Batch idx 29
Batch idx 30
Gradient norm: 1.125
Batch idx 31
Batch idx 32
Batch idx 33
Batch idx 34
Batch idx 35
Batch idx 36
Batch idx 37
Batch idx 38
Gradient norm: 1.0859375
Batch idx 39
Batch idx 40
Batch idx 41
Batch idx 42
Batch idx 43
Batch idx 44
Batch idx 45
Batch idx 46
Gradient norm: 0.98046875
Batch idx 47
Batch idx 48
Batch idx 49
Batch idx 50
Batch idx 51
Batch idx 52
Batch idx 53
Batch idx 54
Gradient norm: 0.984375
Batch idx 55
Batch idx 56
Batch idx 57
Batch idx 58
Batch idx 59
Batch idx 60
Batch idx 61
Batch idx 62
Gradient norm: 1.03125
Batch idx 63
Batch idx 64
Batch idx 65
Batch idx 66
Batch idx 67
Batch idx 68
Batch idx 69
Batch idx 70
Gradient norm: 0.87109375
Batch idx 71
Batch idx 72
Batch idx 73
Batch idx 74
Batch idx 75
Batch idx 76
Batch idx 77
Batch idx 78
Gradient norm: 0.921875
Batch idx 79
Batch idx 80
Batch idx 81
Batch idx 82
Batch idx 83
Batch idx 84
Batch idx 85
Batch idx 86
Gradient norm: 0.82421875
Batch idx 87
Batch idx 88
Batch idx 89
Batch idx 90
Batch idx 91
Batch idx 92
Batch idx 93
Batch idx 94
Gradient norm: 0.79296875
Batch idx 95
Batch idx 96
Batch idx 97
Batch idx 98
Batch idx 99
Batch idx 100
Batch idx 101
Batch idx 102
Gradient norm: 0.6328125
Batch idx 103
Batch idx 104
Batch idx 105
Batch idx 106
Batch idx 107
Batch idx 108
Batch idx 109
Batch idx 110
Gradient norm: 0.546875
Batch idx 111
Batch idx 112
Batch idx 113
Batch idx 114
Batch idx 115
Batch idx 116
Batch idx 117
Batch idx 118
Gradient norm: 0.4453125
Batch idx 119
Batch idx 120
Batch idx 121
Batch idx 122
Batch idx 123
Batch idx 124
Batch idx 125
Batch idx 126
Gradient norm: 0.427734375
Batch idx 127
Batch idx 128
Batch idx 129
Batch idx 130
Batch idx 131
Batch idx 132
Batch idx 133
Batch idx 134
Gradient norm: 0.373046875
Batch idx 135
Batch idx 136
Batch idx 137
Batch idx 138
Batch idx 139
Batch idx 140
Batch idx 141
Batch idx 142
Gradient norm: 0.36328125
Batch idx 143
Batch idx 144
Batch idx 145
Batch idx 146
Batch idx 147
Batch idx 148
Batch idx 149
Batch idx 150
Gradient norm: 0.42578125
Batch idx 151
Batch idx 152
Batch idx 153
Batch idx 154
Batch idx 155
Batch idx 156
Batch idx 157
Batch idx 158
Gradient norm: 0.3359375
Batch idx 159
Batch idx 160
Batch idx 161
Batch idx 162
Batch idx 163
Batch idx 164
Batch idx 165
Batch idx 166
Gradient norm: 0.28515625
Batch idx 167
Batch idx 168
Batch idx 169
Batch idx 170
Batch idx 171
Batch idx 172
Batch idx 173
Batch idx 174
Gradient norm: 0.298828125
Batch idx 175
Batch idx 176
Batch idx 177
Batch idx 178
Batch idx 179
Batch idx 180
Batch idx 181
Batch idx 182
Gradient norm: 0.287109375
Batch idx 183
Batch idx 184
Batch idx 185
Batch idx 186
Batch idx 187
Batch idx 188
Batch idx 189
Batch idx 190
Gradient norm: 0.275390625
Batch idx 191
Batch idx 192
Batch idx 193
Batch idx 194
Batch idx 195
Batch idx 196
Batch idx 197
Batch idx 198
Gradient norm: 0.25
Batch idx 199
Batch idx 200
Batch idx 201
Batch idx 202
Batch idx 203
Batch idx 204
Batch idx 205
Batch idx 206
Gradient norm: 0.224609375
Batch idx 207
Batch idx 208
Batch idx 209
Batch idx 210
Batch idx 211
Batch idx 212
Batch idx 213
Batch idx 214
Gradient norm: 0.197265625
Batch idx 215
Batch idx 216
Batch idx 217
Batch idx 218
Batch idx 219
Batch idx 220
Batch idx 221
Batch idx 222
Gradient norm: 0.2060546875
Batch idx 223
Batch idx 224
Batch idx 225
Batch idx 226
Batch idx 227
Batch idx 228
Batch idx 229
Batch idx 230
Gradient norm: 0.2080078125
Batch idx 231
Batch idx 232
Batch idx 233
Batch idx 234
Batch idx 235
Batch idx 236
Batch idx 237
Batch idx 238
Gradient norm: 0.20703125
Batch idx 239
Batch idx 240
Batch idx 241
Batch idx 242
Batch idx 243
Batch idx 244
Batch idx 245
Batch idx 246
Gradient norm: 0.228515625
Batch idx 247
Batch idx 248
Batch idx 249
Batch idx 250
Batch idx 251
Batch idx 252
Batch idx 253
Batch idx 254
Gradient norm: 0.255859375
Batch idx 255
Batch idx 256
Batch idx 257
Batch idx 258
Batch idx 259
Batch idx 260
Batch idx 261
Batch idx 262
Gradient norm: 0.208984375
Batch idx 263
Batch idx 264
Batch idx 265
Batch idx 266
Batch idx 267
Batch idx 268
Batch idx 269
Batch idx 270
Gradient norm: 0.169921875
Batch idx 271
Batch idx 272
Batch idx 273
Batch idx 274
Batch idx 275
Batch idx 276
Batch idx 277
Batch idx 278
Gradient norm: 0.1806640625
Batch idx 279
Batch idx 280
Batch idx 281
Batch idx 282
Batch idx 283
Batch idx 284
Batch idx 285
Batch idx 286
Gradient norm: 0.2333984375
Batch idx 287
Batch idx 288
Batch idx 289
Batch idx 290
Batch idx 291
Batch idx 292
Batch idx 293
Batch idx 294
Gradient norm: 0.208984375
Batch idx 295
Batch idx 296
Batch idx 297
Batch idx 298
Batch idx 299
Batch idx 300
Batch idx 301
Batch idx 302
Gradient norm: 0.2294921875
Batch idx 303
Batch idx 304
Batch idx 305
Batch idx 306
Batch idx 307
Batch idx 308
Batch idx 309
Batch idx 310
Gradient norm: 0.1669921875
Batch idx 311
Batch idx 312
Batch idx 313
Batch idx 314
Batch idx 315
Batch idx 316
Batch idx 317
Batch idx 318
Gradient norm: 0.21484375
Batch idx 319
Batch idx 320
Batch idx 321
Batch idx 322
Batch idx 323
Batch idx 324
Batch idx 325
Batch idx 326
Gradient norm: 0.20703125
Batch idx 327
Batch idx 328
Batch idx 329
Batch idx 330
Batch idx 331
Batch idx 332
Batch idx 333
Batch idx 334
Gradient norm: 0.2060546875
Batch idx 335
Batch idx 336
Batch idx 337
Batch idx 338
Batch idx 339
Batch idx 340
Batch idx 341
Batch idx 342
Gradient norm: 0.2197265625
Batch idx 343
Batch idx 344
Batch idx 345
Batch idx 346
Batch idx 347
Batch idx 348
Batch idx 349
Batch idx 350
Gradient norm: 0.1767578125
Batch idx 351
Batch idx 352
Batch idx 353
Batch idx 354
Batch idx 355
Batch idx 356
Batch idx 357
Batch idx 358
Gradient norm: 0.23046875
Batch idx 359
Batch idx 360
Batch idx 361
Batch idx 362
Batch idx 363
Batch idx 364
Batch idx 365
Batch idx 366
Gradient norm: 0.2197265625
Batch idx 367
Batch idx 368
Batch idx 369
Batch idx 370
Batch idx 371
Batch idx 372
Batch idx 373
Batch idx 374
Gradient norm: 0.1767578125
Batch idx 375
Batch idx 376
Batch idx 377
Batch idx 378
Batch idx 379
Batch idx 380
Batch idx 381
Batch idx 382
Gradient norm: 0.1630859375
Batch idx 383
Batch idx 384
Batch idx 385
Batch idx 386
Batch idx 387
Batch idx 388
Batch idx 389
Batch idx 390
Gradient norm: 0.1640625
Batch idx 391
Batch idx 392
Batch idx 393
Batch idx 394
Batch idx 395
Batch idx 396
Batch idx 397
Batch idx 398
Gradient norm: 0.181640625
Batch idx 399
Batch idx 400
Batch idx 401
Batch idx 402
Batch idx 403
Batch idx 404
Batch idx 405
Batch idx 406
Gradient norm: 0.1572265625
Batch idx 407
Batch idx 408
Batch idx 409
Batch idx 410
Batch idx 411
Batch idx 412
Batch idx 413
Batch idx 414
Gradient norm: 0.158203125
Batch idx 415
Batch idx 416
Batch idx 417
Batch idx 418
Batch idx 419
Batch idx 420
Batch idx 421
Batch idx 422
Gradient norm: 0.1474609375
Batch idx 423
Batch idx 424
Batch idx 425
Batch idx 426
Batch idx 427
Batch idx 428
Batch idx 429
Batch idx 430
Gradient norm: 0.1875
Batch idx 431
Batch idx 432
Batch idx 433
Batch idx 434
Batch idx 435
Batch idx 436
Batch idx 437
Batch idx 438
Gradient norm: 0.1640625
Batch idx 439
Batch idx 440
Batch idx 441
Batch idx 442
Batch idx 443
Batch idx 444
Batch idx 445
Batch idx 446
Gradient norm: 0.1591796875
Batch idx 447
Batch idx 448
Batch idx 449
Batch idx 450
Batch idx 451
Batch idx 452
Batch idx 453
Batch idx 454
Gradient norm: 0.1591796875
Batch idx 455
Batch idx 456
Batch idx 457
Batch idx 458
Batch idx 459
Batch idx 460
Batch idx 461
Batch idx 462
Gradient norm: 0.1484375
Batch idx 463
Batch idx 464
Batch idx 465
Batch idx 466
Batch idx 467
Batch idx 468
Batch idx 469
Batch idx 470
Gradient norm: 0.1474609375
Batch idx 471
Batch idx 472
Batch idx 473
Batch idx 474
Batch idx 475
Batch idx 476
Batch idx 477
Batch idx 478
Gradient norm: 0.158203125
Batch idx 479
Epoch 0, Loss 1.707, LR 3.50e-05:  46%|████▌     | 61/133 [1:54:57<2:16:18, 113.59s/it]Epoch 0, Loss 1.738, LR 3.45e-05:  46%|████▌     | 61/133 [1:54:57<2:16:18, 113.59s/it]Epoch 0, Loss 1.738, LR 3.45e-05:  47%|████▋     | 62/133 [1:56:47<2:13:07, 112.49s/it]Epoch 0, Loss 1.713, LR 3.39e-05:  47%|████▋     | 62/133 [1:56:47<2:13:07, 112.49s/it]Epoch 0, Loss 1.713, LR 3.39e-05:  47%|████▋     | 63/133 [1:58:39<2:11:07, 112.39s/it]Epoch 0, Loss 1.829, LR 3.33e-05:  47%|████▋     | 63/133 [1:58:39<2:11:07, 112.39s/it]Epoch 0, Loss 1.829, LR 3.33e-05:  48%|████▊     | 64/133 [2:00:33<2:09:43, 112.80s/it]Epoch 0, Loss 1.795, LR 3.28e-05:  48%|████▊     | 64/133 [2:00:33<2:09:43, 112.80s/it]Epoch 0, Loss 1.795, LR 3.28e-05:  49%|████▉     | 65/133 [2:02:22<2:06:40, 111.77s/it]Epoch 0, Loss 1.811, LR 3.22e-05:  49%|████▉     | 65/133 [2:02:22<2:06:40, 111.77s/it]Epoch 0, Loss 1.811, LR 3.22e-05:  50%|████▉     | 66/133 [2:04:11<2:03:45, 110.83s/it]Epoch 0, Loss 1.755, LR 3.16e-05:  50%|████▉     | 66/133 [2:04:11<2:03:45, 110.83s/it]Epoch 0, Loss 1.755, LR 3.16e-05:  50%|█████     | 67/133 [2:05:54<1:59:17, 108.45s/it]Epoch 0, Loss 1.638, LR 3.10e-05:  50%|█████     | 67/133 [2:05:54<1:59:17, 108.45s/it]Epoch 0, Loss 1.638, LR 3.10e-05:  51%|█████     | 68/133 [2:07:46<1:58:45, 109.62s/it]Epoch 0, Loss 1.815, LR 3.04e-05:  51%|█████     | 68/133 [2:07:46<1:58:45, 109.62s/it]Epoch 0, Loss 1.815, LR 3.04e-05:  52%|█████▏    | 69/133 [2:09:45<1:59:57, 112.45s/it]Epoch 0, Loss 1.736, LR 2.99e-05:  52%|█████▏    | 69/133 [2:09:45<1:59:57, 112.45s/it]Epoch 0, Loss 1.736, LR 2.99e-05:  53%|█████▎    | 70/133 [2:11:42<1:59:23, 113.70s/it]Epoch 0, Loss 1.758, LR 2.93e-05:  53%|█████▎    | 70/133 [2:11:42<1:59:23, 113.70s/it]Epoch 0, Loss 1.758, LR 2.93e-05:  53%|█████▎    | 71/133 [2:13:40<1:58:51, 115.03s/it]Epoch 0, Loss 1.757, LR 2.87e-05:  53%|█████▎    | 71/133 [2:13:40<1:58:51, 115.03s/it]Epoch 0, Loss 1.757, LR 2.87e-05:  54%|█████▍    | 72/133 [2:15:39<1:58:21, 116.42s/it]Epoch 0, Loss 1.629, LR 2.81e-05:  54%|█████▍    | 72/133 [2:15:39<1:58:21, 116.42s/it]Epoch 0, Loss 1.629, LR 2.81e-05:  55%|█████▍    | 73/133 [2:17:33<1:55:26, 115.44s/it]Epoch 0, Loss 1.825, LR 2.75e-05:  55%|█████▍    | 73/133 [2:17:33<1:55:26, 115.44s/it]Epoch 0, Loss 1.825, LR 2.75e-05:  56%|█████▌    | 74/133 [2:19:30<1:53:58, 115.90s/it]Epoch 0, Loss 1.810, LR 2.69e-05:  56%|█████▌    | 74/133 [2:19:30<1:53:58, 115.90s/it]Epoch 0, Loss 1.810, LR 2.69e-05:  56%|█████▋    | 75/133 [2:21:35<1:54:52, 118.83s/it]Epoch 0, Loss 1.729, LR 2.63e-05:  56%|█████▋    | 75/133 [2:21:35<1:54:52, 118.83s/it]Epoch 0, Loss 1.729, LR 2.63e-05:  57%|█████▋    | 76/133 [2:23:37<1:53:34, 119.56s/it]Epoch 0, Loss 1.765, LR 2.57e-05:  57%|█████▋    | 76/133 [2:23:37<1:53:34, 119.56s/it]Epoch 0, Loss 1.765, LR 2.57e-05:  58%|█████▊    | 77/133 [2:25:35<1:51:24, 119.37s/it]Epoch 0, Loss 1.748, LR 2.51e-05:  58%|█████▊    | 77/133 [2:25:35<1:51:24, 119.37s/it]Epoch 0, Loss 1.748, LR 2.51e-05:  59%|█████▊    | 78/133 [2:27:42<1:51:32, 121.67s/it]Epoch 0, Loss 1.756, LR 2.46e-05:  59%|█████▊    | 78/133 [2:27:43<1:51:32, 121.67s/it]Epoch 0, Loss 1.756, LR 2.46e-05:  59%|█████▉    | 79/133 [2:29:40<1:48:21, 120.40s/it]Epoch 0, Loss 1.797, LR 2.40e-05:  59%|█████▉    | 79/133 [2:29:40<1:48:21, 120.40s/it]Epoch 0, Loss 1.797, LR 2.40e-05:  60%|██████    | 80/133 [2:31:34<1:44:45, 118.59s/it]Epoch 0, Loss 1.792, LR 2.34e-05:  60%|██████    | 80/133 [2:31:34<1:44:45, 118.59s/it]Epoch 0, Loss 1.792, LR 2.34e-05:  61%|██████    | 81/133 [2:33:19<1:39:15, 114.52s/it]Epoch 0, Loss 1.700, LR 2.28e-05:  61%|██████    | 81/133 [2:33:19<1:39:15, 114.52s/it]Epoch 0, Loss 1.700, LR 2.28e-05:  62%|██████▏   | 82/133 [2:35:02<1:34:20, 110.99s/it]Epoch 0, Loss 1.641, LR 2.22e-05:  62%|██████▏   | 82/133 [2:35:02<1:34:20, 110.99s/it]Epoch 0, Loss 1.641, LR 2.22e-05:  62%|██████▏   | 83/133 [2:36:58<1:33:49, 112.60s/it]Epoch 0, Loss 1.710, LR 2.17e-05:  62%|██████▏   | 83/133 [2:36:59<1:33:49, 112.60s/it]Epoch 0, Loss 1.710, LR 2.17e-05:  63%|██████▎   | 84/133 [2:38:50<1:31:41, 112.28s/it]Epoch 0, Loss 1.797, LR 2.11e-05:  63%|██████▎   | 84/133 [2:38:50<1:31:41, 112.28s/it]Epoch 0, Loss 1.797, LR 2.11e-05:  64%|██████▍   | 85/133 [2:40:34<1:27:57, 109.94s/it]Epoch 0, Loss 1.700, LR 2.05e-05:  64%|██████▍   | 85/133 [2:40:34<1:27:57, 109.94s/it]Epoch 0, Loss 1.700, LR 2.05e-05:  65%|██████▍   | 86/133 [2:42:28<1:26:53, 110.93s/it]Epoch 0, Loss 1.758, LR 2.00e-05:  65%|██████▍   | 86/133 [2:42:28<1:26:53, 110.93s/it]Epoch 0, Loss 1.758, LR 2.00e-05:  65%|██████▌   | 87/133 [2:44:02<1:21:16, 106.02s/it]Epoch 0, Loss 1.696, LR 1.94e-05:  65%|██████▌   | 87/133 [2:44:02<1:21:16, 106.02s/it]Epoch 0, Loss 1.696, LR 1.94e-05:  66%|██████▌   | 88/133 [2:45:54<1:20:53, 107.87s/it]Epoch 0, Loss 1.757, LR 1.89e-05:  66%|██████▌   | 88/133 [2:45:54<1:20:53, 107.87s/it]Epoch 0, Loss 1.757, LR 1.89e-05:  67%|██████▋   | 89/133 [2:47:47<1:20:15, 109.43s/it]Epoch 0, Loss 1.670, LR 1.83e-05:  67%|██████▋   | 89/133 [2:47:48<1:20:15, 109.43s/it]Epoch 0, Loss 1.670, LR 1.83e-05:  68%|██████▊   | 90/133 [2:49:37<1:18:31, 109.57s/it]Epoch 0, Loss 1.755, LR 1.78e-05:  68%|██████▊   | 90/133 [2:49:37<1:18:31, 109.57s/it]Epoch 0, Loss 1.755, LR 1.78e-05:  68%|██████▊   | 91/133 [2:51:31<1:17:37, 110.89s/it]Epoch 0, Loss 1.740, LR 1.73e-05:  68%|██████▊   | 91/133 [2:51:31<1:17:37, 110.89s/it]Epoch 0, Loss 1.740, LR 1.73e-05:  69%|██████▉   | 92/133 [2:53:28<1:16:52, 112.50s/it]Epoch 0, Loss 1.774, LR 1.68e-05:  69%|██████▉   | 92/133 [2:53:28<1:16:52, 112.50s/it]Epoch 0, Loss 1.774, LR 1.68e-05:  70%|██████▉   | 93/133 [2:55:17<1:14:17, 111.44s/it]Epoch 0, Loss 1.727, LR 1.63e-05:  70%|██████▉   | 93/133 [2:55:17<1:14:17, 111.44s/it]Epoch 0, Loss 1.727, LR 1.63e-05:  71%|███████   | 94/133 [2:57:16<1:14:00, 113.86s/it]Epoch 0, Loss 1.702, LR 1.57e-05:  71%|███████   | 94/133 [2:57:16<1:14:00, 113.86s/it]Epoch 0, Loss 1.702, LR 1.57e-05:  71%|███████▏  | 95/133 [2:59:24<1:14:43, 117.99s/it]Epoch 0, Loss 1.736, LR 1.52e-05:  71%|███████▏  | 95/133 [2:59:24<1:14:43, 117.99s/it]Epoch 0, Loss 1.736, LR 1.52e-05:  72%|███████▏  | 96/133 [3:01:12<1:10:55, 115.01s/it]Epoch 0, Loss 1.783, LR 1.48e-05:  72%|███████▏  | 96/133 [3:01:12<1:10:55, 115.01s/it]Epoch 0, Loss 1.783, LR 1.48e-05:  73%|███████▎  | 97/133 [3:03:01<1:07:57, 113.25s/it]Epoch 0, Loss 1.730, LR 1.43e-05:  73%|███████▎  | 97/133 [3:03:01<1:07:57, 113.25s/it]Epoch 0, Loss 1.730, LR 1.43e-05:  74%|███████▎  | 98/133 [3:04:43<1:04:04, 109.84s/it]Epoch 0, Loss 1.705, LR 1.38e-05:  74%|███████▎  | 98/133 [3:04:43<1:04:04, 109.84s/it]Epoch 0, Loss 1.705, LR 1.38e-05:  74%|███████▍  | 99/133 [3:06:34<1:02:31, 110.34s/it]Epoch 0, Loss 1.713, LR 1.33e-05:  74%|███████▍  | 99/133 [3:06:34<1:02:31, 110.34s/it]Epoch 0, Loss 1.713, LR 1.33e-05:  75%|███████▌  | 100/133 [3:08:23<1:00:21, 109.73s/it]Epoch 0, Loss 1.685, LR 1.29e-05:  75%|███████▌  | 100/133 [3:08:23<1:00:21, 109.73s/it]Epoch 0, Loss 1.685, LR 1.29e-05:  76%|███████▌  | 101/133 [3:10:18<59:24, 111.40s/it]  Epoch 0, Loss 1.723, LR 1.24e-05:  76%|███████▌  | 101/133 [3:10:18<59:24, 111.40s/it]Epoch 0, Loss 1.723, LR 1.24e-05:  77%|███████▋  | 102/133 [3:12:12<57:59, 112.23s/it]Epoch 0, Loss 1.707, LR 1.20e-05:  77%|███████▋  | 102/133 [3:12:12<57:59, 112.23s/it]Epoch 0, Loss 1.707, LR 1.20e-05:  77%|███████▋  | 103/133 [3:14:12<57:15, 114.50s/it]Epoch 0, Loss 1.719, LR 1.16e-05:  77%|███████▋  | 103/133 [3:14:12<57:15, 114.50s/it]Epoch 0, Loss 1.719, LR 1.16e-05:  78%|███████▊  | 104/133 [3:15:51<53:09, 110.00s/it]Epoch 0, Loss 1.737, LR 1.12e-05:  78%|███████▊  | 104/133 [3:15:51<53:09, 110.00s/it]Epoch 0, Loss 1.737, LR 1.12e-05:  79%|███████▉  | 105/133 [3:17:42<51:27, 110.28s/it]Epoch 0, Loss 1.748, LR 1.08e-05:  79%|███████▉  | 105/133 [3:17:42<51:27, 110.28s/it]Epoch 0, Loss 1.748, LR 1.08e-05:  80%|███████▉  | 106/133 [3:19:38<50:23, 111.97s/it]Epoch 0, Loss 1.693, LR 1.04e-05:  80%|███████▉  | 106/133 [3:19:38<50:23, 111.97s/it]Epoch 0, Loss 1.693, LR 1.04e-05:  80%|████████  | 107/133 [3:21:14<46:23, 107.07s/it]Epoch 0, Loss 1.756, LR 1.00e-05:  80%|████████  | 107/133 [3:21:14<46:23, 107.07s/it]Epoch 0, Loss 1.756, LR 1.00e-05:  81%|████████  | 108/133 [3:23:09<45:38, 109.54s/it]Epoch 0, Loss 1.755, LR 9.65e-06:  81%|████████  | 108/133 [3:23:09<45:38, 109.54s/it]Epoch 0, Loss 1.755, LR 9.65e-06:  82%|████████▏ | 109/133 [3:25:12<45:22, 113.43s/it]Epoch 0, Loss 1.750, LR 9.30e-06:  82%|████████▏ | 109/133 [3:25:12<45:22, 113.43s/it]Epoch 0, Loss 1.750, LR 9.30e-06:  83%|████████▎ | 110/133 [3:27:18<44:57, 117.28s/it]Epoch 0, Loss 1.652, LR 8.96e-06:  83%|████████▎ | 110/133 [3:27:18<44:57, 117.28s/it]Epoch 0, Loss 1.652, LR 8.96e-06:  83%|████████▎ | 111/133 [3:29:17<43:09, 117.68s/it]Epoch 0, Loss 1.694, LR 8.63e-06:  83%|████████▎ | 111/133 [3:29:17<43:09, 117.68s/it]Epoch 0, Loss 1.694, LR 8.63e-06:  84%|████████▍ | 112/133 [3:31:09<40:35, 115.98s/it]Epoch 0, Loss 1.764, LR 8.32e-06:  84%|████████▍ | 112/133 [3:31:09<40:35, 115.98s/it]Epoch 0, Loss 1.764, LR 8.32e-06:  85%|████████▍ | 113/133 [3:33:09<39:05, 117.29s/it]Epoch 0, Loss 1.753, LR 8.01e-06:  85%|████████▍ | 113/133 [3:33:09<39:05, 117.29s/it]Epoch 0, Loss 1.753, LR 8.01e-06:  86%|████████▌ | 114/133 [3:34:50<35:37, 112.50s/it]Epoch 0, Loss 1.715, LR 7.73e-06:  86%|████████▌ | 114/133 [3:34:50<35:37, 112.50s/it]Epoch 0, Loss 1.715, LR 7.73e-06:  86%|████████▋ | 115/133 [3:36:39<33:25, 111.41s/it]Epoch 0, Loss 1.673, LR 7.45e-06:  86%|████████▋ | 115/133 [3:36:39<33:25, 111.41s/it]Epoch 0, Loss 1.673, LR 7.45e-06:  87%|████████▋ | 116/133 [3:38:24<30:59, 109.36s/it]Epoch 0, Loss 1.766, LR 7.19e-06:  87%|████████▋ | 116/133 [3:38:24<30:59, 109.36s/it]Epoch 0, Loss 1.766, LR 7.19e-06:  88%|████████▊ | 117/133 [3:40:12<29:03, 108.96s/it]Epoch 0, Loss 1.789, LR 6.95e-06:  88%|████████▊ | 117/133 [3:40:12<29:03, 108.96s/it]Epoch 0, Loss 1.789, LR 6.95e-06:  89%|████████▊ | 118/133 [3:42:01<27:16, 109.09s/it]Epoch 0, Loss 1.770, LR 6.71e-06:  89%|████████▊ | 118/133 [3:42:01<27:16, 109.09s/it]Epoch 0, Loss 1.770, LR 6.71e-06:  89%|████████▉ | 119/133 [3:43:57<25:55, 111.11s/it]Epoch 0, Loss 1.775, LR 6.49e-06:  89%|████████▉ | 119/133 [3:43:57<25:55, 111.11s/it]Batch idx 480
Batch idx 481
Batch idx 482
Batch idx 483
Batch idx 484
Batch idx 485
Batch idx 486
Gradient norm: 0.1474609375
Batch idx 487
Batch idx 488
Batch idx 489
Batch idx 490
Batch idx 491
Batch idx 492
Batch idx 493
Batch idx 494
Gradient norm: 0.1767578125
Batch idx 495
Batch idx 496
Batch idx 497
Batch idx 498
Batch idx 499
Batch idx 500
Batch idx 501
Batch idx 502
Gradient norm: 0.169921875
Batch idx 503
Batch idx 504
Batch idx 505
Batch idx 506
Batch idx 507
Batch idx 508
Batch idx 509
Batch idx 510
Gradient norm: 0.140625
Batch idx 511
Batch idx 512
Batch idx 513
Batch idx 514
Batch idx 515
Batch idx 516
Batch idx 517
Batch idx 518
Gradient norm: 0.169921875
Batch idx 519
Batch idx 520
Batch idx 521
Batch idx 522
Batch idx 523
Batch idx 524
Batch idx 525
Batch idx 526
Gradient norm: 0.16796875
Batch idx 527
Batch idx 528
Batch idx 529
Batch idx 530
Batch idx 531
Batch idx 532
Batch idx 533
Batch idx 534
Gradient norm: 0.166015625
Batch idx 535
Batch idx 536
Batch idx 537
Batch idx 538
Batch idx 539
Batch idx 540
Batch idx 541
Batch idx 542
Gradient norm: 0.150390625
Batch idx 543
Batch idx 544
Batch idx 545
Batch idx 546
Batch idx 547
Batch idx 548
Batch idx 549
Batch idx 550
Gradient norm: 0.158203125
Batch idx 551
Batch idx 552
Batch idx 553
Batch idx 554
Batch idx 555
Batch idx 556
Batch idx 557
Batch idx 558
Gradient norm: 0.1474609375
Batch idx 559
Batch idx 560
Batch idx 561
Batch idx 562
Batch idx 563
Batch idx 564
Batch idx 565
Batch idx 566
Gradient norm: 0.13671875
Batch idx 567
Batch idx 568
Batch idx 569
Batch idx 570
Batch idx 571
Batch idx 572
Batch idx 573
Batch idx 574
Gradient norm: 0.1669921875
Batch idx 575
Batch idx 576
Batch idx 577
Batch idx 578
Batch idx 579
Batch idx 580
Batch idx 581
Batch idx 582
Gradient norm: 0.142578125
Batch idx 583
Batch idx 584
Batch idx 585
Batch idx 586
Batch idx 587
Batch idx 588
Batch idx 589
Batch idx 590
Gradient norm: 0.146484375
Batch idx 591
Batch idx 592
Batch idx 593
Batch idx 594
Batch idx 595
Batch idx 596
Batch idx 597
Batch idx 598
Gradient norm: 0.1298828125
Batch idx 599
Batch idx 600
Batch idx 601
Batch idx 602
Batch idx 603
Batch idx 604
Batch idx 605
Batch idx 606
Gradient norm: 0.1669921875
Batch idx 607
Batch idx 608
Batch idx 609
Batch idx 610
Batch idx 611
Batch idx 612
Batch idx 613
Batch idx 614
Gradient norm: 0.1435546875
Batch idx 615
Batch idx 616
Batch idx 617
Batch idx 618
Batch idx 619
Batch idx 620
Batch idx 621
Batch idx 622
Gradient norm: 0.1484375
Batch idx 623
Batch idx 624
Batch idx 625
Batch idx 626
Batch idx 627
Batch idx 628
Batch idx 629
Batch idx 630
Gradient norm: 0.1416015625
Batch idx 631
Batch idx 632
Batch idx 633
Batch idx 634
Batch idx 635
Batch idx 636
Batch idx 637
Batch idx 638
Gradient norm: 0.1337890625
Batch idx 639
Batch idx 640
Batch idx 641
Batch idx 642
Batch idx 643
Batch idx 644
Batch idx 645
Batch idx 646
Gradient norm: 0.166015625
Batch idx 647
Batch idx 648
Batch idx 649
Batch idx 650
Batch idx 651
Batch idx 652
Batch idx 653
Batch idx 654
Gradient norm: 0.18359375
Batch idx 655
Batch idx 656
Batch idx 657
Batch idx 658
Batch idx 659
Batch idx 660
Batch idx 661
Batch idx 662
Gradient norm: 0.14453125
Batch idx 663
Batch idx 664
Batch idx 665
Batch idx 666
Batch idx 667
Batch idx 668
Batch idx 669
Batch idx 670
Gradient norm: 0.1552734375
Batch idx 671
Batch idx 672
Batch idx 673
Batch idx 674
Batch idx 675
Batch idx 676
Batch idx 677
Batch idx 678
Gradient norm: 0.146484375
Batch idx 679
Batch idx 680
Batch idx 681
Batch idx 682
Batch idx 683
Batch idx 684
Batch idx 685
Batch idx 686
Gradient norm: 0.1630859375
Batch idx 687
Batch idx 688
Batch idx 689
Batch idx 690
Batch idx 691
Batch idx 692
Batch idx 693
Batch idx 694
Gradient norm: 0.1650390625
Batch idx 695
Batch idx 696
Batch idx 697
Batch idx 698
Batch idx 699
Batch idx 700
Batch idx 701
Batch idx 702
Gradient norm: 0.138671875
Batch idx 703
Batch idx 704
Batch idx 705
Batch idx 706
Batch idx 707
Batch idx 708
Batch idx 709
Batch idx 710
Gradient norm: 0.142578125
Batch idx 711
Batch idx 712
Batch idx 713
Batch idx 714
Batch idx 715
Batch idx 716
Batch idx 717
Batch idx 718
Gradient norm: 0.1533203125
Batch idx 719
Batch idx 720
Batch idx 721
Batch idx 722
Batch idx 723
Batch idx 724
Batch idx 725
Batch idx 726
Gradient norm: 0.1396484375
Batch idx 727
Batch idx 728
Batch idx 729
Batch idx 730
Batch idx 731
Batch idx 732
Batch idx 733
Batch idx 734
Gradient norm: 0.140625
Batch idx 735
Batch idx 736
Batch idx 737
Batch idx 738
Batch idx 739
Batch idx 740
Batch idx 741
Batch idx 742
Gradient norm: 0.15625
Batch idx 743
Batch idx 744
Batch idx 745
Batch idx 746
Batch idx 747
Batch idx 748
Batch idx 749
Batch idx 750
Gradient norm: 0.1396484375
Batch idx 751
Batch idx 752
Batch idx 753
Batch idx 754
Batch idx 755
Batch idx 756
Batch idx 757
Batch idx 758
Gradient norm: 0.15234375
Batch idx 759
Batch idx 760
Batch idx 761
Batch idx 762
Batch idx 763
Batch idx 764
Batch idx 765
Batch idx 766
Gradient norm: 0.13671875
Batch idx 767
Batch idx 768
Batch idx 769
Batch idx 770
Batch idx 771
Batch idx 772
Batch idx 773
Batch idx 774
Gradient norm: 0.1591796875
Batch idx 775
Batch idx 776
Batch idx 777
Batch idx 778
Batch idx 779
Batch idx 780
Batch idx 781
Batch idx 782
Gradient norm: 0.150390625
Batch idx 783
Batch idx 784
Batch idx 785
Batch idx 786
Batch idx 787
Batch idx 788
Batch idx 789
Batch idx 790
Gradient norm: 0.146484375
Batch idx 791
Batch idx 792
Batch idx 793
Batch idx 794
Batch idx 795
Batch idx 796
Batch idx 797
Batch idx 798
Gradient norm: 0.146484375
Batch idx 799
Batch idx 800
Batch idx 801
Batch idx 802
Batch idx 803
Batch idx 804
Batch idx 805
Batch idx 806
Gradient norm: 0.150390625
Batch idx 807
Batch idx 808
Batch idx 809
Batch idx 810
Batch idx 811
Batch idx 812
Batch idx 813
Batch idx 814
Gradient norm: 0.15234375
Batch idx 815
Batch idx 816
Batch idx 817
Batch idx 818
Batch idx 819
Batch idx 820
Batch idx 821
Batch idx 822
Gradient norm: 0.14453125
Batch idx 823
Batch idx 824
Batch idx 825
Batch idx 826
Batch idx 827
Batch idx 828
Batch idx 829
Batch idx 830
Gradient norm: 0.14453125
Batch idx 831
Batch idx 832
Batch idx 833
Batch idx 834
Batch idx 835
Batch idx 836
Batch idx 837
Batch idx 838
Gradient norm: 0.158203125
Batch idx 839
Batch idx 840
Batch idx 841
Batch idx 842
Batch idx 843
Batch idx 844
Batch idx 845
Batch idx 846
Gradient norm: 0.15234375
Batch idx 847
Batch idx 848
Batch idx 849
Batch idx 850
Batch idx 851
Batch idx 852
Batch idx 853
Batch idx 854
Gradient norm: 0.1630859375
Batch idx 855
Batch idx 856
Batch idx 857
Batch idx 858
Batch idx 859
Batch idx 860
Batch idx 861
Batch idx 862
Gradient norm: 0.1416015625
Batch idx 863
Batch idx 864
Batch idx 865
Batch idx 866
Batch idx 867
Batch idx 868
Batch idx 869
Batch idx 870
Gradient norm: 0.1552734375
Batch idx 871
Batch idx 872
Batch idx 873
Batch idx 874
Batch idx 875
Batch idx 876
Batch idx 877
Batch idx 878
Gradient norm: 0.150390625
Batch idx 879
Batch idx 880
Batch idx 881
Batch idx 882
Batch idx 883
Batch idx 884
Batch idx 885
Batch idx 886
Gradient norm: 0.140625
Batch idx 887
Batch idx 888
Batch idx 889
Batch idx 890
Batch idx 891
Batch idx 892
Batch idx 893
Batch idx 894
Gradient norm: 0.14453125
Batch idx 895
Batch idx 896
Batch idx 897
Batch idx 898
Batch idx 899
Batch idx 900
Batch idx 901
Batch idx 902
Gradient norm: 0.1435546875
Batch idx 903
Batch idx 904
Batch idx 905
Batch idx 906
Batch idx 907
Batch idx 908
Batch idx 909
Batch idx 910
Gradient norm: 0.162109375
Batch idx 911
Batch idx 912
Batch idx 913
Batch idx 914
Batch idx 915
Batch idx 916
Batch idx 917
Batch idx 918
Gradient norm: 0.1533203125
Batch idx 919
Batch idx 920
Batch idx 921
Batch idx 922
Batch idx 923
Batch idx 924
Batch idx 925
Batch idx 926
Gradient norm: 0.1513671875
Batch idx 927
Batch idx 928
Batch idx 929
Batch idx 930
Batch idx 931
Batch idx 932
Batch idx 933
Batch idx 934
Gradient norm: 0.142578125
Batch idx 935
Batch idx 936
Batch idx 937
Batch idx 938
Batch idx 939
Batch idx 940
Batch idx 941
Batch idx 942
Gradient norm: 0.154296875
Batch idx 943
Batch idx 944
Batch idx 945
Batch idx 946
Batch idx 947
Batch idx 948
Batch idx 949
Batch idx 950
Gradient norm: 0.1318359375
Epoch 0, Loss 1.775, LR 6.49e-06:  90%|█████████ | 120/133 [3:45:45<23:50, 110.08s/it]Epoch 0, Loss 1.719, LR 6.29e-06:  90%|█████████ | 120/133 [3:45:45<23:50, 110.08s/it]Epoch 0, Loss 1.719, LR 6.29e-06:  91%|█████████ | 121/133 [3:47:35<22:03, 110.28s/it]Epoch 0, Loss 1.770, LR 6.10e-06:  91%|█████████ | 121/133 [3:47:35<22:03, 110.28s/it]Epoch 0, Loss 1.770, LR 6.10e-06:  92%|█████████▏| 122/133 [3:49:22<20:00, 109.17s/it]Epoch 0, Loss 1.784, LR 5.93e-06:  92%|█████████▏| 122/133 [3:49:22<20:00, 109.17s/it]Epoch 0, Loss 1.784, LR 5.93e-06:  92%|█████████▏| 123/133 [3:51:09<18:04, 108.47s/it]Epoch 0, Loss 1.763, LR 5.77e-06:  92%|█████████▏| 123/133 [3:51:09<18:04, 108.47s/it]Epoch 0, Loss 1.763, LR 5.77e-06:  93%|█████████▎| 124/133 [3:53:09<16:47, 111.96s/it]Epoch 0, Loss 1.737, LR 5.62e-06:  93%|█████████▎| 124/133 [3:53:09<16:47, 111.96s/it]Epoch 0, Loss 1.737, LR 5.62e-06:  94%|█████████▍| 125/133 [3:55:03<15:01, 112.73s/it]Epoch 0, Loss 1.736, LR 5.49e-06:  94%|█████████▍| 125/133 [3:55:03<15:01, 112.73s/it]Epoch 0, Loss 1.736, LR 5.49e-06:  95%|█████████▍| 126/133 [3:56:51<12:58, 111.18s/it]Epoch 0, Loss 1.645, LR 5.38e-06:  95%|█████████▍| 126/133 [3:56:51<12:58, 111.18s/it]Epoch 0, Loss 1.645, LR 5.38e-06:  95%|█████████▌| 127/133 [3:58:49<11:20, 113.34s/it]Epoch 0, Loss 1.665, LR 5.28e-06:  95%|█████████▌| 127/133 [3:58:49<11:20, 113.34s/it]Epoch 0, Loss 1.665, LR 5.28e-06:  96%|█████████▌| 128/133 [4:00:42<09:25, 113.10s/it]Epoch 0, Loss 1.781, LR 5.19e-06:  96%|█████████▌| 128/133 [4:00:42<09:25, 113.10s/it]Epoch 0, Loss 1.781, LR 5.19e-06:  97%|█████████▋| 129/133 [4:02:45<07:44, 116.23s/it]Epoch 0, Loss 1.754, LR 5.12e-06:  97%|█████████▋| 129/133 [4:02:45<07:44, 116.23s/it]Epoch 0, Loss 1.754, LR 5.12e-06:  98%|█████████▊| 130/133 [4:04:42<05:48, 116.24s/it]Epoch 0, Loss 1.748, LR 5.07e-06:  98%|█████████▊| 130/133 [4:04:42<05:48, 116.24s/it]Epoch 0, Loss 1.748, LR 5.07e-06:  98%|█████████▊| 131/133 [4:06:20<03:41, 110.76s/it]Epoch 0, Loss 1.666, LR 5.03e-06:  98%|█████████▊| 131/133 [4:06:20<03:41, 110.76s/it]Epoch 0, Loss 1.666, LR 5.03e-06:  99%|█████████▉| 132/133 [4:08:00<01:47, 107.51s/it]Epoch 0, Loss 1.723, LR 5.01e-06:  99%|█████████▉| 132/133 [4:08:00<01:47, 107.51s/it]Epoch 0, Loss 1.723, LR 5.01e-06: 100%|██████████| 133/133 [4:09:59<00:00, 111.12s/it]Epoch 0, Loss 1.679, LR 5.00e-06: 100%|██████████| 133/133 [4:09:59<00:00, 111.12s/it]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.007 MB of 0.090 MB uploadedwandb: - 0.007 MB of 0.090 MB uploadedwandb: \ 0.090 MB of 0.090 MB uploadedwandb: | 0.090 MB of 0.090 MB uploadedwandb: / 0.090 MB of 0.090 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                            grad_norm ██▇▆▄▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                 loss ███▆▄▄▃▄▃▃▂▂▃▃▂▂▂▂▂▃▃▁▃▂▁▃▂▃▂▂▂▂▂▂▂▃▃▂▃▂
wandb:                                   lr ▁▃▅▇██████▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:      memory/allocated_after_backward ▁
wandb:       memory/allocated_after_forward ▁
wandb: memory/allocated_after_model_created ▁
wandb:    memory/allocated_after_model_wrap ▁
wandb:      memory/allocated_before_forward ▁
wandb:                memory/allocated_peak ▁
wandb:       memory/reserved_after_backward ▁
wandb:        memory/reserved_after_forward ▁
wandb: memory/reserved_after_model_creation ▁
wandb:     memory/reserved_after_model_wrap ▁
wandb:       memory/reserved_before_forward ▁
wandb:                 memory/reserved_peak ▁
wandb:                           time_taken ▁
wandb: 
wandb: Run summary:
wandb:                            grad_norm 0.14551
wandb:                                 loss 1.67893
wandb:                                   lr 1e-05
wandb:      memory/allocated_after_backward 379801600
wandb:       memory/allocated_after_forward 1025223168
wandb: memory/allocated_after_model_created 8652800
wandb:    memory/allocated_after_model_wrap 111200256
wandb:      memory/allocated_before_forward 111201280
wandb:                memory/allocated_peak 3124532224
wandb:       memory/reserved_after_backward 3013607424
wandb:        memory/reserved_after_forward 2860515328
wandb: memory/reserved_after_model_creation 113246208
wandb:     memory/reserved_after_model_wrap 1956642816
wandb:       memory/reserved_before_forward 1956642816
wandb:                 memory/reserved_peak 4018143232
wandb:                           time_taken 15093.754
wandb: 
wandb: 🚀 View run noble-sun-67 at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/fdtxv9xb
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240830_052152-fdtxv9xb/logs
Rank 3: Model created: 0.105 GiB
Wrapping model w/ FSDP 3
Rank 3: Wrapped model: 1.824 GiB
Applying activation checkpointing 3
Rank 3: Before forward: 1.82 GiB
Rank 3: After forward: 2.67 GiB
Rank 3: After backward: 2.81 GiB
Rank 3: Peak allocated memory: 2.91 GiB
Rank 3: Peak reserved memory:  3.68 GiB
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (fdtxv9xb) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
Epoch 0, Loss 1.679, LR 5.00e-06: 100%|██████████| 133/133 [4:11:43<00:00, 113.56s/it]
Batch idx 951
Batch idx 952
Batch idx 953
Batch idx 954
Batch idx 955
Batch idx 956
Batch idx 957
Batch idx 958
Gradient norm: 0.154296875
Batch idx 959
Batch idx 960
Batch idx 961
Batch idx 962
Batch idx 963
Batch idx 964
Batch idx 965
Batch idx 966
Gradient norm: 0.1630859375
Batch idx 967
Batch idx 968
Batch idx 969
Batch idx 970
Batch idx 971
Batch idx 972
Batch idx 973
Batch idx 974
Gradient norm: 0.146484375
Batch idx 975
Batch idx 976
Batch idx 977
Batch idx 978
Batch idx 979
Batch idx 980
Batch idx 981
Batch idx 982
Gradient norm: 0.16015625
Batch idx 983
Batch idx 984
Batch idx 985
Batch idx 986
Batch idx 987
Batch idx 988
Batch idx 989
Batch idx 990
Gradient norm: 0.15234375
Batch idx 991
Batch idx 992
Batch idx 993
Batch idx 994
Batch idx 995
Batch idx 996
Batch idx 997
Batch idx 998
Gradient norm: 0.1435546875
Batch idx 999
Batch idx 1000
Batch idx 1001
Batch idx 1002
Batch idx 1003
Batch idx 1004
Batch idx 1005
Batch idx 1006
Gradient norm: 0.1611328125
Batch idx 1007
Batch idx 1008
Batch idx 1009
Batch idx 1010
Batch idx 1011
Batch idx 1012
Batch idx 1013
Batch idx 1014
Gradient norm: 0.1591796875
Batch idx 1015
Batch idx 1016
Batch idx 1017
Batch idx 1018
Batch idx 1019
Batch idx 1020
Batch idx 1021
Batch idx 1022
Gradient norm: 0.1494140625
Batch idx 1023
Batch idx 1024
Batch idx 1025
Batch idx 1026
Batch idx 1027
Batch idx 1028
Batch idx 1029
Batch idx 1030
Gradient norm: 0.1728515625
Batch idx 1031
Batch idx 1032
Batch idx 1033
Batch idx 1034
Batch idx 1035
Batch idx 1036
Batch idx 1037
Batch idx 1038
Gradient norm: 0.1552734375
Batch idx 1039
Batch idx 1040
Batch idx 1041
Batch idx 1042
Batch idx 1043
Batch idx 1044
Batch idx 1045
Batch idx 1046
Gradient norm: 0.1748046875
Batch idx 1047
Batch idx 1048
Batch idx 1049
Batch idx 1050
Batch idx 1051
Batch idx 1052
Batch idx 1053
Batch idx 1054
Gradient norm: 0.1484375
Batch idx 1055
Batch idx 1056
Batch idx 1057
Batch idx 1058
Batch idx 1059
Batch idx 1060
Batch idx 1061
Batch idx 1062
Gradient norm: 0.1455078125
Batch idx 1063
Batch idx 1064
Batch idx 1065
Batch idx 1066
Batch idx 1067
Batch idx 1068
Batch idx 1069
Saving full model weights.
Done 0
Finished training 0
CUDA event elapsed time: 15093.754 sec
Rank 0: Before forward: 1.82 GiB
Rank 0: After forward: 2.66 GiB
Rank 0: After backward: 2.81 GiB
Rank 0: Peak allocated memory: 2.91 GiB
Rank 0: Peak reserved memory:  3.74 GiB
Rank 2: Model created: 0.105 GiB
Wrapping model w/ FSDP 2
Rank 2: Wrapped model: 1.824 GiB
Applying activation checkpointing 2
Rank 2: Before forward: 1.82 GiB
Rank 2: After forward: 2.66 GiB
Rank 2: After backward: 2.81 GiB
Rank 2: Peak allocated memory: 2.91 GiB
Rank 2: Peak reserved memory:  3.90 GiB
Rank 1: Model created: 0.105 GiB
Wrapping model w/ FSDP 1
Rank 1: Wrapped model: 1.824 GiB
Applying activation checkpointing 1
Rank 1: Before forward: 1.82 GiB
Rank 1: After forward: 2.66 GiB
Rank 1: After backward: 2.81 GiB
Rank 1: Peak allocated memory: 2.91 GiB
Rank 1: Peak reserved memory:  3.72 GiB
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.97s/it]
name : base_model.model.model.embed_tokens.weight | dtype : torch.bfloat16 | shape : torch.Size([32768, 4096]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.0.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.1.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.2.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.3.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.4.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.5.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.6.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.7.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.8.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.9.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.10.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.11.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.12.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.13.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.14.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.15.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.16.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.17.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.18.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.19.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.20.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.21.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.22.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.23.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.24.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.25.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.26.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.27.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.28.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.29.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.30.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.31.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.norm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.lm_head.weight | dtype : torch.bfloat16 | shape : torch.Size([32768, 4096]) | requires_grad : False 


All tensors are equal
---------------------------------------------------
Dataset size : 554
example of the dataset
input_ids : tensor([[    1,     3, 12786,  ..., 29491, 29473,     4]])
prompt : <s>[INST] Write a summary of the source text. The summary should be normal in extractiveness. Extractiveness is defined by the degree of exact copying from the source text. The source text is given below.  (CNN)Jackson Gordon is no ordinary 21-year-old. By day he is an industrial design student at Philadelphia University, but Gordon has another side to him -- a side altogether darker, tougher and more enigmatic. Hanging in his workshop Gordon has a full suit of armor plating, cape and cowl -- matte black and built to stop a knife. Gordon has an alter ego: the Dark Knight himself, Batman. You might expect his origin story to be cloaked in mystery, but speaking to CNN Gordon is quick to explain how the transformation took place. Gordon says his calling came five years ago when he began experimenting with cosplay. "Previously I'd been involved with costume making... I'd made a version of the Batsuit from Christopher Nolan's 'Dark Knight Trilogy' and I really liked that suit," Gordon says. But, as elaborate as his design was, it lacked the functionality or the authenticity of the genuine article. "I was frustrated every time I wore it," Gordon explains. "It really limited my mobility and I didn't like that -- it didn't go with the character." In September 2014 he bit the bullet, deciding "to do another one that wouldn't inhibit my mobility and would actually provide protection and function more like Batman's actual suit." The Batsuit had to be strong -- tough enough to withstand the stab or slash of a knife, the impact of a punch or a baseball bat, but light and articulate enough to make it practical. Striking such a balance required expensive materials, and they didn't come cheap. Gordon therefore fired up a Kickstarter campaign. He "didn't really think anyone would fund it or even be interested in it" -- he raised $1,255 in 6 days. "It was a little surprising," Gordon demurs. Writing out his shopping list, it was important that "everywhere, even places without armor plating, had some sort of protection." Kevlar was sourced as the base fabric, making it "cut and slash resistant to bladed weapons, but breathable and wearable all day." Eschewing conventional materials, Gordon opted for a form of memory foam, built around key areas to "squish and compress," dissipating the impact of blows. After much experimenting with "polycarbonates and extruded PVC materials," ¼" Kydex (or ABS) plastic formed the tough armor plates, located on the torso, forearms and shins. Stab resistant, Gordon says "it can take anything but a gunshot." The cowl was more problematic, being "nearly impossible" to craft out of the same materials within the limits of his workshop. Gordon therefore took a mold of his head using Sintra plastic, "working on top of that with different sculpting clays and soft plastics to get it into a recognizable Batman shape." Using a two part box mold Gordon was able to create a "silicone jacket" of this, into which liquid polyurethane was poured, forming the final, "durable and functional" cowl. Gordon (who doesn't appear to be related to Gotham City's police commissioner, James Gordon) is also an expert in Shaolin Kung Fu: he is both brains and brawn, a cross between Bruce Wayne and Batsuit designer Lucius Fox from Nolan's Batman trilogy. Legendary, the production company behind the films, has taken note of his design and given it their seal of approval. The Batsuit has made appearances at conventions and proved a showstopper among his fellow students and the faculty. "People love the theatricality of it," its designer says. That the product so closely mimics DC's fantastical comic book creation has had resonance. He has already begun manufacturing the cowls for the public, with "fully adjustable" jackets going up for sale on his site Armatus Design "in the next couple of weeks." The jackets have received particular attention. Gordon has received "easily over 50 requests from people," and not just from the cosplay community. "They range from recreational use to martial artists... but also motorcycle and All Terrain Vehicle riders who want protective gear and prefer the look and functionality of [Gordon's] suit." Perhaps because of their versatility and the small matter of copyright issues, those that go on sale will not feature the iconic bat symbol. Gordon says his fledgling business will remain small whilst he's at University -- he has to finish he studies after all, and won't be using the project towards his degree credits. For now the Batsuit and Armatus Design will remain a one man operation: such is the life of a superhero. [/INST]
reference : Jackson Gordon, a 21-year-old industrial design student at Philadelphia University built a Batsuit that is resistant to stabs, knife slashes, and high impacts. According to Gordon, this is a second attempt at building the suit after an earlier attempt five years ago.
---------------------------------------------------
Generating summaries: 0it [00:00, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Generating summaries: 1it [00:20, 20.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 2it [00:29, 13.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 3it [00:43, 13.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 4it [00:51, 11.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 5it [01:01, 11.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 6it [01:10, 10.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 7it [01:15,  8.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 8it [01:27,  9.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 9it [01:43, 11.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 10it [01:49,  9.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 11it [02:02, 11.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 12it [02:11, 10.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 13it [02:23, 10.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 14it [02:36, 11.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 15it [02:49, 11.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 16it [02:57, 10.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 17it [03:04,  9.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 18it [03:11,  8.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 19it [03:22,  9.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 20it [03:30,  9.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 21it [03:41,  9.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 22it [03:48,  8.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 23it [04:00,  9.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 24it [04:04,  8.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 25it [04:11,  7.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 26it [04:14,  6.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 27it [04:17,  5.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 28it [04:23,  5.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 29it [04:30,  6.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 30it [04:34,  5.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 31it [04:44,  6.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 32it [04:49,  6.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 33it [04:59,  7.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 34it [05:06,  7.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 35it [05:14,  7.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 36it [05:23,  7.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 37it [05:33,  8.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 38it [05:40,  8.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 39it [05:46,  7.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 40it [05:53,  7.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 41it [06:02,  7.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 42it [06:08,  7.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 43it [06:16,  7.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 44it [06:23,  7.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 45it [06:26,  6.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 46it [06:33,  6.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 47it [06:40,  6.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 48it [06:45,  6.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 49it [06:51,  6.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 50it [07:01,  7.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 51it [07:21, 10.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 52it [07:31, 10.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 53it [07:39,  9.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 54it [07:51, 10.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 55it [08:00, 10.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 56it [08:19, 12.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 57it [08:34, 13.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 58it [08:41, 11.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 59it [08:55, 12.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 60it [09:05, 11.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 61it [09:16, 11.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 62it [09:24, 10.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 63it [09:33,  9.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 64it [09:41,  9.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 65it [09:49,  9.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 66it [10:08, 12.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 67it [10:14, 10.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 68it [10:23,  9.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 69it [10:30,  8.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 70it [10:38,  8.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 71it [10:45,  8.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 72it [10:51,  7.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 73it [11:11, 11.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 74it [11:20, 10.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 75it [11:28,  9.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 76it [11:39, 10.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 77it [11:45,  9.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 78it [11:56,  9.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 79it [12:06,  9.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 80it [12:17,  9.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 81it [12:29, 10.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 82it [12:37,  9.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 83it [12:48, 10.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 84it [13:00, 10.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 85it [13:08,  9.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 86it [13:17,  9.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 87it [13:26,  9.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 88it [13:36,  9.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 89it [13:45,  9.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 90it [13:50,  8.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 91it [13:56,  7.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 92it [14:09,  9.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 93it [14:16,  8.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 94it [14:25,  8.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 95it [14:31,  7.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 96it [14:40,  8.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 97it [14:51,  8.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 98it [14:58,  8.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 99it [15:11,  9.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 100it [15:19,  9.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 101it [15:23,  7.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 102it [15:33,  8.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 103it [15:38,  7.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 104it [15:46,  7.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 105it [15:56,  8.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 106it [16:01,  7.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 107it [16:04,  6.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 108it [16:13,  6.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 109it [16:16,  5.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 110it [16:20,  5.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 111it [16:26,  5.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 112it [16:33,  5.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 113it [16:36,  5.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 114it [16:40,  4.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 115it [16:44,  4.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 116it [16:52,  5.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 117it [16:59,  6.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 118it [17:05,  6.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 119it [17:18,  8.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 120it [17:28,  8.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 121it [17:35,  8.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 122it [17:43,  8.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 123it [17:51,  7.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 124it [17:54,  6.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 125it [17:59,  6.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 126it [18:09,  7.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 127it [18:21,  8.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 128it [18:32,  9.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 129it [18:44, 10.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 130it [18:50,  8.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 131it [18:59,  8.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 132it [19:09,  9.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 133it [19:12,  7.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 134it [19:14,  5.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 135it [19:17,  4.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 136it [19:23,  5.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 137it [19:26,  4.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 138it [19:30,  4.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 139it [19:34,  4.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 140it [19:40,  4.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 141it [19:45,  4.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 142it [19:49,  4.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 143it [19:57,  5.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 144it [20:10,  7.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 145it [20:16,  7.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 146it [20:33, 10.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 147it [20:37,  8.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 148it [20:44,  7.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 149it [20:48,  6.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 150it [20:54,  6.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 151it [21:06,  8.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 152it [21:11,  7.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 153it [21:21,  8.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 154it [21:27,  7.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 155it [21:35,  7.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 156it [21:39,  6.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 157it [21:51,  8.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 158it [21:58,  7.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 159it [22:05,  7.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 160it [22:19,  9.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 161it [22:30,  9.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 162it [22:39,  9.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 163it [22:47,  9.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 164it [22:55,  8.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 165it [23:03,  8.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 166it [23:10,  8.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 167it [23:15,  7.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 168it [23:26,  8.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 169it [23:30,  7.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 170it [23:38,  7.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 171it [23:45,  7.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 172it [23:57,  8.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 173it [24:09,  9.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 174it [24:25, 11.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 175it [24:35, 11.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 176it [24:43, 10.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 177it [24:56, 11.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 178it [25:08, 11.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 179it [25:15,  9.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 180it [25:27, 10.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 181it [25:36,  9.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 182it [25:54, 12.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 183it [26:07, 12.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 184it [26:22, 13.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 185it [26:27, 10.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 186it [26:34,  9.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 187it [26:38,  8.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 188it [26:43,  7.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 189it [26:47,  6.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 190it [27:04,  9.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 191it [27:08,  7.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 192it [27:14,  7.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 193it [27:23,  7.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 194it [27:28,  7.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 195it [27:50, 11.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 196it [27:52,  8.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 197it [27:57,  7.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 198it [28:00,  6.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 199it [28:05,  5.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 200it [28:14,  6.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 201it [28:27,  8.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 202it [28:32,  7.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 203it [28:42,  8.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 204it [28:52,  8.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 205it [28:57,  7.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 206it [29:08,  8.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 207it [29:15,  8.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 208it [29:20,  7.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 209it [29:30,  8.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 210it [29:36,  7.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 211it [29:44,  7.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 212it [29:51,  7.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 213it [29:53,  5.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 214it [30:00,  6.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 215it [30:06,  6.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 216it [30:15,  7.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 217it [30:26,  8.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 218it [30:45, 11.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 219it [30:57, 11.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 220it [31:07, 11.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 221it [31:22, 12.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 222it [31:29, 10.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 223it [31:37, 10.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 224it [31:48, 10.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 225it [31:53,  8.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 226it [31:59,  7.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 227it [32:07,  7.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 228it [32:13,  7.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 229it [32:18,  6.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 230it [32:38, 10.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 231it [32:45,  9.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 232it [32:53,  9.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 233it [33:03,  9.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 234it [33:23, 12.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 235it [33:41, 14.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 236it [33:46, 11.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 237it [33:50,  9.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 238it [33:57,  8.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 239it [34:05,  8.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 240it [34:14,  8.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 241it [34:22,  8.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 242it [34:30,  8.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 243it [34:40,  8.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 244it [34:53, 10.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 245it [35:03,  9.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 246it [35:08,  8.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 247it [35:14,  7.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 248it [35:26,  9.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 249it [35:44, 11.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 250it [35:56, 11.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 251it [36:16, 14.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 252it [36:34, 15.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 253it [36:45, 13.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 254it [37:19, 20.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 255it [37:32, 17.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 256it [38:17, 26.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 257it [38:28, 21.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 258it [38:50, 21.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 259it [38:52, 15.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 260it [38:58, 12.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 261it [39:11, 12.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 262it [39:18, 10.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 263it [39:23,  9.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 264it [39:33,  9.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 265it [39:47, 10.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 266it [39:55, 10.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 267it [40:05, 10.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 268it [40:10,  8.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 269it [40:17,  7.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 270it [40:26,  8.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 271it [40:41, 10.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 272it [40:55, 11.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 273it [41:02, 10.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 274it [41:10,  9.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 275it [41:18,  9.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 276it [41:27,  9.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 277it [41:34,  8.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 278it [41:40,  7.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 279it [41:45,  6.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 280it [41:54,  7.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 281it [41:57,  6.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 282it [42:02,  5.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 283it [42:07,  5.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 284it [42:14,  5.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 285it [42:24,  7.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 286it [42:32,  7.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 287it [42:44,  8.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 288it [42:54,  9.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 289it [43:04,  9.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 290it [43:17, 10.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 291it [43:22,  8.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 292it [43:27,  7.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 293it [43:33,  6.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 294it [43:44,  8.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 295it [43:51,  7.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 296it [43:57,  7.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 297it [44:06,  7.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 298it [44:11,  7.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 299it [44:16,  6.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 300it [44:22,  6.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 301it [44:31,  7.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 302it [44:49, 10.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 303it [44:59, 10.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 304it [45:19, 13.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 305it [45:29, 12.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 306it [45:42, 12.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 307it [46:02, 14.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 308it [46:08, 11.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 309it [46:19, 11.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 310it [46:23,  9.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 311it [46:27,  7.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 312it [46:35,  7.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 313it [46:43,  7.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 314it [46:49,  7.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 315it [46:55,  6.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 316it [46:59,  6.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 317it [47:11,  7.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 318it [47:44, 15.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 319it [47:51, 12.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 320it [48:01, 12.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 321it [48:15, 12.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 322it [48:32, 14.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 323it [48:43, 13.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 324it [48:56, 13.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 325it [49:09, 13.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 326it [49:20, 12.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 327it [49:31, 12.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 328it [49:35,  9.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 329it [49:43,  9.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 330it [49:50,  8.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 331it [50:02,  9.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 332it [50:15, 10.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 333it [50:29, 11.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 334it [50:45, 13.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 335it [50:56, 12.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 336it [51:01, 10.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 337it [51:12, 10.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 338it [51:26, 11.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 339it [51:33, 10.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 340it [51:42,  9.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 341it [51:49,  8.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 342it [52:10, 12.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 343it [52:16, 10.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 344it [52:22,  9.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 345it [52:35, 10.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 346it [52:38,  8.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 347it [52:46,  8.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 348it [52:50,  6.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 349it [52:54,  6.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 350it [52:59,  5.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 351it [53:07,  6.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 352it [53:11,  5.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 353it [53:19,  6.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 354it [53:29,  7.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 355it [53:34,  6.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 356it [53:43,  7.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 357it [53:50,  7.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 358it [53:58,  7.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 359it [54:01,  6.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 360it [54:07,  6.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 361it [54:14,  6.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 362it [54:30,  9.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 363it [54:41,  9.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 364it [54:47,  8.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 365it [54:53,  8.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 366it [55:01,  7.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 367it [55:12,  8.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 368it [55:18,  7.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 369it [55:36, 10.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 370it [55:39,  8.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 371it [55:53, 10.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 372it [56:06, 11.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 373it [56:27, 14.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 374it [56:44, 14.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 375it [56:47, 11.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 376it [56:55, 10.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 377it [57:02,  9.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 378it [57:12,  9.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 379it [57:17,  8.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 380it [57:23,  7.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 381it [57:26,  6.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 382it [57:37,  7.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 383it [57:50,  9.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 384it [57:58,  8.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 385it [58:04,  7.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 386it [58:09,  7.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 387it [58:20,  8.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 388it [58:28,  8.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 389it [58:37,  8.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 390it [58:57, 11.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 391it [59:10, 12.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 392it [59:24, 12.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 393it [59:39, 13.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 394it [59:55, 14.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 395it [1:00:05, 13.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 396it [1:00:27, 15.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 397it [1:00:46, 16.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 398it [1:00:55, 14.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 399it [1:01:18, 16.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 400it [1:01:29, 15.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 401it [1:01:40, 13.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 402it [1:01:44, 10.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 403it [1:01:54, 10.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 404it [1:01:59,  8.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 405it [1:02:02,  7.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 406it [1:02:07,  6.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 407it [1:02:12,  6.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 408it [1:02:18,  5.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 409it [1:02:24,  6.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 410it [1:02:31,  6.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 411it [1:02:37,  6.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 412it [1:02:48,  7.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 413it [1:02:56,  7.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 414it [1:03:03,  7.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 415it [1:03:13,  8.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 416it [1:03:22,  8.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 417it [1:03:38, 10.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 418it [1:03:44,  9.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 419it [1:03:47,  7.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 420it [1:03:50,  6.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 421it [1:03:55,  5.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 422it [1:04:02,  5.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 423it [1:04:10,  6.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 424it [1:04:27,  9.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 425it [1:04:38, 10.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 426it [1:04:43,  8.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 427it [1:04:55,  9.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 428it [1:05:30, 17.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 429it [1:05:51, 18.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 430it [1:06:07, 17.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 431it [1:06:34, 20.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 432it [1:06:47, 18.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 433it [1:06:57, 15.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 434it [1:07:06, 13.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 435it [1:07:19, 13.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 436it [1:07:34, 14.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 437it [1:08:05, 18.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 438it [1:08:13, 15.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 439it [1:08:23, 14.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 440it [1:08:35, 13.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 441it [1:08:38, 10.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 442it [1:08:45,  9.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 443it [1:08:48,  7.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 444it [1:08:53,  6.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 445it [1:08:57,  5.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 446it [1:09:01,  5.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 447it [1:09:13,  7.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 448it [1:09:18,  6.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 449it [1:09:25,  6.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 450it [1:09:31,  6.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 451it [1:09:43,  8.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 452it [1:09:48,  7.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 453it [1:09:55,  7.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 454it [1:10:00,  6.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 455it [1:10:05,  6.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 456it [1:10:09,  5.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 457it [1:10:16,  5.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 458it [1:10:21,  5.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 459it [1:10:33,  7.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 460it [1:10:42,  7.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 461it [1:10:46,  6.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 462it [1:10:51,  6.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 463it [1:10:57,  6.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 464it [1:11:04,  6.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 465it [1:11:08,  5.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 466it [1:11:12,  5.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 467it [1:11:18,  5.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 468it [1:11:30,  7.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 469it [1:11:39,  7.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 470it [1:11:46,  7.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 471it [1:11:52,  7.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 472it [1:12:04,  8.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 473it [1:12:14,  9.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 474it [1:12:23,  8.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 475it [1:12:36, 10.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 476it [1:12:47, 10.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 477it [1:12:57, 10.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 478it [1:13:10, 11.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 479it [1:13:25, 12.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 480it [1:13:48, 15.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 481it [1:13:59, 14.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 482it [1:14:06, 12.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 483it [1:14:14, 10.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 484it [1:14:20,  9.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 485it [1:14:30,  9.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 486it [1:14:35,  8.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 487it [1:14:42,  7.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 488it [1:15:01, 11.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 489it [1:15:11, 10.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 490it [1:15:19, 10.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 491it [1:15:27,  9.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 492it [1:15:35,  8.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 493it [1:15:40,  7.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 494it [1:15:45,  6.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 495it [1:15:55,  7.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 496it [1:16:05,  8.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 497it [1:16:11,  7.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 498it [1:16:16,  7.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 499it [1:16:26,  7.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 500it [1:16:39,  9.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 501it [1:16:56, 11.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 502it [1:17:04, 10.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 503it [1:17:19, 11.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 504it [1:17:31, 11.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 505it [1:17:35,  9.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 506it [1:17:40,  8.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 507it [1:17:46,  7.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 508it [1:18:01,  9.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 509it [1:18:05,  8.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 510it [1:18:11,  7.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 511it [1:18:27, 10.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 512it [1:18:41, 11.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 513it [1:18:50, 10.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 514it [1:19:01, 10.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 515it [1:19:09,  9.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 516it [1:19:15,  8.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 517it [1:19:24,  8.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 518it [1:19:30,  7.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 519it [1:19:36,  7.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 520it [1:19:46,  8.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 521it [1:19:54,  8.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 522it [1:19:59,  7.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 523it [1:20:08,  7.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 524it [1:20:26, 10.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 525it [1:20:31,  8.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 526it [1:20:40,  9.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 527it [1:20:55, 10.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 528it [1:21:04, 10.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 529it [1:21:16, 10.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 530it [1:21:28, 11.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 531it [1:21:37, 10.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 532it [1:21:44,  9.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 533it [1:21:56, 10.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 534it [1:22:05,  9.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 535it [1:22:16, 10.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 536it [1:22:26, 10.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 537it [1:22:41, 11.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 538it [1:22:54, 12.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 539it [1:23:03, 11.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 540it [1:23:07,  8.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 541it [1:23:12,  7.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 542it [1:23:18,  7.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 543it [1:23:24,  6.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 544it [1:23:31,  6.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 545it [1:23:34,  5.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 546it [1:23:40,  5.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 547it [1:23:47,  6.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 548it [1:23:51,  5.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 549it [1:23:58,  6.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 550it [1:24:12,  8.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 551it [1:24:16,  7.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 552it [1:24:27,  8.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 553it [1:24:36,  8.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 554it [1:24:44,  8.24s/it]Generating summaries: 554it [1:24:44,  9.18s/it]
Total time taken for generating summaries : 5084.171392440796
Average time taken for generating summaries : 9.177204679496022
./run_train.sh: line 50: /scratch/tathagato/fsdp_qlora_experiments_30_August_storm_llama3.1-8b/: No such file or directory
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
World size: 4
{'world_size': 4, 'train_type': 'qlora', 'llama_pro_path': None, 'batch_size': 1, 'context_length': 2048, 'gradient_accumulation_steps': 8, 'num_epochs': 1, 'dataset': 'macsum', 'macsum_path': '/home2/tathagato/summarization/MACSUM/dataset/macdoc/train_dataset.json', 'dataset_samples': -1, 'sharding_strategy': 'full_shard', 'use_gradient_checkpointing': 1, 'reentrant_checkpointing': 1, 'use_cpu_offload': 1, 'use_activation_cpu_offload': False, 'low_memory': True, 'no_sync': False, 'precision': 'bf16', 'model_name': 'akjindal53244/Llama-3.1-Storm-8B', 'save_model': 1, 'output_dir': '/scratch/tathagato/fsdp_qlora_experiments_30_August_storm_llama3.1/extractiveness', 'lora_rank': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'lora_target_modules': 'all', 'verbose': 1, 'lr': 5e-05, 'apply_gradient_clipping': 1, 'grad_norm': 1.0, 'wd': 0.1, 'profile_memory': False, 'optimizer': 'adamw', 'lr_scheduler': 'cosine', 'loading_workers': -1, 'log_to': 'wandb', 'master_addr': 'localhost', 'master_port': '12355', 'seed': 42, 'project_name': 'fsdp_qlora', 'name': None, 'group': None, 'entity': None, 'n_bits': 4, 'profile': False, 'profiling_output': 'profiles', 'overwrite_profiling_output': True, 'with_stack': False, 'with_shapes': False, 'export_trace': True, 'export_memory_timeline': False, 'wait_steps': 0, 'warmup_steps': 1, 'active_steps': 2, 'repeat': 0, 'profiling_frequency': 10, 'max_steps': -1, 'attribute': 'extractiveness'}
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
Creating model 3
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Creating model 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Creating model 1
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSUM/fsdp_lora/wandb/run-20240830_110515-qlkxvgbj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-wind-68
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/qlkxvgbj
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
dataset chosen is :  macsum
Example input_ids shape:  torch.Size([1, 653])
Example labels shape:  torch.Size([1, 653])
example input 
<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an honest and to the point assistant, please follow the instruction and answer to the point<|eot_id|><|start_header_id|>user<|end_header_id|> Write a summary of the source text. The summary should be normal in extractiveness. Extractiveness is defined by the degree of exact copying from the source text. The source text is given below. 

(CNN)Fans of the late actor Paul Walker knew that watching him in "Furious 7" would be bittersweet. Even so, many moviegoers said the final scenes of the new film, which earned a record $146 million over the weekend, still packed an emotional wallop. "Not gonna lie, I shed a few tears at the end of Furious 7. The tribute to Paul Walker was very well done," one woman said Monday on Twitter. Hers was just one of a flood of messages on social media from people who said they got choked up during scenes featuring Walker, who died at 40 in a car crash in November 2013, before filming on "Furious 7" was completed. To finish Walker's scenes, the makers of the movie used body doubles, computer-generated images and even the actor's brothers. But it was the ending that really got to moviegoers. In finishing "Furious 7," the film's producers sought to retire Walker's character, Brian, while paying homage to his role in the blockbuster "Furious" action franchise. But they felt that killing him off might appear exploitative. "If they had gone down the other path, I think I would have refused to finish making this movie," director James Wan told BuzzFeed. Instead, the movie's makers chose to "retire Paul's character in the most sincere and elegant way (they) could," Wan said. Their idea was to have Brian retire from his dangerous, high-octane lifestyle out of a sense of responsibility to his growing family with girlfriend Mia, who is pregnant with their second child. A scene late in the movie shows him and Mia playing on a beach with their son while the crew looks on -- essentially saying goodbye. Then his longtime buddy Dom reminisces about their years together, leading to a montage of Walker scenes from the first six movies. The song that plays over the montage is  "See You Again," a collaboration between Wiz Khalifa and Charlie Puth. Co-star Vin Diesel shared the video for the song late Sunday on his Facebook page, where it has more than 1.5 million likes. Fans on Twitter and Facebook mostly praised the movie's ending as a fitting tribute -- and an emotionally wrenching one. "Man I don't care how tough u are or how gangsta u claim to be....the last five minutes had me choked up in the movie theater... I saw it 3 times in one day......the ending is the deepest ending I've ever seen," one man wrote on the movie's Facebook page.<|eot_id|><|start_header_id|>assistant<|end_header_id|> " The tribute to Paul Walker was very well done," said a fan after watching the Furious 7. The actor was on break from filming "Furious 7" at the time of the fiery accident which also claimed the life of the car's driver, Roger Rodas.<|eot_id|>
tensor([[128000, 128006,   9125, 128007,   1472,    527,    459,  10978,    323,
            311,    279,   1486,  18328,     11,   4587,   1833,    279,   7754,
            323,   4320,    311,    279,   1486, 128009, 128006,    882, 128007,
           9842,    264,  12399,    315,    279,   2592,   1495,     13,    578,
          12399,   1288,    387,   4725,    304,   8819,  13071,     13,  23673,
          13071,    374,   4613,    555,    279,   8547,    315,   4839,  32139,
            505,    279,   2592,   1495,     13,    578,   2592,   1495,    374,
           2728,   3770,     13,   4815,   3100,   9944,      8,  76887,    315,
            279,   3389,  12360,   7043,  23074,   7020,    430,  10307,   1461,
            304,    330,     37,  28626,    220,     22,      1,   1053,    387,
            293,  29163,   4589,     13,   7570,    779,     11,   1690,   5818,
           3427,    388,   1071,    279,   1620,  16451,    315,    279,    502,
           4632,     11,    902,  15662,    264,   3335,    400,  10465,   3610,
            927,    279,   9178,     11,   2103,  19937,    459,  14604,  41926,
          23085,     13,    330,   2688,  16926,  10457,     11,    358,  25351,
            264,   2478,  24014,    520,    279,    842,    315,  93431,    220,
             22,     13,    578,  35491,    311,   7043,  23074,    574,   1633,
           1664,   2884,   1359,    832,   5333,   1071,   7159,    389,   6405,
             13,  65466,    574,   1120,    832,    315,    264,  18197,    315,
           6743,    389,   3674,   3772,    505,   1274,    889,   1071,    814,
           2751,  94743,    709,   2391,  16451,  16850,  23074,     11,    889,
           8636,    520,    220,   1272,    304,    264,   1841,  10121,    304,
           6841,    220,    679,     18,     11,   1603,  39970,    389,    330,
             37,  28626,    220,     22,      1,    574,   8308,     13,   2057,
           6381,  23074,    596,  16451,     11,    279,  29414,    315,    279,
           5818,   1511,   2547,  40396,     11,   6500,  16581,   5448,    323,
           1524,    279,  12360,    596,  20820,     13,   2030,    433,    574,
            279,  13696,    430,   2216,   2751,    311,   5818,   3427,    388,
             13,    763,  25270,    330,     37,  28626,    220,     22,   1359,
            279,   4632,    596,  24190,  16495,    311,  16177,  23074,    596,
           3752,     11,  17520,     11,   1418,  12798,  68089,    311,    813,
           3560,    304,    279,  72097,    330,     37,  28626,      1,   1957,
          19562,     13,   2030,    814,   6612,    430,  13419,   1461,   1022,
           2643,   5101,   7684,  22018,     13,    330,   2746,    814,   1047,
           8208,   1523,    279,   1023,   1853,     11,    358,   1781,    358,
           1053,    617,  16436,    311,   6381,   3339,    420,   5818,   1359,
           7690,   7957,  72118,   3309,  72922,     13,  12361,     11,    279,
           5818,    596,  29414,  14896,    311,    330,   2171,    556,   7043,
            596,   3752,    304,    279,   1455,  49424,    323,  26861,   1648,
            320,  20670,      8,   1436,   1359,  72118,   1071,     13,  11205,
           4623,    574,    311,    617,  17520,  16177,    505,    813,  11660,
             11,   1579,  16405,    302,   2194,  19433,    704,    315,    264,
           5647,    315,  12014,    311,    813,   7982,   3070,    449,  23601,
          61697,     11,    889,    374,  20895,    449,    872,   2132,   1716,
             13,    362,   6237,   3389,    304,    279,   5818,   5039,   1461,
            323,  61697,   5737,    389,    264,  11573,    449,    872,   4538,
           1418,    279,  13941,   5992,    389,   1198,  16168,   5605,  47555,
             13,   5112,    813,  36504,  37772,  21414,  49679,   1634,    922,
            872,   1667,   3871,     11,   6522,    311,    264,  97044,    315,
          23074,  16451,    505,    279,   1176,   4848,   9698,     13,    578,
           5609,    430,  11335,    927,    279,  97044,    374,    220,    330,
          10031,   1472,  14077,   1359,    264,  20632,   1990,    468,    450,
          52709,  34918,    323,  25972,    393,    952,     13,   3623,  21337,
          21016,  54894,   6222,    279,   2835,    369,    279,   5609,   3389,
           7418,    389,    813,   5690,   2199,     11,   1405,    433,    706,
            810,   1109,    220,     16,     13,     20,   3610,  13452,     13,
          42896,    389,   6405,    323,   5690,  10213,  37475,    279,   5818,
            596,  13696,    439,    264,  27442,  35491,   1198,    323,    459,
          38683,  60588,    287,    832,     13,    330,   1692,    358,   1541,
            956,   2512,   1268,  11292,    577,    527,    477,   1268,  13481,
          21127,    577,   3802,    311,    387,    662,   2564,   1820,   1566,
           4330,   4520,   1047,    757,  94743,    709,    304,    279,   5818,
          27803,   2564,    358,   5602,    433,    220,     18,   3115,    304,
            832,   1938,   2564,   2564,   1820,  13696,    374,    279,  51621,
          13696,    358,   3077,   3596,   3970,   1359,    832,    893,   6267,
            389,    279,   5818,    596,   5690,   2199,     13, 128009, 128006,
          78191, 128007,    330,    578,  35491,    311,   7043,  23074,    574,
           1633,   1664,   2884,   1359,   1071,    264,   8571,   1306,  10307,
            279,  93431,    220,     22,     13,    578,  12360,    574,    389,
           1464,    505,  39970,    330,     37,  28626,    220,     22,      1,
            520,    279,    892,    315,    279,  64942,  11677,    902,   1101,
          11922,    279,   2324,    315,    279,   1841,    596,   5696,     11,
          29607,  13611,    300,     13, 128009]])
Creating model 0
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  25%|██▌       | 1/4 [04:16<12:50, 256.94s/it]Downloading shards:  25%|██▌       | 1/4 [04:09<12:27, 249.04s/it]Downloading shards:  25%|██▌       | 1/4 [04:16<12:50, 256.95s/it]Downloading shards:  25%|██▌       | 1/4 [04:16<12:50, 256.98s/it]Downloading shards:  50%|█████     | 2/4 [06:08<05:42, 171.46s/it]Downloading shards:  50%|█████     | 2/4 [06:00<05:36, 168.21s/it]Downloading shards:  50%|█████     | 2/4 [06:08<05:42, 171.48s/it]Downloading shards:  50%|█████     | 2/4 [06:08<05:42, 171.49s/it]Downloading shards:  75%|███████▌  | 3/4 [09:45<03:12, 192.23s/it]Downloading shards:  75%|███████▌  | 3/4 [09:37<03:10, 190.47s/it]Downloading shards:  75%|███████▌  | 3/4 [09:45<03:12, 192.22s/it]Downloading shards:  75%|███████▌  | 3/4 [09:45<03:12, 192.26s/it]Downloading shards: 100%|██████████| 4/4 [10:13<00:00, 127.52s/it]Downloading shards: 100%|██████████| 4/4 [10:13<00:00, 153.46s/it]
Downloading shards: 100%|██████████| 4/4 [10:05<00:00, 126.45s/it]Downloading shards: 100%|██████████| 4/4 [10:05<00:00, 151.48s/it]
Loading model 0
Total model params: 8030261248
Using n_workers: 10 for loading
Loading & Quantizing Model Shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [10:13<00:00, 127.51s/it]Downloading shards: 100%|██████████| 4/4 [10:13<00:00, 153.46s/it]
Loading model 3
Downloading shards: 100%|██████████| 4/4 [10:13<00:00, 127.52s/it]Downloading shards: 100%|██████████| 4/4 [10:13<00:00, 153.47s/it]
Loading model 1
Loading model 2
Loading & Quantizing Model Shards:  25%|██▌       | 1/4 [00:10<00:30, 10.08s/it]Loading & Quantizing Model Shards:  50%|█████     | 2/4 [00:19<00:19,  9.91s/it]Loading & Quantizing Model Shards:  75%|███████▌  | 3/4 [00:29<00:09,  9.75s/it]Loading & Quantizing Model Shards: 100%|██████████| 4/4 [00:37<00:00,  9.03s/it]Loading & Quantizing Model Shards: 100%|██████████| 4/4 [00:37<00:00,  9.34s/it]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Loaded model weights in 37.353 seconds
Rank 0: Model created: 0.107 GiB
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314053700330226
Wrapping model w/ FSDP 0
Rank 0: Wrapped model: 5.830 GiB
Applying activation checkpointing 0
Config:
LlamaConfig {
  "_name_or_path": "akjindal53244/Llama-3.1-Storm-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": false,
  "vocab_size": 128256
}

Model:
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): LlamaDecoderLayer(
                  (self_attn): LlamaSdpaAttention(
                    (q_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (k_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (v_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): LlamaRotaryEmbedding()
                  )
                  (mlp): LlamaMLP(
                    (gate_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (up_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (down_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=14336, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (act_fn): SiLU()
                  )
                  (input_layernorm): LlamaRMSNorm()
                  (post_attention_layernorm): LlamaRMSNorm()
                )
              )
            )
          )
          (norm): LlamaRMSNorm()
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
      )
    )
  )
)
Starting training
Optimizer params:
Shape: torch.Size([262669312]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Total Training Steps: 133
  0%|          | 0/133 [00:00<?, ?it/s]/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Epoch 0, Loss 0.000:   0%|          | 0/133 [00:00<?, ?it/s]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Epoch 0, Loss 0.000:   1%|          | 1/133 [01:36<3:32:38, 96.65s/it]Epoch 0, Loss 2.742, LR 3.85e-06:   1%|          | 1/133 [01:36<3:32:38, 96.65s/it]Epoch 0, Loss 2.742, LR 3.85e-06:   2%|▏         | 2/133 [03:36<4:00:26, 110.12s/it]Epoch 0, Loss 2.699, LR 7.69e-06:   2%|▏         | 2/133 [03:36<4:00:26, 110.12s/it]Epoch 0, Loss 2.699, LR 7.69e-06:   2%|▏         | 3/133 [05:37<4:09:12, 115.02s/it]Epoch 0, Loss 2.624, LR 1.15e-05:   2%|▏         | 3/133 [05:37<4:09:12, 115.02s/it]Epoch 0, Loss 2.624, LR 1.15e-05:   3%|▎         | 4/133 [07:27<4:03:29, 113.25s/it]Epoch 0, Loss 2.691, LR 1.54e-05:   3%|▎         | 4/133 [07:27<4:03:29, 113.25s/it]Epoch 0, Loss 2.691, LR 1.54e-05:   4%|▍         | 5/133 [09:15<3:57:18, 111.24s/it]Epoch 0, Loss 2.655, LR 1.92e-05:   4%|▍         | 5/133 [09:15<3:57:18, 111.24s/it]Epoch 0, Loss 2.655, LR 1.92e-05:   5%|▍         | 6/133 [11:09<3:57:50, 112.37s/it]Epoch 0, Loss 2.604, LR 2.31e-05:   5%|▍         | 6/133 [11:09<3:57:50, 112.37s/it]Epoch 0, Loss 2.604, LR 2.31e-05:   5%|▌         | 7/133 [13:05<3:58:24, 113.53s/it]Epoch 0, Loss 2.604, LR 2.69e-05:   5%|▌         | 7/133 [13:05<3:58:24, 113.53s/it]Epoch 0, Loss 2.604, LR 2.69e-05:   6%|▌         | 8/133 [14:52<3:52:00, 111.36s/it]Epoch 0, Loss 2.775, LR 3.08e-05:   6%|▌         | 8/133 [14:52<3:52:00, 111.36s/it]Epoch 0, Loss 2.775, LR 3.08e-05:   7%|▋         | 9/133 [16:42<3:49:11, 110.90s/it]Epoch 0, Loss 2.634, LR 3.46e-05:   7%|▋         | 9/133 [16:42<3:49:11, 110.90s/it]Epoch 0, Loss 2.634, LR 3.46e-05:   8%|▊         | 10/133 [18:35<3:49:00, 111.71s/it]Epoch 0, Loss 2.633, LR 3.85e-05:   8%|▊         | 10/133 [18:35<3:49:00, 111.71s/it]Epoch 0, Loss 2.633, LR 3.85e-05:   8%|▊         | 11/133 [20:20<3:42:28, 109.41s/it]Epoch 0, Loss 2.614, LR 4.23e-05:   8%|▊         | 11/133 [20:20<3:42:28, 109.41s/it]Epoch 0, Loss 2.614, LR 4.23e-05:   9%|▉         | 12/133 [22:03<3:36:45, 107.49s/it]Epoch 0, Loss 2.607, LR 4.62e-05:   9%|▉         | 12/133 [22:03<3:36:45, 107.49s/it]Epoch 0, Loss 2.607, LR 4.62e-05:  10%|▉         | 13/133 [23:58<3:39:51, 109.93s/it]Epoch 0, Loss 2.516, LR 5.00e-05:  10%|▉         | 13/133 [23:58<3:39:51, 109.93s/it]Epoch 0, Loss 2.516, LR 5.00e-05:  11%|█         | 14/133 [25:55<3:42:01, 111.95s/it]Epoch 0, Loss 2.435, LR 5.00e-05:  11%|█         | 14/133 [25:55<3:42:01, 111.95s/it]Epoch 0, Loss 2.435, LR 5.00e-05:  11%|█▏        | 15/133 [27:43<3:37:53, 110.79s/it]Epoch 0, Loss 2.501, LR 5.00e-05:  11%|█▏        | 15/133 [27:43<3:37:53, 110.79s/it]Epoch 0, Loss 2.501, LR 5.00e-05:  12%|█▏        | 16/133 [29:33<3:35:46, 110.65s/it]Epoch 0, Loss 2.486, LR 4.99e-05:  12%|█▏        | 16/133 [29:33<3:35:46, 110.65s/it]Epoch 0, Loss 2.486, LR 4.99e-05:  13%|█▎        | 17/133 [31:18<3:30:38, 108.95s/it]Epoch 0, Loss 2.492, LR 4.99e-05:  13%|█▎        | 17/133 [31:18<3:30:38, 108.95s/it]Epoch 0, Loss 2.492, LR 4.99e-05:  14%|█▎        | 18/133 [33:11<3:31:14, 110.22s/it]Epoch 0, Loss 2.409, LR 4.98e-05:  14%|█▎        | 18/133 [33:12<3:31:14, 110.22s/it]Epoch 0, Loss 2.409, LR 4.98e-05:  14%|█▍        | 19/133 [34:57<3:26:35, 108.73s/it]Epoch 0, Loss 2.497, LR 4.97e-05:  14%|█▍        | 19/133 [34:57<3:26:35, 108.73s/it]Epoch 0, Loss 2.497, LR 4.97e-05:  15%|█▌        | 20/133 [36:40<3:21:47, 107.14s/it]Epoch 0, Loss 2.445, LR 4.96e-05:  15%|█▌        | 20/133 [36:40<3:21:47, 107.14s/it]Epoch 0, Loss 2.445, LR 4.96e-05:  16%|█▌        | 21/133 [38:33<3:23:29, 109.01s/it]Epoch 0, Loss 2.351, LR 4.95e-05:  16%|█▌        | 21/133 [38:34<3:23:29, 109.01s/it]Epoch 0, Loss 2.351, LR 4.95e-05:  17%|█▋        | 22/133 [40:32<3:26:59, 111.88s/it]Epoch 0, Loss 2.265, LR 4.94e-05:  17%|█▋        | 22/133 [40:32<3:26:59, 111.88s/it]Epoch 0, Loss 2.265, LR 4.94e-05:  17%|█▋        | 23/133 [42:38<3:32:40, 116.01s/it]Epoch 0, Loss 2.322, LR 4.92e-05:  17%|█▋        | 23/133 [42:38<3:32:40, 116.01s/it]Epoch 0, Loss 2.322, LR 4.92e-05:  18%|█▊        | 24/133 [44:30<3:28:42, 114.89s/it]Epoch 0, Loss 2.316, LR 4.91e-05:  18%|█▊        | 24/133 [44:30<3:28:42, 114.89s/it]Epoch 0, Loss 2.316, LR 4.91e-05:  19%|█▉        | 25/133 [46:25<3:27:05, 115.05s/it]Epoch 0, Loss 2.255, LR 4.89e-05:  19%|█▉        | 25/133 [46:26<3:27:05, 115.05s/it]Epoch 0, Loss 2.255, LR 4.89e-05:  20%|█▉        | 26/133 [48:22<3:25:49, 115.42s/it]Epoch 0, Loss 2.220, LR 4.87e-05:  20%|█▉        | 26/133 [48:22<3:25:49, 115.42s/it]Epoch 0, Loss 2.220, LR 4.87e-05:  20%|██        | 27/133 [50:19<3:24:54, 115.99s/it]Epoch 0, Loss 2.188, LR 4.85e-05:  20%|██        | 27/133 [50:19<3:24:54, 115.99s/it]Epoch 0, Loss 2.188, LR 4.85e-05:  21%|██        | 28/133 [52:08<3:19:32, 114.02s/it]Epoch 0, Loss 2.242, LR 4.83e-05:  21%|██        | 28/133 [52:09<3:19:32, 114.02s/it]Epoch 0, Loss 2.242, LR 4.83e-05:  22%|██▏       | 29/133 [53:49<3:10:52, 110.12s/it]Epoch 0, Loss 2.192, LR 4.81e-05:  22%|██▏       | 29/133 [53:50<3:10:52, 110.12s/it]Epoch 0, Loss 2.192, LR 4.81e-05:  23%|██▎       | 30/133 [55:36<3:07:07, 109.00s/it]Epoch 0, Loss 2.131, LR 4.78e-05:  23%|██▎       | 30/133 [55:36<3:07:07, 109.00s/it]Epoch 0, Loss 2.131, LR 4.78e-05:  23%|██▎       | 31/133 [57:40<3:12:50, 113.44s/it]Epoch 0, Loss 2.131, LR 4.75e-05:  23%|██▎       | 31/133 [57:40<3:12:50, 113.44s/it]Epoch 0, Loss 2.131, LR 4.75e-05:  24%|██▍       | 32/133 [59:15<3:01:46, 107.99s/it]Epoch 0, Loss 2.246, LR 4.73e-05:  24%|██▍       | 32/133 [59:15<3:01:46, 107.99s/it]Epoch 0, Loss 2.246, LR 4.73e-05:  25%|██▍       | 33/133 [1:01:10<3:03:20, 110.01s/it]Epoch 0, Loss 2.170, LR 4.70e-05:  25%|██▍       | 33/133 [1:01:10<3:03:20, 110.01s/it]Epoch 0, Loss 2.170, LR 4.70e-05:  26%|██▌       | 34/133 [1:03:06<3:04:25, 111.77s/it]Epoch 0, Loss 2.108, LR 4.67e-05:  26%|██▌       | 34/133 [1:03:06<3:04:25, 111.77s/it]Epoch 0, Loss 2.108, LR 4.67e-05:  26%|██▋       | 35/133 [1:04:57<3:02:38, 111.82s/it]Epoch 0, Loss 2.189, LR 4.64e-05:  26%|██▋       | 35/133 [1:04:58<3:02:38, 111.82s/it]Epoch 0, Loss 2.189, LR 4.64e-05:  27%|██▋       | 36/133 [1:06:47<2:59:42, 111.16s/it]Epoch 0, Loss 2.069, LR 4.60e-05:  27%|██▋       | 36/133 [1:06:47<2:59:42, 111.16s/it]Epoch 0, Loss 2.069, LR 4.60e-05:  28%|██▊       | 37/133 [1:08:35<2:56:31, 110.33s/it]Epoch 0, Loss 2.136, LR 4.57e-05:  28%|██▊       | 37/133 [1:08:36<2:56:31, 110.33s/it]Epoch 0, Loss 2.136, LR 4.57e-05:  29%|██▊       | 38/133 [1:10:34<2:58:34, 112.79s/it]Epoch 0, Loss 2.069, LR 4.54e-05:  29%|██▊       | 38/133 [1:10:34<2:58:34, 112.79s/it]Epoch 0, Loss 2.069, LR 4.54e-05:  29%|██▉       | 39/133 [1:12:27<2:56:34, 112.71s/it]Epoch 0, Loss 2.179, LR 4.50e-05:  29%|██▉       | 39/133 [1:12:27<2:56:34, 112.71s/it]Epoch 0, Loss 2.179, LR 4.50e-05:  30%|███       | 40/133 [1:14:11<2:50:42, 110.14s/it]Epoch 0, Loss 2.161, LR 4.46e-05:  30%|███       | 40/133 [1:14:11<2:50:42, 110.14s/it]Epoch 0, Loss 2.161, LR 4.46e-05:  31%|███       | 41/133 [1:16:00<2:48:41, 110.01s/it]Epoch 0, Loss 2.151, LR 4.42e-05:  31%|███       | 41/133 [1:16:00<2:48:41, 110.01s/it]Epoch 0, Loss 2.151, LR 4.42e-05:  32%|███▏      | 42/133 [1:18:02<2:52:02, 113.43s/it]Epoch 0, Loss 2.101, LR 4.38e-05:  32%|███▏      | 42/133 [1:18:02<2:52:02, 113.43s/it]Epoch 0, Loss 2.101, LR 4.38e-05:  32%|███▏      | 43/133 [1:19:56<2:50:38, 113.77s/it]Epoch 0, Loss 2.076, LR 4.34e-05:  32%|███▏      | 43/133 [1:19:56<2:50:38, 113.77s/it]Epoch 0, Loss 2.076, LR 4.34e-05:  33%|███▎      | 44/133 [1:21:42<2:45:01, 111.25s/it]Epoch 0, Loss 2.095, LR 4.30e-05:  33%|███▎      | 44/133 [1:21:42<2:45:01, 111.25s/it]Epoch 0, Loss 2.095, LR 4.30e-05:  34%|███▍      | 45/133 [1:23:32<2:42:53, 111.06s/it]Epoch 0, Loss 2.112, LR 4.26e-05:  34%|███▍      | 45/133 [1:23:32<2:42:53, 111.06s/it]Epoch 0, Loss 2.112, LR 4.26e-05:  35%|███▍      | 46/133 [1:25:36<2:46:37, 114.91s/it]Epoch 0, Loss 2.085, LR 4.21e-05:  35%|███▍      | 46/133 [1:25:36<2:46:37, 114.91s/it]Epoch 0, Loss 2.085, LR 4.21e-05:  35%|███▌      | 47/133 [1:27:17<2:38:38, 110.68s/it]Epoch 0, Loss 2.065, LR 4.17e-05:  35%|███▌      | 47/133 [1:27:17<2:38:38, 110.68s/it]Epoch 0, Loss 2.065, LR 4.17e-05:  36%|███▌      | 48/133 [1:29:03<2:34:36, 109.14s/it]Epoch 0, Loss 2.016, LR 4.12e-05:  36%|███▌      | 48/133 [1:29:03<2:34:36, 109.14s/it]Epoch 0, Loss 2.016, LR 4.12e-05:  37%|███▋      | 49/133 [1:31:14<2:42:21, 115.97s/it]Epoch 0, Loss 2.114, LR 4.07e-05:  37%|███▋      | 49/133 [1:31:14<2:42:21, 115.97s/it]Epoch 0, Loss 2.114, LR 4.07e-05:  38%|███▊      | 50/133 [1:33:05<2:38:05, 114.28s/it]Epoch 0, Loss 2.061, LR 4.02e-05:  38%|███▊      | 50/133 [1:33:05<2:38:05, 114.28s/it]Epoch 0, Loss 2.061, LR 4.02e-05:  38%|███▊      | 51/133 [1:34:55<2:34:38, 113.15s/it]Epoch 0, Loss 2.077, LR 3.98e-05:  38%|███▊      | 51/133 [1:34:55<2:34:38, 113.15s/it]Epoch 0, Loss 2.077, LR 3.98e-05:  39%|███▉      | 52/133 [1:36:51<2:33:38, 113.81s/it]Epoch 0, Loss 2.059, LR 3.93e-05:  39%|███▉      | 52/133 [1:36:51<2:33:38, 113.81s/it]Epoch 0, Loss 2.059, LR 3.93e-05:  40%|███▉      | 53/133 [1:38:35<2:27:55, 110.94s/it]Epoch 0, Loss 2.072, LR 3.87e-05:  40%|███▉      | 53/133 [1:38:35<2:27:55, 110.94s/it]Epoch 0, Loss 2.072, LR 3.87e-05:  41%|████      | 54/133 [1:40:27<2:26:39, 111.39s/it]Epoch 0, Loss 1.999, LR 3.82e-05:  41%|████      | 54/133 [1:40:27<2:26:39, 111.39s/it]Epoch 0, Loss 1.999, LR 3.82e-05:  41%|████▏     | 55/133 [1:42:16<2:23:47, 110.61s/it]Epoch 0, Loss 2.005, LR 3.77e-05:  41%|████▏     | 55/133 [1:42:16<2:23:47, 110.61s/it]Epoch 0, Loss 2.005, LR 3.77e-05:  42%|████▏     | 56/133 [1:43:58<2:18:35, 107.99s/it]Epoch 0, Loss 2.014, LR 3.72e-05:  42%|████▏     | 56/133 [1:43:58<2:18:35, 107.99s/it]Epoch 0, Loss 2.014, LR 3.72e-05:  43%|████▎     | 57/133 [1:45:53<2:19:26, 110.09s/it]Epoch 0, Loss 2.044, LR 3.67e-05:  43%|████▎     | 57/133 [1:45:53<2:19:26, 110.09s/it]Epoch 0, Loss 2.044, LR 3.67e-05:  44%|████▎     | 58/133 [1:47:46<2:18:49, 111.06s/it]Epoch 0, Loss 2.019, LR 3.61e-05:  44%|████▎     | 58/133 [1:47:46<2:18:49, 111.06s/it]Epoch 0, Loss 2.019, LR 3.61e-05:  44%|████▍     | 59/133 [1:49:46<2:20:17, 113.75s/it]Epoch 0, Loss 1.995, LR 3.56e-05:  44%|████▍     | 59/133 [1:49:46<2:20:17, 113.75s/it]Epoch 0, Loss 1.995, LR 3.56e-05:  45%|████▌     | 60/133 [1:51:35<2:16:27, 112.16s/it]Epoch 0, Loss 1.941, LR 3.50e-05:  45%|████▌     | 60/133 [1:51:35<2:16:27, 112.16s/it]Batch idx 0
Batch idx 1
Batch idx 2
Batch idx 3
Batch idx 4
Batch idx 5
Batch idx 6
Gradient norm: 0.890625
Batch idx 7
Batch idx 8
Batch idx 9
Batch idx 10
Batch idx 11
Batch idx 12
Batch idx 13
Batch idx 14
Gradient norm: 0.78515625
Batch idx 15
Batch idx 16
Batch idx 17
Batch idx 18
Batch idx 19
Batch idx 20
Batch idx 21
Batch idx 22
Gradient norm: 0.671875
Batch idx 23
Batch idx 24
Batch idx 25
Batch idx 26
Batch idx 27
Batch idx 28
Batch idx 29
Batch idx 30
Gradient norm: 0.78515625
Batch idx 31
Batch idx 32
Batch idx 33
Batch idx 34
Batch idx 35
Batch idx 36
Batch idx 37
Batch idx 38
Gradient norm: 0.71484375
Batch idx 39
Batch idx 40
Batch idx 41
Batch idx 42
Batch idx 43
Batch idx 44
Batch idx 45
Batch idx 46
Gradient norm: 0.64453125
Batch idx 47
Batch idx 48
Batch idx 49
Batch idx 50
Batch idx 51
Batch idx 52
Batch idx 53
Batch idx 54
Gradient norm: 0.62890625
Batch idx 55
Batch idx 56
Batch idx 57
Batch idx 58
Batch idx 59
Batch idx 60
Batch idx 61
Batch idx 62
Gradient norm: 0.8125
Batch idx 63
Batch idx 64
Batch idx 65
Batch idx 66
Batch idx 67
Batch idx 68
Batch idx 69
Batch idx 70
Gradient norm: 0.71875
Batch idx 71
Batch idx 72
Batch idx 73
Batch idx 74
Batch idx 75
Batch idx 76
Batch idx 77
Batch idx 78
Gradient norm: 0.8203125
Batch idx 79
Batch idx 80
Batch idx 81
Batch idx 82
Batch idx 83
Batch idx 84
Batch idx 85
Batch idx 86
Gradient norm: 0.71484375
Batch idx 87
Batch idx 88
Batch idx 89
Batch idx 90
Batch idx 91
Batch idx 92
Batch idx 93
Batch idx 94
Gradient norm: 0.78125
Batch idx 95
Batch idx 96
Batch idx 97
Batch idx 98
Batch idx 99
Batch idx 100
Batch idx 101
Batch idx 102
Gradient norm: 0.640625
Batch idx 103
Batch idx 104
Batch idx 105
Batch idx 106
Batch idx 107
Batch idx 108
Batch idx 109
Batch idx 110
Gradient norm: 0.6015625
Batch idx 111
Batch idx 112
Batch idx 113
Batch idx 114
Batch idx 115
Batch idx 116
Batch idx 117
Batch idx 118
Gradient norm: 0.6171875
Batch idx 119
Batch idx 120
Batch idx 121
Batch idx 122
Batch idx 123
Batch idx 124
Batch idx 125
Batch idx 126
Gradient norm: 0.6015625
Batch idx 127
Batch idx 128
Batch idx 129
Batch idx 130
Batch idx 131
Batch idx 132
Batch idx 133
Batch idx 134
Gradient norm: 0.53515625
Batch idx 135
Batch idx 136
Batch idx 137
Batch idx 138
Batch idx 139
Batch idx 140
Batch idx 141
Batch idx 142
Gradient norm: 0.490234375
Batch idx 143
Batch idx 144
Batch idx 145
Batch idx 146
Batch idx 147
Batch idx 148
Batch idx 149
Batch idx 150
Gradient norm: 0.5
Batch idx 151
Batch idx 152
Batch idx 153
Batch idx 154
Batch idx 155
Batch idx 156
Batch idx 157
Batch idx 158
Gradient norm: 0.466796875
Batch idx 159
Batch idx 160
Batch idx 161
Batch idx 162
Batch idx 163
Batch idx 164
Batch idx 165
Batch idx 166
Gradient norm: 0.3984375
Batch idx 167
Batch idx 168
Batch idx 169
Batch idx 170
Batch idx 171
Batch idx 172
Batch idx 173
Batch idx 174
Gradient norm: 0.37109375
Batch idx 175
Batch idx 176
Batch idx 177
Batch idx 178
Batch idx 179
Batch idx 180
Batch idx 181
Batch idx 182
Gradient norm: 0.369140625
Batch idx 183
Batch idx 184
Batch idx 185
Batch idx 186
Batch idx 187
Batch idx 188
Batch idx 189
Batch idx 190
Gradient norm: 0.37109375
Batch idx 191
Batch idx 192
Batch idx 193
Batch idx 194
Batch idx 195
Batch idx 196
Batch idx 197
Batch idx 198
Gradient norm: 0.35546875
Batch idx 199
Batch idx 200
Batch idx 201
Batch idx 202
Batch idx 203
Batch idx 204
Batch idx 205
Batch idx 206
Gradient norm: 0.298828125
Batch idx 207
Batch idx 208
Batch idx 209
Batch idx 210
Batch idx 211
Batch idx 212
Batch idx 213
Batch idx 214
Gradient norm: 0.259765625
Batch idx 215
Batch idx 216
Batch idx 217
Batch idx 218
Batch idx 219
Batch idx 220
Batch idx 221
Batch idx 222
Gradient norm: 0.271484375
Batch idx 223
Batch idx 224
Batch idx 225
Batch idx 226
Batch idx 227
Batch idx 228
Batch idx 229
Batch idx 230
Gradient norm: 0.25
Batch idx 231
Batch idx 232
Batch idx 233
Batch idx 234
Batch idx 235
Batch idx 236
Batch idx 237
Batch idx 238
Gradient norm: 0.25
Batch idx 239
Batch idx 240
Batch idx 241
Batch idx 242
Batch idx 243
Batch idx 244
Batch idx 245
Batch idx 246
Gradient norm: 0.1904296875
Batch idx 247
Batch idx 248
Batch idx 249
Batch idx 250
Batch idx 251
Batch idx 252
Batch idx 253
Batch idx 254
Gradient norm: 0.2265625
Batch idx 255
Batch idx 256
Batch idx 257
Batch idx 258
Batch idx 259
Batch idx 260
Batch idx 261
Batch idx 262
Gradient norm: 0.205078125
Batch idx 263
Batch idx 264
Batch idx 265
Batch idx 266
Batch idx 267
Batch idx 268
Batch idx 269
Batch idx 270
Gradient norm: 0.1708984375
Batch idx 271
Batch idx 272
Batch idx 273
Batch idx 274
Batch idx 275
Batch idx 276
Batch idx 277
Batch idx 278
Gradient norm: 0.1748046875
Batch idx 279
Batch idx 280
Batch idx 281
Batch idx 282
Batch idx 283
Batch idx 284
Batch idx 285
Batch idx 286
Gradient norm: 0.1982421875
Batch idx 287
Batch idx 288
Batch idx 289
Batch idx 290
Batch idx 291
Batch idx 292
Batch idx 293
Batch idx 294
Gradient norm: 0.1845703125
Batch idx 295
Batch idx 296
Batch idx 297
Batch idx 298
Batch idx 299
Batch idx 300
Batch idx 301
Batch idx 302
Gradient norm: 0.1650390625
Batch idx 303
Batch idx 304
Batch idx 305
Batch idx 306
Batch idx 307
Batch idx 308
Batch idx 309
Batch idx 310
Gradient norm: 0.154296875
Batch idx 311
Batch idx 312
Batch idx 313
Batch idx 314
Batch idx 315
Batch idx 316
Batch idx 317
Batch idx 318
Gradient norm: 0.1591796875
Batch idx 319
Batch idx 320
Batch idx 321
Batch idx 322
Batch idx 323
Batch idx 324
Batch idx 325
Batch idx 326
Gradient norm: 0.1455078125
Batch idx 327
Batch idx 328
Batch idx 329
Batch idx 330
Batch idx 331
Batch idx 332
Batch idx 333
Batch idx 334
Gradient norm: 0.14453125
Batch idx 335
Batch idx 336
Batch idx 337
Batch idx 338
Batch idx 339
Batch idx 340
Batch idx 341
Batch idx 342
Gradient norm: 0.1826171875
Batch idx 343
Batch idx 344
Batch idx 345
Batch idx 346
Batch idx 347
Batch idx 348
Batch idx 349
Batch idx 350
Gradient norm: 0.15625
Batch idx 351
Batch idx 352
Batch idx 353
Batch idx 354
Batch idx 355
Batch idx 356
Batch idx 357
Batch idx 358
Gradient norm: 0.1796875
Batch idx 359
Batch idx 360
Batch idx 361
Batch idx 362
Batch idx 363
Batch idx 364
Batch idx 365
Batch idx 366
Gradient norm: 0.1494140625
Batch idx 367
Batch idx 368
Batch idx 369
Batch idx 370
Batch idx 371
Batch idx 372
Batch idx 373
Batch idx 374
Gradient norm: 0.1689453125
Batch idx 375
Batch idx 376
Batch idx 377
Batch idx 378
Batch idx 379
Batch idx 380
Batch idx 381
Batch idx 382
Gradient norm: 0.14453125
Batch idx 383
Batch idx 384
Batch idx 385
Batch idx 386
Batch idx 387
Batch idx 388
Batch idx 389
Batch idx 390
Gradient norm: 0.13671875
Batch idx 391
Batch idx 392
Batch idx 393
Batch idx 394
Batch idx 395
Batch idx 396
Batch idx 397
Batch idx 398
Gradient norm: 0.1396484375
Batch idx 399
Batch idx 400
Batch idx 401
Batch idx 402
Batch idx 403
Batch idx 404
Batch idx 405
Batch idx 406
Gradient norm: 0.14453125
Batch idx 407
Batch idx 408
Batch idx 409
Batch idx 410
Batch idx 411
Batch idx 412
Batch idx 413
Batch idx 414
Gradient norm: 0.1572265625
Batch idx 415
Batch idx 416
Batch idx 417
Batch idx 418
Batch idx 419
Batch idx 420
Batch idx 421
Batch idx 422
Gradient norm: 0.1640625
Batch idx 423
Batch idx 424
Batch idx 425
Batch idx 426
Batch idx 427
Batch idx 428
Batch idx 429
Batch idx 430
Gradient norm: 0.181640625
Batch idx 431
Batch idx 432
Batch idx 433
Batch idx 434
Batch idx 435
Batch idx 436
Batch idx 437
Batch idx 438
Gradient norm: 0.1533203125
Batch idx 439
Batch idx 440
Batch idx 441
Batch idx 442
Batch idx 443
Batch idx 444
Batch idx 445
Batch idx 446
Gradient norm: 0.14453125
Batch idx 447
Batch idx 448
Batch idx 449
Batch idx 450
Batch idx 451
Batch idx 452
Batch idx 453
Batch idx 454
Gradient norm: 0.138671875
Batch idx 455
Batch idx 456
Batch idx 457
Batch idx 458
Batch idx 459
Batch idx 460
Batch idx 461
Batch idx 462
Gradient norm: 0.12451171875
Batch idx 463
Batch idx 464
Batch idx 465
Batch idx 466
Batch idx 467
Batch idx 468
Batch idx 469
Batch idx 470
Gradient norm: 0.1328125
Batch idx 471
Batch idx 472
Batch idx 473
Batch idx 474
Batch idx 475
Batch idx 476
Batch idx 477
Batch idx 478
Gradient norm: 0.16796875
Batch idx 479
Batch idx 480
Batch idx 481
Epoch 0, Loss 1.941, LR 3.50e-05:  46%|████▌     | 61/133 [1:53:27<2:14:31, 112.10s/it]Epoch 0, Loss 2.014, LR 3.45e-05:  46%|████▌     | 61/133 [1:53:27<2:14:31, 112.10s/it]Epoch 0, Loss 2.014, LR 3.45e-05:  47%|████▋     | 62/133 [1:55:14<2:10:50, 110.57s/it]Epoch 0, Loss 1.958, LR 3.39e-05:  47%|████▋     | 62/133 [1:55:14<2:10:50, 110.57s/it]Epoch 0, Loss 1.958, LR 3.39e-05:  47%|████▋     | 63/133 [1:57:03<2:08:36, 110.24s/it]Epoch 0, Loss 2.076, LR 3.33e-05:  47%|████▋     | 63/133 [1:57:03<2:08:36, 110.24s/it]Epoch 0, Loss 2.076, LR 3.33e-05:  48%|████▊     | 64/133 [1:58:55<2:07:16, 110.68s/it]Epoch 0, Loss 2.049, LR 3.28e-05:  48%|████▊     | 64/133 [1:58:55<2:07:16, 110.68s/it]Epoch 0, Loss 2.049, LR 3.28e-05:  49%|████▉     | 65/133 [2:00:43<2:04:28, 109.82s/it]Epoch 0, Loss 2.064, LR 3.22e-05:  49%|████▉     | 65/133 [2:00:43<2:04:28, 109.82s/it]Epoch 0, Loss 2.064, LR 3.22e-05:  50%|████▉     | 66/133 [2:02:29<2:01:25, 108.74s/it]Epoch 0, Loss 2.002, LR 3.16e-05:  50%|████▉     | 66/133 [2:02:29<2:01:25, 108.74s/it]Epoch 0, Loss 2.002, LR 3.16e-05:  50%|█████     | 67/133 [2:04:09<1:56:45, 106.14s/it]Epoch 0, Loss 1.881, LR 3.10e-05:  50%|█████     | 67/133 [2:04:09<1:56:45, 106.14s/it]Epoch 0, Loss 1.881, LR 3.10e-05:  51%|█████     | 68/133 [2:05:59<1:56:16, 107.33s/it]Epoch 0, Loss 2.052, LR 3.04e-05:  51%|█████     | 68/133 [2:05:59<1:56:16, 107.33s/it]Epoch 0, Loss 2.052, LR 3.04e-05:  52%|█████▏    | 69/133 [2:07:57<1:57:50, 110.48s/it]Epoch 0, Loss 1.975, LR 2.99e-05:  52%|█████▏    | 69/133 [2:07:57<1:57:50, 110.48s/it]Epoch 0, Loss 1.975, LR 2.99e-05:  53%|█████▎    | 70/133 [2:09:53<1:57:36, 112.00s/it]Epoch 0, Loss 1.998, LR 2.93e-05:  53%|█████▎    | 70/133 [2:09:53<1:57:36, 112.00s/it]Epoch 0, Loss 1.998, LR 2.93e-05:  53%|█████▎    | 71/133 [2:11:49<1:57:06, 113.33s/it]Epoch 0, Loss 1.999, LR 2.87e-05:  53%|█████▎    | 71/133 [2:11:49<1:57:06, 113.33s/it]Epoch 0, Loss 1.999, LR 2.87e-05:  54%|█████▍    | 72/133 [2:13:48<1:57:03, 115.14s/it]Epoch 0, Loss 1.842, LR 2.81e-05:  54%|█████▍    | 72/133 [2:13:48<1:57:03, 115.14s/it]Epoch 0, Loss 1.842, LR 2.81e-05:  55%|█████▍    | 73/133 [2:15:40<1:54:08, 114.15s/it]Epoch 0, Loss 2.052, LR 2.75e-05:  55%|█████▍    | 73/133 [2:15:40<1:54:08, 114.15s/it]Epoch 0, Loss 2.052, LR 2.75e-05:  56%|█████▌    | 74/133 [2:17:37<1:53:07, 115.05s/it]Epoch 0, Loss 2.065, LR 2.69e-05:  56%|█████▌    | 74/133 [2:17:37<1:53:07, 115.05s/it]Epoch 0, Loss 2.065, LR 2.69e-05:  56%|█████▋    | 75/133 [2:19:44<1:54:31, 118.48s/it]Epoch 0, Loss 1.990, LR 2.63e-05:  56%|█████▋    | 75/133 [2:19:44<1:54:31, 118.48s/it]Epoch 0, Loss 1.990, LR 2.63e-05:  57%|█████▋    | 76/133 [2:21:41<1:52:04, 117.98s/it]Epoch 0, Loss 2.002, LR 2.57e-05:  57%|█████▋    | 76/133 [2:21:41<1:52:04, 117.98s/it]Epoch 0, Loss 2.002, LR 2.57e-05:  58%|█████▊    | 77/133 [2:23:37<1:49:45, 117.60s/it]Epoch 0, Loss 1.990, LR 2.51e-05:  58%|█████▊    | 77/133 [2:23:37<1:49:45, 117.60s/it]Epoch 0, Loss 1.990, LR 2.51e-05:  59%|█████▊    | 78/133 [2:25:42<1:49:49, 119.80s/it]Epoch 0, Loss 2.007, LR 2.46e-05:  59%|█████▊    | 78/133 [2:25:42<1:49:49, 119.80s/it]Epoch 0, Loss 2.007, LR 2.46e-05:  59%|█████▉    | 79/133 [2:27:41<1:47:26, 119.39s/it]Epoch 0, Loss 2.038, LR 2.40e-05:  59%|█████▉    | 79/133 [2:27:41<1:47:26, 119.39s/it]Epoch 0, Loss 2.038, LR 2.40e-05:  60%|██████    | 80/133 [2:29:32<1:43:23, 117.04s/it]Epoch 0, Loss 2.038, LR 2.34e-05:  60%|██████    | 80/133 [2:29:32<1:43:23, 117.04s/it]Epoch 0, Loss 2.038, LR 2.34e-05:  61%|██████    | 81/133 [2:31:16<1:37:55, 113.00s/it]Epoch 0, Loss 1.928, LR 2.28e-05:  61%|██████    | 81/133 [2:31:16<1:37:55, 113.00s/it]Epoch 0, Loss 1.928, LR 2.28e-05:  62%|██████▏   | 82/133 [2:32:58<1:33:13, 109.67s/it]Epoch 0, Loss 1.874, LR 2.22e-05:  62%|██████▏   | 82/133 [2:32:58<1:33:13, 109.67s/it]Epoch 0, Loss 1.874, LR 2.22e-05:  62%|██████▏   | 83/133 [2:34:54<1:33:02, 111.65s/it]Epoch 0, Loss 1.954, LR 2.17e-05:  62%|██████▏   | 83/133 [2:34:54<1:33:02, 111.65s/it]Epoch 0, Loss 1.954, LR 2.17e-05:  63%|██████▎   | 84/133 [2:36:42<1:30:22, 110.66s/it]Epoch 0, Loss 2.038, LR 2.11e-05:  63%|██████▎   | 84/133 [2:36:42<1:30:22, 110.66s/it]Epoch 0, Loss 2.038, LR 2.11e-05:  64%|██████▍   | 85/133 [2:38:23<1:26:12, 107.76s/it]Epoch 0, Loss 1.949, LR 2.05e-05:  64%|██████▍   | 85/133 [2:38:23<1:26:12, 107.76s/it]Epoch 0, Loss 1.949, LR 2.05e-05:  65%|██████▍   | 86/133 [2:40:17<1:25:49, 109.55s/it]Epoch 0, Loss 1.987, LR 2.00e-05:  65%|██████▍   | 86/133 [2:40:17<1:25:49, 109.55s/it]Epoch 0, Loss 1.987, LR 2.00e-05:  65%|██████▌   | 87/133 [2:41:51<1:20:18, 104.74s/it]Epoch 0, Loss 1.906, LR 1.94e-05:  65%|██████▌   | 87/133 [2:41:51<1:20:18, 104.74s/it]Epoch 0, Loss 1.906, LR 1.94e-05:  66%|██████▌   | 88/133 [2:43:41<1:19:45, 106.35s/it]Epoch 0, Loss 2.004, LR 1.89e-05:  66%|██████▌   | 88/133 [2:43:41<1:19:45, 106.35s/it]Epoch 0, Loss 2.004, LR 1.89e-05:  67%|██████▋   | 89/133 [2:45:31<1:18:48, 107.47s/it]Epoch 0, Loss 1.898, LR 1.83e-05:  67%|██████▋   | 89/133 [2:45:31<1:18:48, 107.47s/it]Epoch 0, Loss 1.898, LR 1.83e-05:  68%|██████▊   | 90/133 [2:47:18<1:16:54, 107.32s/it]Epoch 0, Loss 1.991, LR 1.78e-05:  68%|██████▊   | 90/133 [2:47:18<1:16:54, 107.32s/it]Epoch 0, Loss 1.991, LR 1.78e-05:  68%|██████▊   | 91/133 [2:49:09<1:15:58, 108.53s/it]Epoch 0, Loss 1.990, LR 1.73e-05:  68%|██████▊   | 91/133 [2:49:09<1:15:58, 108.53s/it]Epoch 0, Loss 1.990, LR 1.73e-05:  69%|██████▉   | 92/133 [2:51:02<1:14:58, 109.71s/it]Epoch 0, Loss 2.015, LR 1.68e-05:  69%|██████▉   | 92/133 [2:51:02<1:14:58, 109.71s/it]Epoch 0, Loss 2.015, LR 1.68e-05:  70%|██████▉   | 93/133 [2:52:48<1:12:26, 108.65s/it]Epoch 0, Loss 1.949, LR 1.63e-05:  70%|██████▉   | 93/133 [2:52:48<1:12:26, 108.65s/it]Epoch 0, Loss 1.949, LR 1.63e-05:  71%|███████   | 94/133 [2:54:47<1:12:44, 111.90s/it]Epoch 0, Loss 1.960, LR 1.57e-05:  71%|███████   | 94/133 [2:54:47<1:12:44, 111.90s/it]Epoch 0, Loss 1.960, LR 1.57e-05:  71%|███████▏  | 95/133 [2:56:55<1:13:51, 116.62s/it]Epoch 0, Loss 1.944, LR 1.52e-05:  71%|███████▏  | 95/133 [2:56:55<1:13:51, 116.62s/it]Epoch 0, Loss 1.944, LR 1.52e-05:  72%|███████▏  | 96/133 [2:58:40<1:09:51, 113.30s/it]Epoch 0, Loss 2.019, LR 1.48e-05:  72%|███████▏  | 96/133 [2:58:41<1:09:51, 113.30s/it]Epoch 0, Loss 2.019, LR 1.48e-05:  73%|███████▎  | 97/133 [3:00:28<1:06:58, 111.63s/it]Epoch 0, Loss 1.962, LR 1.43e-05:  73%|███████▎  | 97/133 [3:00:28<1:06:58, 111.63s/it]Epoch 0, Loss 1.962, LR 1.43e-05:  74%|███████▎  | 98/133 [3:02:08<1:03:03, 108.09s/it]Epoch 0, Loss 1.945, LR 1.38e-05:  74%|███████▎  | 98/133 [3:02:08<1:03:03, 108.09s/it]Epoch 0, Loss 1.945, LR 1.38e-05:  74%|███████▍  | 99/133 [3:03:57<1:01:26, 108.43s/it]Epoch 0, Loss 1.928, LR 1.33e-05:  74%|███████▍  | 99/133 [3:03:57<1:01:26, 108.43s/it]Epoch 0, Loss 1.928, LR 1.33e-05:  75%|███████▌  | 100/133 [3:05:44<59:24, 108.02s/it] Epoch 0, Loss 1.925, LR 1.29e-05:  75%|███████▌  | 100/133 [3:05:44<59:24, 108.02s/it]Epoch 0, Loss 1.925, LR 1.29e-05:  76%|███████▌  | 101/133 [3:07:37<58:23, 109.48s/it]Epoch 0, Loss 1.947, LR 1.24e-05:  76%|███████▌  | 101/133 [3:07:37<58:23, 109.48s/it]Epoch 0, Loss 1.947, LR 1.24e-05:  77%|███████▋  | 102/133 [3:09:30<57:01, 110.37s/it]Epoch 0, Loss 1.947, LR 1.20e-05:  77%|███████▋  | 102/133 [3:09:30<57:01, 110.37s/it]Epoch 0, Loss 1.947, LR 1.20e-05:  77%|███████▋  | 103/133 [3:11:28<56:20, 112.69s/it]Epoch 0, Loss 1.948, LR 1.16e-05:  77%|███████▋  | 103/133 [3:11:28<56:20, 112.69s/it]Epoch 0, Loss 1.948, LR 1.16e-05:  78%|███████▊  | 104/133 [3:13:07<52:28, 108.58s/it]Epoch 0, Loss 1.973, LR 1.12e-05:  78%|███████▊  | 104/133 [3:13:07<52:28, 108.58s/it]Epoch 0, Loss 1.973, LR 1.12e-05:  79%|███████▉  | 105/133 [3:14:55<50:35, 108.41s/it]Epoch 0, Loss 1.971, LR 1.08e-05:  79%|███████▉  | 105/133 [3:14:55<50:35, 108.41s/it]Epoch 0, Loss 1.971, LR 1.08e-05:  80%|███████▉  | 106/133 [3:16:50<49:46, 110.61s/it]Epoch 0, Loss 1.892, LR 1.04e-05:  80%|███████▉  | 106/133 [3:16:51<49:46, 110.61s/it]Epoch 0, Loss 1.892, LR 1.04e-05:  80%|████████  | 107/133 [3:18:24<45:41, 105.45s/it]Epoch 0, Loss 1.984, LR 1.00e-05:  80%|████████  | 107/133 [3:18:24<45:41, 105.45s/it]Epoch 0, Loss 1.984, LR 1.00e-05:  81%|████████  | 108/133 [3:20:17<44:55, 107.83s/it]Epoch 0, Loss 1.988, LR 9.65e-06:  81%|████████  | 108/133 [3:20:17<44:55, 107.83s/it]Epoch 0, Loss 1.988, LR 9.65e-06:  82%|████████▏ | 109/133 [3:22:18<44:37, 111.56s/it]Epoch 0, Loss 1.970, LR 9.30e-06:  82%|████████▏ | 109/133 [3:22:18<44:37, 111.56s/it]Epoch 0, Loss 1.970, LR 9.30e-06:  83%|████████▎ | 110/133 [3:24:21<44:10, 115.22s/it]Epoch 0, Loss 1.877, LR 8.96e-06:  83%|████████▎ | 110/133 [3:24:21<44:10, 115.22s/it]Epoch 0, Loss 1.877, LR 8.96e-06:  83%|████████▎ | 111/133 [3:26:18<42:22, 115.55s/it]Epoch 0, Loss 1.935, LR 8.63e-06:  83%|████████▎ | 111/133 [3:26:18<42:22, 115.55s/it]Epoch 0, Loss 1.935, LR 8.63e-06:  84%|████████▍ | 112/133 [3:28:10<40:03, 114.48s/it]Epoch 0, Loss 1.979, LR 8.32e-06:  84%|████████▍ | 112/133 [3:28:10<40:03, 114.48s/it]Epoch 0, Loss 1.979, LR 8.32e-06:  85%|████████▍ | 113/133 [3:30:08<38:31, 115.60s/it]Epoch 0, Loss 1.997, LR 8.01e-06:  85%|████████▍ | 113/133 [3:30:08<38:31, 115.60s/it]Epoch 0, Loss 1.997, LR 8.01e-06:  86%|████████▌ | 114/133 [3:31:49<35:11, 111.14s/it]Epoch 0, Loss 1.971, LR 7.73e-06:  86%|████████▌ | 114/133 [3:31:49<35:11, 111.14s/it]Epoch 0, Loss 1.971, LR 7.73e-06:  86%|████████▋ | 115/133 [3:33:38<33:08, 110.49s/it]Epoch 0, Loss 1.903, LR 7.45e-06:  86%|████████▋ | 115/133 [3:33:38<33:08, 110.49s/it]Epoch 0, Loss 1.903, LR 7.45e-06:  87%|████████▋ | 116/133 [3:35:19<30:33, 107.83s/it]Epoch 0, Loss 1.992, LR 7.19e-06:  87%|████████▋ | 116/133 [3:35:19<30:33, 107.83s/it]Epoch 0, Loss 1.992, LR 7.19e-06:  88%|████████▊ | 117/133 [3:37:05<28:33, 107.11s/it]Epoch 0, Loss 2.021, LR 6.95e-06:  88%|████████▊ | 117/133 [3:37:05<28:33, 107.11s/it]Epoch 0, Loss 2.021, LR 6.95e-06:  89%|████████▊ | 118/133 [3:38:52<26:48, 107.23s/it]Epoch 0, Loss 1.991, LR 6.71e-06:  89%|████████▊ | 118/133 [3:38:52<26:48, 107.23s/it]Batch idx 482
Batch idx 483
Batch idx 484
Batch idx 485
Batch idx 486
Gradient norm: 0.1455078125
Batch idx 487
Batch idx 488
Batch idx 489
Batch idx 490
Batch idx 491
Batch idx 492
Batch idx 493
Batch idx 494
Gradient norm: 0.177734375
Batch idx 495
Batch idx 496
Batch idx 497
Batch idx 498
Batch idx 499
Batch idx 500
Batch idx 501
Batch idx 502
Gradient norm: 0.1923828125
Batch idx 503
Batch idx 504
Batch idx 505
Batch idx 506
Batch idx 507
Batch idx 508
Batch idx 509
Batch idx 510
Gradient norm: 0.12060546875
Batch idx 511
Batch idx 512
Batch idx 513
Batch idx 514
Batch idx 515
Batch idx 516
Batch idx 517
Batch idx 518
Gradient norm: 0.12890625
Batch idx 519
Batch idx 520
Batch idx 521
Batch idx 522
Batch idx 523
Batch idx 524
Batch idx 525
Batch idx 526
Gradient norm: 0.1337890625
Batch idx 527
Batch idx 528
Batch idx 529
Batch idx 530
Batch idx 531
Batch idx 532
Batch idx 533
Batch idx 534
Gradient norm: 0.1474609375
Batch idx 535
Batch idx 536
Batch idx 537
Batch idx 538
Batch idx 539
Batch idx 540
Batch idx 541
Batch idx 542
Gradient norm: 0.11572265625
Batch idx 543
Batch idx 544
Batch idx 545
Batch idx 546
Batch idx 547
Batch idx 548
Batch idx 549
Batch idx 550
Gradient norm: 0.107421875
Batch idx 551
Batch idx 552
Batch idx 553
Batch idx 554
Batch idx 555
Batch idx 556
Batch idx 557
Batch idx 558
Gradient norm: 0.1171875
Batch idx 559
Batch idx 560
Batch idx 561
Batch idx 562
Batch idx 563
Batch idx 564
Batch idx 565
Batch idx 566
Gradient norm: 0.10791015625
Batch idx 567
Batch idx 568
Batch idx 569
Batch idx 570
Batch idx 571
Batch idx 572
Batch idx 573
Batch idx 574
Gradient norm: 0.1337890625
Batch idx 575
Batch idx 576
Batch idx 577
Batch idx 578
Batch idx 579
Batch idx 580
Batch idx 581
Batch idx 582
Gradient norm: 0.09716796875
Batch idx 583
Batch idx 584
Batch idx 585
Batch idx 586
Batch idx 587
Batch idx 588
Batch idx 589
Batch idx 590
Gradient norm: 0.11669921875
Batch idx 591
Batch idx 592
Batch idx 593
Batch idx 594
Batch idx 595
Batch idx 596
Batch idx 597
Batch idx 598
Gradient norm: 0.146484375
Batch idx 599
Batch idx 600
Batch idx 601
Batch idx 602
Batch idx 603
Batch idx 604
Batch idx 605
Batch idx 606
Gradient norm: 0.1328125
Batch idx 607
Batch idx 608
Batch idx 609
Batch idx 610
Batch idx 611
Batch idx 612
Batch idx 613
Batch idx 614
Gradient norm: 0.1083984375
Batch idx 615
Batch idx 616
Batch idx 617
Batch idx 618
Batch idx 619
Batch idx 620
Batch idx 621
Batch idx 622
Gradient norm: 0.11328125
Batch idx 623
Batch idx 624
Batch idx 625
Batch idx 626
Batch idx 627
Batch idx 628
Batch idx 629
Batch idx 630
Gradient norm: 0.11181640625
Batch idx 631
Batch idx 632
Batch idx 633
Batch idx 634
Batch idx 635
Batch idx 636
Batch idx 637
Batch idx 638
Gradient norm: 0.10302734375
Batch idx 639
Batch idx 640
Batch idx 641
Batch idx 642
Batch idx 643
Batch idx 644
Batch idx 645
Batch idx 646
Gradient norm: 0.123046875
Batch idx 647
Batch idx 648
Batch idx 649
Batch idx 650
Batch idx 651
Batch idx 652
Batch idx 653
Batch idx 654
Gradient norm: 0.11279296875
Batch idx 655
Batch idx 656
Batch idx 657
Batch idx 658
Batch idx 659
Batch idx 660
Batch idx 661
Batch idx 662
Gradient norm: 0.09716796875
Batch idx 663
Batch idx 664
Batch idx 665
Batch idx 666
Batch idx 667
Batch idx 668
Batch idx 669
Batch idx 670
Gradient norm: 0.107421875
Batch idx 671
Batch idx 672
Batch idx 673
Batch idx 674
Batch idx 675
Batch idx 676
Batch idx 677
Batch idx 678
Gradient norm: 0.1279296875
Batch idx 679
Batch idx 680
Batch idx 681
Batch idx 682
Batch idx 683
Batch idx 684
Batch idx 685
Batch idx 686
Gradient norm: 0.1162109375
Batch idx 687
Batch idx 688
Batch idx 689
Batch idx 690
Batch idx 691
Batch idx 692
Batch idx 693
Batch idx 694
Gradient norm: 0.12451171875
Batch idx 695
Batch idx 696
Batch idx 697
Batch idx 698
Batch idx 699
Batch idx 700
Batch idx 701
Batch idx 702
Gradient norm: 0.11279296875
Batch idx 703
Batch idx 704
Batch idx 705
Batch idx 706
Batch idx 707
Batch idx 708
Batch idx 709
Batch idx 710
Gradient norm: 0.1708984375
Batch idx 711
Batch idx 712
Batch idx 713
Batch idx 714
Batch idx 715
Batch idx 716
Batch idx 717
Batch idx 718
Gradient norm: 0.1572265625
Batch idx 719
Batch idx 720
Batch idx 721
Batch idx 722
Batch idx 723
Batch idx 724
Batch idx 725
Batch idx 726
Gradient norm: 0.10400390625
Batch idx 727
Batch idx 728
Batch idx 729
Batch idx 730
Batch idx 731
Batch idx 732
Batch idx 733
Batch idx 734
Gradient norm: 0.09326171875
Batch idx 735
Batch idx 736
Batch idx 737
Batch idx 738
Batch idx 739
Batch idx 740
Batch idx 741
Batch idx 742
Gradient norm: 0.123046875
Batch idx 743
Batch idx 744
Batch idx 745
Batch idx 746
Batch idx 747
Batch idx 748
Batch idx 749
Batch idx 750
Gradient norm: 0.10400390625
Batch idx 751
Batch idx 752
Batch idx 753
Batch idx 754
Batch idx 755
Batch idx 756
Batch idx 757
Batch idx 758
Gradient norm: 0.0986328125
Batch idx 759
Batch idx 760
Batch idx 761
Batch idx 762
Batch idx 763
Batch idx 764
Batch idx 765
Batch idx 766
Gradient norm: 0.08447265625
Batch idx 767
Batch idx 768
Batch idx 769
Batch idx 770
Batch idx 771
Batch idx 772
Batch idx 773
Batch idx 774
Gradient norm: 0.11865234375
Batch idx 775
Batch idx 776
Batch idx 777
Batch idx 778
Batch idx 779
Batch idx 780
Batch idx 781
Batch idx 782
Gradient norm: 0.10693359375
Batch idx 783
Batch idx 784
Batch idx 785
Batch idx 786
Batch idx 787
Batch idx 788
Batch idx 789
Batch idx 790
Gradient norm: 0.103515625
Batch idx 791
Batch idx 792
Batch idx 793
Batch idx 794
Batch idx 795
Batch idx 796
Batch idx 797
Batch idx 798
Gradient norm: 0.09912109375
Batch idx 799
Batch idx 800
Batch idx 801
Batch idx 802
Batch idx 803
Batch idx 804
Batch idx 805
Batch idx 806
Gradient norm: 0.1015625
Batch idx 807
Batch idx 808
Batch idx 809
Batch idx 810
Batch idx 811
Batch idx 812
Batch idx 813
Batch idx 814
Gradient norm: 0.10595703125
Batch idx 815
Batch idx 816
Batch idx 817
Batch idx 818
Batch idx 819
Batch idx 820
Batch idx 821
Batch idx 822
Gradient norm: 0.1025390625
Batch idx 823
Batch idx 824
Batch idx 825
Batch idx 826
Batch idx 827
Batch idx 828
Batch idx 829
Batch idx 830
Gradient norm: 0.10205078125
Batch idx 831
Batch idx 832
Batch idx 833
Batch idx 834
Batch idx 835
Batch idx 836
Batch idx 837
Batch idx 838
Gradient norm: 0.1240234375
Batch idx 839
Batch idx 840
Batch idx 841
Batch idx 842
Batch idx 843
Batch idx 844
Batch idx 845
Batch idx 846
Gradient norm: 0.1328125
Batch idx 847
Batch idx 848
Batch idx 849
Batch idx 850
Batch idx 851
Batch idx 852
Batch idx 853
Batch idx 854
Gradient norm: 0.1279296875
Batch idx 855
Batch idx 856
Batch idx 857
Batch idx 858
Batch idx 859
Batch idx 860
Batch idx 861
Batch idx 862
Gradient norm: 0.10888671875
Batch idx 863
Batch idx 864
Batch idx 865
Batch idx 866
Batch idx 867
Batch idx 868
Batch idx 869
Batch idx 870
Gradient norm: 0.11669921875
Batch idx 871
Batch idx 872
Batch idx 873
Batch idx 874
Batch idx 875
Batch idx 876
Batch idx 877
Batch idx 878
Gradient norm: 0.1123046875
Batch idx 879
Batch idx 880
Batch idx 881
Batch idx 882
Batch idx 883
Batch idx 884
Batch idx 885
Batch idx 886
Gradient norm: 0.10302734375
Batch idx 887
Batch idx 888
Batch idx 889
Batch idx 890
Batch idx 891
Batch idx 892
Batch idx 893
Batch idx 894
Gradient norm: 0.095703125
Batch idx 895
Batch idx 896
Batch idx 897
Batch idx 898
Batch idx 899
Batch idx 900
Batch idx 901
Batch idx 902
Gradient norm: 0.099609375
Batch idx 903
Batch idx 904
Batch idx 905
Batch idx 906
Batch idx 907
Batch idx 908
Batch idx 909
Batch idx 910
Gradient norm: 0.12353515625
Batch idx 911
Batch idx 912
Batch idx 913
Batch idx 914
Batch idx 915
Batch idx 916
Batch idx 917
Batch idx 918
Gradient norm: 0.1103515625
Batch idx 919
Batch idx 920
Batch idx 921
Batch idx 922
Batch idx 923
Batch idx 924
Batch idx 925
Batch idx 926
Gradient norm: 0.11572265625
Batch idx 927
Batch idx 928
Batch idx 929
Batch idx 930
Batch idx 931
Batch idx 932
Batch idx 933
Batch idx 934
Gradient norm: 0.08642578125
Batch idx 935
Batch idx 936
Batch idx 937
Batch idx 938
Batch idx 939
Batch idx 940
Batch idx 941
Batch idx 942
Gradient norm: 0.1201171875
Batch idx 943
Batch idx 944
Batch idx 945
Batch idx 946
Batch idx 947
Batch idx 948
Batch idx 949
Batch idx 950
Epoch 0, Loss 1.991, LR 6.71e-06:  89%|████████▉ | 119/133 [3:40:46<25:28, 109.15s/it]Epoch 0, Loss 2.019, LR 6.49e-06:  89%|████████▉ | 119/133 [3:40:46<25:28, 109.15s/it]Epoch 0, Loss 2.019, LR 6.49e-06:  90%|█████████ | 120/133 [3:42:32<23:26, 108.19s/it]Epoch 0, Loss 1.931, LR 6.29e-06:  90%|█████████ | 120/133 [3:42:32<23:26, 108.19s/it]Epoch 0, Loss 1.931, LR 6.29e-06:  91%|█████████ | 121/133 [3:44:22<21:46, 108.89s/it]Epoch 0, Loss 1.987, LR 6.10e-06:  91%|█████████ | 121/133 [3:44:22<21:46, 108.89s/it]Epoch 0, Loss 1.987, LR 6.10e-06:  92%|█████████▏| 122/133 [3:46:08<19:46, 107.82s/it]Epoch 0, Loss 2.017, LR 5.93e-06:  92%|█████████▏| 122/133 [3:46:08<19:46, 107.82s/it]Epoch 0, Loss 2.017, LR 5.93e-06:  92%|█████████▏| 123/133 [3:47:52<17:49, 106.97s/it]Epoch 0, Loss 2.000, LR 5.77e-06:  92%|█████████▏| 123/133 [3:47:53<17:49, 106.97s/it]Epoch 0, Loss 2.000, LR 5.77e-06:  93%|█████████▎| 124/133 [3:49:53<16:40, 111.15s/it]Epoch 0, Loss 1.980, LR 5.62e-06:  93%|█████████▎| 124/133 [3:49:53<16:40, 111.15s/it]Epoch 0, Loss 1.980, LR 5.62e-06:  94%|█████████▍| 125/133 [3:51:47<14:54, 111.82s/it]Epoch 0, Loss 1.953, LR 5.49e-06:  94%|█████████▍| 125/133 [3:51:47<14:54, 111.82s/it]Epoch 0, Loss 1.953, LR 5.49e-06:  95%|█████████▍| 126/133 [3:53:34<12:53, 110.49s/it]Epoch 0, Loss 1.877, LR 5.38e-06:  95%|█████████▍| 126/133 [3:53:34<12:53, 110.49s/it]Epoch 0, Loss 1.877, LR 5.38e-06:  95%|█████████▌| 127/133 [3:55:31<11:14, 112.40s/it]Epoch 0, Loss 1.878, LR 5.28e-06:  95%|█████████▌| 127/133 [3:55:31<11:14, 112.40s/it]Epoch 0, Loss 1.878, LR 5.28e-06:  96%|█████████▌| 128/133 [3:57:24<09:23, 112.68s/it]Epoch 0, Loss 1.994, LR 5.19e-06:  96%|█████████▌| 128/133 [3:57:24<09:23, 112.68s/it]Epoch 0, Loss 1.994, LR 5.19e-06:  97%|█████████▋| 129/133 [3:59:30<07:45, 116.49s/it]Epoch 0, Loss 1.974, LR 5.12e-06:  97%|█████████▋| 129/133 [3:59:30<07:45, 116.49s/it]Epoch 0, Loss 1.974, LR 5.12e-06:  98%|█████████▊| 130/133 [4:01:25<05:48, 116.26s/it]Epoch 0, Loss 1.978, LR 5.07e-06:  98%|█████████▊| 130/133 [4:01:26<05:48, 116.26s/it]Epoch 0, Loss 1.978, LR 5.07e-06:  98%|█████████▊| 131/133 [4:03:01<03:40, 110.05s/it]Epoch 0, Loss 1.867, LR 5.03e-06:  98%|█████████▊| 131/133 [4:03:01<03:40, 110.05s/it]Epoch 0, Loss 1.867, LR 5.03e-06:  99%|█████████▉| 132/133 [4:04:40<01:46, 106.73s/it]Epoch 0, Loss 1.958, LR 5.01e-06:  99%|█████████▉| 132/133 [4:04:40<01:46, 106.73s/it]Epoch 0, Loss 1.958, LR 5.01e-06: 100%|██████████| 133/133 [4:06:38<00:00, 110.26s/it]Epoch 0, Loss 1.909, LR 5.00e-06: 100%|██████████| 133/133 [4:06:39<00:00, 110.26s/it]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.091 MB uploadedwandb: / 0.011 MB of 0.097 MB uploadedwandb: - 0.097 MB of 0.097 MB uploadedwandb: \ 0.097 MB of 0.097 MB uploadedwandb: | 0.097 MB of 0.097 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                            grad_norm ████▆▅▄▄▃▂▂▂▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                 loss ▇▇█▇▅▅▄▅▄▄▃▃▃▃▂▃▂▂▂▃▃▁▃▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:                                   lr ▁▃▅▇██████▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:      memory/allocated_after_backward ▁
wandb:       memory/allocated_after_forward ▁
wandb: memory/allocated_after_model_created ▁
wandb:    memory/allocated_after_model_wrap ▁
wandb:      memory/allocated_before_forward ▁
wandb:                memory/allocated_peak ▁
wandb:       memory/reserved_after_backward ▁
wandb:        memory/reserved_after_forward ▁
wandb: memory/reserved_after_model_creation ▁
wandb:     memory/reserved_after_model_wrap ▁
wandb:       memory/reserved_before_forward ▁
wandb:                 memory/reserved_peak ▁
wandb:                           time_taken ▁
wandb: 
wandb: Run summary:
wandb:                            grad_norm 0.10352
wandb:                                 loss 1.90889
wandb:                                   lr 1e-05
wandb:      memory/allocated_after_backward 625275392
wandb:       memory/allocated_after_forward 3072801792
wandb: memory/allocated_after_model_created 8652800
wandb:    memory/allocated_after_model_wrap 111200768
wandb:      memory/allocated_before_forward 111201792
wandb:                memory/allocated_peak 7262825984
wandb:       memory/reserved_after_backward 9269411840
wandb:        memory/reserved_after_forward 9116319744
wandb: memory/reserved_after_model_creation 115343360
wandb:     memory/reserved_after_model_wrap 6259998720
wandb:       memory/reserved_before_forward 6259998720
wandb:                 memory/reserved_peak 9344909312
wandb:                           time_taken 14896.251
wandb: 
wandb: 🚀 View run bumbling-wind-68 at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/qlkxvgbj
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240830_110515-qlkxvgbj/logs
Rank 3: Model created: 0.105 GiB
Wrapping model w/ FSDP 3
Rank 3: Wrapped model: 5.830 GiB
Applying activation checkpointing 3
Rank 3: Before forward: 5.83 GiB
Rank 3: After forward: 8.49 GiB
Rank 3: After backward: 8.64 GiB
Rank 3: Peak allocated memory: 6.76 GiB
Rank 3: Peak reserved memory:  8.70 GiB
Rank 1: Model created: 0.105 GiB
Wrapping model w/ FSDP 1
Rank 1: Wrapped model: 5.830 GiB
Applying activation checkpointing 1
Rank 1: Before forward: 5.83 GiB
Rank 1: After forward: 8.49 GiB
Rank 1: After backward: 8.63 GiB
Rank 1: Peak allocated memory: 6.76 GiB
Rank 1: Peak reserved memory:  8.70 GiB
Rank 2: Model created: 0.105 GiB
Wrapping model w/ FSDP 2
Rank 2: Wrapped model: 5.830 GiB
Applying activation checkpointing 2
Rank 2: Before forward: 5.83 GiB
Rank 2: After forward: 8.49 GiB
Rank 2: After backward: 8.63 GiB
Rank 2: Peak allocated memory: 6.76 GiB
Rank 2: Peak reserved memory:  8.70 GiB
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (qlkxvgbj) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
Epoch 0, Loss 1.909, LR 5.00e-06: 100%|██████████| 133/133 [4:08:26<00:00, 112.08s/it]
Gradient norm: 0.1064453125
Batch idx 951
Batch idx 952
Batch idx 953
Batch idx 954
Batch idx 955
Batch idx 956
Batch idx 957
Batch idx 958
Gradient norm: 0.10546875
Batch idx 959
Batch idx 960
Batch idx 961
Batch idx 962
Batch idx 963
Batch idx 964
Batch idx 965
Batch idx 966
Gradient norm: 0.1103515625
Batch idx 967
Batch idx 968
Batch idx 969
Batch idx 970
Batch idx 971
Batch idx 972
Batch idx 973
Batch idx 974
Gradient norm: 0.11279296875
Batch idx 975
Batch idx 976
Batch idx 977
Batch idx 978
Batch idx 979
Batch idx 980
Batch idx 981
Batch idx 982
Gradient norm: 0.134765625
Batch idx 983
Batch idx 984
Batch idx 985
Batch idx 986
Batch idx 987
Batch idx 988
Batch idx 989
Batch idx 990
Gradient norm: 0.11279296875
Batch idx 991
Batch idx 992
Batch idx 993
Batch idx 994
Batch idx 995
Batch idx 996
Batch idx 997
Batch idx 998
Gradient norm: 0.10009765625
Batch idx 999
Batch idx 1000
Batch idx 1001
Batch idx 1002
Batch idx 1003
Batch idx 1004
Batch idx 1005
Batch idx 1006
Gradient norm: 0.12060546875
Batch idx 1007
Batch idx 1008
Batch idx 1009
Batch idx 1010
Batch idx 1011
Batch idx 1012
Batch idx 1013
Batch idx 1014
Gradient norm: 0.1025390625
Batch idx 1015
Batch idx 1016
Batch idx 1017
Batch idx 1018
Batch idx 1019
Batch idx 1020
Batch idx 1021
Batch idx 1022
Gradient norm: 0.10791015625
Batch idx 1023
Batch idx 1024
Batch idx 1025
Batch idx 1026
Batch idx 1027
Batch idx 1028
Batch idx 1029
Batch idx 1030
Gradient norm: 0.1259765625
Batch idx 1031
Batch idx 1032
Batch idx 1033
Batch idx 1034
Batch idx 1035
Batch idx 1036
Batch idx 1037
Batch idx 1038
Gradient norm: 0.1240234375
Batch idx 1039
Batch idx 1040
Batch idx 1041
Batch idx 1042
Batch idx 1043
Batch idx 1044
Batch idx 1045
Batch idx 1046
Gradient norm: 0.1435546875
Batch idx 1047
Batch idx 1048
Batch idx 1049
Batch idx 1050
Batch idx 1051
Batch idx 1052
Batch idx 1053
Batch idx 1054
Gradient norm: 0.10986328125
Batch idx 1055
Batch idx 1056
Batch idx 1057
Batch idx 1058
Batch idx 1059
Batch idx 1060
Batch idx 1061
Batch idx 1062
Gradient norm: 0.103515625
Batch idx 1063
Batch idx 1064
Batch idx 1065
Batch idx 1066
Batch idx 1067
Batch idx 1068
Batch idx 1069
Saving full model weights.
Done 0
Finished training 0
CUDA event elapsed time: 14896.251 sec
Rank 0: Before forward: 5.83 GiB
Rank 0: After forward: 8.49 GiB
Rank 0: After backward: 8.63 GiB
Rank 0: Peak allocated memory: 6.76 GiB
Rank 0: Peak reserved memory:  8.70 GiB
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.76s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.80s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.79s/it]
name : base_model.model.model.embed_tokens.weight | dtype : torch.bfloat16 | shape : torch.Size([32768, 4096]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.0.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.1.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.2.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.3.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.4.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.5.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.6.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.7.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.8.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.9.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.10.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.11.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.12.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.13.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.14.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.15.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.16.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.17.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.18.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.19.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.20.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.21.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.22.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.23.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.24.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.25.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.26.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.27.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.28.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.29.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.30.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


Generating summaries: 0it [00:00, ?it/s]name : base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.31.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.norm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.lm_head.weight | dtype : torch.bfloat16 | shape : torch.Size([32768, 4096]) | requires_grad : False 


All tensors are equal
---------------------------------------------------
Dataset size : 554
example of the dataset
input_ids : tensor([[    1,     3, 12786,  ..., 29491, 29473,     4]])
prompt : <s>[INST] Write a summary of the source text. The summary should be normal in extractiveness. Extractiveness is defined by the degree of exact copying from the source text. The source text is given below.  (CNN)Jackson Gordon is no ordinary 21-year-old. By day he is an industrial design student at Philadelphia University, but Gordon has another side to him -- a side altogether darker, tougher and more enigmatic. Hanging in his workshop Gordon has a full suit of armor plating, cape and cowl -- matte black and built to stop a knife. Gordon has an alter ego: the Dark Knight himself, Batman. You might expect his origin story to be cloaked in mystery, but speaking to CNN Gordon is quick to explain how the transformation took place. Gordon says his calling came five years ago when he began experimenting with cosplay. "Previously I'd been involved with costume making... I'd made a version of the Batsuit from Christopher Nolan's 'Dark Knight Trilogy' and I really liked that suit," Gordon says. But, as elaborate as his design was, it lacked the functionality or the authenticity of the genuine article. "I was frustrated every time I wore it," Gordon explains. "It really limited my mobility and I didn't like that -- it didn't go with the character." In September 2014 he bit the bullet, deciding "to do another one that wouldn't inhibit my mobility and would actually provide protection and function more like Batman's actual suit." The Batsuit had to be strong -- tough enough to withstand the stab or slash of a knife, the impact of a punch or a baseball bat, but light and articulate enough to make it practical. Striking such a balance required expensive materials, and they didn't come cheap. Gordon therefore fired up a Kickstarter campaign. He "didn't really think anyone would fund it or even be interested in it" -- he raised $1,255 in 6 days. "It was a little surprising," Gordon demurs. Writing out his shopping list, it was important that "everywhere, even places without armor plating, had some sort of protection." Kevlar was sourced as the base fabric, making it "cut and slash resistant to bladed weapons, but breathable and wearable all day." Eschewing conventional materials, Gordon opted for a form of memory foam, built around key areas to "squish and compress," dissipating the impact of blows. After much experimenting with "polycarbonates and extruded PVC materials," ¼" Kydex (or ABS) plastic formed the tough armor plates, located on the torso, forearms and shins. Stab resistant, Gordon says "it can take anything but a gunshot." The cowl was more problematic, being "nearly impossible" to craft out of the same materials within the limits of his workshop. Gordon therefore took a mold of his head using Sintra plastic, "working on top of that with different sculpting clays and soft plastics to get it into a recognizable Batman shape." Using a two part box mold Gordon was able to create a "silicone jacket" of this, into which liquid polyurethane was poured, forming the final, "durable and functional" cowl. Gordon (who doesn't appear to be related to Gotham City's police commissioner, James Gordon) is also an expert in Shaolin Kung Fu: he is both brains and brawn, a cross between Bruce Wayne and Batsuit designer Lucius Fox from Nolan's Batman trilogy. Legendary, the production company behind the films, has taken note of his design and given it their seal of approval. The Batsuit has made appearances at conventions and proved a showstopper among his fellow students and the faculty. "People love the theatricality of it," its designer says. That the product so closely mimics DC's fantastical comic book creation has had resonance. He has already begun manufacturing the cowls for the public, with "fully adjustable" jackets going up for sale on his site Armatus Design "in the next couple of weeks." The jackets have received particular attention. Gordon has received "easily over 50 requests from people," and not just from the cosplay community. "They range from recreational use to martial artists... but also motorcycle and All Terrain Vehicle riders who want protective gear and prefer the look and functionality of [Gordon's] suit." Perhaps because of their versatility and the small matter of copyright issues, those that go on sale will not feature the iconic bat symbol. Gordon says his fledgling business will remain small whilst he's at University -- he has to finish he studies after all, and won't be using the project towards his degree credits. For now the Batsuit and Armatus Design will remain a one man operation: such is the life of a superhero. [/INST]
reference : Jackson Gordon, a 21-year-old industrial design student at Philadelphia University built a Batsuit that is resistant to stabs, knife slashes, and high impacts. According to Gordon, this is a second attempt at building the suit after an earlier attempt five years ago.
---------------------------------------------------
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Generating summaries: 1it [00:23, 23.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 2it [00:55, 28.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 3it [01:25, 29.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 4it [01:46, 25.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 5it [02:16, 27.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 6it [02:49, 29.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 7it [03:21, 30.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 8it [03:50, 29.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 9it [04:20, 29.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 10it [04:53, 30.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 11it [05:26, 31.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 12it [06:00, 32.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 13it [06:33, 32.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 14it [07:06, 32.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 15it [07:34, 31.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 16it [08:07, 31.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 17it [08:38, 31.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 18it [09:11, 32.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 19it [09:33, 29.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 20it [10:06, 30.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 21it [10:29, 28.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 22it [11:01, 29.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 23it [11:31, 29.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 24it [12:02, 29.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 25it [12:33, 30.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 26it [13:01, 29.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 27it [13:24, 27.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 28it [13:48, 26.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 29it [14:08, 24.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 30it [14:30, 23.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 31it [14:49, 22.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 32it [15:13, 22.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 33it [15:45, 25.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 34it [16:13, 26.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 35it [16:45, 27.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 36it [17:17, 29.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 37it [17:50, 30.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 38it [18:22, 31.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 39it [18:55, 31.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 40it [19:28, 31.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 41it [20:01, 32.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 42it [20:30, 31.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 43it [21:03, 31.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 44it [21:30, 30.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 45it [22:01, 30.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 46it [22:33, 30.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 47it [23:05, 31.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 48it [23:37, 31.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 49it [24:08, 31.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 50it [24:44, 32.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 51it [25:19, 33.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 52it [25:46, 31.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 53it [26:16, 30.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 54it [26:45, 30.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 55it [27:05, 27.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 56it [27:28, 25.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 57it [28:04, 28.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 58it [28:40, 31.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 59it [29:09, 30.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 60it [29:29, 27.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 61it [29:54, 26.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 62it [30:25, 27.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 63it [31:00, 29.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 64it [31:22, 27.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 65it [31:47, 26.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 66it [32:20, 28.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 67it [32:46, 27.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 68it [33:16, 28.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 69it [33:48, 29.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 70it [34:16, 28.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 71it [34:45, 29.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 72it [35:10, 28.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 73it [35:44, 29.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 74it [36:17, 30.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 75it [36:43, 29.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 76it [37:14, 29.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 77it [37:42, 29.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 78it [38:10, 28.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 79it [38:40, 29.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 80it [39:15, 30.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 81it [39:49, 31.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 82it [40:24, 32.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 83it [40:51, 31.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 84it [41:10, 27.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 85it [41:32, 25.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 86it [41:54, 24.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 87it [42:14, 23.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 88it [42:32, 21.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 89it [42:54, 21.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 90it [43:13, 20.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 91it [43:32, 20.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 92it [44:01, 22.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 93it [44:31, 25.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 94it [45:03, 27.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 95it [45:32, 27.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 96it [45:59, 27.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 97it [46:24, 26.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 98it [46:52, 26.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 99it [47:26, 29.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 100it [47:56, 29.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 101it [48:20, 27.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 102it [48:43, 26.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 103it [49:10, 26.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 104it [49:38, 26.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 105it [50:01, 25.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 106it [50:26, 25.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 107it [50:50, 25.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 108it [51:17, 25.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 109it [51:41, 25.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 110it [52:08, 25.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 111it [52:38, 27.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 112it [53:09, 28.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 113it [53:40, 29.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 114it [54:12, 29.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 115it [54:43, 30.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 116it [55:15, 30.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 117it [55:48, 31.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 118it [56:19, 31.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 119it [56:43, 29.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 120it [57:15, 30.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 121it [57:47, 30.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 122it [58:18, 30.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 123it [58:50, 31.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 124it [59:22, 31.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 125it [59:53, 31.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 126it [1:00:26, 31.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 127it [1:01:00, 32.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 128it [1:01:31, 31.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 129it [1:01:58, 30.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 130it [1:02:31, 31.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 131it [1:03:04, 31.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 132it [1:03:38, 32.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 133it [1:04:00, 29.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 134it [1:04:20, 26.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 135it [1:04:39, 24.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 136it [1:04:56, 22.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 137it [1:05:14, 20.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 138it [1:05:46, 24.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 139it [1:06:18, 26.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 140it [1:06:47, 27.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 141it [1:07:16, 27.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 142it [1:07:48, 28.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 143it [1:08:13, 27.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 144it [1:08:37, 26.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 145it [1:09:02, 26.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 146it [1:09:18, 23.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 147it [1:09:47, 24.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 148it [1:10:20, 27.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 149it [1:10:52, 28.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 150it [1:11:25, 29.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 151it [1:11:57, 30.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 152it [1:12:24, 29.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 153it [1:12:57, 30.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 154it [1:13:28, 30.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 155it [1:14:00, 31.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 156it [1:14:31, 31.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 157it [1:15:03, 31.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 158it [1:15:26, 28.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 159it [1:15:54, 28.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 160it [1:16:28, 30.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 161it [1:16:59, 30.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 162it [1:17:27, 29.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 163it [1:17:57, 29.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 164it [1:18:25, 29.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 165it [1:18:54, 29.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 166it [1:19:17, 27.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 167it [1:19:46, 27.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 168it [1:20:17, 28.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 169it [1:20:44, 28.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 170it [1:21:14, 28.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 171it [1:21:43, 28.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 172it [1:22:14, 29.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 173it [1:22:45, 30.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 174it [1:23:05, 27.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 175it [1:23:27, 25.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 176it [1:23:58, 27.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 177it [1:24:20, 25.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 178it [1:24:46, 25.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 179it [1:25:16, 26.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 180it [1:25:46, 27.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 181it [1:26:18, 29.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 182it [1:26:51, 30.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 183it [1:27:22, 30.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 184it [1:27:55, 31.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 185it [1:28:24, 30.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 186it [1:28:47, 28.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 187it [1:29:18, 29.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 188it [1:29:44, 28.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 189it [1:30:15, 28.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 190it [1:30:39, 27.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 191it [1:31:05, 26.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 192it [1:31:22, 24.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 193it [1:31:46, 24.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 194it [1:32:14, 25.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 195it [1:32:38, 24.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 196it [1:32:58, 23.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 197it [1:33:16, 21.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 198it [1:33:35, 21.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 199it [1:33:54, 20.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 200it [1:34:12, 19.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 201it [1:34:43, 23.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 202it [1:35:15, 25.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 203it [1:35:47, 27.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 204it [1:36:19, 28.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 205it [1:36:50, 29.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 206it [1:37:19, 29.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 207it [1:37:52, 30.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 208it [1:38:24, 30.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 209it [1:38:48, 28.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 210it [1:39:21, 30.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 211it [1:39:53, 30.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 212it [1:40:15, 28.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 213it [1:40:29, 23.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 214it [1:40:44, 21.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 215it [1:40:59, 19.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 216it [1:41:33, 23.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 217it [1:42:06, 26.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 218it [1:42:39, 28.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 219it [1:43:13, 30.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 220it [1:43:46, 31.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 221it [1:44:16, 30.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 222it [1:44:49, 31.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 223it [1:45:10, 28.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 224it [1:45:42, 29.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 225it [1:46:14, 30.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 226it [1:46:46, 30.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 227it [1:47:15, 30.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 228it [1:47:44, 29.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 229it [1:48:02, 26.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 230it [1:48:35, 28.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 231it [1:49:00, 27.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 232it [1:49:31, 28.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 233it [1:49:57, 27.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 234it [1:50:29, 28.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 235it [1:50:59, 29.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 236it [1:51:18, 26.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 237it [1:51:49, 27.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 238it [1:52:23, 29.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 239it [1:52:56, 30.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 240it [1:53:29, 31.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 241it [1:53:54, 29.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 242it [1:54:25, 29.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 243it [1:54:58, 30.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 244it [1:55:32, 31.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 245it [1:55:56, 29.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 246it [1:56:19, 27.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 247it [1:56:47, 27.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 248it [1:57:18, 28.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 249it [1:57:57, 31.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 250it [1:58:33, 33.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 251it [1:59:12, 34.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 252it [1:59:39, 32.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 253it [2:00:07, 30.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 254it [2:00:40, 31.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 255it [2:01:12, 31.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 256it [2:01:39, 30.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 257it [2:02:05, 28.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 258it [2:02:37, 29.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 259it [2:02:54, 25.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 260it [2:03:11, 23.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 261it [2:03:29, 21.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 262it [2:03:51, 21.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 263it [2:04:11, 21.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 264it [2:04:43, 24.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 265it [2:05:15, 26.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 266it [2:05:46, 28.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 267it [2:06:19, 29.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 268it [2:06:44, 28.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 269it [2:07:16, 29.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 270it [2:07:49, 30.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 271it [2:08:20, 30.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 272it [2:08:45, 29.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 273it [2:09:17, 29.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 274it [2:09:49, 30.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 275it [2:10:21, 31.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 276it [2:10:47, 29.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 277it [2:11:18, 29.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 278it [2:11:50, 30.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 279it [2:12:10, 27.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 280it [2:12:33, 25.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 281it [2:12:51, 23.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 282it [2:13:08, 21.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 283it [2:13:30, 21.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 284it [2:14:03, 25.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 285it [2:14:31, 26.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 286it [2:15:05, 28.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 287it [2:15:38, 29.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 288it [2:16:11, 30.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 289it [2:16:43, 31.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 290it [2:17:16, 31.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 291it [2:17:42, 30.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 292it [2:18:11, 29.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 293it [2:18:36, 28.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 294it [2:19:05, 28.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 295it [2:19:33, 28.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 296it [2:19:57, 27.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 297it [2:20:20, 25.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 298it [2:20:54, 28.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 299it [2:21:27, 29.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 300it [2:21:57, 29.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 301it [2:22:20, 27.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 302it [2:22:47, 27.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 303it [2:23:11, 26.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 304it [2:23:37, 26.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 305it [2:24:03, 26.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 306it [2:24:34, 27.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 307it [2:24:59, 26.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 308it [2:25:23, 26.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 309it [2:25:47, 25.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 310it [2:26:17, 26.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 311it [2:26:41, 26.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 312it [2:27:06, 25.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 313it [2:27:27, 24.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 314it [2:27:54, 24.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 315it [2:28:23, 26.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 316it [2:28:55, 28.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 317it [2:29:21, 27.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 318it [2:29:54, 29.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 319it [2:30:25, 29.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 320it [2:30:44, 26.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 321it [2:31:17, 28.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 322it [2:31:41, 27.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 323it [2:32:14, 28.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 324it [2:32:41, 28.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 325it [2:33:14, 29.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 326it [2:33:33, 26.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 327it [2:33:57, 25.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 328it [2:34:30, 27.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 329it [2:34:57, 27.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 330it [2:35:20, 26.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 331it [2:35:52, 27.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 332it [2:36:18, 27.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 333it [2:36:41, 26.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 334it [2:37:02, 24.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 335it [2:37:22, 23.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 336it [2:37:48, 23.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 337it [2:38:19, 26.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 338it [2:38:51, 27.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 339it [2:39:13, 26.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 340it [2:39:45, 28.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 341it [2:40:17, 29.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 342it [2:40:42, 27.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 343it [2:41:15, 29.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 344it [2:41:38, 27.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 345it [2:42:09, 28.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 346it [2:42:35, 27.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 347it [2:42:59, 26.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 348it [2:43:29, 27.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 349it [2:43:51, 26.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 350it [2:44:10, 23.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 351it [2:44:30, 22.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 352it [2:44:45, 20.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 353it [2:45:03, 19.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 354it [2:45:22, 19.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 355it [2:45:54, 23.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 356it [2:46:18, 23.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 357it [2:46:49, 25.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 358it [2:47:15, 25.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 359it [2:47:39, 25.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 360it [2:48:04, 25.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 361it [2:48:22, 22.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 362it [2:48:54, 25.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 363it [2:49:26, 27.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 364it [2:49:58, 28.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 365it [2:50:25, 28.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 366it [2:50:57, 29.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 367it [2:51:24, 28.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 368it [2:51:44, 26.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 369it [2:52:05, 24.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 370it [2:52:25, 23.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 371it [2:52:41, 21.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 372it [2:53:06, 22.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 373it [2:53:23, 20.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 374it [2:53:42, 20.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 375it [2:54:04, 20.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 376it [2:54:30, 22.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 377it [2:54:47, 20.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 378it [2:55:06, 20.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 379it [2:55:21, 18.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 380it [2:55:39, 18.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 381it [2:56:02, 19.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 382it [2:56:26, 21.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 383it [2:56:51, 22.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 384it [2:57:09, 21.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 385it [2:57:27, 20.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 386it [2:57:57, 22.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 387it [2:58:22, 23.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 388it [2:58:55, 26.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 389it [2:59:21, 26.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 390it [2:59:42, 24.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 391it [3:00:15, 27.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 392it [3:00:42, 27.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 393it [3:01:15, 28.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 394it [3:01:48, 30.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 395it [3:02:21, 30.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 396it [3:02:54, 31.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 397it [3:03:27, 31.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 398it [3:04:00, 32.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 399it [3:04:33, 32.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 400it [3:05:06, 32.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 401it [3:05:29, 29.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 402it [3:05:51, 27.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 403it [3:06:29, 30.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 404it [3:07:01, 31.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 405it [3:07:39, 33.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 406it [3:08:18, 34.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 407it [3:08:54, 35.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 408it [3:09:33, 36.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 409it [3:10:11, 36.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 410it [3:10:50, 37.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 411it [3:11:29, 38.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 412it [3:12:09, 38.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 413it [3:12:48, 38.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 414it [3:13:28, 39.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 415it [3:14:07, 39.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 416it [3:14:47, 39.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 417it [3:15:26, 39.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 418it [3:15:51, 35.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 419it [3:16:21, 33.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 420it [3:16:59, 34.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 421it [3:17:26, 32.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 422it [3:17:56, 31.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 423it [3:18:36, 34.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 424it [3:19:17, 36.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 425it [3:19:59, 37.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 426it [3:20:42, 39.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 427it [3:21:24, 40.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 428it [3:22:06, 40.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 429it [3:22:35, 37.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 430it [3:23:09, 36.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 431it [3:23:44, 35.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 432it [3:24:10, 33.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 433it [3:24:47, 34.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 434it [3:25:30, 36.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 435it [3:26:12, 38.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 436it [3:26:42, 35.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 437it [3:27:19, 36.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 438it [3:27:51, 34.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 439it [3:28:31, 36.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 440it [3:29:03, 35.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 441it [3:29:18, 29.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 442it [3:29:34, 25.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 443it [3:29:49, 22.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 444it [3:30:06, 20.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 445it [3:30:28, 20.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 446it [3:31:00, 24.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 447it [3:31:32, 26.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 448it [3:32:01, 27.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 449it [3:32:41, 31.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 450it [3:33:06, 29.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 451it [3:33:46, 32.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 452it [3:34:09, 29.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 453it [3:34:39, 29.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 454it [3:35:10, 30.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 455it [3:35:46, 31.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 456it [3:36:10, 29.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 457it [3:36:48, 31.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 458it [3:37:21, 32.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 459it [3:37:43, 29.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 460it [3:38:09, 28.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 461it [3:38:46, 31.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 462it [3:39:13, 29.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 463it [3:39:45, 30.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 464it [3:40:20, 31.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 465it [3:40:46, 30.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 466it [3:41:26, 33.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 467it [3:41:54, 31.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 468it [3:42:34, 33.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 469it [3:43:07, 33.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 470it [3:43:39, 33.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 471it [3:44:16, 34.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 472it [3:44:54, 35.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 473it [3:45:22, 33.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 474it [3:45:52, 32.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 475it [3:46:22, 31.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 476it [3:46:53, 31.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 477it [3:47:33, 33.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 478it [3:48:14, 36.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 479it [3:48:37, 32.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 480it [3:49:09, 32.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 481it [3:49:50, 34.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 482it [3:50:18, 32.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 483it [3:50:56, 34.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 484it [3:51:22, 31.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 485it [3:52:00, 33.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 486it [3:52:30, 32.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 487it [3:53:09, 34.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 488it [3:53:41, 33.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 489it [3:54:18, 34.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 490it [3:54:52, 34.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 491it [3:55:31, 35.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 492it [3:56:03, 34.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 493it [3:56:42, 35.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 494it [3:57:12, 34.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 495it [3:57:40, 32.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 496it [3:58:19, 34.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 497it [3:58:58, 35.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 498it [3:59:36, 36.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 499it [4:00:01, 32.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 500it [4:00:34, 33.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 501it [4:01:13, 34.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 502it [4:01:51, 35.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 503it [4:02:31, 36.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 504it [4:03:10, 37.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 505it [4:03:41, 35.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 506it [4:04:06, 32.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 507it [4:04:36, 31.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 508it [4:05:00, 29.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 509it [4:05:39, 32.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 510it [4:06:17, 34.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 511it [4:06:56, 35.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 512it [4:07:35, 36.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 513it [4:08:14, 37.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 514it [4:08:54, 38.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 515it [4:09:33, 38.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 516it [4:10:13, 38.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 517it [4:10:47, 37.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 518it [4:11:27, 38.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 519it [4:12:06, 38.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 520it [4:12:46, 38.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 521it [4:13:17, 36.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 522it [4:13:55, 36.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 523it [4:14:31, 36.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 524it [4:14:58, 33.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 525it [4:15:36, 35.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 526it [4:16:15, 36.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 527it [4:16:54, 37.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 528it [4:17:23, 34.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 529it [4:18:02, 36.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 530it [4:18:38, 35.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 531it [4:19:01, 31.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 532it [4:19:40, 34.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 533it [4:20:05, 31.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 534it [4:20:30, 29.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 535it [4:21:00, 29.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 536it [4:21:23, 27.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 537it [4:22:00, 30.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 538it [4:22:24, 28.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 539it [4:22:57, 29.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 540it [4:23:20, 27.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 541it [4:23:58, 30.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 542it [4:24:27, 30.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 543it [4:25:02, 31.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 544it [4:25:29, 30.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 545it [4:25:55, 29.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 546it [4:26:13, 25.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 547it [4:26:36, 24.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 548it [4:27:14, 28.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 549it [4:27:50, 31.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 550it [4:28:29, 33.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 551it [4:29:08, 35.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 552it [4:29:45, 35.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 553it [4:30:24, 36.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 554it [4:31:03, 37.40s/it]Generating summaries: 554it [4:31:03, 29.36s/it]
Total time taken for generating summaries : 16263.569112062454
Average time taken for generating summaries : 29.35662294596111
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]
Traceback (most recent call last):
All tensors are equal
---------------------------------------------------
Dataset size : 554
example of the dataset
  File "/home2/tathagato/summarization/MACSUM/fsdp_lora/zero_shot_inference.py", line 143, in <module>
    input_ids = dataset[0]["input_ids"]
  File "/home2/tathagato/summarization/MACSUM/fsdp_lora/dataset.py", line 125, in __getitem__
    self.tokenizer.encode(prompt, add_special_tokens = False), dtype=torch.int64
UnboundLocalError: local variable 'prompt' referenced before assignment
World size: 4
{'world_size': 4, 'train_type': 'qlora', 'llama_pro_path': None, 'batch_size': 1, 'context_length': 2048, 'gradient_accumulation_steps': 8, 'num_epochs': 1, 'dataset': 'macsum', 'macsum_path': '/home2/tathagato/summarization/MACSUM/dataset/macdoc/train_dataset.json', 'dataset_samples': -1, 'sharding_strategy': 'full_shard', 'use_gradient_checkpointing': 1, 'reentrant_checkpointing': 1, 'use_cpu_offload': 1, 'use_activation_cpu_offload': False, 'low_memory': True, 'no_sync': False, 'precision': 'bf16', 'model_name': 'akjindal53244/Llama-3.1-Storm-8B', 'save_model': 1, 'output_dir': '/scratch/tathagato/fsdp_qlora_experiments_30_August_storm_llama3.1/length', 'lora_rank': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'lora_target_modules': 'all', 'verbose': 1, 'lr': 5e-05, 'apply_gradient_clipping': 1, 'grad_norm': 1.0, 'wd': 0.1, 'profile_memory': False, 'optimizer': 'adamw', 'lr_scheduler': 'cosine', 'loading_workers': -1, 'log_to': 'wandb', 'master_addr': 'localhost', 'master_port': '12355', 'seed': 42, 'project_name': 'fsdp_qlora', 'name': None, 'group': None, 'entity': None, 'n_bits': 4, 'profile': False, 'profiling_output': 'profiles', 'overwrite_profiling_output': True, 'with_stack': False, 'with_shapes': False, 'export_trace': True, 'export_memory_timeline': False, 'wait_steps': 0, 'warmup_steps': 1, 'active_steps': 2, 'repeat': 0, 'profiling_frequency': 10, 'max_steps': -1, 'attribute': 'length'}
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
Creating model 1
Loading model 1
Creating model 2
Loading model 2
Creating model 3
Loading model 3
wandb: wandb version 0.17.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSUM/fsdp_lora/wandb/run-20240830_195728-6aib0nqw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-frog-69
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/6aib0nqw
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
dataset chosen is :  macsum
Example input_ids shape:  torch.Size([1, 652])
Example labels shape:  torch.Size([1, 652])
example input 
<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an honest and to the point assistant, please follow the instruction and answer to the point<|eot_id|><|start_header_id|>user<|end_header_id|> Write a summary of the source text. The summary should be normal in length. The length is defined in terms of number of words used in the summary. The source text is given below. 

(CNN)Fans of the late actor Paul Walker knew that watching him in "Furious 7" would be bittersweet. Even so, many moviegoers said the final scenes of the new film, which earned a record $146 million over the weekend, still packed an emotional wallop. "Not gonna lie, I shed a few tears at the end of Furious 7. The tribute to Paul Walker was very well done," one woman said Monday on Twitter. Hers was just one of a flood of messages on social media from people who said they got choked up during scenes featuring Walker, who died at 40 in a car crash in November 2013, before filming on "Furious 7" was completed. To finish Walker's scenes, the makers of the movie used body doubles, computer-generated images and even the actor's brothers. But it was the ending that really got to moviegoers. In finishing "Furious 7," the film's producers sought to retire Walker's character, Brian, while paying homage to his role in the blockbuster "Furious" action franchise. But they felt that killing him off might appear exploitative. "If they had gone down the other path, I think I would have refused to finish making this movie," director James Wan told BuzzFeed. Instead, the movie's makers chose to "retire Paul's character in the most sincere and elegant way (they) could," Wan said. Their idea was to have Brian retire from his dangerous, high-octane lifestyle out of a sense of responsibility to his growing family with girlfriend Mia, who is pregnant with their second child. A scene late in the movie shows him and Mia playing on a beach with their son while the crew looks on -- essentially saying goodbye. Then his longtime buddy Dom reminisces about their years together, leading to a montage of Walker scenes from the first six movies. The song that plays over the montage is  "See You Again," a collaboration between Wiz Khalifa and Charlie Puth. Co-star Vin Diesel shared the video for the song late Sunday on his Facebook page, where it has more than 1.5 million likes. Fans on Twitter and Facebook mostly praised the movie's ending as a fitting tribute -- and an emotionally wrenching one. "Man I don't care how tough u are or how gangsta u claim to be....the last five minutes had me choked up in the movie theater... I saw it 3 times in one day......the ending is the deepest ending I've ever seen," one man wrote on the movie's Facebook page.<|eot_id|><|start_header_id|>assistant<|end_header_id|> " The tribute to Paul Walker was very well done," said a fan after watching the Furious 7. The actor was on break from filming "Furious 7" at the time of the fiery accident which also claimed the life of the car's driver, Roger Rodas.<|eot_id|>
tensor([[128000, 128006,   9125, 128007,   1472,    527,    459,  10978,    323,
            311,    279,   1486,  18328,     11,   4587,   1833,    279,   7754,
            323,   4320,    311,    279,   1486, 128009, 128006,    882, 128007,
           9842,    264,  12399,    315,    279,   2592,   1495,     13,    578,
          12399,   1288,    387,   4725,    304,   3160,     13,    578,   3160,
            374,   4613,    304,   3878,    315,   1396,    315,   4339,   1511,
            304,    279,  12399,     13,    578,   2592,   1495,    374,   2728,
           3770,     13,   4815,   3100,   9944,      8,  76887,    315,    279,
           3389,  12360,   7043,  23074,   7020,    430,  10307,   1461,    304,
            330,     37,  28626,    220,     22,      1,   1053,    387,    293,
          29163,   4589,     13,   7570,    779,     11,   1690,   5818,   3427,
            388,   1071,    279,   1620,  16451,    315,    279,    502,   4632,
             11,    902,  15662,    264,   3335,    400,  10465,   3610,    927,
            279,   9178,     11,   2103,  19937,    459,  14604,  41926,  23085,
             13,    330,   2688,  16926,  10457,     11,    358,  25351,    264,
           2478,  24014,    520,    279,    842,    315,  93431,    220,     22,
             13,    578,  35491,    311,   7043,  23074,    574,   1633,   1664,
           2884,   1359,    832,   5333,   1071,   7159,    389,   6405,     13,
          65466,    574,   1120,    832,    315,    264,  18197,    315,   6743,
            389,   3674,   3772,    505,   1274,    889,   1071,    814,   2751,
          94743,    709,   2391,  16451,  16850,  23074,     11,    889,   8636,
            520,    220,   1272,    304,    264,   1841,  10121,    304,   6841,
            220,    679,     18,     11,   1603,  39970,    389,    330,     37,
          28626,    220,     22,      1,    574,   8308,     13,   2057,   6381,
          23074,    596,  16451,     11,    279,  29414,    315,    279,   5818,
           1511,   2547,  40396,     11,   6500,  16581,   5448,    323,   1524,
            279,  12360,    596,  20820,     13,   2030,    433,    574,    279,
          13696,    430,   2216,   2751,    311,   5818,   3427,    388,     13,
            763,  25270,    330,     37,  28626,    220,     22,   1359,    279,
           4632,    596,  24190,  16495,    311,  16177,  23074,    596,   3752,
             11,  17520,     11,   1418,  12798,  68089,    311,    813,   3560,
            304,    279,  72097,    330,     37,  28626,      1,   1957,  19562,
             13,   2030,    814,   6612,    430,  13419,   1461,   1022,   2643,
           5101,   7684,  22018,     13,    330,   2746,    814,   1047,   8208,
           1523,    279,   1023,   1853,     11,    358,   1781,    358,   1053,
            617,  16436,    311,   6381,   3339,    420,   5818,   1359,   7690,
           7957,  72118,   3309,  72922,     13,  12361,     11,    279,   5818,
            596,  29414,  14896,    311,    330,   2171,    556,   7043,    596,
           3752,    304,    279,   1455,  49424,    323,  26861,   1648,    320,
          20670,      8,   1436,   1359,  72118,   1071,     13,  11205,   4623,
            574,    311,    617,  17520,  16177,    505,    813,  11660,     11,
           1579,  16405,    302,   2194,  19433,    704,    315,    264,   5647,
            315,  12014,    311,    813,   7982,   3070,    449,  23601,  61697,
             11,    889,    374,  20895,    449,    872,   2132,   1716,     13,
            362,   6237,   3389,    304,    279,   5818,   5039,   1461,    323,
          61697,   5737,    389,    264,  11573,    449,    872,   4538,   1418,
            279,  13941,   5992,    389,   1198,  16168,   5605,  47555,     13,
           5112,    813,  36504,  37772,  21414,  49679,   1634,    922,    872,
           1667,   3871,     11,   6522,    311,    264,  97044,    315,  23074,
          16451,    505,    279,   1176,   4848,   9698,     13,    578,   5609,
            430,  11335,    927,    279,  97044,    374,    220,    330,  10031,
           1472,  14077,   1359,    264,  20632,   1990,    468,    450,  52709,
          34918,    323,  25972,    393,    952,     13,   3623,  21337,  21016,
          54894,   6222,    279,   2835,    369,    279,   5609,   3389,   7418,
            389,    813,   5690,   2199,     11,   1405,    433,    706,    810,
           1109,    220,     16,     13,     20,   3610,  13452,     13,  42896,
            389,   6405,    323,   5690,  10213,  37475,    279,   5818,    596,
          13696,    439,    264,  27442,  35491,   1198,    323,    459,  38683,
          60588,    287,    832,     13,    330,   1692,    358,   1541,    956,
           2512,   1268,  11292,    577,    527,    477,   1268,  13481,  21127,
            577,   3802,    311,    387,    662,   2564,   1820,   1566,   4330,
           4520,   1047,    757,  94743,    709,    304,    279,   5818,  27803,
           2564,    358,   5602,    433,    220,     18,   3115,    304,    832,
           1938,   2564,   2564,   1820,  13696,    374,    279,  51621,  13696,
            358,   3077,   3596,   3970,   1359,    832,    893,   6267,    389,
            279,   5818,    596,   5690,   2199,     13, 128009, 128006,  78191,
         128007,    330,    578,  35491,    311,   7043,  23074,    574,   1633,
           1664,   2884,   1359,   1071,    264,   8571,   1306,  10307,    279,
          93431,    220,     22,     13,    578,  12360,    574,    389,   1464,
            505,  39970,    330,     37,  28626,    220,     22,      1,    520,
            279,    892,    315,    279,  64942,  11677,    902,   1101,  11922,
            279,   2324,    315,    279,   1841,    596,   5696,     11,  29607,
          13611,    300,     13, 128009]])
Creating model 0
Loading model 0
Total model params: 8030261248
Using n_workers: 10 for loading
Loading & Quantizing Model Shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading & Quantizing Model Shards:  25%|██▌       | 1/4 [00:09<00:27,  9.23s/it]Loading & Quantizing Model Shards:  50%|█████     | 2/4 [00:19<00:19,  9.68s/it]Loading & Quantizing Model Shards:  75%|███████▌  | 3/4 [00:29<00:09,  9.82s/it]Loading & Quantizing Model Shards: 100%|██████████| 4/4 [00:37<00:00,  9.34s/it]Loading & Quantizing Model Shards: 100%|██████████| 4/4 [00:37<00:00,  9.45s/it]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:402: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:403: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:404: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Loaded model weights in 37.818 seconds
Rank 0: Model created: 0.107 GiB
trainable params: 75,497,472 || all params: 8,105,758,720 || trainable%: 0.9314053700330226
Wrapping model w/ FSDP 0
Rank 0: Wrapped model: 5.830 GiB
Applying activation checkpointing 0
Config:
LlamaConfig {
  "_name_or_path": "akjindal53244/Llama-3.1-Storm-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": false,
  "vocab_size": 128256
}

Model:
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): LlamaDecoderLayer(
                  (self_attn): LlamaSdpaAttention(
                    (q_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (k_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (v_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): LlamaRotaryEmbedding()
                  )
                  (mlp): LlamaMLP(
                    (gate_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (up_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (down_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=14336, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (act_fn): SiLU()
                  )
                  (input_layernorm): LlamaRMSNorm()
                  (post_attention_layernorm): LlamaRMSNorm()
                )
              )
            )
          )
          (norm): LlamaRMSNorm()
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
      )
    )
  )
)
Starting training
Optimizer params:
Shape: torch.Size([262669312]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Total Training Steps: 133
  0%|          | 0/133 [00:00<?, ?it/s]/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:858: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Epoch 0, Loss 0.000:   0%|          | 0/133 [00:00<?, ?it/s]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Epoch 0, Loss 0.000:   1%|          | 1/133 [01:36<3:32:20, 96.52s/it]Epoch 0, Loss 2.731, LR 3.85e-06:   1%|          | 1/133 [01:36<3:32:20, 96.52s/it]Epoch 0, Loss 2.731, LR 3.85e-06:   2%|▏         | 2/133 [03:35<3:59:36, 109.74s/it]Epoch 0, Loss 2.689, LR 7.69e-06:   2%|▏         | 2/133 [03:35<3:59:36, 109.74s/it]Epoch 0, Loss 2.689, LR 7.69e-06:   2%|▏         | 3/133 [05:36<4:08:44, 114.81s/it]Epoch 0, Loss 2.615, LR 1.15e-05:   2%|▏         | 3/133 [05:36<4:08:44, 114.81s/it]Epoch 0, Loss 2.615, LR 1.15e-05:   3%|▎         | 4/133 [07:26<4:03:08, 113.09s/it]Epoch 0, Loss 2.678, LR 1.54e-05:   3%|▎         | 4/133 [07:26<4:03:08, 113.09s/it]Epoch 0, Loss 2.678, LR 1.54e-05:   4%|▍         | 5/133 [09:13<3:56:40, 110.94s/it]Epoch 0, Loss 2.643, LR 1.92e-05:   4%|▍         | 5/133 [09:14<3:56:40, 110.94s/it]Epoch 0, Loss 2.643, LR 1.92e-05:   5%|▍         | 6/133 [11:08<3:57:19, 112.12s/it]Epoch 0, Loss 2.595, LR 2.31e-05:   5%|▍         | 6/133 [11:08<3:57:19, 112.12s/it]Epoch 0, Loss 2.595, LR 2.31e-05:   5%|▌         | 7/133 [13:04<3:58:12, 113.43s/it]Epoch 0, Loss 2.590, LR 2.69e-05:   5%|▌         | 7/133 [13:04<3:58:12, 113.43s/it]Epoch 0, Loss 2.590, LR 2.69e-05:   6%|▌         | 8/133 [14:50<3:51:20, 111.04s/it]Epoch 0, Loss 2.762, LR 3.08e-05:   6%|▌         | 8/133 [14:50<3:51:20, 111.04s/it]Epoch 0, Loss 2.762, LR 3.08e-05:   7%|▋         | 9/133 [16:39<3:48:29, 110.56s/it]Epoch 0, Loss 2.619, LR 3.46e-05:   7%|▋         | 9/133 [16:40<3:48:29, 110.56s/it]Epoch 0, Loss 2.619, LR 3.46e-05:   8%|▊         | 10/133 [18:33<3:48:34, 111.50s/it]Epoch 0, Loss 2.615, LR 3.85e-05:   8%|▊         | 10/133 [18:33<3:48:34, 111.50s/it]Epoch 0, Loss 2.615, LR 3.85e-05:   8%|▊         | 11/133 [20:17<3:42:18, 109.33s/it]Epoch 0, Loss 2.597, LR 4.23e-05:   8%|▊         | 11/133 [20:18<3:42:18, 109.33s/it]Epoch 0, Loss 2.597, LR 4.23e-05:   9%|▉         | 12/133 [22:01<3:36:47, 107.50s/it]Epoch 0, Loss 2.584, LR 4.62e-05:   9%|▉         | 12/133 [22:01<3:36:47, 107.50s/it]Epoch 0, Loss 2.584, LR 4.62e-05:  10%|▉         | 13/133 [23:56<3:39:53, 109.95s/it]Epoch 0, Loss 2.494, LR 5.00e-05:  10%|▉         | 13/133 [23:56<3:39:53, 109.95s/it]Epoch 0, Loss 2.494, LR 5.00e-05:  11%|█         | 14/133 [25:53<3:42:16, 112.07s/it]Epoch 0, Loss 2.415, LR 5.00e-05:  11%|█         | 14/133 [25:53<3:42:16, 112.07s/it]Epoch 0, Loss 2.415, LR 5.00e-05:  11%|█▏        | 15/133 [27:42<3:38:23, 111.05s/it]Epoch 0, Loss 2.480, LR 5.00e-05:  11%|█▏        | 15/133 [27:42<3:38:23, 111.05s/it]Epoch 0, Loss 2.480, LR 5.00e-05:  12%|█▏        | 16/133 [29:32<3:36:13, 110.88s/it]Epoch 0, Loss 2.468, LR 4.99e-05:  12%|█▏        | 16/133 [29:33<3:36:13, 110.88s/it]Epoch 0, Loss 2.468, LR 4.99e-05:  13%|█▎        | 17/133 [31:18<3:31:11, 109.23s/it]Epoch 0, Loss 2.473, LR 4.99e-05:  13%|█▎        | 17/133 [31:18<3:31:11, 109.23s/it]Epoch 0, Loss 2.473, LR 4.99e-05:  14%|█▎        | 18/133 [33:12<3:31:58, 110.60s/it]Epoch 0, Loss 2.389, LR 4.98e-05:  14%|█▎        | 18/133 [33:12<3:31:58, 110.60s/it]Epoch 0, Loss 2.389, LR 4.98e-05:  14%|█▍        | 19/133 [34:58<3:27:29, 109.21s/it]Epoch 0, Loss 2.473, LR 4.97e-05:  14%|█▍        | 19/133 [34:58<3:27:29, 109.21s/it]Epoch 0, Loss 2.473, LR 4.97e-05:  15%|█▌        | 20/133 [36:41<3:22:27, 107.50s/it]Epoch 0, Loss 2.425, LR 4.96e-05:  15%|█▌        | 20/133 [36:41<3:22:27, 107.50s/it]Epoch 0, Loss 2.425, LR 4.96e-05:  16%|█▌        | 21/133 [38:35<3:24:10, 109.38s/it]Epoch 0, Loss 2.336, LR 4.95e-05:  16%|█▌        | 21/133 [38:35<3:24:10, 109.38s/it]Epoch 0, Loss 2.336, LR 4.95e-05:  17%|█▋        | 22/133 [40:34<3:27:32, 112.18s/it]Epoch 0, Loss 2.252, LR 4.94e-05:  17%|█▋        | 22/133 [40:34<3:27:32, 112.18s/it]Epoch 0, Loss 2.252, LR 4.94e-05:  17%|█▋        | 23/133 [42:39<3:33:07, 116.25s/it]Epoch 0, Loss 2.313, LR 4.92e-05:  17%|█▋        | 23/133 [42:39<3:33:07, 116.25s/it]Epoch 0, Loss 2.313, LR 4.92e-05:  18%|█▊        | 24/133 [44:32<3:29:06, 115.11s/it]Epoch 0, Loss 2.308, LR 4.91e-05:  18%|█▊        | 24/133 [44:32<3:29:06, 115.11s/it]Epoch 0, Loss 2.308, LR 4.91e-05:  19%|█▉        | 25/133 [46:27<3:27:27, 115.26s/it]Epoch 0, Loss 2.245, LR 4.89e-05:  19%|█▉        | 25/133 [46:28<3:27:27, 115.26s/it]Epoch 0, Loss 2.245, LR 4.89e-05:  20%|█▉        | 26/133 [48:24<3:26:08, 115.60s/it]Epoch 0, Loss 2.210, LR 4.87e-05:  20%|█▉        | 26/133 [48:24<3:26:08, 115.60s/it]Epoch 0, Loss 2.210, LR 4.87e-05:  20%|██        | 27/133 [50:21<3:25:10, 116.14s/it]Epoch 0, Loss 2.179, LR 4.85e-05:  20%|██        | 27/133 [50:21<3:25:10, 116.14s/it]Epoch 0, Loss 2.179, LR 4.85e-05:  21%|██        | 28/133 [52:11<3:19:43, 114.13s/it]Epoch 0, Loss 2.230, LR 4.83e-05:  21%|██        | 28/133 [52:11<3:19:43, 114.13s/it]Epoch 0, Loss 2.230, LR 4.83e-05:  22%|██▏       | 29/133 [53:52<3:11:04, 110.23s/it]Epoch 0, Loss 2.182, LR 4.81e-05:  22%|██▏       | 29/133 [53:52<3:11:04, 110.23s/it]Epoch 0, Loss 2.182, LR 4.81e-05:  23%|██▎       | 30/133 [55:38<3:07:03, 108.96s/it]Epoch 0, Loss 2.119, LR 4.78e-05:  23%|██▎       | 30/133 [55:38<3:07:03, 108.96s/it]Epoch 0, Loss 2.119, LR 4.78e-05:  23%|██▎       | 31/133 [57:42<3:12:54, 113.48s/it]Epoch 0, Loss 2.126, LR 4.75e-05:  23%|██▎       | 31/133 [57:42<3:12:54, 113.48s/it]Epoch 0, Loss 2.126, LR 4.75e-05:  24%|██▍       | 32/133 [59:17<3:01:48, 108.01s/it]Epoch 0, Loss 2.237, LR 4.73e-05:  24%|██▍       | 32/133 [59:17<3:01:48, 108.01s/it]Epoch 0, Loss 2.237, LR 4.73e-05:  25%|██▍       | 33/133 [1:01:12<3:03:21, 110.02s/it]Epoch 0, Loss 2.161, LR 4.70e-05:  25%|██▍       | 33/133 [1:01:12<3:03:21, 110.02s/it]Epoch 0, Loss 2.161, LR 4.70e-05:  26%|██▌       | 34/133 [1:03:07<3:04:14, 111.66s/it]Epoch 0, Loss 2.103, LR 4.67e-05:  26%|██▌       | 34/133 [1:03:07<3:04:14, 111.66s/it]Epoch 0, Loss 2.103, LR 4.67e-05:  26%|██▋       | 35/133 [1:04:59<3:02:28, 111.72s/it]Epoch 0, Loss 2.185, LR 4.64e-05:  26%|██▋       | 35/133 [1:04:59<3:02:28, 111.72s/it]Epoch 0, Loss 2.185, LR 4.64e-05:  27%|██▋       | 36/133 [1:06:48<2:59:22, 110.95s/it]Epoch 0, Loss 2.063, LR 4.60e-05:  27%|██▋       | 36/133 [1:06:48<2:59:22, 110.95s/it]Epoch 0, Loss 2.063, LR 4.60e-05:  28%|██▊       | 37/133 [1:08:37<2:56:19, 110.20s/it]Epoch 0, Loss 2.129, LR 4.57e-05:  28%|██▊       | 37/133 [1:08:37<2:56:19, 110.20s/it]Epoch 0, Loss 2.129, LR 4.57e-05:  29%|██▊       | 38/133 [1:10:36<2:58:36, 112.81s/it]Epoch 0, Loss 2.063, LR 4.54e-05:  29%|██▊       | 38/133 [1:10:36<2:58:36, 112.81s/it]Epoch 0, Loss 2.063, LR 4.54e-05:  29%|██▉       | 39/133 [1:12:28<2:56:33, 112.70s/it]Epoch 0, Loss 2.175, LR 4.50e-05:  29%|██▉       | 39/133 [1:12:28<2:56:33, 112.70s/it]Epoch 0, Loss 2.175, LR 4.50e-05:  30%|███       | 40/133 [1:14:12<2:50:48, 110.20s/it]Epoch 0, Loss 2.159, LR 4.46e-05:  30%|███       | 40/133 [1:14:13<2:50:48, 110.20s/it]Epoch 0, Loss 2.159, LR 4.46e-05:  31%|███       | 41/133 [1:16:03<2:48:59, 110.21s/it]Epoch 0, Loss 2.148, LR 4.42e-05:  31%|███       | 41/133 [1:16:03<2:48:59, 110.21s/it]Epoch 0, Loss 2.148, LR 4.42e-05:  32%|███▏      | 42/133 [1:18:05<2:52:32, 113.77s/it]Epoch 0, Loss 2.100, LR 4.38e-05:  32%|███▏      | 42/133 [1:18:05<2:52:32, 113.77s/it]Epoch 0, Loss 2.100, LR 4.38e-05:  32%|███▏      | 43/133 [1:19:59<2:51:06, 114.07s/it]Epoch 0, Loss 2.073, LR 4.34e-05:  32%|███▏      | 43/133 [1:20:00<2:51:06, 114.07s/it]Epoch 0, Loss 2.073, LR 4.34e-05:  33%|███▎      | 44/133 [1:21:45<2:45:29, 111.57s/it]Epoch 0, Loss 2.096, LR 4.30e-05:  33%|███▎      | 44/133 [1:21:45<2:45:29, 111.57s/it]Epoch 0, Loss 2.096, LR 4.30e-05:  34%|███▍      | 45/133 [1:23:36<2:43:09, 111.25s/it]Epoch 0, Loss 2.116, LR 4.26e-05:  34%|███▍      | 45/133 [1:23:36<2:43:09, 111.25s/it]Epoch 0, Loss 2.116, LR 4.26e-05:  35%|███▍      | 46/133 [1:25:40<2:46:55, 115.12s/it]Epoch 0, Loss 2.086, LR 4.21e-05:  35%|███▍      | 46/133 [1:25:40<2:46:55, 115.12s/it]Epoch 0, Loss 2.086, LR 4.21e-05:  35%|███▌      | 47/133 [1:27:21<2:39:02, 110.96s/it]Epoch 0, Loss 2.070, LR 4.17e-05:  35%|███▌      | 47/133 [1:27:21<2:39:02, 110.96s/it]Epoch 0, Loss 2.070, LR 4.17e-05:  36%|███▌      | 48/133 [1:29:07<2:34:52, 109.32s/it]Epoch 0, Loss 2.020, LR 4.12e-05:  36%|███▌      | 48/133 [1:29:07<2:34:52, 109.32s/it]Epoch 0, Loss 2.020, LR 4.12e-05:  37%|███▋      | 49/133 [1:31:19<2:42:35, 116.13s/it]Epoch 0, Loss 2.120, LR 4.07e-05:  37%|███▋      | 49/133 [1:31:19<2:42:35, 116.13s/it]Epoch 0, Loss 2.120, LR 4.07e-05:  38%|███▊      | 50/133 [1:33:09<2:38:09, 114.33s/it]Epoch 0, Loss 2.065, LR 4.02e-05:  38%|███▊      | 50/133 [1:33:09<2:38:09, 114.33s/it]Epoch 0, Loss 2.065, LR 4.02e-05:  38%|███▊      | 51/133 [1:34:59<2:34:43, 113.21s/it]Epoch 0, Loss 2.083, LR 3.98e-05:  38%|███▊      | 51/133 [1:34:59<2:34:43, 113.21s/it]Epoch 0, Loss 2.083, LR 3.98e-05:  39%|███▉      | 52/133 [1:36:55<2:33:50, 113.95s/it]Epoch 0, Loss 2.063, LR 3.93e-05:  39%|███▉      | 52/133 [1:36:55<2:33:50, 113.95s/it]Epoch 0, Loss 2.063, LR 3.93e-05:  40%|███▉      | 53/133 [1:38:39<2:27:48, 110.85s/it]Epoch 0, Loss 2.074, LR 3.87e-05:  40%|███▉      | 53/133 [1:38:39<2:27:48, 110.85s/it]Epoch 0, Loss 2.074, LR 3.87e-05:  41%|████      | 54/133 [1:40:30<2:26:14, 111.07s/it]Epoch 0, Loss 2.006, LR 3.82e-05:  41%|████      | 54/133 [1:40:30<2:26:14, 111.07s/it]Epoch 0, Loss 2.006, LR 3.82e-05:  41%|████▏     | 55/133 [1:42:19<2:23:27, 110.35s/it]Epoch 0, Loss 2.008, LR 3.77e-05:  41%|████▏     | 55/133 [1:42:19<2:23:27, 110.35s/it]Epoch 0, Loss 2.008, LR 3.77e-05:  42%|████▏     | 56/133 [1:44:01<2:18:23, 107.83s/it]Epoch 0, Loss 2.021, LR 3.72e-05:  42%|████▏     | 56/133 [1:44:01<2:18:23, 107.83s/it]Epoch 0, Loss 2.021, LR 3.72e-05:  43%|████▎     | 57/133 [1:45:56<2:19:20, 110.00s/it]Epoch 0, Loss 2.049, LR 3.67e-05:  43%|████▎     | 57/133 [1:45:56<2:19:20, 110.00s/it]Epoch 0, Loss 2.049, LR 3.67e-05:  44%|████▎     | 58/133 [1:47:49<2:18:42, 110.97s/it]Epoch 0, Loss 2.022, LR 3.61e-05:  44%|████▎     | 58/133 [1:47:49<2:18:42, 110.97s/it]Epoch 0, Loss 2.022, LR 3.61e-05:  44%|████▍     | 59/133 [1:49:49<2:20:13, 113.70s/it]Epoch 0, Loss 2.003, LR 3.56e-05:  44%|████▍     | 59/133 [1:49:49<2:20:13, 113.70s/it]Epoch 0, Loss 2.003, LR 3.56e-05:  45%|████▌     | 60/133 [1:51:38<2:16:29, 112.18s/it]Epoch 0, Loss 1.953, LR 3.50e-05:  45%|████▌     | 60/133 [1:51:38<2:16:29, 112.18s/it]Batch idx 0
Batch idx 1
Batch idx 2
Batch idx 3
Batch idx 4
Batch idx 5
Batch idx 6
Gradient norm: 0.90234375
Batch idx 7
Batch idx 8
Batch idx 9
Batch idx 10
Batch idx 11
Batch idx 12
Batch idx 13
Batch idx 14
Gradient norm: 0.75
Batch idx 15
Batch idx 16
Batch idx 17
Batch idx 18
Batch idx 19
Batch idx 20
Batch idx 21
Batch idx 22
Gradient norm: 0.66796875
Batch idx 23
Batch idx 24
Batch idx 25
Batch idx 26
Batch idx 27
Batch idx 28
Batch idx 29
Batch idx 30
Gradient norm: 0.80859375
Batch idx 31
Batch idx 32
Batch idx 33
Batch idx 34
Batch idx 35
Batch idx 36
Batch idx 37
Batch idx 38
Gradient norm: 0.7578125
Batch idx 39
Batch idx 40
Batch idx 41
Batch idx 42
Batch idx 43
Batch idx 44
Batch idx 45
Batch idx 46
Gradient norm: 0.65625
Batch idx 47
Batch idx 48
Batch idx 49
Batch idx 50
Batch idx 51
Batch idx 52
Batch idx 53
Batch idx 54
Gradient norm: 0.69140625
Batch idx 55
Batch idx 56
Batch idx 57
Batch idx 58
Batch idx 59
Batch idx 60
Batch idx 61
Batch idx 62
Gradient norm: 0.8671875
Batch idx 63
Batch idx 64
Batch idx 65
Batch idx 66
Batch idx 67
Batch idx 68
Batch idx 69
Batch idx 70
Gradient norm: 0.77734375
Batch idx 71
Batch idx 72
Batch idx 73
Batch idx 74
Batch idx 75
Batch idx 76
Batch idx 77
Batch idx 78
Gradient norm: 0.83984375
Batch idx 79
Batch idx 80
Batch idx 81
Batch idx 82
Batch idx 83
Batch idx 84
Batch idx 85
Batch idx 86
Gradient norm: 0.78125
Batch idx 87
Batch idx 88
Batch idx 89
Batch idx 90
Batch idx 91
Batch idx 92
Batch idx 93
Batch idx 94
Gradient norm: 0.796875
Batch idx 95
Batch idx 96
Batch idx 97
Batch idx 98
Batch idx 99
Batch idx 100
Batch idx 101
Batch idx 102
Gradient norm: 0.640625
Batch idx 103
Batch idx 104
Batch idx 105
Batch idx 106
Batch idx 107
Batch idx 108
Batch idx 109
Batch idx 110
Gradient norm: 0.5859375
Batch idx 111
Batch idx 112
Batch idx 113
Batch idx 114
Batch idx 115
Batch idx 116
Batch idx 117
Batch idx 118
Gradient norm: 0.59375
Batch idx 119
Batch idx 120
Batch idx 121
Batch idx 122
Batch idx 123
Batch idx 124
Batch idx 125
Batch idx 126
Gradient norm: 0.57421875
Batch idx 127
Batch idx 128
Batch idx 129
Batch idx 130
Batch idx 131
Batch idx 132
Batch idx 133
Batch idx 134
Gradient norm: 0.53515625
Batch idx 135
Batch idx 136
Batch idx 137
Batch idx 138
Batch idx 139
Batch idx 140
Batch idx 141
Batch idx 142
Gradient norm: 0.50390625
Batch idx 143
Batch idx 144
Batch idx 145
Batch idx 146
Batch idx 147
Batch idx 148
Batch idx 149
Batch idx 150
Gradient norm: 0.53125
Batch idx 151
Batch idx 152
Batch idx 153
Batch idx 154
Batch idx 155
Batch idx 156
Batch idx 157
Batch idx 158
Gradient norm: 0.384765625
Batch idx 159
Batch idx 160
Batch idx 161
Batch idx 162
Batch idx 163
Batch idx 164
Batch idx 165
Batch idx 166
Gradient norm: 0.373046875
Batch idx 167
Batch idx 168
Batch idx 169
Batch idx 170
Batch idx 171
Batch idx 172
Batch idx 173
Batch idx 174
Gradient norm: 0.357421875
Batch idx 175
Batch idx 176
Batch idx 177
Batch idx 178
Batch idx 179
Batch idx 180
Batch idx 181
Batch idx 182
Gradient norm: 0.353515625
Batch idx 183
Batch idx 184
Batch idx 185
Batch idx 186
Batch idx 187
Batch idx 188
Batch idx 189
Batch idx 190
Gradient norm: 0.306640625
Batch idx 191
Batch idx 192
Batch idx 193
Batch idx 194
Batch idx 195
Batch idx 196
Batch idx 197
Batch idx 198
Gradient norm: 0.32421875
Batch idx 199
Batch idx 200
Batch idx 201
Batch idx 202
Batch idx 203
Batch idx 204
Batch idx 205
Batch idx 206
Gradient norm: 0.28515625
Batch idx 207
Batch idx 208
Batch idx 209
Batch idx 210
Batch idx 211
Batch idx 212
Batch idx 213
Batch idx 214
Gradient norm: 0.271484375
Batch idx 215
Batch idx 216
Batch idx 217
Batch idx 218
Batch idx 219
Batch idx 220
Batch idx 221
Batch idx 222
Gradient norm: 0.263671875
Batch idx 223
Batch idx 224
Batch idx 225
Batch idx 226
Batch idx 227
Batch idx 228
Batch idx 229
Batch idx 230
Gradient norm: 0.2265625
Batch idx 231
Batch idx 232
Batch idx 233
Batch idx 234
Batch idx 235
Batch idx 236
Batch idx 237
Batch idx 238
Gradient norm: 0.255859375
Batch idx 239
Batch idx 240
Batch idx 241
Batch idx 242
Batch idx 243
Batch idx 244
Batch idx 245
Batch idx 246
Gradient norm: 0.2041015625
Batch idx 247
Batch idx 248
Batch idx 249
Batch idx 250
Batch idx 251
Batch idx 252
Batch idx 253
Batch idx 254
Gradient norm: 0.2373046875
Batch idx 255
Batch idx 256
Batch idx 257
Batch idx 258
Batch idx 259
Batch idx 260
Batch idx 261
Batch idx 262
Gradient norm: 0.205078125
Batch idx 263
Batch idx 264
Batch idx 265
Batch idx 266
Batch idx 267
Batch idx 268
Batch idx 269
Batch idx 270
Gradient norm: 0.1767578125
Batch idx 271
Batch idx 272
Batch idx 273
Batch idx 274
Batch idx 275
Batch idx 276
Batch idx 277
Batch idx 278
Gradient norm: 0.1767578125
Batch idx 279
Batch idx 280
Batch idx 281
Batch idx 282
Batch idx 283
Batch idx 284
Batch idx 285
Batch idx 286
Gradient norm: 0.19140625
Batch idx 287
Batch idx 288
Batch idx 289
Batch idx 290
Batch idx 291
Batch idx 292
Batch idx 293
Batch idx 294
Gradient norm: 0.1728515625
Batch idx 295
Batch idx 296
Batch idx 297
Batch idx 298
Batch idx 299
Batch idx 300
Batch idx 301
Batch idx 302
Gradient norm: 0.15234375
Batch idx 303
Batch idx 304
Batch idx 305
Batch idx 306
Batch idx 307
Batch idx 308
Batch idx 309
Batch idx 310
Gradient norm: 0.146484375
Batch idx 311
Batch idx 312
Batch idx 313
Batch idx 314
Batch idx 315
Batch idx 316
Batch idx 317
Batch idx 318
Gradient norm: 0.1533203125
Batch idx 319
Batch idx 320
Batch idx 321
Batch idx 322
Batch idx 323
Batch idx 324
Batch idx 325
Batch idx 326
Gradient norm: 0.138671875
Batch idx 327
Batch idx 328
Batch idx 329
Batch idx 330
Batch idx 331
Batch idx 332
Batch idx 333
Batch idx 334
Gradient norm: 0.1328125
Batch idx 335
Batch idx 336
Batch idx 337
Batch idx 338
Batch idx 339
Batch idx 340
Batch idx 341
Batch idx 342
Gradient norm: 0.1640625
Batch idx 343
Batch idx 344
Batch idx 345
Batch idx 346
Batch idx 347
Batch idx 348
Batch idx 349
Batch idx 350
Gradient norm: 0.142578125
Batch idx 351
Batch idx 352
Batch idx 353
Batch idx 354
Batch idx 355
Batch idx 356
Batch idx 357
Batch idx 358
Gradient norm: 0.154296875
Batch idx 359
Batch idx 360
Batch idx 361
Batch idx 362
Batch idx 363
Batch idx 364
Batch idx 365
Batch idx 366
Gradient norm: 0.138671875
Batch idx 367
Batch idx 368
Batch idx 369
Batch idx 370
Batch idx 371
Batch idx 372
Batch idx 373
Batch idx 374
Gradient norm: 0.1494140625
Batch idx 375
Batch idx 376
Batch idx 377
Batch idx 378
Batch idx 379
Batch idx 380
Batch idx 381
Batch idx 382
Gradient norm: 0.1337890625
Batch idx 383
Batch idx 384
Batch idx 385
Batch idx 386
Batch idx 387
Batch idx 388
Batch idx 389
Batch idx 390
Gradient norm: 0.142578125
Batch idx 391
Batch idx 392
Batch idx 393
Batch idx 394
Batch idx 395
Batch idx 396
Batch idx 397
Batch idx 398
Gradient norm: 0.1455078125
Batch idx 399
Batch idx 400
Batch idx 401
Batch idx 402
Batch idx 403
Batch idx 404
Batch idx 405
Batch idx 406
Gradient norm: 0.142578125
Batch idx 407
Batch idx 408
Batch idx 409
Batch idx 410
Batch idx 411
Batch idx 412
Batch idx 413
Batch idx 414
Gradient norm: 0.140625
Batch idx 415
Batch idx 416
Batch idx 417
Batch idx 418
Batch idx 419
Batch idx 420
Batch idx 421
Batch idx 422
Gradient norm: 0.1513671875
Batch idx 423
Batch idx 424
Batch idx 425
Batch idx 426
Batch idx 427
Batch idx 428
Batch idx 429
Batch idx 430
Gradient norm: 0.1748046875
Batch idx 431
Batch idx 432
Batch idx 433
Batch idx 434
Batch idx 435
Batch idx 436
Batch idx 437
Batch idx 438
Gradient norm: 0.1484375
Batch idx 439
Batch idx 440
Batch idx 441
Batch idx 442
Batch idx 443
Batch idx 444
Batch idx 445
Batch idx 446
Gradient norm: 0.126953125
Batch idx 447
Batch idx 448
Batch idx 449
Batch idx 450
Batch idx 451
Batch idx 452
Batch idx 453
Batch idx 454
Gradient norm: 0.12109375
Batch idx 455
Batch idx 456
Batch idx 457
Batch idx 458
Batch idx 459
Batch idx 460
Batch idx 461
Batch idx 462
Gradient norm: 0.1142578125
Batch idx 463
Batch idx 464
Batch idx 465
Batch idx 466
Batch idx 467
Batch idx 468
Batch idx 469
Batch idx 470
Gradient norm: 0.11279296875
Batch idx 471
Batch idx 472
Batch idx 473
Batch idx 474
Batch idx 475
Batch idx 476
Batch idx 477
Batch idx 478
Gradient norm: 0.142578125
Batch idx 479
Epoch 0, Loss 1.953, LR 3.50e-05:  46%|████▌     | 61/133 [1:53:30<2:14:36, 112.18s/it]Epoch 0, Loss 2.025, LR 3.45e-05:  46%|████▌     | 61/133 [1:53:30<2:14:36, 112.18s/it]Epoch 0, Loss 2.025, LR 3.45e-05:  47%|████▋     | 62/133 [1:55:17<2:10:57, 110.67s/it]Epoch 0, Loss 1.970, LR 3.39e-05:  47%|████▋     | 62/133 [1:55:17<2:10:57, 110.67s/it]Epoch 0, Loss 1.970, LR 3.39e-05:  47%|████▋     | 63/133 [1:57:07<2:08:52, 110.47s/it]Epoch 0, Loss 2.090, LR 3.33e-05:  47%|████▋     | 63/133 [1:57:07<2:08:52, 110.47s/it]Epoch 0, Loss 2.090, LR 3.33e-05:  48%|████▊     | 64/133 [1:58:59<2:07:29, 110.86s/it]Epoch 0, Loss 2.061, LR 3.28e-05:  48%|████▊     | 64/133 [1:58:59<2:07:29, 110.86s/it]Epoch 0, Loss 2.061, LR 3.28e-05:  49%|████▉     | 65/133 [2:00:47<2:04:43, 110.05s/it]Epoch 0, Loss 2.079, LR 3.22e-05:  49%|████▉     | 65/133 [2:00:47<2:04:43, 110.05s/it]Epoch 0, Loss 2.079, LR 3.22e-05:  50%|████▉     | 66/133 [2:02:33<2:01:31, 108.83s/it]Epoch 0, Loss 2.019, LR 3.16e-05:  50%|████▉     | 66/133 [2:02:33<2:01:31, 108.83s/it]Epoch 0, Loss 2.019, LR 3.16e-05:  50%|█████     | 67/133 [2:04:13<1:56:46, 106.17s/it]Epoch 0, Loss 1.897, LR 3.10e-05:  50%|█████     | 67/133 [2:04:13<1:56:46, 106.17s/it]Epoch 0, Loss 1.897, LR 3.10e-05:  51%|█████     | 68/133 [2:06:03<1:56:16, 107.32s/it]Epoch 0, Loss 2.064, LR 3.04e-05:  51%|█████     | 68/133 [2:06:03<1:56:16, 107.32s/it]Epoch 0, Loss 2.064, LR 3.04e-05:  52%|█████▏    | 69/133 [2:08:01<1:57:58, 110.60s/it]Epoch 0, Loss 1.987, LR 2.99e-05:  52%|█████▏    | 69/133 [2:08:01<1:57:58, 110.60s/it]Epoch 0, Loss 1.987, LR 2.99e-05:  53%|█████▎    | 70/133 [2:09:56<1:57:32, 111.94s/it]Epoch 0, Loss 2.009, LR 2.93e-05:  53%|█████▎    | 70/133 [2:09:56<1:57:32, 111.94s/it]Epoch 0, Loss 2.009, LR 2.93e-05:  53%|█████▎    | 71/133 [2:11:53<1:57:01, 113.25s/it]Epoch 0, Loss 2.011, LR 2.87e-05:  53%|█████▎    | 71/133 [2:11:53<1:57:01, 113.25s/it]Epoch 0, Loss 2.011, LR 2.87e-05:  54%|█████▍    | 72/133 [2:13:51<1:56:49, 114.91s/it]Epoch 0, Loss 1.852, LR 2.81e-05:  54%|█████▍    | 72/133 [2:13:52<1:56:49, 114.91s/it]Epoch 0, Loss 1.852, LR 2.81e-05:  55%|█████▍    | 73/133 [2:15:43<1:53:58, 113.98s/it]Epoch 0, Loss 2.061, LR 2.75e-05:  55%|█████▍    | 73/133 [2:15:43<1:53:58, 113.98s/it]Epoch 0, Loss 2.061, LR 2.75e-05:  56%|█████▌    | 74/133 [2:17:40<1:52:56, 114.85s/it]Epoch 0, Loss 2.071, LR 2.69e-05:  56%|█████▌    | 74/133 [2:17:40<1:52:56, 114.85s/it]Epoch 0, Loss 2.071, LR 2.69e-05:  56%|█████▋    | 75/133 [2:19:46<1:54:19, 118.26s/it]Epoch 0, Loss 1.994, LR 2.63e-05:  56%|█████▋    | 75/133 [2:19:47<1:54:19, 118.26s/it]Epoch 0, Loss 1.994, LR 2.63e-05:  57%|█████▋    | 76/133 [2:21:43<1:51:57, 117.85s/it]Epoch 0, Loss 2.006, LR 2.57e-05:  57%|█████▋    | 76/133 [2:21:43<1:51:57, 117.85s/it]Epoch 0, Loss 2.006, LR 2.57e-05:  58%|█████▊    | 77/133 [2:23:41<1:50:05, 117.95s/it]Epoch 0, Loss 1.995, LR 2.51e-05:  58%|█████▊    | 77/133 [2:23:41<1:50:05, 117.95s/it]Epoch 0, Loss 1.995, LR 2.51e-05:  59%|█████▊    | 78/133 [2:25:48<1:50:24, 120.44s/it]Epoch 0, Loss 2.012, LR 2.46e-05:  59%|█████▊    | 78/133 [2:25:48<1:50:24, 120.44s/it]Epoch 0, Loss 2.012, LR 2.46e-05:  59%|█████▉    | 79/133 [2:27:47<1:48:02, 120.05s/it]Epoch 0, Loss 2.042, LR 2.40e-05:  59%|█████▉    | 79/133 [2:27:47<1:48:02, 120.05s/it]Epoch 0, Loss 2.042, LR 2.40e-05:  60%|██████    | 80/133 [2:29:39<1:43:57, 117.69s/it]Epoch 0, Loss 2.044, LR 2.34e-05:  60%|██████    | 80/133 [2:29:39<1:43:57, 117.69s/it]Epoch 0, Loss 2.044, LR 2.34e-05:  61%|██████    | 81/133 [2:31:23<1:38:25, 113.57s/it]Epoch 0, Loss 1.933, LR 2.28e-05:  61%|██████    | 81/133 [2:31:23<1:38:25, 113.57s/it]Epoch 0, Loss 1.933, LR 2.28e-05:  62%|██████▏   | 82/133 [2:33:05<1:33:32, 110.05s/it]Epoch 0, Loss 1.882, LR 2.22e-05:  62%|██████▏   | 82/133 [2:33:05<1:33:32, 110.05s/it]Epoch 0, Loss 1.882, LR 2.22e-05:  62%|██████▏   | 83/133 [2:35:01<1:33:14, 111.88s/it]Epoch 0, Loss 1.959, LR 2.17e-05:  62%|██████▏   | 83/133 [2:35:01<1:33:14, 111.88s/it]Epoch 0, Loss 1.959, LR 2.17e-05:  63%|██████▎   | 84/133 [2:36:49<1:30:30, 110.82s/it]Epoch 0, Loss 2.043, LR 2.11e-05:  63%|██████▎   | 84/133 [2:36:49<1:30:30, 110.82s/it]Epoch 0, Loss 2.043, LR 2.11e-05:  64%|██████▍   | 85/133 [2:38:30<1:26:17, 107.87s/it]Epoch 0, Loss 1.952, LR 2.05e-05:  64%|██████▍   | 85/133 [2:38:30<1:26:17, 107.87s/it]Epoch 0, Loss 1.952, LR 2.05e-05:  65%|██████▍   | 86/133 [2:40:24<1:25:53, 109.66s/it]Epoch 0, Loss 1.989, LR 2.00e-05:  65%|██████▍   | 86/133 [2:40:24<1:25:53, 109.66s/it]Epoch 0, Loss 1.989, LR 2.00e-05:  65%|██████▌   | 87/133 [2:41:58<1:20:21, 104.82s/it]Epoch 0, Loss 1.909, LR 1.94e-05:  65%|██████▌   | 87/133 [2:41:58<1:20:21, 104.82s/it]Epoch 0, Loss 1.909, LR 1.94e-05:  66%|██████▌   | 88/133 [2:43:48<1:19:47, 106.39s/it]Epoch 0, Loss 2.007, LR 1.89e-05:  66%|██████▌   | 88/133 [2:43:48<1:19:47, 106.39s/it]Epoch 0, Loss 2.007, LR 1.89e-05:  67%|██████▋   | 89/133 [2:45:38<1:18:54, 107.61s/it]Epoch 0, Loss 1.900, LR 1.83e-05:  67%|██████▋   | 89/133 [2:45:38<1:18:54, 107.61s/it]Epoch 0, Loss 1.900, LR 1.83e-05:  68%|██████▊   | 90/133 [2:47:25<1:16:59, 107.44s/it]Epoch 0, Loss 1.996, LR 1.78e-05:  68%|██████▊   | 90/133 [2:47:25<1:16:59, 107.44s/it]Epoch 0, Loss 1.996, LR 1.78e-05:  68%|██████▊   | 91/133 [2:49:16<1:15:59, 108.56s/it]Epoch 0, Loss 1.994, LR 1.73e-05:  68%|██████▊   | 91/133 [2:49:17<1:15:59, 108.56s/it]Epoch 0, Loss 1.994, LR 1.73e-05:  69%|██████▉   | 92/133 [2:51:09<1:14:55, 109.65s/it]Epoch 0, Loss 2.018, LR 1.68e-05:  69%|██████▉   | 92/133 [2:51:09<1:14:55, 109.65s/it]Epoch 0, Loss 2.018, LR 1.68e-05:  70%|██████▉   | 93/133 [2:52:55<1:12:24, 108.61s/it]Epoch 0, Loss 1.952, LR 1.63e-05:  70%|██████▉   | 93/133 [2:52:55<1:12:24, 108.61s/it]Epoch 0, Loss 1.952, LR 1.63e-05:  71%|███████   | 94/133 [2:54:54<1:12:41, 111.83s/it]Epoch 0, Loss 1.964, LR 1.57e-05:  71%|███████   | 94/133 [2:54:54<1:12:41, 111.83s/it]Epoch 0, Loss 1.964, LR 1.57e-05:  71%|███████▏  | 95/133 [2:57:02<1:13:50, 116.59s/it]Epoch 0, Loss 1.947, LR 1.52e-05:  71%|███████▏  | 95/133 [2:57:02<1:13:50, 116.59s/it]Epoch 0, Loss 1.947, LR 1.52e-05:  72%|███████▏  | 96/133 [2:58:47<1:09:47, 113.17s/it]Epoch 0, Loss 2.022, LR 1.48e-05:  72%|███████▏  | 96/133 [2:58:47<1:09:47, 113.17s/it]Epoch 0, Loss 2.022, LR 1.48e-05:  73%|███████▎  | 97/133 [3:00:34<1:06:51, 111.42s/it]Epoch 0, Loss 1.965, LR 1.43e-05:  73%|███████▎  | 97/133 [3:00:34<1:06:51, 111.42s/it]Epoch 0, Loss 1.965, LR 1.43e-05:  74%|███████▎  | 98/133 [3:02:14<1:02:56, 107.89s/it]Epoch 0, Loss 1.949, LR 1.38e-05:  74%|███████▎  | 98/133 [3:02:14<1:02:56, 107.89s/it]Epoch 0, Loss 1.949, LR 1.38e-05:  74%|███████▍  | 99/133 [3:04:03<1:01:21, 108.27s/it]Epoch 0, Loss 1.932, LR 1.33e-05:  74%|███████▍  | 99/133 [3:04:03<1:01:21, 108.27s/it]Epoch 0, Loss 1.932, LR 1.33e-05:  75%|███████▌  | 100/133 [3:05:50<59:21, 107.91s/it] Epoch 0, Loss 1.929, LR 1.29e-05:  75%|███████▌  | 100/133 [3:05:50<59:21, 107.91s/it]Epoch 0, Loss 1.929, LR 1.29e-05:  76%|███████▌  | 101/133 [3:07:43<58:20, 109.40s/it]Epoch 0, Loss 1.951, LR 1.24e-05:  76%|███████▌  | 101/133 [3:07:43<58:20, 109.40s/it]Epoch 0, Loss 1.951, LR 1.24e-05:  77%|███████▋  | 102/133 [3:09:36<57:02, 110.39s/it]Epoch 0, Loss 1.951, LR 1.20e-05:  77%|███████▋  | 102/133 [3:09:36<57:02, 110.39s/it]Epoch 0, Loss 1.951, LR 1.20e-05:  77%|███████▋  | 103/133 [3:11:34<56:23, 112.78s/it]Epoch 0, Loss 1.951, LR 1.16e-05:  77%|███████▋  | 103/133 [3:11:34<56:23, 112.78s/it]Epoch 0, Loss 1.951, LR 1.16e-05:  78%|███████▊  | 104/133 [3:13:13<52:30, 108.62s/it]Epoch 0, Loss 1.977, LR 1.12e-05:  78%|███████▊  | 104/133 [3:13:13<52:30, 108.62s/it]Epoch 0, Loss 1.977, LR 1.12e-05:  79%|███████▉  | 105/133 [3:15:01<50:34, 108.36s/it]Epoch 0, Loss 1.974, LR 1.08e-05:  79%|███████▉  | 105/133 [3:15:01<50:34, 108.36s/it]Epoch 0, Loss 1.974, LR 1.08e-05:  80%|███████▉  | 106/133 [3:16:57<49:48, 110.67s/it]Epoch 0, Loss 1.897, LR 1.04e-05:  80%|███████▉  | 106/133 [3:16:57<49:48, 110.67s/it]Epoch 0, Loss 1.897, LR 1.04e-05:  80%|████████  | 107/133 [3:18:30<45:42, 105.50s/it]Epoch 0, Loss 1.986, LR 1.00e-05:  80%|████████  | 107/133 [3:18:30<45:42, 105.50s/it]Epoch 0, Loss 1.986, LR 1.00e-05:  81%|████████  | 108/133 [3:20:24<44:56, 107.86s/it]Epoch 0, Loss 1.990, LR 9.65e-06:  81%|████████  | 108/133 [3:20:24<44:56, 107.86s/it]Epoch 0, Loss 1.990, LR 9.65e-06:  82%|████████▏ | 109/133 [3:22:24<44:39, 111.66s/it]Epoch 0, Loss 1.973, LR 9.30e-06:  82%|████████▏ | 109/133 [3:22:24<44:39, 111.66s/it]Epoch 0, Loss 1.973, LR 9.30e-06:  83%|████████▎ | 110/133 [3:24:28<44:13, 115.37s/it]Epoch 0, Loss 1.882, LR 8.96e-06:  83%|████████▎ | 110/133 [3:24:28<44:13, 115.37s/it]Epoch 0, Loss 1.882, LR 8.96e-06:  83%|████████▎ | 111/133 [3:26:25<42:23, 115.63s/it]Epoch 0, Loss 1.939, LR 8.63e-06:  83%|████████▎ | 111/133 [3:26:25<42:23, 115.63s/it]Epoch 0, Loss 1.939, LR 8.63e-06:  84%|████████▍ | 112/133 [3:28:17<40:07, 114.63s/it]Epoch 0, Loss 1.982, LR 8.32e-06:  84%|████████▍ | 112/133 [3:28:17<40:07, 114.63s/it]Epoch 0, Loss 1.982, LR 8.32e-06:  85%|████████▍ | 113/133 [3:30:15<38:35, 115.76s/it]Epoch 0, Loss 2.000, LR 8.01e-06:  85%|████████▍ | 113/133 [3:30:15<38:35, 115.76s/it]Epoch 0, Loss 2.000, LR 8.01e-06:  86%|████████▌ | 114/133 [3:31:56<35:12, 111.18s/it]Epoch 0, Loss 1.974, LR 7.73e-06:  86%|████████▌ | 114/133 [3:31:56<35:12, 111.18s/it]Epoch 0, Loss 1.974, LR 7.73e-06:  86%|████████▋ | 115/133 [3:33:45<33:09, 110.55s/it]Epoch 0, Loss 1.906, LR 7.45e-06:  86%|████████▋ | 115/133 [3:33:45<33:09, 110.55s/it]Epoch 0, Loss 1.906, LR 7.45e-06:  87%|████████▋ | 116/133 [3:35:27<30:35, 107.97s/it]Epoch 0, Loss 1.996, LR 7.19e-06:  87%|████████▋ | 116/133 [3:35:27<30:35, 107.97s/it]Epoch 0, Loss 1.996, LR 7.19e-06:  88%|████████▊ | 117/133 [3:37:13<28:38, 107.38s/it]Epoch 0, Loss 2.023, LR 6.95e-06:  88%|████████▊ | 117/133 [3:37:13<28:38, 107.38s/it]Epoch 0, Loss 2.023, LR 6.95e-06:  89%|████████▊ | 118/133 [3:39:00<26:52, 107.47s/it]Epoch 0, Loss 1.995, LR 6.71e-06:  89%|████████▊ | 118/133 [3:39:00<26:52, 107.47s/it]Batch idx 480
Batch idx 481
Batch idx 482
Batch idx 483
Batch idx 484
Batch idx 485
Batch idx 486
Gradient norm: 0.1298828125
Batch idx 487
Batch idx 488
Batch idx 489
Batch idx 490
Batch idx 491
Batch idx 492
Batch idx 493
Batch idx 494
Gradient norm: 0.14453125
Batch idx 495
Batch idx 496
Batch idx 497
Batch idx 498
Batch idx 499
Batch idx 500
Batch idx 501
Batch idx 502
Gradient norm: 0.1435546875
Batch idx 503
Batch idx 504
Batch idx 505
Batch idx 506
Batch idx 507
Batch idx 508
Batch idx 509
Batch idx 510
Gradient norm: 0.1220703125
Batch idx 511
Batch idx 512
Batch idx 513
Batch idx 514
Batch idx 515
Batch idx 516
Batch idx 517
Batch idx 518
Gradient norm: 0.1376953125
Batch idx 519
Batch idx 520
Batch idx 521
Batch idx 522
Batch idx 523
Batch idx 524
Batch idx 525
Batch idx 526
Gradient norm: 0.1494140625
Batch idx 527
Batch idx 528
Batch idx 529
Batch idx 530
Batch idx 531
Batch idx 532
Batch idx 533
Batch idx 534
Gradient norm: 0.158203125
Batch idx 535
Batch idx 536
Batch idx 537
Batch idx 538
Batch idx 539
Batch idx 540
Batch idx 541
Batch idx 542
Gradient norm: 0.1318359375
Batch idx 543
Batch idx 544
Batch idx 545
Batch idx 546
Batch idx 547
Batch idx 548
Batch idx 549
Batch idx 550
Gradient norm: 0.12890625
Batch idx 551
Batch idx 552
Batch idx 553
Batch idx 554
Batch idx 555
Batch idx 556
Batch idx 557
Batch idx 558
Gradient norm: 0.1318359375
Batch idx 559
Batch idx 560
Batch idx 561
Batch idx 562
Batch idx 563
Batch idx 564
Batch idx 565
Batch idx 566
Gradient norm: 0.140625
Batch idx 567
Batch idx 568
Batch idx 569
Batch idx 570
Batch idx 571
Batch idx 572
Batch idx 573
Batch idx 574
Gradient norm: 0.158203125
Batch idx 575
Batch idx 576
Batch idx 577
Batch idx 578
Batch idx 579
Batch idx 580
Batch idx 581
Batch idx 582
Gradient norm: 0.11083984375
Batch idx 583
Batch idx 584
Batch idx 585
Batch idx 586
Batch idx 587
Batch idx 588
Batch idx 589
Batch idx 590
Gradient norm: 0.11181640625
Batch idx 591
Batch idx 592
Batch idx 593
Batch idx 594
Batch idx 595
Batch idx 596
Batch idx 597
Batch idx 598
Gradient norm: 0.10205078125
Batch idx 599
Batch idx 600
Batch idx 601
Batch idx 602
Batch idx 603
Batch idx 604
Batch idx 605
Batch idx 606
Gradient norm: 0.12255859375
Batch idx 607
Batch idx 608
Batch idx 609
Batch idx 610
Batch idx 611
Batch idx 612
Batch idx 613
Batch idx 614
Gradient norm: 0.11083984375
Batch idx 615
Batch idx 616
Batch idx 617
Batch idx 618
Batch idx 619
Batch idx 620
Batch idx 621
Batch idx 622
Gradient norm: 0.1123046875
Batch idx 623
Batch idx 624
Batch idx 625
Batch idx 626
Batch idx 627
Batch idx 628
Batch idx 629
Batch idx 630
Gradient norm: 0.11328125
Batch idx 631
Batch idx 632
Batch idx 633
Batch idx 634
Batch idx 635
Batch idx 636
Batch idx 637
Batch idx 638
Gradient norm: 0.0947265625
Batch idx 639
Batch idx 640
Batch idx 641
Batch idx 642
Batch idx 643
Batch idx 644
Batch idx 645
Batch idx 646
Gradient norm: 0.12109375
Batch idx 647
Batch idx 648
Batch idx 649
Batch idx 650
Batch idx 651
Batch idx 652
Batch idx 653
Batch idx 654
Gradient norm: 0.1494140625
Batch idx 655
Batch idx 656
Batch idx 657
Batch idx 658
Batch idx 659
Batch idx 660
Batch idx 661
Batch idx 662
Gradient norm: 0.171875
Batch idx 663
Batch idx 664
Batch idx 665
Batch idx 666
Batch idx 667
Batch idx 668
Batch idx 669
Batch idx 670
Gradient norm: 0.14453125
Batch idx 671
Batch idx 672
Batch idx 673
Batch idx 674
Batch idx 675
Batch idx 676
Batch idx 677
Batch idx 678
Gradient norm: 0.10791015625
Batch idx 679
Batch idx 680
Batch idx 681
Batch idx 682
Batch idx 683
Batch idx 684
Batch idx 685
Batch idx 686
Gradient norm: 0.115234375
Batch idx 687
Batch idx 688
Batch idx 689
Batch idx 690
Batch idx 691
Batch idx 692
Batch idx 693
Batch idx 694
Gradient norm: 0.1298828125
Batch idx 695
Batch idx 696
Batch idx 697
Batch idx 698
Batch idx 699
Batch idx 700
Batch idx 701
Batch idx 702
Gradient norm: 0.111328125
Batch idx 703
Batch idx 704
Batch idx 705
Batch idx 706
Batch idx 707
Batch idx 708
Batch idx 709
Batch idx 710
Gradient norm: 0.1103515625
Batch idx 711
Batch idx 712
Batch idx 713
Batch idx 714
Batch idx 715
Batch idx 716
Batch idx 717
Batch idx 718
Gradient norm: 0.10205078125
Batch idx 719
Batch idx 720
Batch idx 721
Batch idx 722
Batch idx 723
Batch idx 724
Batch idx 725
Batch idx 726
Gradient norm: 0.099609375
Batch idx 727
Batch idx 728
Batch idx 729
Batch idx 730
Batch idx 731
Batch idx 732
Batch idx 733
Batch idx 734
Gradient norm: 0.09375
Batch idx 735
Batch idx 736
Batch idx 737
Batch idx 738
Batch idx 739
Batch idx 740
Batch idx 741
Batch idx 742
Gradient norm: 0.10693359375
Batch idx 743
Batch idx 744
Batch idx 745
Batch idx 746
Batch idx 747
Batch idx 748
Batch idx 749
Batch idx 750
Gradient norm: 0.10302734375
Batch idx 751
Batch idx 752
Batch idx 753
Batch idx 754
Batch idx 755
Batch idx 756
Batch idx 757
Batch idx 758
Gradient norm: 0.10009765625
Batch idx 759
Batch idx 760
Batch idx 761
Batch idx 762
Batch idx 763
Batch idx 764
Batch idx 765
Batch idx 766
Gradient norm: 0.1123046875
Batch idx 767
Batch idx 768
Batch idx 769
Batch idx 770
Batch idx 771
Batch idx 772
Batch idx 773
Batch idx 774
Gradient norm: 0.123046875
Batch idx 775
Batch idx 776
Batch idx 777
Batch idx 778
Batch idx 779
Batch idx 780
Batch idx 781
Batch idx 782
Gradient norm: 0.10791015625
Batch idx 783
Batch idx 784
Batch idx 785
Batch idx 786
Batch idx 787
Batch idx 788
Batch idx 789
Batch idx 790
Gradient norm: 0.134765625
Batch idx 791
Batch idx 792
Batch idx 793
Batch idx 794
Batch idx 795
Batch idx 796
Batch idx 797
Batch idx 798
Gradient norm: 0.1591796875
Batch idx 799
Batch idx 800
Batch idx 801
Batch idx 802
Batch idx 803
Batch idx 804
Batch idx 805
Batch idx 806
Gradient norm: 0.103515625
Batch idx 807
Batch idx 808
Batch idx 809
Batch idx 810
Batch idx 811
Batch idx 812
Batch idx 813
Batch idx 814
Gradient norm: 0.09423828125
Batch idx 815
Batch idx 816
Batch idx 817
Batch idx 818
Batch idx 819
Batch idx 820
Batch idx 821
Batch idx 822
Gradient norm: 0.10888671875
Batch idx 823
Batch idx 824
Batch idx 825
Batch idx 826
Batch idx 827
Batch idx 828
Batch idx 829
Batch idx 830
Gradient norm: 0.12158203125
Batch idx 831
Batch idx 832
Batch idx 833
Batch idx 834
Batch idx 835
Batch idx 836
Batch idx 837
Batch idx 838
Gradient norm: 0.0966796875
Batch idx 839
Batch idx 840
Batch idx 841
Batch idx 842
Batch idx 843
Batch idx 844
Batch idx 845
Batch idx 846
Gradient norm: 0.10400390625
Batch idx 847
Batch idx 848
Batch idx 849
Batch idx 850
Batch idx 851
Batch idx 852
Batch idx 853
Batch idx 854
Gradient norm: 0.11474609375
Batch idx 855
Batch idx 856
Batch idx 857
Batch idx 858
Batch idx 859
Batch idx 860
Batch idx 861
Batch idx 862
Gradient norm: 0.12353515625
Batch idx 863
Batch idx 864
Batch idx 865
Batch idx 866
Batch idx 867
Batch idx 868
Batch idx 869
Batch idx 870
Gradient norm: 0.10595703125
Batch idx 871
Batch idx 872
Batch idx 873
Batch idx 874
Batch idx 875
Batch idx 876
Batch idx 877
Batch idx 878
Gradient norm: 0.11181640625
Batch idx 879
Batch idx 880
Batch idx 881
Batch idx 882
Batch idx 883
Batch idx 884
Batch idx 885
Batch idx 886
Gradient norm: 0.1201171875
Batch idx 887
Batch idx 888
Batch idx 889
Batch idx 890
Batch idx 891
Batch idx 892
Batch idx 893
Batch idx 894
Gradient norm: 0.09033203125
Batch idx 895
Batch idx 896
Batch idx 897
Batch idx 898
Batch idx 899
Batch idx 900
Batch idx 901
Batch idx 902
Gradient norm: 0.09375
Batch idx 903
Batch idx 904
Batch idx 905
Batch idx 906
Batch idx 907
Batch idx 908
Batch idx 909
Batch idx 910
Gradient norm: 0.12890625
Batch idx 911
Batch idx 912
Batch idx 913
Batch idx 914
Batch idx 915
Batch idx 916
Batch idx 917
Batch idx 918
Gradient norm: 0.1181640625
Batch idx 919
Batch idx 920
Batch idx 921
Batch idx 922
Batch idx 923
Batch idx 924
Batch idx 925
Batch idx 926
Gradient norm: 0.10791015625
Batch idx 927
Batch idx 928
Batch idx 929
Batch idx 930
Batch idx 931
Batch idx 932
Batch idx 933
Batch idx 934
Gradient norm: 0.0869140625
Batch idx 935
Batch idx 936
Batch idx 937
Batch idx 938
Batch idx 939
Batch idx 940
Batch idx 941
Batch idx 942
Gradient norm: 0.107421875
Batch idx 943
Batch idx 944
Batch idx 945
Batch idx 946
Batch idx 947
Batch idx 948
Batch idx 949
Epoch 0, Loss 1.995, LR 6.71e-06:  89%|████████▉ | 119/133 [3:40:54<25:31, 109.40s/it]Epoch 0, Loss 2.022, LR 6.49e-06:  89%|████████▉ | 119/133 [3:40:54<25:31, 109.40s/it]Epoch 0, Loss 2.022, LR 6.49e-06:  90%|█████████ | 120/133 [3:42:41<23:30, 108.49s/it]Epoch 0, Loss 1.935, LR 6.29e-06:  90%|█████████ | 120/133 [3:42:41<23:30, 108.49s/it]Epoch 0, Loss 1.935, LR 6.29e-06:  91%|█████████ | 121/133 [3:44:32<21:52, 109.34s/it]Epoch 0, Loss 1.990, LR 6.10e-06:  91%|█████████ | 121/133 [3:44:32<21:52, 109.34s/it]Epoch 0, Loss 1.990, LR 6.10e-06:  92%|█████████▏| 122/133 [3:46:17<19:50, 108.18s/it]Epoch 0, Loss 2.019, LR 5.93e-06:  92%|█████████▏| 122/133 [3:46:18<19:50, 108.18s/it]Epoch 0, Loss 2.019, LR 5.93e-06:  92%|█████████▏| 123/133 [3:48:02<17:50, 107.09s/it]Epoch 0, Loss 2.003, LR 5.77e-06:  92%|█████████▏| 123/133 [3:48:02<17:50, 107.09s/it]Epoch 0, Loss 2.003, LR 5.77e-06:  93%|█████████▎| 124/133 [3:50:03<16:40, 111.19s/it]Epoch 0, Loss 1.985, LR 5.62e-06:  93%|█████████▎| 124/133 [3:50:03<16:40, 111.19s/it]Epoch 0, Loss 1.985, LR 5.62e-06:  94%|█████████▍| 125/133 [3:51:56<14:55, 111.90s/it]Epoch 0, Loss 1.957, LR 5.49e-06:  94%|█████████▍| 125/133 [3:51:56<14:55, 111.90s/it]Epoch 0, Loss 1.957, LR 5.49e-06:  95%|█████████▍| 126/133 [3:53:44<12:54, 110.60s/it]Epoch 0, Loss 1.881, LR 5.38e-06:  95%|█████████▍| 126/133 [3:53:44<12:54, 110.60s/it]Epoch 0, Loss 1.881, LR 5.38e-06:  95%|█████████▌| 127/133 [3:55:41<11:14, 112.46s/it]Epoch 0, Loss 1.881, LR 5.28e-06:  95%|█████████▌| 127/133 [3:55:41<11:14, 112.46s/it]Epoch 0, Loss 1.881, LR 5.28e-06:  96%|█████████▌| 128/133 [3:57:34<09:23, 112.77s/it]Epoch 0, Loss 1.996, LR 5.19e-06:  96%|█████████▌| 128/133 [3:57:34<09:23, 112.77s/it]Epoch 0, Loss 1.996, LR 5.19e-06:  97%|█████████▋| 129/133 [3:59:40<07:46, 116.66s/it]Epoch 0, Loss 1.977, LR 5.12e-06:  97%|█████████▋| 129/133 [3:59:40<07:46, 116.66s/it]Epoch 0, Loss 1.977, LR 5.12e-06:  98%|█████████▊| 130/133 [4:01:36<05:49, 116.34s/it]Epoch 0, Loss 1.980, LR 5.07e-06:  98%|█████████▊| 130/133 [4:01:36<05:49, 116.34s/it]Epoch 0, Loss 1.980, LR 5.07e-06:  98%|█████████▊| 131/133 [4:03:11<03:40, 110.13s/it]Epoch 0, Loss 1.871, LR 5.03e-06:  98%|█████████▊| 131/133 [4:03:11<03:40, 110.13s/it]Epoch 0, Loss 1.871, LR 5.03e-06:  99%|█████████▉| 132/133 [4:04:50<01:46, 106.85s/it]Epoch 0, Loss 1.961, LR 5.01e-06:  99%|█████████▉| 132/133 [4:04:50<01:46, 106.85s/it]Epoch 0, Loss 1.961, LR 5.01e-06: 100%|██████████| 133/133 [4:06:49<00:00, 110.36s/it]Epoch 0, Loss 1.912, LR 5.00e-06: 100%|██████████| 133/133 [4:06:49<00:00, 110.36s/it]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.007 MB of 0.097 MB uploadedwandb: - 0.007 MB of 0.097 MB uploadedwandb: \ 0.097 MB of 0.097 MB uploadedwandb: | 0.097 MB of 0.097 MB uploadedwandb: / 0.097 MB of 0.097 MB uploadedwandb: - 0.097 MB of 0.097 MB uploadedwandb: \ 0.097 MB of 0.097 MB uploadedwandb: | 0.097 MB of 0.097 MB uploadedwandb: / 0.097 MB of 0.097 MB uploadedwandb: - 0.097 MB of 0.097 MB uploadedwandb: \ 0.097 MB of 0.097 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                            grad_norm ▇▇█▇▅▅▃▃▃▂▂▂▁▁▁▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                                 loss ▇▇█▇▅▅▄▅▄▄▃▃▃▃▂▃▂▂▂▃▃▁▃▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
wandb:                                   lr ▁▃▅▇██████▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:      memory/allocated_after_backward ▁
wandb:       memory/allocated_after_forward ▁
wandb: memory/allocated_after_model_created ▁
wandb:    memory/allocated_after_model_wrap ▁
wandb:      memory/allocated_before_forward ▁
wandb:                memory/allocated_peak ▁
wandb:       memory/reserved_after_backward ▁
wandb:        memory/reserved_after_forward ▁
wandb: memory/reserved_after_model_creation ▁
wandb:     memory/reserved_after_model_wrap ▁
wandb:       memory/reserved_before_forward ▁
wandb:                 memory/reserved_peak ▁
wandb:                           time_taken ▁
wandb: 
wandb: Run summary:
wandb:                            grad_norm 0.09863
wandb:                                 loss 1.91221
wandb:                                   lr 1e-05
wandb:      memory/allocated_after_backward 624744960
wandb:       memory/allocated_after_forward 3071496192
wandb: memory/allocated_after_model_created 8652800
wandb:    memory/allocated_after_model_wrap 111200768
wandb:      memory/allocated_before_forward 111201792
wandb:                memory/allocated_peak 7262825984
wandb:       memory/reserved_after_backward 9269411840
wandb:        memory/reserved_after_forward 9116319744
wandb: memory/reserved_after_model_creation 115343360
wandb:     memory/reserved_after_model_wrap 6259998720
wandb:       memory/reserved_before_forward 6259998720
wandb:                 memory/reserved_peak 9344909312
wandb:                           time_taken 14905.579
wandb: 
wandb: 🚀 View run solar-frog-69 at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/6aib0nqw
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240830_195728-6aib0nqw/logs
Rank 2: Model created: 0.107 GiB
Wrapping model w/ FSDP 2
Rank 2: Wrapped model: 5.830 GiB
Applying activation checkpointing 2
Rank 2: Before forward: 5.83 GiB
Rank 2: After forward: 8.49 GiB
Rank 2: After backward: 8.63 GiB
Rank 2: Peak allocated memory: 6.76 GiB
Rank 2: Peak reserved memory:  8.70 GiB
Rank 3: Model created: 0.107 GiB
Wrapping model w/ FSDP 3
Rank 3: Wrapped model: 5.830 GiB
Applying activation checkpointing 3
Rank 3: Before forward: 5.83 GiB
Rank 3: After forward: 8.49 GiB
Rank 3: After backward: 8.64 GiB
Rank 3: Peak allocated memory: 6.76 GiB
Rank 3: Peak reserved memory:  8.70 GiB
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (6aib0nqw) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
Epoch 0, Loss 1.912, LR 5.00e-06: 100%|██████████| 133/133 [4:08:41<00:00, 112.19s/it]
Rank 1: Model created: 0.107 GiB
Wrapping model w/ FSDP 1
Rank 1: Wrapped model: 5.830 GiB
Applying activation checkpointing 1
Rank 1: Before forward: 5.83 GiB
Rank 1: After forward: 8.49 GiB
Rank 1: After backward: 8.63 GiB
Rank 1: Peak allocated memory: 6.76 GiB
Rank 1: Peak reserved memory:  8.70 GiB
Batch idx 950
Gradient norm: 0.10009765625
Batch idx 951
Batch idx 952
Batch idx 953
Batch idx 954
Batch idx 955
Batch idx 956
Batch idx 957
Batch idx 958
Gradient norm: 0.10693359375
Batch idx 959
Batch idx 960
Batch idx 961
Batch idx 962
Batch idx 963
Batch idx 964
Batch idx 965
Batch idx 966
Gradient norm: 0.111328125
Batch idx 967
Batch idx 968
Batch idx 969
Batch idx 970
Batch idx 971
Batch idx 972
Batch idx 973
Batch idx 974
Gradient norm: 0.0927734375
Batch idx 975
Batch idx 976
Batch idx 977
Batch idx 978
Batch idx 979
Batch idx 980
Batch idx 981
Batch idx 982
Gradient norm: 0.1328125
Batch idx 983
Batch idx 984
Batch idx 985
Batch idx 986
Batch idx 987
Batch idx 988
Batch idx 989
Batch idx 990
Gradient norm: 0.12060546875
Batch idx 991
Batch idx 992
Batch idx 993
Batch idx 994
Batch idx 995
Batch idx 996
Batch idx 997
Batch idx 998
Gradient norm: 0.11328125
Batch idx 999
Batch idx 1000
Batch idx 1001
Batch idx 1002
Batch idx 1003
Batch idx 1004
Batch idx 1005
Batch idx 1006
Gradient norm: 0.1298828125
Batch idx 1007
Batch idx 1008
Batch idx 1009
Batch idx 1010
Batch idx 1011
Batch idx 1012
Batch idx 1013
Batch idx 1014
Gradient norm: 0.11767578125
Batch idx 1015
Batch idx 1016
Batch idx 1017
Batch idx 1018
Batch idx 1019
Batch idx 1020
Batch idx 1021
Batch idx 1022
Gradient norm: 0.1044921875
Batch idx 1023
Batch idx 1024
Batch idx 1025
Batch idx 1026
Batch idx 1027
Batch idx 1028
Batch idx 1029
Batch idx 1030
Gradient norm: 0.1259765625
Batch idx 1031
Batch idx 1032
Batch idx 1033
Batch idx 1034
Batch idx 1035
Batch idx 1036
Batch idx 1037
Batch idx 1038
Gradient norm: 0.11181640625
Batch idx 1039
Batch idx 1040
Batch idx 1041
Batch idx 1042
Batch idx 1043
Batch idx 1044
Batch idx 1045
Batch idx 1046
Gradient norm: 0.1298828125
Batch idx 1047
Batch idx 1048
Batch idx 1049
Batch idx 1050
Batch idx 1051
Batch idx 1052
Batch idx 1053
Batch idx 1054
Gradient norm: 0.103515625
Batch idx 1055
Batch idx 1056
Batch idx 1057
Batch idx 1058
Batch idx 1059
Batch idx 1060
Batch idx 1061
Batch idx 1062
Gradient norm: 0.0986328125
Batch idx 1063
Batch idx 1064
Batch idx 1065
Batch idx 1066
Batch idx 1067
Batch idx 1068
Batch idx 1069
Saving full model weights.
Done 0
Finished training 0
CUDA event elapsed time: 14905.579 sec
Rank 0: Before forward: 5.83 GiB
Rank 0: After forward: 8.49 GiB
Rank 0: After backward: 8.63 GiB
Rank 0: Peak allocated memory: 6.76 GiB
Rank 0: Peak reserved memory:  8.70 GiB
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.97s/it]
name : base_model.model.model.embed_tokens.weight | dtype : torch.bfloat16 | shape : torch.Size([32768, 4096]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.0.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.0.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.1.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.1.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.2.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.2.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.3.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.3.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.4.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.4.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.5.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.5.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.6.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.6.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.7.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.7.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.8.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.8.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.9.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.9.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.10.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.10.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.11.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.11.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.12.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.12.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.13.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.13.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.14.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.14.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.15.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.15.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.16.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.16.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.17.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.17.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.18.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.18.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.19.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.19.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.20.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.20.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.21.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.21.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.22.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.22.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.23.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.23.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.24.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.24.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.25.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.25.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.26.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.26.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.27.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.27.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.28.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.28.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.29.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.29.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.30.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.30.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([2097152, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([1024, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.self_attn.o_proj.weight | dtype : torch.uint8 | shape : torch.Size([8388608, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.up_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 4096]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([14336, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.down_proj.base_layer.weight | dtype : torch.uint8 | shape : torch.Size([29360128, 1]) | requires_grad : False 


name : base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight | dtype : torch.bfloat16 | shape : torch.Size([32, 14336]) | requires_grad : True 


name : base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight | dtype : torch.bfloat16 | shape : torch.Size([4096, 32]) | requires_grad : True 


name : base_model.model.model.layers.31.input_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.layers.31.post_attention_layernorm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.model.norm.weight | dtype : torch.bfloat16 | shape : torch.Size([4096]) | requires_grad : False 


name : base_model.model.lm_head.weight | dtype : torch.bfloat16 | shape : torch.Size([32768, 4096]) | requires_grad : False 


All tensors are equal
---------------------------------------------------
Dataset size : 554
example of the dataset
input_ids : tensor([[    1,     3, 12786,  ..., 29491, 29473,     4]])
prompt : <s>[INST] Write a summary of the source text. The summary should be short in length. The length is defined in terms of number of words used in the summary. The source text is given below.  (CNN)Jackson Gordon is no ordinary 21-year-old. By day he is an industrial design student at Philadelphia University, but Gordon has another side to him -- a side altogether darker, tougher and more enigmatic. Hanging in his workshop Gordon has a full suit of armor plating, cape and cowl -- matte black and built to stop a knife. Gordon has an alter ego: the Dark Knight himself, Batman. You might expect his origin story to be cloaked in mystery, but speaking to CNN Gordon is quick to explain how the transformation took place. Gordon says his calling came five years ago when he began experimenting with cosplay. "Previously I'd been involved with costume making... I'd made a version of the Batsuit from Christopher Nolan's 'Dark Knight Trilogy' and I really liked that suit," Gordon says. But, as elaborate as his design was, it lacked the functionality or the authenticity of the genuine article. "I was frustrated every time I wore it," Gordon explains. "It really limited my mobility and I didn't like that -- it didn't go with the character." In September 2014 he bit the bullet, deciding "to do another one that wouldn't inhibit my mobility and would actually provide protection and function more like Batman's actual suit." The Batsuit had to be strong -- tough enough to withstand the stab or slash of a knife, the impact of a punch or a baseball bat, but light and articulate enough to make it practical. Striking such a balance required expensive materials, and they didn't come cheap. Gordon therefore fired up a Kickstarter campaign. He "didn't really think anyone would fund it or even be interested in it" -- he raised $1,255 in 6 days. "It was a little surprising," Gordon demurs. Writing out his shopping list, it was important that "everywhere, even places without armor plating, had some sort of protection." Kevlar was sourced as the base fabric, making it "cut and slash resistant to bladed weapons, but breathable and wearable all day." Eschewing conventional materials, Gordon opted for a form of memory foam, built around key areas to "squish and compress," dissipating the impact of blows. After much experimenting with "polycarbonates and extruded PVC materials," ¼" Kydex (or ABS) plastic formed the tough armor plates, located on the torso, forearms and shins. Stab resistant, Gordon says "it can take anything but a gunshot." The cowl was more problematic, being "nearly impossible" to craft out of the same materials within the limits of his workshop. Gordon therefore took a mold of his head using Sintra plastic, "working on top of that with different sculpting clays and soft plastics to get it into a recognizable Batman shape." Using a two part box mold Gordon was able to create a "silicone jacket" of this, into which liquid polyurethane was poured, forming the final, "durable and functional" cowl. Gordon (who doesn't appear to be related to Gotham City's police commissioner, James Gordon) is also an expert in Shaolin Kung Fu: he is both brains and brawn, a cross between Bruce Wayne and Batsuit designer Lucius Fox from Nolan's Batman trilogy. Legendary, the production company behind the films, has taken note of his design and given it their seal of approval. The Batsuit has made appearances at conventions and proved a showstopper among his fellow students and the faculty. "People love the theatricality of it," its designer says. That the product so closely mimics DC's fantastical comic book creation has had resonance. He has already begun manufacturing the cowls for the public, with "fully adjustable" jackets going up for sale on his site Armatus Design "in the next couple of weeks." The jackets have received particular attention. Gordon has received "easily over 50 requests from people," and not just from the cosplay community. "They range from recreational use to martial artists... but also motorcycle and All Terrain Vehicle riders who want protective gear and prefer the look and functionality of [Gordon's] suit." Perhaps because of their versatility and the small matter of copyright issues, those that go on sale will not feature the iconic bat symbol. Gordon says his fledgling business will remain small whilst he's at University -- he has to finish he studies after all, and won't be using the project towards his degree credits. For now the Batsuit and Armatus Design will remain a one man operation: such is the life of a superhero. [/INST]
reference : Jackson Gordon, a 21-year-old industrial design student at Philadelphia University built a Batsuit that is resistant to stabs, knife slashes, and high impacts. According to Gordon, this is a second attempt at building the suit after an earlier attempt five years ago.
---------------------------------------------------
Generating summaries: 0it [00:00, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Generating summaries: 1it [00:20, 20.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 2it [00:52, 27.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 3it [01:24, 29.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 4it [01:56, 30.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 5it [02:28, 30.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 6it [02:50, 27.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 7it [03:22, 29.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 8it [03:52, 29.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 9it [04:25, 30.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 10it [04:58, 31.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 11it [05:31, 31.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 12it [06:04, 32.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 13it [06:28, 29.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 14it [07:01, 30.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 15it [07:20, 27.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 16it [07:45, 26.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 17it [08:18, 28.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 18it [08:40, 26.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 19it [09:05, 26.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 20it [09:28, 25.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 21it [09:49, 23.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 22it [10:09, 22.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 23it [10:29, 21.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 24it [10:59, 24.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 25it [11:21, 23.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 26it [11:47, 24.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 27it [12:04, 22.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 28it [12:22, 20.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 29it [12:53, 23.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 30it [13:07, 21.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 31it [13:20, 18.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 32it [13:35, 17.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 33it [13:57, 18.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 34it [14:29, 22.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 35it [14:53, 23.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 36it [15:08, 20.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 37it [15:28, 20.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 38it [15:52, 21.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 39it [16:24, 24.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 40it [16:57, 27.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 41it [17:29, 28.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 42it [17:53, 27.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 43it [18:16, 26.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 44it [18:35, 23.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 45it [18:57, 23.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 46it [19:28, 25.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 47it [19:47, 23.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 48it [20:12, 23.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 49it [20:32, 22.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 50it [20:55, 22.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 51it [21:31, 26.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 52it [22:06, 29.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 53it [22:32, 28.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 54it [22:59, 27.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 55it [23:34, 30.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 56it [24:10, 31.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 57it [24:32, 28.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 58it [24:53, 26.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 59it [25:22, 27.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 60it [25:49, 27.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 61it [26:11, 25.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 62it [26:40, 26.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 63it [27:02, 25.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 64it [27:28, 25.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 65it [27:52, 25.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 66it [28:20, 25.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 67it [28:43, 25.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 68it [29:11, 25.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 69it [29:38, 26.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 70it [30:10, 28.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 71it [30:41, 28.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 72it [31:02, 26.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 73it [31:35, 28.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 74it [31:57, 26.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 75it [32:31, 28.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 76it [33:05, 30.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 77it [33:38, 31.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 78it [34:12, 32.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 79it [34:37, 29.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 80it [35:02, 28.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 81it [35:36, 30.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 82it [36:10, 31.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 83it [36:30, 27.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 84it [36:54, 26.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 85it [37:25, 27.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 86it [37:47, 26.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 87it [38:10, 25.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 88it [38:30, 23.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 89it [39:00, 25.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 90it [39:26, 25.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 91it [39:49, 24.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 92it [40:12, 24.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 93it [40:36, 24.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 94it [41:10, 27.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 95it [41:40, 28.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 96it [42:02, 26.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 97it [42:27, 25.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 98it [43:01, 28.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 99it [43:20, 25.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 100it [43:44, 25.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 101it [44:07, 24.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 102it [44:25, 22.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 103it [44:57, 25.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 104it [45:14, 22.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 105it [45:46, 25.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 106it [46:02, 22.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 107it [46:30, 24.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 108it [47:00, 26.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 109it [47:22, 24.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 110it [47:37, 21.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 111it [47:54, 20.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 112it [48:22, 22.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 113it [48:53, 25.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 114it [49:20, 25.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 115it [49:44, 25.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 116it [50:02, 23.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 117it [50:29, 24.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 118it [51:01, 26.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 119it [51:25, 25.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 120it [51:47, 24.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 121it [52:12, 24.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 122it [52:33, 23.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 123it [53:04, 25.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 124it [53:35, 27.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 125it [53:55, 25.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 126it [54:15, 23.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 127it [54:39, 23.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 128it [55:12, 26.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 129it [55:36, 25.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 130it [56:09, 27.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 131it [56:37, 27.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 132it [57:07, 28.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 133it [57:22, 24.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 134it [57:45, 24.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 135it [58:14, 25.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 136it [58:28, 21.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 137it [58:41, 19.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 138it [59:03, 20.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 139it [59:22, 19.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 140it [59:54, 23.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 141it [1:00:21, 24.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 142it [1:00:51, 26.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 143it [1:01:13, 24.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 144it [1:01:36, 24.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 145it [1:02:08, 26.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 146it [1:02:33, 26.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 147it [1:02:50, 23.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 148it [1:03:13, 23.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 149it [1:03:37, 23.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 150it [1:04:08, 25.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 151it [1:04:40, 27.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 152it [1:05:13, 29.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 153it [1:05:44, 29.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 154it [1:06:16, 30.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 155it [1:06:34, 26.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 156it [1:06:53, 24.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 157it [1:07:25, 26.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 158it [1:07:51, 26.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 159it [1:08:23, 28.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 160it [1:08:40, 24.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 161it [1:09:03, 24.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 162it [1:09:37, 27.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 163it [1:10:04, 27.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 164it [1:10:29, 26.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 165it [1:10:55, 26.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 166it [1:11:28, 28.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 167it [1:11:45, 24.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 168it [1:12:16, 26.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 169it [1:12:47, 28.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 170it [1:13:08, 25.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 171it [1:13:28, 24.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 172it [1:13:46, 22.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 173it [1:14:10, 22.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 174it [1:14:43, 25.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 175it [1:15:11, 26.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 176it [1:15:35, 25.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 177it [1:16:05, 27.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 178it [1:16:26, 25.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 179it [1:16:46, 23.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 180it [1:17:05, 22.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 181it [1:17:27, 22.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 182it [1:17:56, 24.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 183it [1:18:23, 25.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 184it [1:18:44, 23.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 185it [1:19:16, 26.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 186it [1:19:31, 22.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 187it [1:19:47, 20.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 188it [1:20:18, 23.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 189it [1:20:37, 22.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 190it [1:20:58, 21.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 191it [1:21:14, 20.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 192it [1:21:35, 20.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 193it [1:22:04, 23.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 194it [1:22:25, 22.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 195it [1:22:56, 24.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 196it [1:23:07, 20.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 197it [1:23:21, 18.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 198it [1:23:52, 22.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 199it [1:24:10, 21.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 200it [1:24:40, 23.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 201it [1:25:05, 24.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 202it [1:25:24, 22.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 203it [1:25:55, 25.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 204it [1:26:25, 26.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 205it [1:26:57, 28.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 206it [1:27:15, 25.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 207it [1:27:45, 26.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 208it [1:28:17, 28.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 209it [1:28:47, 28.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 210it [1:29:19, 29.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 211it [1:29:51, 30.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 212it [1:30:23, 30.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 213it [1:30:35, 25.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 214it [1:30:49, 21.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 215it [1:31:07, 20.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 216it [1:31:30, 21.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 217it [1:32:00, 23.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 218it [1:32:33, 26.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 219it [1:33:05, 28.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 220it [1:33:38, 29.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 221it [1:34:06, 29.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 222it [1:34:39, 30.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 223it [1:34:56, 26.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 224it [1:35:19, 25.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 225it [1:35:51, 27.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 226it [1:36:18, 27.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 227it [1:36:50, 28.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 228it [1:37:09, 25.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 229it [1:37:30, 24.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 230it [1:38:02, 26.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 231it [1:38:29, 26.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 232it [1:39:00, 28.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 233it [1:39:16, 24.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 234it [1:39:44, 25.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 235it [1:40:16, 27.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 236it [1:40:34, 24.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 237it [1:40:55, 23.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 238it [1:41:22, 24.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 239it [1:41:46, 24.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 240it [1:42:19, 26.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 241it [1:42:44, 26.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 242it [1:43:12, 26.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 243it [1:43:38, 26.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 244it [1:44:11, 28.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 245it [1:44:30, 25.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 246it [1:44:48, 23.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 247it [1:45:18, 25.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 248it [1:45:49, 27.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 249it [1:46:14, 26.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 250it [1:46:52, 29.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 251it [1:47:17, 28.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 252it [1:47:55, 31.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 253it [1:48:23, 30.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 254it [1:48:47, 28.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 255it [1:49:19, 29.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 256it [1:49:55, 31.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 257it [1:50:19, 29.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 258it [1:50:57, 31.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 259it [1:51:11, 26.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 260it [1:51:28, 23.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 261it [1:51:48, 22.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 262it [1:52:11, 22.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 263it [1:52:36, 23.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 264it [1:53:07, 25.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 265it [1:53:29, 24.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 266it [1:53:48, 22.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 267it [1:54:09, 22.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 268it [1:54:35, 23.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 269it [1:55:07, 26.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 270it [1:55:39, 27.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 271it [1:56:11, 29.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 272it [1:56:40, 28.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 273it [1:57:07, 28.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 274it [1:57:25, 25.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 275it [1:57:52, 25.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 276it [1:58:18, 26.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 277it [1:58:46, 26.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 278it [1:59:17, 27.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 279it [1:59:31, 23.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 280it [1:59:44, 20.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 281it [2:00:11, 22.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 282it [2:00:30, 21.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 283it [2:00:56, 22.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 284it [2:01:19, 22.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 285it [2:01:52, 25.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 286it [2:02:25, 28.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 287it [2:02:53, 28.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 288it [2:03:21, 27.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 289it [2:03:54, 29.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 290it [2:04:20, 28.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 291it [2:04:33, 23.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 292it [2:05:00, 24.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 293it [2:05:31, 26.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 294it [2:05:51, 24.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 295it [2:06:20, 25.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 296it [2:06:53, 28.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 297it [2:07:18, 27.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 298it [2:07:42, 26.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 299it [2:08:01, 23.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 300it [2:08:20, 22.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 301it [2:08:43, 22.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 302it [2:09:09, 23.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 303it [2:09:41, 26.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 304it [2:10:05, 25.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 305it [2:10:31, 25.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 306it [2:10:58, 26.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 307it [2:11:18, 24.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 308it [2:11:37, 22.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 309it [2:12:01, 23.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 310it [2:12:29, 24.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 311it [2:12:51, 23.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 312it [2:13:10, 22.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 313it [2:13:26, 20.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 314it [2:13:51, 21.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 315it [2:14:23, 24.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 316it [2:14:43, 23.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 317it [2:15:15, 25.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 318it [2:15:39, 25.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 319it [2:15:57, 23.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 320it [2:16:30, 26.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 321it [2:16:55, 25.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 322it [2:17:28, 27.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 323it [2:17:50, 26.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 324it [2:18:22, 27.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 325it [2:18:39, 24.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 326it [2:19:08, 26.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 327it [2:19:40, 27.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 328it [2:20:08, 27.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 329it [2:20:29, 25.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 330it [2:20:57, 26.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 331it [2:21:22, 25.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 332it [2:21:37, 22.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 333it [2:21:57, 22.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 334it [2:22:22, 22.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 335it [2:22:46, 23.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 336it [2:23:00, 20.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 337it [2:23:23, 21.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 338it [2:23:51, 23.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 339it [2:24:23, 25.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 340it [2:24:50, 26.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 341it [2:25:07, 23.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 342it [2:25:29, 22.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 343it [2:25:48, 21.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 344it [2:26:06, 20.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 345it [2:26:32, 22.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 346it [2:27:03, 24.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 347it [2:27:19, 22.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 348it [2:27:50, 24.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 349it [2:28:07, 22.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 350it [2:28:22, 20.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 351it [2:28:49, 22.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 352it [2:29:07, 20.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 353it [2:29:23, 19.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 354it [2:29:43, 19.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 355it [2:30:12, 22.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 356it [2:30:43, 25.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 357it [2:31:07, 24.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 358it [2:31:39, 26.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 359it [2:31:58, 24.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 360it [2:32:15, 22.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 361it [2:32:42, 23.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 362it [2:33:05, 23.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 363it [2:33:26, 22.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 364it [2:33:58, 25.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 365it [2:34:17, 23.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 366it [2:34:38, 22.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 367it [2:34:53, 20.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 368it [2:35:14, 20.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 369it [2:35:45, 23.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 370it [2:36:08, 23.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 371it [2:36:28, 22.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 372it [2:36:43, 20.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 373it [2:37:01, 19.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 374it [2:37:30, 22.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 375it [2:37:49, 21.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 376it [2:38:11, 21.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 377it [2:38:23, 18.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 378it [2:38:38, 17.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 379it [2:39:00, 18.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 380it [2:39:12, 16.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 381it [2:39:35, 18.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 382it [2:39:57, 19.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 383it [2:40:27, 22.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 384it [2:40:49, 22.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 385it [2:41:15, 23.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 386it [2:41:39, 23.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 387it [2:41:57, 22.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 388it [2:42:29, 25.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 389it [2:43:02, 27.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 390it [2:43:24, 25.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 391it [2:43:45, 24.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 392it [2:44:08, 23.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 393it [2:44:25, 21.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 394it [2:44:49, 22.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 395it [2:45:21, 25.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 396it [2:45:54, 27.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 397it [2:46:19, 26.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 398it [2:46:52, 28.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 399it [2:47:19, 28.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 400it [2:47:51, 29.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 401it [2:48:17, 28.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 402it [2:48:42, 27.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 403it [2:49:13, 28.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 404it [2:49:32, 25.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 405it [2:50:03, 27.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 406it [2:50:22, 24.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 407it [2:50:48, 25.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 408it [2:51:19, 26.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 409it [2:51:45, 26.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 410it [2:52:16, 27.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 411it [2:52:36, 25.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 412it [2:53:08, 27.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 413it [2:53:40, 28.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 414it [2:54:12, 29.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 415it [2:54:42, 29.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 416it [2:55:15, 30.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 417it [2:55:34, 27.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 418it [2:55:51, 24.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 419it [2:56:18, 25.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 420it [2:56:49, 26.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 421it [2:57:10, 24.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 422it [2:57:29, 23.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 423it [2:57:47, 21.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 424it [2:58:20, 25.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 425it [2:58:53, 27.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 426it [2:59:19, 26.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 427it [2:59:37, 24.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 428it [2:59:59, 23.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 429it [3:00:24, 24.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 430it [3:00:50, 24.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 431it [3:01:21, 26.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 432it [3:01:56, 29.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 433it [3:02:22, 27.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 434it [3:02:57, 30.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 435it [3:03:18, 27.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 436it [3:03:48, 28.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 437it [3:04:20, 29.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 438it [3:04:40, 26.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 439it [3:05:13, 28.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 440it [3:05:36, 26.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 441it [3:05:45, 21.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 442it [3:05:56, 18.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 443it [3:06:17, 19.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 444it [3:06:26, 16.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 445it [3:06:53, 19.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 446it [3:07:06, 17.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 447it [3:07:26, 18.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 448it [3:07:57, 21.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 449it [3:08:20, 22.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 450it [3:08:38, 21.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 451it [3:08:56, 20.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 452it [3:09:18, 20.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 453it [3:09:49, 23.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 454it [3:10:15, 24.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 455it [3:10:34, 22.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 456it [3:10:48, 20.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 457it [3:11:08, 19.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 458it [3:11:30, 20.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 459it [3:11:48, 20.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 460it [3:12:08, 19.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 461it [3:12:23, 18.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 462it [3:12:50, 21.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 463it [3:13:21, 23.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 464it [3:13:44, 23.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 465it [3:14:06, 23.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 466it [3:14:27, 22.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 467it [3:14:51, 22.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 468it [3:15:23, 25.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 469it [3:15:45, 24.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 470it [3:16:13, 25.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 471it [3:16:36, 24.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 472it [3:16:55, 23.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 473it [3:17:12, 21.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 474it [3:17:34, 21.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 475it [3:18:08, 25.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 476it [3:18:34, 25.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 477it [3:18:57, 24.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 478it [3:19:22, 24.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 479it [3:19:56, 27.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 480it [3:20:30, 29.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 481it [3:20:57, 28.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 482it [3:21:13, 24.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 483it [3:21:33, 23.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 484it [3:22:04, 25.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 485it [3:22:31, 26.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 486it [3:23:00, 26.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 487it [3:23:19, 24.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 488it [3:23:38, 23.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 489it [3:24:06, 24.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 490it [3:24:30, 24.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 491it [3:25:02, 26.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 492it [3:25:19, 23.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 493it [3:25:43, 23.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 494it [3:26:15, 26.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 495it [3:26:39, 25.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 496it [3:27:02, 25.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 497it [3:27:20, 22.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 498it [3:27:41, 22.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 499it [3:28:14, 25.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 500it [3:28:35, 24.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 501it [3:29:08, 26.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 502it [3:29:40, 28.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 503it [3:30:13, 29.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 504it [3:30:34, 27.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 505it [3:30:55, 25.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 506it [3:31:27, 27.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 507it [3:31:48, 25.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 508it [3:32:12, 25.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 509it [3:32:31, 22.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 510it [3:32:53, 22.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 511it [3:33:25, 25.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 512it [3:33:52, 26.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 513it [3:34:14, 24.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 514it [3:34:40, 25.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 515it [3:35:12, 27.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 516it [3:35:45, 28.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 517it [3:36:12, 28.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 518it [3:36:44, 29.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 519it [3:37:17, 30.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 520it [3:37:49, 31.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 521it [3:38:02, 25.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 522it [3:38:21, 23.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 523it [3:38:51, 25.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 524it [3:39:12, 24.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 525it [3:39:31, 22.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 526it [3:39:51, 21.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 527it [3:40:16, 22.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 528it [3:40:48, 25.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 529it [3:41:14, 25.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 530it [3:41:46, 27.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 531it [3:42:10, 26.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 532it [3:42:41, 27.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 533it [3:43:02, 25.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 534it [3:43:27, 25.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 535it [3:43:59, 27.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 536it [3:44:16, 24.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 537it [3:44:36, 23.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 538it [3:45:08, 25.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 539it [3:45:40, 27.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 540it [3:45:53, 23.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 541it [3:46:12, 22.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 542it [3:46:40, 23.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 543it [3:46:57, 21.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 544it [3:47:23, 23.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 545it [3:47:35, 19.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 546it [3:47:50, 18.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 547it [3:48:21, 22.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 548it [3:48:37, 20.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 549it [3:48:50, 18.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 550it [3:49:11, 19.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 551it [3:49:43, 22.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 552it [3:50:15, 25.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 553it [3:50:38, 24.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Generating summaries: 554it [3:51:10, 26.93s/it]Generating summaries: 554it [3:51:10, 25.04s/it]
Total time taken for generating summaries : 13870.265419006348
Average time taken for generating summaries : 25.03658017871182
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]
Traceback (most recent call last):
  File "/home2/tathagato/summarization/MACSUM/fsdp_lora/zero_shot_inference.py", line 143, in <module>
All tensors are equal
---------------------------------------------------
Dataset size : 554
example of the dataset
    input_ids = dataset[0]["input_ids"]
  File "/home2/tathagato/summarization/MACSUM/fsdp_lora/dataset.py", line 125, in __getitem__
    self.tokenizer.encode(prompt, add_special_tokens = False), dtype=torch.int64
UnboundLocalError: local variable 'prompt' referenced before assignment
Completed
