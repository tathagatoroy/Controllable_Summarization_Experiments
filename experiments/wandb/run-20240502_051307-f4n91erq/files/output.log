size of the dataset:  4278
loading the model and the tokenizer and preparing peft training
loading from /scratch/tathagato/openelm_adapter experiments/2024-05-01-19-01-27_extractiveness/checkpoint-8556
Traceback (most recent call last):
  File "/home2/tathagato/summarization/MACSum/experiments/qlora_train.py", line 154, in <module>
    model = AutoModelForCausalLM.from_pretrained(args.previous_model_path,
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 558, in from_pretrained
    return model_class.from_pretrained(
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2977, in from_pretrained
    raise ValueError(
ValueError: You can't pass `load_in_4bit`or `load_in_8bit` as a kwarg when passing `quantization_config` argument at the same time.