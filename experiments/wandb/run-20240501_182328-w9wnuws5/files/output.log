size of the dataset:  4278
loading the model and the tokenizer and preparing peft training
config.json: 100%|██████████████████████████████████████████████████████| 1.41k/1.41k [00:00<00:00, 520kB/s]
configuration_openelm.py: 100%|████████████████████████████████████████| 14.3k/14.3k [00:00<00:00, 6.21MB/s]
A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-450M-Instruct:
- configuration_openelm.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
modeling_openelm.py: 100%|██████████████████████████████████████████████| 39.3k/39.3k [00:00<00:00, 191kB/s]
A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-450M-Instruct:
- modeling_openelm.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.

















model.safetensors: 100%|█████████████████████████████████████████████████| 914M/914M [00:34<00:00, 26.9MB/s]
generation_config.json: 100%|██████████████████████████████████████████████| 111/111 [00:00<00:00, 34.5kB/s]
Map:   0%|                                                                  | 0/4278 [00:00<?, ? examples/s]
trainable params: 2101248 || all params: 255299584 || trainable%: 0.8230518699160905

Map:  94%|█████████████████████████████████████████████████▌   | 4000/4278 [00:02<00:00, 1705.47 examples/s]
prepare the model for distributed training
Map: 100%|█████████████████████████████████████████████████████| 4278/4278 [00:02<00:00, 1653.58 examples/s]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|                                                                              | 0/7130 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '



  0%|                                                                    | 4/7130 [00:23<9:21:34,  4.73s/it]
  0%|                                                                    | 4/7130 [00:23<9:21:34,  4.73s/it]Traceback (most recent call last):
  File "/home2/tathagato/summarization/MACSum/experiments/qlora_train.py", line 195, in <module>
    trainer.train()
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py", line 361, in train
    output = super().train(*args, **kwargs)
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/transformers/trainer.py", line 2118, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/transformers/trainer.py", line 3036, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/transformers/trainer.py", line 3059, in compute_loss
    outputs = model(**inputs)
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 189, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/nn/parallel/replicate.py", line 125, in replicate
    buffer_copies_not_rg = _broadcast_coalesced_reshape(buffers_not_rg, devices, detach=True)
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/nn/parallel/replicate.py", line 79, in _broadcast_coalesced_reshape
    return comm.broadcast_coalesced(tensors, devices)
  File "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 57, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
RuntimeError: All tensors must be on devices[0]: 0