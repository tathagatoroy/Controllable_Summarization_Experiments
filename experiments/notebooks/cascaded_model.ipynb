{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.nn import Linear4bit\n",
    "import bitsandbytes as bnb\n",
    "import tqdm\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig,PeftConfig, PeftModel, PeftModelForCausalLM\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "import safetensors\n",
    "import torch.nn as nn\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "safetensor_path = \"/scratch/tathagato/redo_adapter_experiments/length/length/adapter_model.safetensors\"\n",
    "adapter_path = \"/scratch/tathagato/redo_adapter_experiments/length/length/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/murali/miniconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec7020482604d4c82a952bc4f7ae628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaccaa040dfc42eba3e4f981bc0bc421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/murali/miniconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da73894150e4e10b68e8c327b6cf67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b6ec29b2dc4457b5cee5dd5ea97005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d74f8880c7e4af5bfedd0162cac3aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ae9937d01040008a07ebbb7babf282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0903a551243c40ad82f2dae74d12df3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "model_kwargs = dict(\n",
    "        use_cache=False,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=None,\n",
    "        cache_dir = \"/scratch/tathagato\",\n",
    "        attn_implementation = \"eager\",\n",
    "        quantization_config = nf4_config, \n",
    "\n",
    "    )\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model,cache_dir = \"/scratch/tathagato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n",
      "torch.Size([256, 2048])\n"
     ]
    }
   ],
   "source": [
    "non_quantized_model = AutoModelForCausalLM.from_pretrained(base_model, trust_remote_code=True, use_cache=False, cache_dir = \"/scratch/tathagato\")\n",
    "print(non_quantized_model)\n",
    "print(non_quantized_model.model.layers[0].self_attn.k_proj.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CascadedLoRALinear4bit(torch.nn.Module):\n",
    "    def __init__(self, linear, in_dim, out_dim, rank_1 = 64, rank_2 = 32, alpha_1 = 16, alpha_2 = 16, adapter_name = \"default\" , dropout = None):\n",
    "        super().__init__()\n",
    "        self.base_layer = linear\n",
    "        std_dev_1 = 1 / torch.sqrt(torch.tensor(rank_1).float())\n",
    "        std_dev_2 = 1 / torch.sqrt(torch.tensor(rank_2).float())\n",
    "        if dropout is not None:\n",
    "            self.lora_dropout = nn.ModuleDict(\n",
    "                {\n",
    "                    adapter_name : torch.nn.Dropout(dropout)\n",
    "                }\n",
    "            )\n",
    "        #first dimension\n",
    "        self.lora_A = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(in_dim, rank_1, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_B = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(rank_1, out_dim, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_A[adapter_name].weight = torch.nn.Parameter(torch.randn(rank_1, in_dim) * std_dev_1)\n",
    "        self.lora_B[adapter_name].weight = torch.nn.Parameter(torch.zeros(out_dim, rank_1))  \n",
    "\n",
    "        self.lora_A1 = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(in_dim, rank_2, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_A2 = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(rank_2, rank_1, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_B1 = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(rank_1, rank_2, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_B2 = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(rank_2, out_dim, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_A1[adapter_name].weight = torch.nn.Parameter(torch.randn(rank_2, in_dim) * std_dev_2)\n",
    "        self.lora_A2[adapter_name].weight = torch.nn.Parameter(torch.zeros(rank_1, rank_2))\n",
    "        self.lora_B1[adapter_name].weight = torch.nn.Parameter(torch.zeros(rank_2, rank_1))\n",
    "        self.lora_B2[adapter_name].weight = torch.nn.Parameter(torch.zeros(out_dim, rank_2) * std_dev_2)  \n",
    "        self.alpha_1 = alpha_1\n",
    "        self.alpha_2 = alpha_2\n",
    "        self.rank_1 = rank_1\n",
    "        self.rank_2 = rank_2\n",
    "        self.is_second_layar_being_trained = False\n",
    "        self.is_first_layer_being_trained = False\n",
    "        self.is_first_layer_being_used_for_inference = True\n",
    "        self.is_first_layer_being_used_for_inference = True\n",
    "        self.scaling_1 = self.rank_1 / self.alpha_1\n",
    "        self.scaling_2 = self.rank_2 / self.alpha_1\n",
    "        self.adapter_name = adapter_name\n",
    "\n",
    "\n",
    "\n",
    "    def set_gradients_for_all_layer(self):\n",
    "        if self.is_second_layar_being_trained:\n",
    "            self.lora_A1[self.adapter_name].requires_grad = True\n",
    "            self.lora_A2[self.adapter_name].requires_grad = True\n",
    "            self.lora_B1[self.adapter_name].requires_grad = True\n",
    "            self.lora_B2[self.adapter_name].requires_grad = True\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.lora_A1[self.adapter_name].requires_grad = False\n",
    "            self.lora_A2[self.adapter_name].requires_grad = False\n",
    "            self.lora_B1[self.adapter_name].requires_grad = False\n",
    "            self.lora_B2[self.adapter_name].requires_grad = False\n",
    "            \n",
    "        if self.is_first_layer_being_trained:\n",
    "            self.lora_A[self.adapter_name].requires_grad = True\n",
    "            self.lora_B[self.adapter_name].requires_grad = True\n",
    "        else:\n",
    "            self.lora_A[self.adapter_name].requires_grad = False\n",
    "            self.lora_B[self.adapter_name].requires_grad = False  \n",
    "    \n",
    "    def tune_the_first_adapter(self):\n",
    "        self.is_first_layer_being_trained = True\n",
    "    \n",
    "    def freeze_the_first_adapter(self):\n",
    "        self.is_first_layer_being_trained = False\n",
    "    \n",
    "    def tune_the_second_adapter(self):\n",
    "        self.is_second_layar_being_trained = True\n",
    "    \n",
    "    def freeze_the_second_adapter(self):\n",
    "        self.is_second_layar_being_trained = False\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.set_gradients_for_all_layer()\n",
    "        if self.is_first_layer_being_used_for_inference and self.is_second_layer_being_used_for_inference:\n",
    "            #x = self.scaling_1 * (x @ self.W1_a @ self.W1_b) + self.scaling_2 * (x @ self.W2_a1 @ self.W2_a2 @ self.W2_b1 @ self.W2_b2)\n",
    "            output  = self.linear(x) + self.scaling_1 * (self.W1['A'](self.W1['B'](x))) + self.scaling_2 * (self.W2['B2'](self.W2['A2'](self.W2['B1'](self.W2['A1'](x)))))\n",
    "        if self.is_first_layer_being_used_for_inference and not self.is_second_layer_being_used_for_inference:\n",
    "            #x = self.scaling_2 * (x @ self.W2_a1 @ self.W2_a2) \n",
    "            output  =  self.linear(x)  + self.scaling_1 * (self.W1['A'](self.W1['B'](x))) \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rank_1 = 64\n",
    "rank_2 = 32\n",
    "alpha_1 = 16\n",
    "alpha_2 = 16\n",
    "adapter_name = \"test\"\n",
    "dropout = 0.05\n",
    "target_modules = [\n",
    "                    'q_proj',\n",
    "                    'k_proj',\n",
    "                    'v_proj',\n",
    "                    'o_proj',\n",
    "                    'gate_proj',\n",
    "                    'up_proj',\n",
    "                    'down_proj'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_cascaded_lora(module, target_modules = target_modules, rank_1 = 64, rank_2 = 32, alpha_1 = 16 , alpha_2 = 16 , adapter_name = \"default\" , dropout = None):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, bnb.nn.Linear4bit) and name in target_modules:\n",
    "            #setattr(module, name, CascadedLoRALinear4bit(child, in_dim, out_dim, **kwargs))\n",
    "            #print(name)\n",
    "            #print(child.in_features, child.out_features)\n",
    "            #get the device of the child\n",
    "            setattr(module, name, CascadedLoRALinear4bit(child, child.in_features, child.out_features, rank_1, rank_2, alpha_1, alpha_2, adapter_name , dropout = dropout))\n",
    "            #put everything in device \n",
    "        else:\n",
    "            replace_with_cascaded_lora(child, target_modules, rank_1, rank_2, alpha_1, alpha_2, adapter_name , dropout = None)\n",
    "def print_device_and_dtype(model, file = sys.stdout):\n",
    "    if file == sys.stdout:\n",
    "            for name, module in model.named_modules():\n",
    "            # Get the device and dtype of the module's parameters\n",
    "            #file = open(file, \"a\")\n",
    "                try:\n",
    "                    param = next(module.parameters())\n",
    "                    device = param.device\n",
    "                    dtype = param.dtype\n",
    "                    type = param.type()\n",
    "                except StopIteration:\n",
    "                    device = 'No parameters'\n",
    "                    dtype = 'No parameters'\n",
    "                    type = 'No parameters'\n",
    "\n",
    "                \n",
    "                # Print the name, device, and dtype of the module\n",
    "                print(f\"Module: {name}\", file = file)\n",
    "                print(f\"  Device: {device}\", file = file)\n",
    "                print(f\"  Dtype: {dtype}\", file = file)\n",
    "                print(f\"  Type: {type}\", file = file)\n",
    "                print(\" \",file = file )\n",
    "            return \n",
    "\n",
    "    with open(file, \"w\") as file:\n",
    "\n",
    "        for name, module in model.named_modules():\n",
    "            # Get the device and dtype of the module's parameters\n",
    "            #file = open(file, \"a\")\n",
    "            try:\n",
    "                param = next(module.parameters())\n",
    "                device = param.device\n",
    "                dtype = param.dtype\n",
    "                type = param.type()\n",
    "            except StopIteration:\n",
    "                device = 'No parameters'\n",
    "                dtype = 'No parameters'\n",
    "                type = 'No parameters'\n",
    "\n",
    "            \n",
    "            # Print the name, device, and dtype of the module\n",
    "            print(f\"Module: {name}\", file = file)\n",
    "            print(f\"  Device: {device}\", file = file)\n",
    "            print(f\"  Dtype: {dtype}\", file = file)\n",
    "            print(f\"  Type: {type}\", file = file)\n",
    "            print(\" \",file = file )\n",
    "# Function to ensure all submodules are on GPU\n",
    "def move_to_device(model, device):\n",
    "    for name, module in model.named_modules():\n",
    "        try:\n",
    "            # Check if the module is already on the device\n",
    "            param = next(module.parameters())\n",
    "            if param.device != device:\n",
    "                # Move the module to the specified device\n",
    "                module.to(device)\n",
    "                #print(f\"Moved module: {name} to {device}\")\n",
    "        except StopIteration:\n",
    "            # No parameters in the module\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "replace_with_cascaded_lora(quantized_model)\n",
    "#print(quantized_model)\n",
    "move_to_device(quantized_model, torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print_device_and_dtype(quantized_model, file = \"cascaded_lora_structure.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cascaded_lora.ipynb          clean_config.ipynb     test_model.ipynb\n",
      "cascaded_lora_structure.txt  extract_outputs.ipynb  test_tinyllama.ipynb\n",
      "cascaded_model.ipynb         test_dataset.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at '/scratch/tathagato/redo_adapter_experiments/length/length/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/peft/config.py:197\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/scratch/tathagato/redo_adapter_experiments/length/length/'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m base_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m base_model_quantized \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m----> 3\u001b[0m adapter_model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print(adapter_model)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m print_device_and_dtype(adapter_model, file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./peft_adapter_structure.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/peft/peft_model.py:372\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 372\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[1;32m    382\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/peft/config.py:203\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    198\u001b[0m             model_id,\n\u001b[1;32m    199\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[1;32m    201\u001b[0m         )\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/scratch/tathagato/redo_adapter_experiments/length/length/'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "base_model_quantized = AutoModelForCausalLM.from_pretrained(base_model_path, **model_kwargs)\n",
    "adapter_model = PeftModelForCausalLM.from_pretrained(base_model_quantized, adapter_path, \"test\")\n",
    "#print(adapter_model)\n",
    "print_device_and_dtype(adapter_model, file = \"./peft_adapter_structure.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
