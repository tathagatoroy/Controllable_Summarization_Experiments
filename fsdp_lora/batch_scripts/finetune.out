==========================================
SLURM_JOB_ID = 1195560
SLURM_NODELIST = gnode051
SLURM_JOB_GPUS = 0,1,2,3
==========================================
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
World size: 4
{'world_size': 4, 'train_type': 'qlora', 'llama_pro_path': None, 'batch_size': 1, 'context_length': 2048, 'gradient_accumulation_steps': 4, 'num_epochs': 1, 'dataset': 'macsum', 'macsum_path': '/home2/tathagato/summarization/MACSUM/dataset/macdoc/train_dataset.json', 'dataset_samples': -1, 'sharding_strategy': 'full_shard', 'use_gradient_checkpointing': 1, 'reentrant_checkpointing': 1, 'use_cpu_offload': 1, 'use_activation_cpu_offload': False, 'low_memory': True, 'no_sync': False, 'precision': 'bf16', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'save_model': 1, 'output_dir': '/scratch/tathagato/fsdp_qlora_experiments_25_August_test_mistral/length', 'lora_rank': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'lora_target_modules': 'all', 'verbose': 1, 'lr': 5e-05, 'apply_gradient_clipping': 1, 'grad_norm': 1.0, 'wd': 0.1, 'profile_memory': False, 'optimizer': 'adamw', 'lr_scheduler': 'cosine', 'loading_workers': -1, 'log_to': 'wandb', 'master_addr': 'localhost', 'master_port': '12355', 'seed': 42, 'project_name': 'fsdp_qlora', 'name': None, 'group': None, 'entity': None, 'n_bits': 4, 'profile': False, 'profiling_output': 'profiles', 'overwrite_profiling_output': True, 'with_stack': False, 'with_shapes': False, 'export_trace': True, 'export_memory_timeline': False, 'wait_steps': 0, 'warmup_steps': 1, 'active_steps': 2, 'repeat': 0, 'profiling_frequency': 10, 'max_steps': -1, 'attribute': 'length'}
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
Creating model 1
Loading model 1
Creating model 2
Loading model 2
Creating model 3
Loading model 3
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSUM/fsdp_lora/wandb/run-20240826_034729-lgs1naa4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-energy-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: üöÄ View run at https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/lgs1naa4
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
dataset chosen is :  macsum
Example input_ids shape:  torch.Size([1, 679])
Example labels shape:  torch.Size([1, 679])
example input 
<s>[INST] Write a summary of the source text. The summary should be normal in length. The length is defined in terms of number of words used in the summary. The source text is given below.  (CNN)Fans of the late actor Paul Walker knew that watching him in "Furious 7" would be bittersweet. Even so, many moviegoers said the final scenes of the new film, which earned a record $146 million over the weekend, still packed an emotional wallop. "Not gonna lie, I shed a few tears at the end of Furious 7. The tribute to Paul Walker was very well done," one woman said Monday on Twitter. Hers was just one of a flood of messages on social media from people who said they got choked up during scenes featuring Walker, who died at 40 in a car crash in November 2013, before filming on "Furious 7" was completed. To finish Walker's scenes, the makers of the movie used body doubles, computer-generated images and even the actor's brothers. But it was the ending that really got to moviegoers. In finishing "Furious 7," the film's producers sought to retire Walker's character, Brian, while paying homage to his role in the blockbuster "Furious" action franchise. But they felt that killing him off might appear exploitative. "If they had gone down the other path, I think I would have refused to finish making this movie," director James Wan told BuzzFeed. Instead, the movie's makers chose to "retire Paul's character in the most sincere and elegant way (they) could," Wan said. Their idea was to have Brian retire from his dangerous, high-octane lifestyle out of a sense of responsibility to his growing family with girlfriend Mia, who is pregnant with their second child. A scene late in the movie shows him and Mia playing on a beach with their son while the crew looks on -- essentially saying goodbye. Then his longtime buddy Dom reminisces about their years together, leading to a montage of Walker scenes from the first six movies. The song that plays over the montage is  "See You Again," a collaboration between Wiz Khalifa and Charlie Puth. Co-star Vin Diesel shared the video for the song late Sunday on his Facebook page, where it has more than 1.5 million likes. Fans on Twitter and Facebook mostly praised the movie's ending as a fitting tribute -- and an emotionally wrenching one. "Man I don't care how tough u are or how gangsta u claim to be . ...the last five minutes had me choked up in the movie theater ... I saw it 3 times in one day ... ...the ending is the deepest ending I've ever seen," one man wrote on the movie's Facebook page. [/INST]" The tribute to Paul Walker was very well done," said a fan after watching the Furious 7. The actor was on break from filming "Furious 7" at the time of the fiery accident which also claimed the life of the car's driver, Roger Rodas.</s>
tensor([[    1,     3, 12786,  1032, 14828,  1070,  1040,  3600,  3013, 29491,
          1183, 14828,  1791,  1115,  4891,  1065,  4343, 29491,  1183,  4343,
          1117,  4825,  1065,  4239,  1070,  2242,  1070,  3853,  2075,  1065,
          1040, 14828, 29491,  1183,  3600,  3013,  1117,  2846,  4392, 29491,
         29473,  1093, 29511, 12116, 29499, 29533,  1277,  1070,  1040,  4677,
         11732,  4688, 18502,  3348,  1137,  7033,  1481,  1065,  1113, 29533,
         26863, 29473, 29555, 29507,  1450,  1115,  3054,  2300, 13291, 29491,
          4895,  1347, 29493,  2055,  6762,  2412,  1172,  1541,  1040,  2248,
         15148,  1070,  1040,  1401,  3734, 29493,  1458, 13607,  1032,  3163,
          1197, 29508, 29549, 29552,  4609,  1522,  1040,  9839, 29493,  2077,
         15471,  1164, 11294,  4268,  1178, 29491,  1113,  3369, 10343,  5620,
         29493,  1083, 20619,  1032,  2432, 11793,  1206,  1040,  1716,  1070,
          8947,  1693, 29473, 29555, 29491,  1183,  1029,  2751,  1066,  4688,
         18502,  1171,  1983,  1930,  2971,  1630,  1392,  3739,  1541, 10030,
          1124, 12458, 29491,  1150,  1172,  1171,  1544,  1392,  1070,  1032,
         18036,  1070,  9338,  1124,  3577,  4845,  1245,  1673,  1461,  1541,
          1358,  2201,  1252,  9409,  1350,  2706, 15148, 17080, 18502, 29493,
          1461,  5615,  1206, 29473, 29549, 29502,  1065,  1032,  2021, 15224,
          1065,  5117, 29473, 29518, 29502, 29508, 29538, 29493,  1927,  3734,
          1056,  1124,  1113, 29533, 26863, 29473, 29555, 29507,  1171,  8136,
         29491,  2559,  8214, 18502, 29510, 29481, 15148, 29493,  1040,  1058,
          9542,  1070,  1040,  6762,  2075,  2955,  5027, 10111, 29493,  6842,
         29501, 12937,  6971,  1072,  1787,  1040, 11732, 29510, 29481, 13414,
         29491,  1860,  1146,  1171,  1040, 13594,  1137,  2296,  2201,  1066,
          6762,  2412,  1172, 29491,  1328, 20332,  1113, 29533, 26863, 29473,
         29555,  1630,  1040,  3734, 29510, 29481, 23630, 13801,  1066, 26801,
         18502, 29510, 29481,  4001, 29493, 13337, 29493,  2080, 10728,  3921,
          1233,  1066,  1284,  4673,  1065,  1040,  3492, 29494,  4631,  1113,
         29533, 26863, 29507,  3760, 21782, 29491,  1860,  1358,  3538,  1137,
         11703,  1481,  1573,  2427,  5073, 13341, 17230, 29491,  1113,  4149,
          1358,  1321,  4982,  1828,  1040,  1567,  3207, 29493,  1083,  1841,
          1083,  1450,  1274, 12158,  1066,  8214,  3260,  1224,  6762,  1630,
          7627,  5565,  1162,  1044,  3008,  1133,  9814, 21735, 29491,  8930,
         29493,  1040,  6762, 29510, 29481,  1058,  9542, 10776,  1066,  1113,
          2209,  1304,  4688, 29510, 29481,  4001,  1065,  1040,  1848, 19793,
          1165,  1072, 21351,  1837,  1093, 14028, 29499,  1597,  1630,  1162,
          1044,  1541, 29491,  7491,  3796,  1171,  1066,  1274, 13337, 26801,
          1245,  1284, 10027, 29493,  2254, 29501, 22776,  2332, 16986,  1343,
          1070,  1032,  4135,  1070, 10448,  1066,  1284,  7253,  2773,  1163,
         17159,  1119,  1283, 29493,  1461,  1117, 16214,  1163,  1420,  2444,
          2270, 29491,  1098,  7105,  4677,  1065,  1040,  6762,  5138,  1481,
          1072,  1119,  1283,  5311,  1124,  1032, 11073,  1163,  1420,  2734,
          2080,  1040, 10273,  5442,  1124,  2707, 14083,  4445,  1947, 18432,
         29491,  3247,  1284,  1811,  2304, 26891,  7479,  1771, 25247,  2145,
          1452,  1420,  2035,  3321, 29493,  6142,  1066,  1032, 12663,  1233,
          1070, 18502, 15148,  1245,  1040,  1675,  4290, 11383, 29491,  1183,
          4802,  1137,  9696,  1522,  1040, 12663,  1233,  1117, 29473,  1113,
          8760,  1763, 10474,  1630,  1032, 16377,  2212,  1162,  1231,  1292,
          6787, 26390,  1072, 13963,  1135,  3425, 29491,  3860, 29501,  8658,
         13438, 12171,  1069,  7199,  1040,  4566,  1122,  1040,  4802,  4677,
          8497,  1124,  1284,  9256,  3652, 29493,  1738,  1146,  1427,  1448,
          1589, 29473, 29508, 29491, 29550,  4609, 13440, 29491,  1169,  1277,
          1124, 12458,  1072,  9256,  8212, 29061,  1040,  6762, 29510, 29481,
         13594,  1158,  1032, 20836,  1029,  2751,  2707,  1072,  1164, 27126,
          1043,  5052,  1056,  1392, 29491,  1113,  3124,  1083,  1717, 29510,
         29475,  2424,  1678, 11110,  1100,  1228,  1210,  1678, 13839,  7282,
          1100,  4220,  1066,  1115,  1610,  4618,  2005,  2200,  4127,  4254,
          1321,  1296,  1252,  9409,  1350,  1065,  1040,  6762, 18131,  4618,
          1083,  3440,  1146, 29473, 29538,  3189,  1065,  1392,  2138,  4618,
          4618,  2005, 13594,  1117,  1040,  4302,  1142, 13594,  1083, 29510,
          1101,  3038,  3366,  1630,  1392,  1444,  5445,  1124,  1040,  6762,
         29510, 29481,  9256,  3652, 29491, 29473,     4, 29507,  1183,  1029,
          2751,  1066,  4688, 18502,  1171,  1983,  1930,  2971,  1630,  1541,
          1032,  8422,  1792,  7033,  1040,  8947,  1693, 29473, 29555, 29491,
          1183, 11732,  1171,  1124,  2489,  1245,  3734,  1056,  1113, 29533,
         26863, 29473, 29555, 29507,  1206,  1040,  1495,  1070,  1040, 17075,
         29492,  9086,  1458,  1603, 11103,  1040,  2179,  1070,  1040,  2021,
         29510, 29481,  7071, 29493, 14883,  9583,  1061, 29491,     2]])
Creating model 0
Loading model 0
Total model params: 7248023552
Using n_workers: 10 for loading
Loading & Quantizing Model Shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading & Quantizing Model Shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:20<00:41, 20.91s/it]Loading & Quantizing Model Shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:58<00:30, 30.71s/it]Loading & Quantizing Model Shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:08<00:00, 21.41s/it]Loading & Quantizing Model Shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:08<00:00, 22.94s/it]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:855: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:855: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:855: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Loaded model weights in 68.823 seconds
Rank 0: Model created: 0.107 GiB
trainable params: 75,497,472 || all params: 7,323,521,024 || trainable%: 1.0308903566001424
Wrapping model w/ FSDP 0
Rank 0: Wrapped model: 1.824 GiB
Applying activation checkpointing 0
Config:
MistralConfig {
  "_name_or_path": "mistralai/Mistral-7B-Instruct-v0.3",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": false,
  "vocab_size": 32768
}

Model:
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): MistralForCausalLM(
        (model): MistralModel(
          (embed_tokens): Embedding(32768, 4096)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): MistralDecoderLayer(
                  (self_attn): MistralSdpaAttention(
                    (q_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (k_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (v_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): MistralRotaryEmbedding()
                  )
                  (mlp): MistralMLP(
                    (gate_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (up_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (down_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=14336, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (act_fn): SiLU()
                  )
                  (input_layernorm): MistralRMSNorm()
                  (post_attention_layernorm): MistralRMSNorm()
                )
              )
            )
          )
          (norm): MistralRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32768, bias=False)
      )
    )
  )
)
Starting training
Optimizer params:
Shape: torch.Size([67109888]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Total Training Steps: 267
  0%|          | 0/267 [00:00<?, ?it/s]/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:855: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Epoch 0, Loss 0.000:   0%|          | 0/267 [00:00<?, ?it/s]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Epoch 0, Loss 0.000:   0%|          | 1/267 [00:46<3:24:17, 46.08s/it]Epoch 0, Loss 2.303, LR 1.92e-06:   0%|          | 1/267 [00:46<3:24:17, 46.08s/it]Epoch 0, Loss 2.303, LR 1.92e-06:   1%|          | 2/267 [01:37<3:36:06, 48.93s/it]Epoch 0, Loss 2.170, LR 3.85e-06:   1%|          | 2/267 [01:37<3:36:06, 48.93s/it]Epoch 0, Loss 2.170, LR 3.85e-06:   1%|          | 3/267 [02:38<4:01:06, 54.80s/it]Epoch 0, Loss 2.245, LR 5.77e-06:   1%|          | 3/267 [02:38<4:01:06, 54.80s/it]Epoch 0, Loss 2.245, LR 5.77e-06:   1%|‚ñè         | 4/267 [03:33<3:59:31, 54.65s/it]Epoch 0, Loss 2.269, LR 7.69e-06:   1%|‚ñè         | 4/267 [03:33<3:59:31, 54.65s/it]Epoch 0, Loss 2.269, LR 7.69e-06:   2%|‚ñè         | 5/267 [04:34<4:08:39, 56.95s/it]Epoch 0, Loss 2.157, LR 9.62e-06:   2%|‚ñè         | 5/267 [04:34<4:08:39, 56.95s/it]Epoch 0, Loss 2.157, LR 9.62e-06:   2%|‚ñè         | 6/267 [05:30<4:07:20, 56.86s/it]Epoch 0, Loss 2.230, LR 1.15e-05:   2%|‚ñè         | 6/267 [05:30<4:07:20, 56.86s/it]Epoch 0, Loss 2.230, LR 1.15e-05:   3%|‚ñé         | 7/267 [06:27<4:06:05, 56.79s/it]Epoch 0, Loss 2.137, LR 1.35e-05:   3%|‚ñé         | 7/267 [06:27<4:06:05, 56.79s/it]Epoch 0, Loss 2.137, LR 1.35e-05:   3%|‚ñé         | 8/267 [07:18<3:57:11, 54.95s/it]Epoch 0, Loss 2.312, LR 1.54e-05:   3%|‚ñé         | 8/267 [07:18<3:57:11, 54.95s/it]Epoch 0, Loss 2.312, LR 1.54e-05:   3%|‚ñé         | 9/267 [08:02<3:41:56, 51.62s/it]Epoch 0, Loss 2.222, LR 1.73e-05:   3%|‚ñé         | 9/267 [08:02<3:41:56, 51.62s/it]Epoch 0, Loss 2.222, LR 1.73e-05:   4%|‚ñé         | 10/267 [09:05<3:55:34, 55.00s/it]Epoch 0, Loss 2.124, LR 1.92e-05:   4%|‚ñé         | 10/267 [09:05<3:55:34, 55.00s/it]Epoch 0, Loss 2.124, LR 1.92e-05:   4%|‚ñç         | 11/267 [10:05<4:00:42, 56.42s/it]Epoch 0, Loss 2.099, LR 2.12e-05:   4%|‚ñç         | 11/267 [10:05<4:00:42, 56.42s/it]Epoch 0, Loss 2.099, LR 2.12e-05:   4%|‚ñç         | 12/267 [11:00<3:58:35, 56.14s/it]Epoch 0, Loss 2.114, LR 2.31e-05:   4%|‚ñç         | 12/267 [11:00<3:58:35, 56.14s/it]Epoch 0, Loss 2.114, LR 2.31e-05:   5%|‚ñç         | 13/267 [11:55<3:55:54, 55.73s/it]Epoch 0, Loss 2.080, LR 2.50e-05:   5%|‚ñç         | 13/267 [11:55<3:55:54, 55.73s/it]Epoch 0, Loss 2.080, LR 2.50e-05:   5%|‚ñå         | 14/267 [12:56<4:01:16, 57.22s/it]Epoch 0, Loss 2.068, LR 2.69e-05:   5%|‚ñå         | 14/267 [12:56<4:01:16, 57.22s/it]Epoch 0, Loss 2.068, LR 2.69e-05:   6%|‚ñå         | 15/267 [13:52<3:59:22, 57.00s/it]Epoch 0, Loss 2.184, LR 2.88e-05:   6%|‚ñå         | 15/267 [13:52<3:59:22, 57.00s/it]Epoch 0, Loss 2.184, LR 2.88e-05:   6%|‚ñå         | 16/267 [14:41<3:47:58, 54.49s/it]Epoch 0, Loss 2.132, LR 3.08e-05:   6%|‚ñå         | 16/267 [14:41<3:47:58, 54.49s/it]Epoch 0, Loss 2.132, LR 3.08e-05:   6%|‚ñã         | 17/267 [15:37<3:49:32, 55.09s/it]Epoch 0, Loss 1.999, LR 3.27e-05:   6%|‚ñã         | 17/267 [15:37<3:49:32, 55.09s/it]Epoch 0, Loss 1.999, LR 3.27e-05:   7%|‚ñã         | 18/267 [16:30<3:45:13, 54.27s/it]Epoch 0, Loss 2.025, LR 3.46e-05:   7%|‚ñã         | 18/267 [16:30<3:45:13, 54.27s/it]Epoch 0, Loss 2.025, LR 3.46e-05:   7%|‚ñã         | 19/267 [17:27<3:48:16, 55.23s/it]Epoch 0, Loss 1.995, LR 3.65e-05:   7%|‚ñã         | 19/267 [17:27<3:48:16, 55.23s/it]Epoch 0, Loss 1.995, LR 3.65e-05:   7%|‚ñã         | 20/267 [18:21<3:45:29, 54.78s/it]Epoch 0, Loss 1.878, LR 3.85e-05:   7%|‚ñã         | 20/267 [18:21<3:45:29, 54.78s/it]Epoch 0, Loss 1.878, LR 3.85e-05:   8%|‚ñä         | 21/267 [19:17<3:46:44, 55.30s/it]Epoch 0, Loss 1.975, LR 4.04e-05:   8%|‚ñä         | 21/267 [19:17<3:46:44, 55.30s/it]Epoch 0, Loss 1.975, LR 4.04e-05:   8%|‚ñä         | 22/267 [20:08<3:39:53, 53.85s/it]Epoch 0, Loss 1.926, LR 4.23e-05:   8%|‚ñä         | 22/267 [20:08<3:39:53, 53.85s/it]Epoch 0, Loss 1.926, LR 4.23e-05:   9%|‚ñä         | 23/267 [20:59<3:36:11, 53.16s/it]Epoch 0, Loss 1.906, LR 4.42e-05:   9%|‚ñä         | 23/267 [20:59<3:36:11, 53.16s/it]Epoch 0, Loss 1.906, LR 4.42e-05:   9%|‚ñâ         | 24/267 [21:50<3:32:37, 52.50s/it]Epoch 0, Loss 1.889, LR 4.62e-05:   9%|‚ñâ         | 24/267 [21:50<3:32:37, 52.50s/it]Epoch 0, Loss 1.889, LR 4.62e-05:   9%|‚ñâ         | 25/267 [22:47<3:37:11, 53.85s/it]Epoch 0, Loss 1.878, LR 4.81e-05:   9%|‚ñâ         | 25/267 [22:47<3:37:11, 53.85s/it]Epoch 0, Loss 1.878, LR 4.81e-05:  10%|‚ñâ         | 26/267 [23:44<3:40:23, 54.87s/it]Epoch 0, Loss 1.852, LR 5.00e-05:  10%|‚ñâ         | 26/267 [23:45<3:40:23, 54.87s/it]Epoch 0, Loss 1.852, LR 5.00e-05:  10%|‚ñà         | 27/267 [24:44<3:45:41, 56.42s/it]Epoch 0, Loss 1.751, LR 5.00e-05:  10%|‚ñà         | 27/267 [24:45<3:45:41, 56.42s/it]Epoch 0, Loss 1.751, LR 5.00e-05:  10%|‚ñà         | 28/267 [25:43<3:46:57, 56.98s/it]Epoch 0, Loss 1.825, LR 5.00e-05:  10%|‚ñà         | 28/267 [25:43<3:46:57, 56.98s/it]Epoch 0, Loss 1.825, LR 5.00e-05:  11%|‚ñà         | 29/267 [26:43<3:49:29, 57.85s/it]Epoch 0, Loss 1.803, LR 5.00e-05:  11%|‚ñà         | 29/267 [26:43<3:49:29, 57.85s/it]Epoch 0, Loss 1.803, LR 5.00e-05:  11%|‚ñà         | 30/267 [27:30<3:36:21, 54.77s/it]Epoch 0, Loss 1.885, LR 5.00e-05:  11%|‚ñà         | 30/267 [27:30<3:36:21, 54.77s/it]Epoch 0, Loss 1.885, LR 5.00e-05:  12%|‚ñà‚ñè        | 31/267 [28:24<3:33:46, 54.35s/it]Epoch 0, Loss 1.785, LR 5.00e-05:  12%|‚ñà‚ñè        | 31/267 [28:24<3:33:46, 54.35s/it]Epoch 0, Loss 1.785, LR 5.00e-05:  12%|‚ñà‚ñè        | 32/267 [29:20<3:35:08, 54.93s/it]Epoch 0, Loss 1.837, LR 4.99e-05:  12%|‚ñà‚ñè        | 32/267 [29:20<3:35:08, 54.93s/it]Epoch 0, Loss 1.837, LR 4.99e-05:  12%|‚ñà‚ñè        | 33/267 [30:09<3:27:25, 53.19s/it]Epoch 0, Loss 1.852, LR 4.99e-05:  12%|‚ñà‚ñè        | 33/267 [30:09<3:27:25, 53.19s/it]Epoch 0, Loss 1.852, LR 4.99e-05:  13%|‚ñà‚ñé        | 34/267 [31:04<3:28:08, 53.60s/it]Epoch 0, Loss 1.844, LR 4.99e-05:  13%|‚ñà‚ñé        | 34/267 [31:04<3:28:08, 53.60s/it]Epoch 0, Loss 1.844, LR 4.99e-05:  13%|‚ñà‚ñé        | 35/267 [31:58<3:28:29, 53.92s/it]Epoch 0, Loss 1.825, LR 4.98e-05:  13%|‚ñà‚ñé        | 35/267 [31:58<3:28:29, 53.92s/it]Epoch 0, Loss 1.825, LR 4.98e-05:  13%|‚ñà‚ñé        | 36/267 [32:56<3:32:10, 55.11s/it]Epoch 0, Loss 1.720, LR 4.98e-05:  13%|‚ñà‚ñé        | 36/267 [32:56<3:32:10, 55.11s/it]Epoch 0, Loss 1.720, LR 4.98e-05:  14%|‚ñà‚ñç        | 37/267 [33:56<3:37:06, 56.64s/it]Epoch 0, Loss 1.921, LR 4.98e-05:  14%|‚ñà‚ñç        | 37/267 [33:56<3:37:06, 56.64s/it]Epoch 0, Loss 1.921, LR 4.98e-05:  14%|‚ñà‚ñç        | 38/267 [34:41<3:22:21, 53.02s/it]Epoch 0, Loss 1.817, LR 4.97e-05:  14%|‚ñà‚ñç        | 38/267 [34:41<3:22:21, 53.02s/it]Epoch 0, Loss 1.817, LR 4.97e-05:  15%|‚ñà‚ñç        | 39/267 [35:36<3:23:21, 53.52s/it]Epoch 0, Loss 1.881, LR 4.97e-05:  15%|‚ñà‚ñç        | 39/267 [35:36<3:23:21, 53.52s/it]Epoch 0, Loss 1.881, LR 4.97e-05:  15%|‚ñà‚ñç        | 40/267 [36:25<3:17:47, 52.28s/it]Epoch 0, Loss 1.854, LR 4.96e-05:  15%|‚ñà‚ñç        | 40/267 [36:25<3:17:47, 52.28s/it]Epoch 0, Loss 1.854, LR 4.96e-05:  15%|‚ñà‚ñå        | 41/267 [37:20<3:19:45, 53.03s/it]Epoch 0, Loss 1.901, LR 4.96e-05:  15%|‚ñà‚ñå        | 41/267 [37:20<3:19:45, 53.03s/it]Epoch 0, Loss 1.901, LR 4.96e-05:  16%|‚ñà‚ñå        | 42/267 [38:17<3:23:51, 54.36s/it]Epoch 0, Loss 1.741, LR 4.95e-05:  16%|‚ñà‚ñå        | 42/267 [38:17<3:23:51, 54.36s/it]Epoch 0, Loss 1.741, LR 4.95e-05:  16%|‚ñà‚ñå        | 43/267 [39:14<3:25:39, 55.09s/it]Epoch 0, Loss 1.756, LR 4.94e-05:  16%|‚ñà‚ñå        | 43/267 [39:14<3:25:39, 55.09s/it]Epoch 0, Loss 1.756, LR 4.94e-05:  16%|‚ñà‚ñã        | 44/267 [40:16<3:31:53, 57.01s/it]Epoch 0, Loss 1.707, LR 4.94e-05:  16%|‚ñà‚ñã        | 44/267 [40:16<3:31:53, 57.01s/it]Epoch 0, Loss 1.707, LR 4.94e-05:  17%|‚ñà‚ñã        | 45/267 [41:15<3:34:11, 57.89s/it]Epoch 0, Loss 1.749, LR 4.93e-05:  17%|‚ñà‚ñã        | 45/267 [41:15<3:34:11, 57.89s/it]Epoch 0, Loss 1.749, LR 4.93e-05:  17%|‚ñà‚ñã        | 46/267 [42:19<3:39:05, 59.48s/it]Epoch 0, Loss 1.812, LR 4.92e-05:  17%|‚ñà‚ñã        | 46/267 [42:19<3:39:05, 59.48s/it]Epoch 0, Loss 1.812, LR 4.92e-05:  18%|‚ñà‚ñä        | 47/267 [43:15<3:34:13, 58.42s/it]Epoch 0, Loss 1.771, LR 4.92e-05:  18%|‚ñà‚ñä        | 47/267 [43:15<3:34:13, 58.42s/it]Epoch 0, Loss 1.771, LR 4.92e-05:  18%|‚ñà‚ñä        | 48/267 [44:12<3:31:58, 58.08s/it]Epoch 0, Loss 1.909, LR 4.91e-05:  18%|‚ñà‚ñä        | 48/267 [44:12<3:31:58, 58.08s/it]Epoch 0, Loss 1.909, LR 4.91e-05:  18%|‚ñà‚ñä        | 49/267 [45:13<3:34:09, 58.94s/it]Epoch 0, Loss 1.785, LR 4.90e-05:  18%|‚ñà‚ñä        | 49/267 [45:13<3:34:09, 58.94s/it]Epoch 0, Loss 1.785, LR 4.90e-05:  19%|‚ñà‚ñä        | 50/267 [46:06<3:26:54, 57.21s/it]Epoch 0, Loss 1.760, LR 4.89e-05:  19%|‚ñà‚ñä        | 50/267 [46:06<3:26:54, 57.21s/it]Epoch 0, Loss 1.760, LR 4.89e-05:  19%|‚ñà‚ñâ        | 51/267 [47:04<3:26:38, 57.40s/it]Epoch 0, Loss 1.728, LR 4.88e-05:  19%|‚ñà‚ñâ        | 51/267 [47:04<3:26:38, 57.40s/it]Epoch 0, Loss 1.728, LR 4.88e-05:  19%|‚ñà‚ñâ        | 52/267 [48:02<3:26:38, 57.67s/it]Epoch 0, Loss 1.741, LR 4.87e-05:  19%|‚ñà‚ñâ        | 52/267 [48:02<3:26:38, 57.67s/it]Epoch 0, Loss 1.741, LR 4.87e-05:  20%|‚ñà‚ñâ        | 53/267 [48:58<3:23:43, 57.12s/it]Epoch 0, Loss 1.746, LR 4.86e-05:  20%|‚ñà‚ñâ        | 53/267 [48:58<3:23:43, 57.12s/it]Epoch 0, Loss 1.746, LR 4.86e-05:  20%|‚ñà‚ñà        | 54/267 [49:59<3:26:50, 58.26s/it]Epoch 0, Loss 1.714, LR 4.85e-05:  20%|‚ñà‚ñà        | 54/267 [49:59<3:26:50, 58.26s/it]Epoch 0, Loss 1.714, LR 4.85e-05:  21%|‚ñà‚ñà        | 55/267 [50:56<3:24:36, 57.91s/it]Epoch 0, Loss 1.766, LR 4.84e-05:  21%|‚ñà‚ñà        | 55/267 [50:56<3:24:36, 57.91s/it]Epoch 0, Loss 1.766, LR 4.84e-05:  21%|‚ñà‚ñà        | 56/267 [51:48<3:17:35, 56.19s/it]Epoch 0, Loss 1.752, LR 4.83e-05:  21%|‚ñà‚ñà        | 56/267 [51:48<3:17:35, 56.19s/it]Epoch 0, Loss 1.752, LR 4.83e-05:  21%|‚ñà‚ñà‚ñè       | 57/267 [52:45<3:17:39, 56.47s/it]Epoch 0, Loss 1.794, LR 4.82e-05:  21%|‚ñà‚ñà‚ñè       | 57/267 [52:45<3:17:39, 56.47s/it]Epoch 0, Loss 1.794, LR 4.82e-05:  22%|‚ñà‚ñà‚ñè       | 58/267 [53:29<3:03:42, 52.74s/it]Epoch 0, Loss 1.710, LR 4.81e-05:  22%|‚ñà‚ñà‚ñè       | 58/267 [53:29<3:03:42, 52.74s/it]Epoch 0, Loss 1.710, LR 4.81e-05:  22%|‚ñà‚ñà‚ñè       | 59/267 [54:25<3:05:35, 53.54s/it]Epoch 0, Loss 1.641, LR 4.80e-05:  22%|‚ñà‚ñà‚ñè       | 59/267 [54:25<3:05:35, 53.54s/it]Epoch 0, Loss 1.641, LR 4.80e-05:  22%|‚ñà‚ñà‚ñè       | 60/267 [55:14<3:00:16, 52.25s/it]Epoch 0, Loss 1.715, LR 4.78e-05:  22%|‚ñà‚ñà‚ñè       | 60/267 [55:14<3:00:16, 52.25s/it]Epoch 0, Loss 1.715, LR 4.78e-05:  23%|‚ñà‚ñà‚ñé       | 61/267 [56:17<3:10:22, 55.45s/it]Epoch 0, Loss 1.660, LR 4.77e-05:  23%|‚ñà‚ñà‚ñé       | 61/267 [56:17<3:10:22, 55.45s/it]Epoch 0, Loss 1.660, LR 4.77e-05:  23%|‚ñà‚ñà‚ñé       | 62/267 [57:16<3:13:22, 56.60s/it]Epoch 0, Loss 1.773, LR 4.76e-05:  23%|‚ñà‚ñà‚ñé       | 62/267 [57:16<3:13:22, 56.60s/it]Epoch 0, Loss 1.773, LR 4.76e-05:  24%|‚ñà‚ñà‚ñé       | 63/267 [58:01<3:00:52, 53.20s/it]Epoch 0, Loss 1.827, LR 4.74e-05:  24%|‚ñà‚ñà‚ñé       | 63/267 [58:02<3:00:52, 53.20s/it]Epoch 0, Loss 1.827, LR 4.74e-05:  24%|‚ñà‚ñà‚ñç       | 64/267 [58:50<2:54:52, 51.69s/it]Epoch 0, Loss 1.783, LR 4.73e-05:  24%|‚ñà‚ñà‚ñç       | 64/267 [58:50<2:54:52, 51.69s/it]Epoch 0, Loss 1.783, LR 4.73e-05:  24%|‚ñà‚ñà‚ñç       | 65/267 [59:39<2:52:04, 51.11s/it]Epoch 0, Loss 1.717, LR 4.72e-05:  24%|‚ñà‚ñà‚ñç       | 65/267 [59:39<2:52:04, 51.11s/it]Epoch 0, Loss 1.717, LR 4.72e-05:  25%|‚ñà‚ñà‚ñç       | 66/267 [1:00:43<3:03:54, 54.90s/it]Epoch 0, Loss 1.803, LR 4.70e-05:  25%|‚ñà‚ñà‚ñç       | 66/267 [1:00:43<3:03:54, 54.90s/it]Epoch 0, Loss 1.803, LR 4.70e-05:  25%|‚ñà‚ñà‚ñå       | 67/267 [1:01:45<3:09:50, 56.95s/it]Epoch 0, Loss 1.822, LR 4.69e-05:  25%|‚ñà‚ñà‚ñå       | 67/267 [1:01:45<3:09:50, 56.95s/it]Epoch 0, Loss 1.822, LR 4.69e-05:  25%|‚ñà‚ñà‚ñå       | 68/267 [1:02:36<3:03:33, 55.34s/it]Epoch 0, Loss 1.621, LR 4.67e-05:  25%|‚ñà‚ñà‚ñå       | 68/267 [1:02:37<3:03:33, 55.34s/it]Epoch 0, Loss 1.621, LR 4.67e-05:  26%|‚ñà‚ñà‚ñå       | 69/267 [1:03:41<3:12:02, 58.19s/it]Epoch 0, Loss 1.740, LR 4.66e-05:  26%|‚ñà‚ñà‚ñå       | 69/267 [1:03:41<3:12:02, 58.19s/it]Epoch 0, Loss 1.740, LR 4.66e-05:  26%|‚ñà‚ñà‚ñå       | 70/267 [1:04:30<3:01:35, 55.30s/it]Epoch 0, Loss 1.860, LR 4.64e-05:  26%|‚ñà‚ñà‚ñå       | 70/267 [1:04:30<3:01:35, 55.30s/it]Epoch 0, Loss 1.860, LR 4.64e-05:  27%|‚ñà‚ñà‚ñã       | 71/267 [1:05:32<3:06:56, 57.23s/it]Epoch 0, Loss 1.637, LR 4.62e-05:  27%|‚ñà‚ñà‚ñã       | 71/267 [1:05:32<3:06:56, 57.23s/it]Epoch 0, Loss 1.637, LR 4.62e-05:  27%|‚ñà‚ñà‚ñã       | 72/267 [1:06:20<2:57:34, 54.64s/it]Epoch 0, Loss 1.719, LR 4.61e-05:  27%|‚ñà‚ñà‚ñã       | 72/267 [1:06:20<2:57:34, 54.64s/it]Epoch 0, Loss 1.719, LR 4.61e-05:  27%|‚ñà‚ñà‚ñã       | 73/267 [1:07:17<2:59:02, 55.37s/it]Epoch 0, Loss 1.707, LR 4.59e-05:  27%|‚ñà‚ñà‚ñã       | 73/267 [1:07:17<2:59:02, 55.37s/it]Epoch 0, Loss 1.707, LR 4.59e-05:  28%|‚ñà‚ñà‚ñä       | 74/267 [1:08:07<2:52:29, 53.62s/it]Epoch 0, Loss 1.779, LR 4.57e-05:  28%|‚ñà‚ñà‚ñä       | 74/267 [1:08:07<2:52:29, 53.62s/it]Epoch 0, Loss 1.779, LR 4.57e-05:  28%|‚ñà‚ñà‚ñä       | 75/267 [1:09:02<2:53:30, 54.22s/it]Epoch 0, Loss 1.676, LR 4.56e-05:  28%|‚ñà‚ñà‚ñä       | 75/267 [1:09:02<2:53:30, 54.22s/it]Epoch 0, Loss 1.676, LR 4.56e-05:  28%|‚ñà‚ñà‚ñä       | 76/267 [1:10:04<2:59:14, 56.31s/it]Epoch 0, Loss 1.711, LR 4.54e-05:  28%|‚ñà‚ñà‚ñä       | 76/267 [1:10:04<2:59:14, 56.31s/it]Epoch 0, Loss 1.711, LR 4.54e-05:  29%|‚ñà‚ñà‚ñâ       | 77/267 [1:10:58<2:56:03, 55.60s/it]Epoch 0, Loss 1.871, LR 4.52e-05:  29%|‚ñà‚ñà‚ñâ       | 77/267 [1:10:58<2:56:03, 55.60s/it]Epoch 0, Loss 1.871, LR 4.52e-05:  29%|‚ñà‚ñà‚ñâ       | 78/267 [1:11:54<2:55:59, 55.87s/it]Epoch 0, Loss 1.760, LR 4.50e-05:  29%|‚ñà‚ñà‚ñâ       | 78/267 [1:11:54<2:55:59, 55.87s/it]Epoch 0, Loss 1.760, LR 4.50e-05:  30%|‚ñà‚ñà‚ñâ       | 79/267 [1:12:48<2:53:18, 55.31s/it]Epoch 0, Loss 1.718, LR 4.48e-05:  30%|‚ñà‚ñà‚ñâ       | 79/267 [1:12:48<2:53:18, 55.31s/it]Epoch 0, Loss 1.718, LR 4.48e-05:  30%|‚ñà‚ñà‚ñâ       | 80/267 [1:13:39<2:48:13, 53.97s/it]Epoch 0, Loss 1.876, LR 4.47e-05:  30%|‚ñà‚ñà‚ñâ       | 80/267 [1:13:39<2:48:13, 53.97s/it]Epoch 0, Loss 1.876, LR 4.47e-05:  30%|‚ñà‚ñà‚ñà       | 81/267 [1:14:41<2:54:52, 56.41s/it]Epoch 0, Loss 1.827, LR 4.45e-05:  30%|‚ñà‚ñà‚ñà       | 81/267 [1:14:41<2:54:52, 56.41s/it]Epoch 0, Loss 1.827, LR 4.45e-05:  31%|‚ñà‚ñà‚ñà       | 82/267 [1:15:30<2:47:19, 54.27s/it]Epoch 0, Loss 1.758, LR 4.43e-05:  31%|‚ñà‚ñà‚ñà       | 82/267 [1:15:30<2:47:19, 54.27s/it]Epoch 0, Loss 1.758, LR 4.43e-05:  31%|‚ñà‚ñà‚ñà       | 83/267 [1:16:33<2:54:32, 56.91s/it]Epoch 0, Loss 1.821, LR 4.41e-05:  31%|‚ñà‚ñà‚ñà       | 83/267 [1:16:33<2:54:32, 56.91s/it]Epoch 0, Loss 1.821, LR 4.41e-05:  31%|‚ñà‚ñà‚ñà‚ñè      | 84/267 [1:17:31<2:54:25, 57.19s/it]Epoch 0, Loss 1.686, LR 4.39e-05:  31%|‚ñà‚ñà‚ñà‚ñè      | 84/267 [1:17:31<2:54:25, 57.19s/it]Epoch 0, Loss 1.686, LR 4.39e-05:  32%|‚ñà‚ñà‚ñà‚ñè      | 85/267 [1:18:35<2:59:47, 59.27s/it]Epoch 0, Loss 1.649, LR 4.37e-05:  32%|‚ñà‚ñà‚ñà‚ñè      | 85/267 [1:18:35<2:59:47, 59.27s/it]Epoch 0, Loss 1.649, LR 4.37e-05:  32%|‚ñà‚ñà‚ñà‚ñè      | 86/267 [1:19:24<2:48:59, 56.02s/it]Epoch 0, Loss 1.761, LR 4.35e-05:  32%|‚ñà‚ñà‚ñà‚ñè      | 86/267 [1:19:24<2:48:59, 56.02s/it]Epoch 0, Loss 1.761, LR 4.35e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 87/267 [1:20:16<2:44:31, 54.84s/it]Epoch 0, Loss 1.771, LR 4.33e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 87/267 [1:20:16<2:44:31, 54.84s/it]Epoch 0, Loss 1.771, LR 4.33e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 88/267 [1:21:09<2:41:56, 54.28s/it]Epoch 0, Loss 1.775, LR 4.30e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 88/267 [1:21:09<2:41:56, 54.28s/it]Epoch 0, Loss 1.775, LR 4.30e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 89/267 [1:22:04<2:42:18, 54.71s/it]Epoch 0, Loss 1.767, LR 4.28e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 89/267 [1:22:04<2:42:18, 54.71s/it]Epoch 0, Loss 1.767, LR 4.28e-05:  34%|‚ñà‚ñà‚ñà‚ñé      | 90/267 [1:22:59<2:41:23, 54.71s/it]Epoch 0, Loss 1.776, LR 4.26e-05:  34%|‚ñà‚ñà‚ñà‚ñé      | 90/267 [1:22:59<2:41:23, 54.71s/it]Epoch 0, Loss 1.776, LR 4.26e-05:  34%|‚ñà‚ñà‚ñà‚ñç      | 91/267 [1:23:59<2:45:11, 56.32s/it]Epoch 0, Loss 1.657, LR 4.24e-05:  34%|‚ñà‚ñà‚ñà‚ñç      | 91/267 [1:23:59<2:45:11, 56.32s/it]Epoch 0, Loss 1.657, LR 4.24e-05:  34%|‚ñà‚ñà‚ñà‚ñç      | 92/267 [1:25:00<2:48:19, 57.71s/it]Epoch 0, Loss 1.834, LR 4.22e-05:  34%|‚ñà‚ñà‚ñà‚ñç      | 92/267 [1:25:00<2:48:19, 57.71s/it]Epoch 0, Loss 1.834, LR 4.22e-05:  35%|‚ñà‚ñà‚ñà‚ñç      | 93/267 [1:25:51<2:40:55, 55.49s/it]Epoch 0, Loss 1.690, LR 4.20e-05:  35%|‚ñà‚ñà‚ñà‚ñç      | 93/267 [1:25:51<2:40:55, 55.49s/it]Epoch 0, Loss 1.690, LR 4.20e-05:  35%|‚ñà‚ñà‚ñà‚ñå      | 94/267 [1:26:41<2:35:28, 53.92s/it]Epoch 0, Loss 1.790, LR 4.17e-05:  35%|‚ñà‚ñà‚ñà‚ñå      | 94/267 [1:26:41<2:35:28, 53.92s/it]Epoch 0, Loss 1.790, LR 4.17e-05:  36%|‚ñà‚ñà‚ñà‚ñå      | 95/267 [1:27:42<2:40:47, 56.09s/it]Epoch 0, Loss 1.698, LR 4.15e-05:  36%|‚ñà‚ñà‚ñà‚ñå      | 95/267 [1:27:42<2:40:47, 56.09s/it]Epoch 0, Loss 1.698, LR 4.15e-05:  36%|‚ñà‚ñà‚ñà‚ñå      | 96/267 [1:28:27<2:30:36, 52.85s/it]Epoch 0, Loss 1.688, LR 4.13e-05:  36%|‚ñà‚ñà‚ñà‚ñå      | 96/267 [1:28:27<2:30:36, 52.85s/it]Epoch 0, Loss 1.688, LR 4.13e-05:  36%|‚ñà‚ñà‚ñà‚ñã      | 97/267 [1:29:29<2:37:36, 55.63s/it]Epoch 0, Loss 1.669, LR 4.10e-05:  36%|‚ñà‚ñà‚ñà‚ñã      | 97/267 [1:29:29<2:37:36, 55.63s/it]Epoch 0, Loss 1.669, LR 4.10e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 98/267 [1:30:36<2:46:18, 59.04s/it]Epoch 0, Loss 1.916, LR 4.08e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 98/267 [1:30:36<2:46:18, 59.04s/it]Epoch 0, Loss 1.916, LR 4.08e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 99/267 [1:31:24<2:36:05, 55.75s/it]Epoch 0, Loss 1.701, LR 4.06e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 99/267 [1:31:25<2:36:05, 55.75s/it]Epoch 0, Loss 1.701, LR 4.06e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 100/267 [1:32:24<2:38:35, 56.98s/it]Epoch 0, Loss 1.728, LR 4.03e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 100/267 [1:32:24<2:38:35, 56.98s/it]Batch idx 0
Batch idx 1
Batch idx 2
Gradient norm: 1.3671875
Batch idx 3
Batch idx 4
Batch idx 5
Batch idx 6
Gradient norm: 1.265625
Batch idx 7
Batch idx 8
Batch idx 9
Batch idx 10
Gradient norm: 1.171875
Batch idx 11
Batch idx 12
Batch idx 13
Batch idx 14
Gradient norm: 1.1953125
Batch idx 15
Batch idx 16
Batch idx 17
Batch idx 18
Gradient norm: 1.0625
Batch idx 19
Batch idx 20
Batch idx 21
Batch idx 22
Gradient norm: 1.0546875
Batch idx 23
Batch idx 24
Batch idx 25
Batch idx 26
Gradient norm: 1.09375
Batch idx 27
Batch idx 28
Batch idx 29
Batch idx 30
Gradient norm: 1.140625
Batch idx 31
Batch idx 32
Batch idx 33
Batch idx 34
Gradient norm: 1.171875
Batch idx 35
Batch idx 36
Batch idx 37
Batch idx 38
Gradient norm: 0.94921875
Batch idx 39
Batch idx 40
Batch idx 41
Batch idx 42
Gradient norm: 0.8828125
Batch idx 43
Batch idx 44
Batch idx 45
Batch idx 46
Gradient norm: 0.8359375
Batch idx 47
Batch idx 48
Batch idx 49
Batch idx 50
Gradient norm: 0.90234375
Batch idx 51
Batch idx 52
Batch idx 53
Batch idx 54
Gradient norm: 0.76953125
Batch idx 55
Batch idx 56
Batch idx 57
Batch idx 58
Gradient norm: 0.8046875
Batch idx 59
Batch idx 60
Batch idx 61
Batch idx 62
Gradient norm: 0.8828125
Batch idx 63
Batch idx 64
Batch idx 65
Batch idx 66
Gradient norm: 0.67578125
Batch idx 67
Batch idx 68
Batch idx 69
Batch idx 70
Gradient norm: 0.65234375
Batch idx 71
Batch idx 72
Batch idx 73
Batch idx 74
Gradient norm: 0.6953125
Batch idx 75
Batch idx 76
Batch idx 77
Batch idx 78
Gradient norm: 0.6171875
Batch idx 79
Batch idx 80
Batch idx 81
Batch idx 82
Gradient norm: 0.470703125
Batch idx 83
Batch idx 84
Batch idx 85
Batch idx 86
Gradient norm: 0.41015625
Batch idx 87
Batch idx 88
Batch idx 89
Batch idx 90
Gradient norm: 0.4921875
Batch idx 91
Batch idx 92
Batch idx 93
Batch idx 94
Gradient norm: 0.369140625
Batch idx 95
Batch idx 96
Batch idx 97
Batch idx 98
Gradient norm: 0.298828125
Batch idx 99
Batch idx 100
Batch idx 101
Batch idx 102
Gradient norm: 0.306640625
Batch idx 103
Batch idx 104
Batch idx 105
Batch idx 106
Gradient norm: 0.294921875
Batch idx 107
Batch idx 108
Batch idx 109
Batch idx 110
Gradient norm: 0.298828125
Batch idx 111
Batch idx 112
Batch idx 113
Batch idx 114
Gradient norm: 0.287109375
Batch idx 115
Batch idx 116
Batch idx 117
Batch idx 118
Gradient norm: 0.349609375
Batch idx 119
Batch idx 120
Batch idx 121
Batch idx 122
Gradient norm: 0.376953125
Batch idx 123
Batch idx 124
Batch idx 125
Batch idx 126
Gradient norm: 0.419921875
Batch idx 127
Batch idx 128
Batch idx 129
Batch idx 130
Gradient norm: 0.423828125
Batch idx 131
Batch idx 132
Batch idx 133
Batch idx 134
Gradient norm: 0.328125
Batch idx 135
Batch idx 136
Batch idx 137
Batch idx 138
Gradient norm: 0.330078125
Batch idx 139
Batch idx 140
Batch idx 141
Batch idx 142
Gradient norm: 0.44921875
Batch idx 143
Batch idx 144
Batch idx 145
Batch idx 146
Gradient norm: 0.2890625
Batch idx 147
Batch idx 148
Batch idx 149
Batch idx 150
Gradient norm: 0.458984375
Batch idx 151
Batch idx 152
Batch idx 153
Batch idx 154
Gradient norm: 0.28515625
Batch idx 155
Batch idx 156
Batch idx 157
Batch idx 158
Gradient norm: 0.3359375
Batch idx 159
Batch idx 160
Batch idx 161
Batch idx 162
Gradient norm: 0.2578125
Batch idx 163
Batch idx 164
Batch idx 165
Batch idx 166
Gradient norm: 0.248046875
Batch idx 167
Batch idx 168
Batch idx 169
Batch idx 170
Gradient norm: 0.271484375
Batch idx 171
Batch idx 172
Batch idx 173
Batch idx 174
Gradient norm: 0.248046875
Batch idx 175
Batch idx 176
Batch idx 177
Batch idx 178
Gradient norm: 0.2451171875
Batch idx 179
Batch idx 180
Batch idx 181
Batch idx 182
Gradient norm: 0.2294921875
Batch idx 183
Batch idx 184
Batch idx 185
Batch idx 186
Gradient norm: 0.224609375
Batch idx 187
Batch idx 188
Batch idx 189
Batch idx 190
Gradient norm: 0.2490234375
Batch idx 191
Batch idx 192
Batch idx 193
Batch idx 194
Gradient norm: 0.23046875
Batch idx 195
Batch idx 196
Batch idx 197
Batch idx 198
Gradient norm: 0.2412109375
Batch idx 199
Batch idx 200
Batch idx 201
Batch idx 202
Gradient norm: 0.21875
Batch idx 203
Batch idx 204
Batch idx 205
Batch idx 206
Gradient norm: 0.19140625
Batch idx 207
Batch idx 208
Batch idx 209
Batch idx 210
Gradient norm: 0.197265625
Batch idx 211
Batch idx 212
Batch idx 213
Batch idx 214
Gradient norm: 0.232421875
Batch idx 215
Batch idx 216
Batch idx 217
Batch idx 218
Gradient norm: 0.220703125
Batch idx 219
Batch idx 220
Batch idx 221
Batch idx 222
Gradient norm: 0.2412109375
Batch idx 223
Batch idx 224
Batch idx 225
Batch idx 226
Gradient norm: 0.2177734375
Batch idx 227
Batch idx 228
Batch idx 229
Batch idx 230
Gradient norm: 0.2490234375
Batch idx 231
Batch idx 232
Batch idx 233
Batch idx 234
Gradient norm: 0.23828125
Batch idx 235
Batch idx 236
Batch idx 237
Batch idx 238
Gradient norm: 0.2578125
Batch idx 239
Batch idx 240
Batch idx 241
Batch idx 242
Gradient norm: 0.2177734375
Batch idx 243
Batch idx 244
Batch idx 245
Batch idx 246
Gradient norm: 0.20703125
Batch idx 247
Batch idx 248
Batch idx 249
Batch idx 250
Gradient norm: 0.2314453125
Batch idx 251
Batch idx 252
Batch idx 253
Batch idx 254
Gradient norm: 0.2333984375
Batch idx 255
Batch idx 256
Batch idx 257
Batch idx 258
Gradient norm: 0.2412109375
Batch idx 259
Batch idx 260
Batch idx 261
Batch idx 262
Gradient norm: 0.2236328125
Batch idx 263
Batch idx 264
Batch idx 265
Batch idx 266
Gradient norm: 0.189453125
Batch idx 267
Batch idx 268
Batch idx 269
Batch idx 270
Gradient norm: 0.2109375
Batch idx 271
Batch idx 272
Batch idx 273
Batch idx 274
Gradient norm: 0.2138671875
Batch idx 275
Batch idx 276
Batch idx 277
Batch idx 278
Gradient norm: 0.2060546875
Batch idx 279
Batch idx 280
Batch idx 281
Batch idx 282
Gradient norm: 0.2314453125
Batch idx 283
Batch idx 284
Batch idx 285
Batch idx 286
Gradient norm: 0.2236328125
Batch idx 287
Batch idx 288
Batch idx 289
Batch idx 290
Gradient norm: 0.2578125
Batch idx 291
Batch idx 292
Batch idx 293
Batch idx 294
Gradient norm: 0.208984375
Batch idx 295
Batch idx 296
Batch idx 297
Batch idx 298
Gradient norm: 0.2080078125
Batch idx 299
Batch idx 300
Batch idx 301
Batch idx 302
Gradient norm: 0.20703125
Batch idx 303
Batch idx 304
Batch idx 305
Batch idx 306
Gradient norm: 0.181640625
Batch idx 307
Batch idx 308
Batch idx 309
Batch idx 310
Gradient norm: 0.2080078125
Batch idx 311
Batch idx 312
Batch idx 313
Batch idx 314
Gradient norm: 0.2431640625
Batch idx 315
Batch idx 316
Batch idx 317
Batch idx 318
Gradient norm: 0.1875
Batch idx 319
Batch idx 320
Batch idx 321
Batch idx 322
Gradient norm: 0.197265625
Batch idx 323
Batch idx 324
Batch idx 325
Batch idx 326
Gradient norm: 0.23828125
Batch idx 327
Batch idx 328
Batch idx 329
Batch idx 330
Gradient norm: 0.22265625
Batch idx 331
Batch idx 332
Batch idx 333
Batch idx 334
Gradient norm: 0.2080078125
Batch idx 335
Batch idx 336
Batch idx 337
Batch idx 338
Gradient norm: 0.2294921875
Batch idx 339
Batch idx 340
Batch idx 341
Batch idx 342
Gradient norm: 0.2353515625
Batch idx 343
Batch idx 344
Batch idx 345
Batch idx 346
Gradient norm: 0.21484375
Batch idx 347
Batch idx 348
Batch idx 349
Batch idx 350
Gradient norm: 0.197265625
Batch idx 351
Batch idx 352
Batch idx 353
Batch idx 354
Gradient norm: 0.228515625
Batch idx 355
Batch idx 356
Batch idx 357
Batch idx 358
Gradient norm: 0.2373046875
Batch idx 359
Batch idx 360
Batch idx 361
Batch idx 362
Gradient norm: 0.205078125
Batch idx 363
Batch idx 364
Batch idx 365
Batch idx 366
Gradient norm: 0.2373046875
Batch idx 367
Batch idx 368
Batch idx 369
Batch idx 370
Gradient norm: 0.20703125
Batch idx 371
Batch idx 372
Batch idx 373
Batch idx 374
Gradient norm: 0.2236328125
Batch idx 375
Batch idx 376
Batch idx 377
Batch idx 378
Gradient norm: 0.1904296875
Batch idx 379
Batch idx 380
Batch idx 381
Batch idx 382
Gradient norm: 0.228515625
Batch idx 383
Batch idx 384
Batch idx 385
Batch idx 386
Gradient norm: 0.251953125
Batch idx 387
Batch idx 388
Batch idx 389
Batch idx 390
Gradient norm: 0.1787109375
Batch idx 391
Batch idx 392
Batch idx 393
Batch idx 394
Gradient norm: 0.236328125
Batch idx 395
Batch idx 396
Batch idx 397
Batch idx 398
Gradient norm: 0.2216796875
Batch idx 399
Batch idx 400
Batch idx 401
Batch idx 402
Epoch 0, Loss 1.728, LR 4.03e-05:  38%|‚ñà‚ñà‚ñà‚ñä      | 101/267 [1:33:20<2:36:21, 56.52s/it]Epoch 0, Loss 1.681, LR 4.01e-05:  38%|‚ñà‚ñà‚ñà‚ñä      | 101/267 [1:33:20<2:36:21, 56.52s/it]Epoch 0, Loss 1.681, LR 4.01e-05:  38%|‚ñà‚ñà‚ñà‚ñä      | 102/267 [1:34:15<2:34:29, 56.18s/it]Epoch 0, Loss 1.789, LR 3.98e-05:  38%|‚ñà‚ñà‚ñà‚ñä      | 102/267 [1:34:15<2:34:29, 56.18s/it]Epoch 0, Loss 1.789, LR 3.98e-05:  39%|‚ñà‚ñà‚ñà‚ñä      | 103/267 [1:35:08<2:30:33, 55.08s/it]Epoch 0, Loss 1.697, LR 3.96e-05:  39%|‚ñà‚ñà‚ñà‚ñä      | 103/267 [1:35:08<2:30:33, 55.08s/it]Epoch 0, Loss 1.697, LR 3.96e-05:  39%|‚ñà‚ñà‚ñà‚ñâ      | 104/267 [1:36:09<2:35:05, 57.09s/it]Epoch 0, Loss 1.775, LR 3.93e-05:  39%|‚ñà‚ñà‚ñà‚ñâ      | 104/267 [1:36:09<2:35:05, 57.09s/it]Epoch 0, Loss 1.775, LR 3.93e-05:  39%|‚ñà‚ñà‚ñà‚ñâ      | 105/267 [1:37:05<2:32:55, 56.64s/it]Epoch 0, Loss 1.741, LR 3.91e-05:  39%|‚ñà‚ñà‚ñà‚ñâ      | 105/267 [1:37:05<2:32:55, 56.64s/it]Epoch 0, Loss 1.741, LR 3.91e-05:  40%|‚ñà‚ñà‚ñà‚ñâ      | 106/267 [1:37:54<2:25:50, 54.35s/it]Epoch 0, Loss 1.821, LR 3.88e-05:  40%|‚ñà‚ñà‚ñà‚ñâ      | 106/267 [1:37:54<2:25:50, 54.35s/it]Epoch 0, Loss 1.821, LR 3.88e-05:  40%|‚ñà‚ñà‚ñà‚ñà      | 107/267 [1:38:46<2:23:08, 53.68s/it]Epoch 0, Loss 1.645, LR 3.86e-05:  40%|‚ñà‚ñà‚ñà‚ñà      | 107/267 [1:38:46<2:23:08, 53.68s/it]Epoch 0, Loss 1.645, LR 3.86e-05:  40%|‚ñà‚ñà‚ñà‚ñà      | 108/267 [1:39:44<2:25:39, 54.97s/it]Epoch 0, Loss 1.731, LR 3.83e-05:  40%|‚ñà‚ñà‚ñà‚ñà      | 108/267 [1:39:44<2:25:39, 54.97s/it]Epoch 0, Loss 1.731, LR 3.83e-05:  41%|‚ñà‚ñà‚ñà‚ñà      | 109/267 [1:40:38<2:24:03, 54.71s/it]Epoch 0, Loss 1.671, LR 3.81e-05:  41%|‚ñà‚ñà‚ñà‚ñà      | 109/267 [1:40:38<2:24:03, 54.71s/it]Epoch 0, Loss 1.671, LR 3.81e-05:  41%|‚ñà‚ñà‚ñà‚ñà      | 110/267 [1:41:31<2:21:39, 54.14s/it]Epoch 0, Loss 1.771, LR 3.78e-05:  41%|‚ñà‚ñà‚ñà‚ñà      | 110/267 [1:41:31<2:21:39, 54.14s/it]Epoch 0, Loss 1.771, LR 3.78e-05:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 111/267 [1:42:25<2:20:19, 53.97s/it]Epoch 0, Loss 1.698, LR 3.75e-05:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 111/267 [1:42:25<2:20:19, 53.97s/it]Epoch 0, Loss 1.698, LR 3.75e-05:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 112/267 [1:43:15<2:16:22, 52.79s/it]Epoch 0, Loss 1.774, LR 3.73e-05:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 112/267 [1:43:15<2:16:22, 52.79s/it]Epoch 0, Loss 1.774, LR 3.73e-05:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 113/267 [1:44:11<2:18:22, 53.92s/it]Epoch 0, Loss 1.742, LR 3.70e-05:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 113/267 [1:44:11<2:18:22, 53.92s/it]Epoch 0, Loss 1.742, LR 3.70e-05:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 114/267 [1:45:08<2:19:39, 54.77s/it]Epoch 0, Loss 1.774, LR 3.67e-05:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 114/267 [1:45:08<2:19:39, 54.77s/it]Epoch 0, Loss 1.774, LR 3.67e-05:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 115/267 [1:46:04<2:19:48, 55.19s/it]Epoch 0, Loss 1.647, LR 3.65e-05:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 115/267 [1:46:04<2:19:48, 55.19s/it]Epoch 0, Loss 1.647, LR 3.65e-05:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 116/267 [1:47:01<2:20:08, 55.69s/it]Epoch 0, Loss 1.757, LR 3.62e-05:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 116/267 [1:47:01<2:20:08, 55.69s/it]Epoch 0, Loss 1.757, LR 3.62e-05:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 117/267 [1:48:05<2:25:44, 58.29s/it]Epoch 0, Loss 1.656, LR 3.59e-05:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 117/267 [1:48:05<2:25:44, 58.29s/it]Epoch 0, Loss 1.656, LR 3.59e-05:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 118/267 [1:48:59<2:20:58, 56.77s/it]Epoch 0, Loss 1.735, LR 3.57e-05:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 118/267 [1:48:59<2:20:58, 56.77s/it]Epoch 0, Loss 1.735, LR 3.57e-05:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 119/267 [1:49:59<2:22:50, 57.91s/it]Epoch 0, Loss 1.755, LR 3.54e-05:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 119/267 [1:49:59<2:22:50, 57.91s/it]Epoch 0, Loss 1.755, LR 3.54e-05:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 120/267 [1:50:48<2:15:36, 55.35s/it]Epoch 0, Loss 1.621, LR 3.51e-05:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 120/267 [1:50:49<2:15:36, 55.35s/it]Epoch 0, Loss 1.621, LR 3.51e-05:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 121/267 [1:51:41<2:12:43, 54.54s/it]Epoch 0, Loss 1.684, LR 3.48e-05:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 121/267 [1:51:41<2:12:43, 54.54s/it]Epoch 0, Loss 1.684, LR 3.48e-05:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 122/267 [1:52:39<2:13:57, 55.43s/it]Epoch 0, Loss 1.745, LR 3.46e-05:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 122/267 [1:52:39<2:13:57, 55.43s/it]Epoch 0, Loss 1.745, LR 3.46e-05:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 123/267 [1:53:34<2:13:02, 55.44s/it]Epoch 0, Loss 1.739, LR 3.43e-05:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 123/267 [1:53:34<2:13:02, 55.44s/it]Epoch 0, Loss 1.739, LR 3.43e-05:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 124/267 [1:54:25<2:08:55, 54.09s/it]Epoch 0, Loss 1.634, LR 3.40e-05:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 124/267 [1:54:25<2:08:55, 54.09s/it]Epoch 0, Loss 1.634, LR 3.40e-05:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 125/267 [1:55:17<2:06:23, 53.41s/it]Epoch 0, Loss 1.822, LR 3.37e-05:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 125/267 [1:55:17<2:06:23, 53.41s/it]Epoch 0, Loss 1.822, LR 3.37e-05:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 126/267 [1:56:15<2:08:53, 54.85s/it]Epoch 0, Loss 1.786, LR 3.34e-05:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 126/267 [1:56:15<2:08:53, 54.85s/it]Epoch 0, Loss 1.786, LR 3.34e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 127/267 [1:57:13<2:09:51, 55.65s/it]Epoch 0, Loss 1.733, LR 3.32e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 127/267 [1:57:13<2:09:51, 55.65s/it]Epoch 0, Loss 1.733, LR 3.32e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 128/267 [1:58:06<2:07:33, 55.06s/it]Epoch 0, Loss 1.812, LR 3.29e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 128/267 [1:58:06<2:07:33, 55.06s/it]Epoch 0, Loss 1.812, LR 3.29e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 129/267 [1:59:04<2:08:47, 56.00s/it]Epoch 0, Loss 1.820, LR 3.26e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 129/267 [1:59:04<2:08:47, 56.00s/it]Epoch 0, Loss 1.820, LR 3.26e-05:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 130/267 [1:59:54<2:03:33, 54.11s/it]Epoch 0, Loss 1.752, LR 3.23e-05:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 130/267 [1:59:54<2:03:33, 54.11s/it]Epoch 0, Loss 1.752, LR 3.23e-05:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 131/267 [2:00:45<2:00:16, 53.06s/it]Epoch 0, Loss 1.689, LR 3.20e-05:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 131/267 [2:00:45<2:00:16, 53.06s/it]Epoch 0, Loss 1.689, LR 3.20e-05:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 132/267 [2:01:40<2:00:51, 53.71s/it]Epoch 0, Loss 1.767, LR 3.17e-05:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 132/267 [2:01:40<2:00:51, 53.71s/it]Epoch 0, Loss 1.767, LR 3.17e-05:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 133/267 [2:02:31<1:57:48, 52.75s/it]Epoch 0, Loss 1.654, LR 3.14e-05:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 133/267 [2:02:31<1:57:48, 52.75s/it]Epoch 0, Loss 1.654, LR 3.14e-05:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 134/267 [2:03:22<1:55:49, 52.25s/it]Epoch 0, Loss 1.568, LR 3.12e-05:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 134/267 [2:03:22<1:55:49, 52.25s/it]Epoch 0, Loss 1.568, LR 3.12e-05:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 135/267 [2:04:21<1:59:57, 54.53s/it]Epoch 0, Loss 1.837, LR 3.09e-05:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 135/267 [2:04:22<1:59:57, 54.53s/it]Epoch 0, Loss 1.837, LR 3.09e-05:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 136/267 [2:05:12<1:56:24, 53.32s/it]Epoch 0, Loss 1.744, LR 3.06e-05:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 136/267 [2:05:12<1:56:24, 53.32s/it]Epoch 0, Loss 1.744, LR 3.06e-05:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 137/267 [2:06:13<2:00:21, 55.55s/it]Epoch 0, Loss 1.672, LR 3.03e-05:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 137/267 [2:06:13<2:00:21, 55.55s/it]Epoch 0, Loss 1.672, LR 3.03e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 138/267 [2:07:08<1:59:35, 55.62s/it]Epoch 0, Loss 1.752, LR 3.00e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 138/267 [2:07:09<1:59:35, 55.62s/it]Epoch 0, Loss 1.752, LR 3.00e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 139/267 [2:08:06<1:59:39, 56.09s/it]Epoch 0, Loss 1.699, LR 2.97e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 139/267 [2:08:06<1:59:39, 56.09s/it]Epoch 0, Loss 1.699, LR 2.97e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 140/267 [2:09:03<1:59:13, 56.33s/it]Epoch 0, Loss 1.765, LR 2.94e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 140/267 [2:09:03<1:59:13, 56.33s/it]Epoch 0, Loss 1.765, LR 2.94e-05:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 141/267 [2:10:02<2:00:26, 57.35s/it]Epoch 0, Loss 1.744, LR 2.91e-05:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 141/267 [2:10:02<2:00:26, 57.35s/it]Epoch 0, Loss 1.744, LR 2.91e-05:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 142/267 [2:10:59<1:59:07, 57.18s/it]Epoch 0, Loss 1.724, LR 2.88e-05:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 142/267 [2:10:59<1:59:07, 57.18s/it]Epoch 0, Loss 1.724, LR 2.88e-05:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 143/267 [2:12:00<2:00:16, 58.20s/it]Epoch 0, Loss 1.605, LR 2.85e-05:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 143/267 [2:12:00<2:00:16, 58.20s/it]Epoch 0, Loss 1.605, LR 2.85e-05:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 144/267 [2:12:56<1:58:10, 57.65s/it]Epoch 0, Loss 1.600, LR 2.82e-05:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 144/267 [2:12:56<1:58:10, 57.65s/it]Epoch 0, Loss 1.600, LR 2.82e-05:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 145/267 [2:13:55<1:57:53, 57.98s/it]Epoch 0, Loss 1.851, LR 2.79e-05:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 145/267 [2:13:55<1:57:53, 57.98s/it]Epoch 0, Loss 1.851, LR 2.79e-05:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 146/267 [2:14:48<1:53:57, 56.51s/it]Epoch 0, Loss 1.752, LR 2.76e-05:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 146/267 [2:14:48<1:53:57, 56.51s/it]Epoch 0, Loss 1.752, LR 2.76e-05:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 147/267 [2:15:43<1:52:13, 56.11s/it]Epoch 0, Loss 1.805, LR 2.74e-05:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 147/267 [2:15:43<1:52:13, 56.11s/it]Epoch 0, Loss 1.805, LR 2.74e-05:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 148/267 [2:16:43<1:53:31, 57.24s/it]Epoch 0, Loss 1.765, LR 2.71e-05:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 148/267 [2:16:43<1:53:31, 57.24s/it]Epoch 0, Loss 1.765, LR 2.71e-05:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 149/267 [2:17:41<1:53:09, 57.54s/it]Epoch 0, Loss 1.689, LR 2.68e-05:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 149/267 [2:17:41<1:53:09, 57.54s/it]Epoch 0, Loss 1.689, LR 2.68e-05:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 150/267 [2:18:46<1:56:39, 59.83s/it]Epoch 0, Loss 1.725, LR 2.65e-05:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 150/267 [2:18:46<1:56:39, 59.83s/it]Epoch 0, Loss 1.725, LR 2.65e-05:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 151/267 [2:19:48<1:56:42, 60.37s/it]Epoch 0, Loss 1.723, LR 2.62e-05:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 151/267 [2:19:48<1:56:42, 60.37s/it]Epoch 0, Loss 1.723, LR 2.62e-05:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 152/267 [2:20:44<1:53:30, 59.22s/it]Epoch 0, Loss 1.753, LR 2.59e-05:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 152/267 [2:20:45<1:53:30, 59.22s/it]Epoch 0, Loss 1.753, LR 2.59e-05:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 153/267 [2:21:42<1:51:27, 58.66s/it]Epoch 0, Loss 1.741, LR 2.56e-05:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 153/267 [2:21:42<1:51:27, 58.66s/it]Epoch 0, Loss 1.741, LR 2.56e-05:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 154/267 [2:22:39<1:49:44, 58.27s/it]Epoch 0, Loss 1.700, LR 2.53e-05:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 154/267 [2:22:39<1:49:44, 58.27s/it]Epoch 0, Loss 1.700, LR 2.53e-05:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 155/267 [2:23:43<1:51:50, 59.92s/it]Epoch 0, Loss 1.705, LR 2.50e-05:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 155/267 [2:23:43<1:51:50, 59.92s/it]Epoch 0, Loss 1.705, LR 2.50e-05:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 156/267 [2:24:43<1:50:42, 59.84s/it]Epoch 0, Loss 1.758, LR 2.47e-05:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 156/267 [2:24:43<1:50:42, 59.84s/it]Epoch 0, Loss 1.758, LR 2.47e-05:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 157/267 [2:25:39<1:47:59, 58.90s/it]Epoch 0, Loss 1.809, LR 2.44e-05:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 157/267 [2:25:39<1:47:59, 58.90s/it]Epoch 0, Loss 1.809, LR 2.44e-05:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 158/267 [2:26:37<1:46:35, 58.67s/it]Epoch 0, Loss 1.736, LR 2.41e-05:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 158/267 [2:26:37<1:46:35, 58.67s/it]Epoch 0, Loss 1.736, LR 2.41e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 159/267 [2:27:32<1:43:31, 57.52s/it]Epoch 0, Loss 1.759, LR 2.38e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 159/267 [2:27:32<1:43:31, 57.52s/it]Epoch 0, Loss 1.759, LR 2.38e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 160/267 [2:28:30<1:42:25, 57.44s/it]Epoch 0, Loss 1.775, LR 2.36e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 160/267 [2:28:30<1:42:25, 57.44s/it]Epoch 0, Loss 1.775, LR 2.36e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 161/267 [2:29:18<1:36:56, 54.87s/it]Epoch 0, Loss 1.699, LR 2.33e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 161/267 [2:29:19<1:36:56, 54.87s/it]Epoch 0, Loss 1.699, LR 2.33e-05:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 162/267 [2:30:13<1:35:43, 54.70s/it]Epoch 0, Loss 1.652, LR 2.30e-05:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 162/267 [2:30:13<1:35:43, 54.70s/it]Epoch 0, Loss 1.652, LR 2.30e-05:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 163/267 [2:30:54<1:27:43, 50.61s/it]Epoch 0, Loss 1.654, LR 2.27e-05:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 163/267 [2:30:54<1:27:43, 50.61s/it]Epoch 0, Loss 1.654, LR 2.27e-05:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 164/267 [2:31:54<1:32:05, 53.64s/it]Epoch 0, Loss 1.569, LR 2.24e-05:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 164/267 [2:31:54<1:32:05, 53.64s/it]Epoch 0, Loss 1.569, LR 2.24e-05:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 165/267 [2:32:54<1:34:01, 55.31s/it]Epoch 0, Loss 1.611, LR 2.21e-05:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 165/267 [2:32:54<1:34:01, 55.31s/it]Epoch 0, Loss 1.611, LR 2.21e-05:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 166/267 [2:33:49<1:32:58, 55.23s/it]Epoch 0, Loss 1.757, LR 2.18e-05:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 166/267 [2:33:49<1:32:58, 55.23s/it]Epoch 0, Loss 1.757, LR 2.18e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 167/267 [2:34:42<1:30:59, 54.59s/it]Epoch 0, Loss 1.770, LR 2.16e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 167/267 [2:34:42<1:30:59, 54.59s/it]Epoch 0, Loss 1.770, LR 2.16e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 168/267 [2:35:38<1:30:56, 55.11s/it]Epoch 0, Loss 1.773, LR 2.13e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 168/267 [2:35:38<1:30:56, 55.11s/it]Epoch 0, Loss 1.773, LR 2.13e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 169/267 [2:36:28<1:27:15, 53.43s/it]Epoch 0, Loss 1.693, LR 2.10e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 169/267 [2:36:28<1:27:15, 53.43s/it]Epoch 0, Loss 1.693, LR 2.10e-05:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 170/267 [2:37:21<1:26:17, 53.38s/it]Epoch 0, Loss 1.655, LR 2.07e-05:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 170/267 [2:37:21<1:26:17, 53.38s/it]Epoch 0, Loss 1.655, LR 2.07e-05:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 171/267 [2:38:15<1:25:36, 53.51s/it]Epoch 0, Loss 1.708, LR 2.04e-05:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 171/267 [2:38:15<1:25:36, 53.51s/it]Epoch 0, Loss 1.708, LR 2.04e-05:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 172/267 [2:39:13<1:27:11, 55.06s/it]Epoch 0, Loss 1.754, LR 2.02e-05:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 172/267 [2:39:14<1:27:11, 55.06s/it]Epoch 0, Loss 1.754, LR 2.02e-05:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 173/267 [2:40:00<1:22:22, 52.58s/it]Epoch 0, Loss 1.729, LR 1.99e-05:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 173/267 [2:40:00<1:22:22, 52.58s/it]Epoch 0, Loss 1.729, LR 1.99e-05:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 174/267 [2:40:47<1:18:54, 50.91s/it]Epoch 0, Loss 1.600, LR 1.96e-05:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 174/267 [2:40:47<1:18:54, 50.91s/it]Epoch 0, Loss 1.600, LR 1.96e-05:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 175/267 [2:41:43<1:20:29, 52.50s/it]Epoch 0, Loss 1.741, LR 1.93e-05:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 175/267 [2:41:43<1:20:29, 52.50s/it]Epoch 0, Loss 1.741, LR 1.93e-05:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 176/267 [2:42:37<1:20:13, 52.89s/it]Epoch 0, Loss 1.720, LR 1.91e-05:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 176/267 [2:42:37<1:20:13, 52.89s/it]Epoch 0, Loss 1.720, LR 1.91e-05:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 177/267 [2:43:36<1:22:02, 54.70s/it]Epoch 0, Loss 1.688, LR 1.88e-05:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 177/267 [2:43:36<1:22:02, 54.70s/it]Epoch 0, Loss 1.688, LR 1.88e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 178/267 [2:44:28<1:19:57, 53.91s/it]Epoch 0, Loss 1.595, LR 1.85e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 178/267 [2:44:28<1:19:57, 53.91s/it]Epoch 0, Loss 1.595, LR 1.85e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 179/267 [2:45:26<1:20:40, 55.01s/it]Epoch 0, Loss 1.835, LR 1.83e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 179/267 [2:45:26<1:20:40, 55.01s/it]Epoch 0, Loss 1.835, LR 1.83e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 180/267 [2:46:16<1:17:44, 53.61s/it]Epoch 0, Loss 1.624, LR 1.80e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 180/267 [2:46:16<1:17:44, 53.61s/it]Epoch 0, Loss 1.624, LR 1.80e-05:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 181/267 [2:47:08<1:16:11, 53.15s/it]Epoch 0, Loss 1.801, LR 1.77e-05:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 181/267 [2:47:08<1:16:11, 53.15s/it]Epoch 0, Loss 1.801, LR 1.77e-05:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 182/267 [2:48:07<1:17:34, 54.76s/it]Epoch 0, Loss 1.628, LR 1.75e-05:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 182/267 [2:48:07<1:17:34, 54.76s/it]Epoch 0, Loss 1.628, LR 1.75e-05:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 183/267 [2:49:04<1:17:41, 55.50s/it]Epoch 0, Loss 1.730, LR 1.72e-05:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 183/267 [2:49:04<1:17:41, 55.50s/it]Epoch 0, Loss 1.730, LR 1.72e-05:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 184/267 [2:50:00<1:17:05, 55.73s/it]Epoch 0, Loss 1.764, LR 1.69e-05:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 184/267 [2:50:00<1:17:05, 55.73s/it]Epoch 0, Loss 1.764, LR 1.69e-05:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 185/267 [2:50:48<1:12:47, 53.27s/it]Epoch 0, Loss 1.686, LR 1.67e-05:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 185/267 [2:50:48<1:12:47, 53.27s/it]Epoch 0, Loss 1.686, LR 1.67e-05:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 186/267 [2:51:48<1:14:36, 55.27s/it]Epoch 0, Loss 1.714, LR 1.64e-05:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 186/267 [2:51:48<1:14:36, 55.27s/it]Epoch 0, Loss 1.714, LR 1.64e-05:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 187/267 [2:52:49<1:16:09, 57.12s/it]Epoch 0, Loss 1.612, LR 1.62e-05:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 187/267 [2:52:49<1:16:09, 57.12s/it]Epoch 0, Loss 1.612, LR 1.62e-05:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 188/267 [2:53:44<1:14:13, 56.37s/it]Epoch 0, Loss 1.736, LR 1.59e-05:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 188/267 [2:53:44<1:14:13, 56.37s/it]Epoch 0, Loss 1.736, LR 1.59e-05:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 189/267 [2:54:42<1:13:50, 56.80s/it]Epoch 0, Loss 1.682, LR 1.57e-05:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 189/267 [2:54:42<1:13:50, 56.80s/it]Epoch 0, Loss 1.682, LR 1.57e-05:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 190/267 [2:55:48<1:16:31, 59.63s/it]Epoch 0, Loss 1.740, LR 1.54e-05:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 190/267 [2:55:48<1:16:31, 59.63s/it]Epoch 0, Loss 1.740, LR 1.54e-05:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 191/267 [2:56:40<1:12:32, 57.27s/it]Epoch 0, Loss 1.800, LR 1.52e-05:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 191/267 [2:56:40<1:12:32, 57.27s/it]Epoch 0, Loss 1.800, LR 1.52e-05:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 192/267 [2:57:33<1:10:16, 56.22s/it]Epoch 0, Loss 1.714, LR 1.49e-05:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 192/267 [2:57:33<1:10:16, 56.22s/it]Epoch 0, Loss 1.714, LR 1.49e-05:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 193/267 [2:58:24<1:07:13, 54.50s/it]Epoch 0, Loss 1.739, LR 1.47e-05:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 193/267 [2:58:24<1:07:13, 54.50s/it]Epoch 0, Loss 1.739, LR 1.47e-05:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 194/267 [2:59:20<1:07:05, 55.15s/it]Epoch 0, Loss 1.667, LR 1.44e-05:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 194/267 [2:59:21<1:07:05, 55.15s/it]Epoch 0, Loss 1.667, LR 1.44e-05:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 195/267 [3:00:16<1:06:13, 55.19s/it]Epoch 0, Loss 1.669, LR 1.42e-05:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 195/267 [3:00:16<1:06:13, 55.19s/it]Epoch 0, Loss 1.669, LR 1.42e-05:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 196/267 [3:01:00<1:01:35, 52.04s/it]Epoch 0, Loss 1.688, LR 1.40e-05:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 196/267 [3:01:01<1:01:35, 52.04s/it]Epoch 0, Loss 1.688, LR 1.40e-05:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 197/267 [3:01:57<1:02:21, 53.45s/it]Epoch 0, Loss 1.714, LR 1.37e-05:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 197/267 [3:01:57<1:02:21, 53.45s/it]Epoch 0, Loss 1.714, LR 1.37e-05:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 198/267 [3:02:49<1:01:00, 53.05s/it]Epoch 0, Loss 1.657, LR 1.35e-05:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 198/267 [3:02:49<1:01:00, 53.05s/it]Epoch 0, Loss 1.657, LR 1.35e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 199/267 [3:03:45<1:01:10, 53.98s/it]Epoch 0, Loss 1.629, LR 1.33e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 199/267 [3:03:46<1:01:10, 53.98s/it]Gradient norm: 0.2041015625
Batch idx 403
Batch idx 404
Batch idx 405
Batch idx 406
Gradient norm: 0.208984375
Batch idx 407
Batch idx 408
Batch idx 409
Batch idx 410
Gradient norm: 0.2451171875
Batch idx 411
Batch idx 412
Batch idx 413
Batch idx 414
Gradient norm: 0.1884765625
Batch idx 415
Batch idx 416
Batch idx 417
Batch idx 418
Gradient norm: 0.2177734375
Batch idx 419
Batch idx 420
Batch idx 421
Batch idx 422
Gradient norm: 0.2021484375
Batch idx 423
Batch idx 424
Batch idx 425
Batch idx 426
Gradient norm: 0.255859375
Batch idx 427
Batch idx 428
Batch idx 429
Batch idx 430
Gradient norm: 0.2314453125
Batch idx 431
Batch idx 432
Batch idx 433
Batch idx 434
Gradient norm: 0.2275390625
Batch idx 435
Batch idx 436
Batch idx 437
Batch idx 438
Gradient norm: 0.2412109375
Batch idx 439
Batch idx 440
Batch idx 441
Batch idx 442
Gradient norm: 0.2119140625
Batch idx 443
Batch idx 444
Batch idx 445
Batch idx 446
Gradient norm: 0.2021484375
Batch idx 447
Batch idx 448
Batch idx 449
Batch idx 450
Gradient norm: 0.2080078125
Batch idx 451
Batch idx 452
Batch idx 453
Batch idx 454
Gradient norm: 0.228515625
Batch idx 455
Batch idx 456
Batch idx 457
Batch idx 458
Gradient norm: 0.2041015625
Batch idx 459
Batch idx 460
Batch idx 461
Batch idx 462
Gradient norm: 0.19140625
Batch idx 463
Batch idx 464
Batch idx 465
Batch idx 466
Gradient norm: 0.189453125
Batch idx 467
Batch idx 468
Batch idx 469
Batch idx 470
Gradient norm: 0.193359375
Batch idx 471
Batch idx 472
Batch idx 473
Batch idx 474
Gradient norm: 0.205078125
Batch idx 475
Batch idx 476
Batch idx 477
Batch idx 478
Gradient norm: 0.25390625
Batch idx 479
Batch idx 480
Batch idx 481
Batch idx 482
Gradient norm: 0.21875
Batch idx 483
Batch idx 484
Batch idx 485
Batch idx 486
Gradient norm: 0.2041015625
Batch idx 487
Batch idx 488
Batch idx 489
Batch idx 490
Gradient norm: 0.236328125
Batch idx 491
Batch idx 492
Batch idx 493
Batch idx 494
Gradient norm: 0.240234375
Batch idx 495
Batch idx 496
Batch idx 497
Batch idx 498
Gradient norm: 0.271484375
Batch idx 499
Batch idx 500
Batch idx 501
Batch idx 502
Gradient norm: 0.2158203125
Batch idx 503
Batch idx 504
Batch idx 505
Batch idx 506
Gradient norm: 0.1943359375
Batch idx 507
Batch idx 508
Batch idx 509
Batch idx 510
Gradient norm: 0.220703125
Batch idx 511
Batch idx 512
Batch idx 513
Batch idx 514
Gradient norm: 0.21484375
Batch idx 515
Batch idx 516
Batch idx 517
Batch idx 518
Gradient norm: 0.28515625
Batch idx 519
Batch idx 520
Batch idx 521
Batch idx 522
Gradient norm: 0.2470703125
Batch idx 523
Batch idx 524
Batch idx 525
Batch idx 526
Gradient norm: 0.2177734375
Batch idx 527
Batch idx 528
Batch idx 529
Batch idx 530
Gradient norm: 0.228515625
Batch idx 531
Batch idx 532
Batch idx 533
Batch idx 534
Gradient norm: 0.2470703125
Batch idx 535
Batch idx 536
Batch idx 537
Batch idx 538
Gradient norm: 0.462890625
Batch idx 539
Batch idx 540
Batch idx 541
Batch idx 542
Gradient norm: 0.2294921875
Batch idx 543
Batch idx 544
Batch idx 545
Batch idx 546
Gradient norm: 0.2119140625
Batch idx 547
Batch idx 548
Batch idx 549
Batch idx 550
Gradient norm: 0.203125
Batch idx 551
Batch idx 552
Batch idx 553
Batch idx 554
Gradient norm: 0.2373046875
Batch idx 555
Batch idx 556
Batch idx 557
Batch idx 558
Gradient norm: 0.2080078125
Batch idx 559
Batch idx 560
Batch idx 561
Batch idx 562
Gradient norm: 0.2138671875
Batch idx 563
Batch idx 564
Batch idx 565
Batch idx 566
Gradient norm: 0.2041015625
Batch idx 567
Batch idx 568
Batch idx 569
Batch idx 570
Gradient norm: 0.255859375
Batch idx 571
Batch idx 572
Batch idx 573
Batch idx 574
Gradient norm: 0.2353515625
Batch idx 575
Batch idx 576
Batch idx 577
Batch idx 578
Gradient norm: 0.2119140625
Batch idx 579
Batch idx 580
Batch idx 581
Batch idx 582
Gradient norm: 0.2021484375
Batch idx 583
Batch idx 584
Batch idx 585
Batch idx 586
Gradient norm: 0.220703125
Batch idx 587
Batch idx 588
Batch idx 589
Batch idx 590
Gradient norm: 0.2060546875
Batch idx 591
Batch idx 592
Batch idx 593
Batch idx 594
Gradient norm: 0.2099609375
Batch idx 595
Batch idx 596
Batch idx 597
Batch idx 598
Gradient norm: 0.177734375
Batch idx 599
Batch idx 600
Batch idx 601
Batch idx 602
Gradient norm: 0.25390625
Batch idx 603
Batch idx 604
Batch idx 605
Batch idx 606
Gradient norm: 0.2158203125
Batch idx 607
Batch idx 608
Batch idx 609
Batch idx 610
Gradient norm: 0.212890625
Batch idx 611
Batch idx 612
Batch idx 613
Batch idx 614
Gradient norm: 0.2080078125
Batch idx 615
Batch idx 616
Batch idx 617
Batch idx 618
Gradient norm: 0.2412109375
Batch idx 619
Batch idx 620
Batch idx 621
Batch idx 622
Gradient norm: 0.2109375
Batch idx 623
Batch idx 624
Batch idx 625
Batch idx 626
Gradient norm: 0.20703125
Batch idx 627
Batch idx 628
Batch idx 629
Batch idx 630
Gradient norm: 0.2412109375
Batch idx 631
Batch idx 632
Batch idx 633
Batch idx 634
Gradient norm: 0.20703125
Batch idx 635
Batch idx 636
Batch idx 637
Batch idx 638
Gradient norm: 0.20703125
Batch idx 639
Batch idx 640
Batch idx 641
Batch idx 642
Gradient norm: 0.251953125
Batch idx 643
Batch idx 644
Batch idx 645
Batch idx 646
Gradient norm: 0.251953125
Batch idx 647
Batch idx 648
Batch idx 649
Batch idx 650
Gradient norm: 0.265625
Batch idx 651
Batch idx 652
Batch idx 653
Batch idx 654
Gradient norm: 0.2353515625
Batch idx 655
Batch idx 656
Batch idx 657
Batch idx 658
Gradient norm: 0.21875
Batch idx 659
Batch idx 660
Batch idx 661
Batch idx 662
Gradient norm: 0.2275390625
Batch idx 663
Batch idx 664
Batch idx 665
Batch idx 666
Gradient norm: 0.25
Batch idx 667
Batch idx 668
Batch idx 669
Batch idx 670
Gradient norm: 0.2099609375
Batch idx 671
Batch idx 672
Batch idx 673
Batch idx 674
Gradient norm: 0.21875
Batch idx 675
Batch idx 676
Batch idx 677
Batch idx 678
Gradient norm: 0.2275390625
Batch idx 679
Batch idx 680
Batch idx 681
Batch idx 682
Gradient norm: 0.208984375
Batch idx 683
Batch idx 684
Batch idx 685
Batch idx 686
Gradient norm: 0.2734375
Batch idx 687
Batch idx 688
Batch idx 689
Batch idx 690
Gradient norm: 0.2431640625
Batch idx 691
Batch idx 692
Batch idx 693
Batch idx 694
Gradient norm: 0.24609375
Batch idx 695
Batch idx 696
Batch idx 697
Batch idx 698
Gradient norm: 0.2109375
Batch idx 699
Batch idx 700
Batch idx 701
Batch idx 702
Gradient norm: 0.2177734375
Batch idx 703
Batch idx 704
Batch idx 705
Batch idx 706
Gradient norm: 0.1953125
Batch idx 707
Batch idx 708
Batch idx 709
Batch idx 710
Gradient norm: 0.25390625
Batch idx 711
Batch idx 712
Batch idx 713
Batch idx 714
Gradient norm: 0.23046875
Batch idx 715
Batch idx 716
Batch idx 717
Batch idx 718
Gradient norm: 0.2265625
Batch idx 719
Batch idx 720
Batch idx 721
Batch idx 722
Gradient norm: 0.205078125
Batch idx 723
Batch idx 724
Batch idx 725
Batch idx 726
Gradient norm: 0.2099609375
Batch idx 727
Batch idx 728
Batch idx 729
Batch idx 730
Gradient norm: 0.205078125
Batch idx 731
Batch idx 732
Batch idx 733
Batch idx 734
Gradient norm: 0.22265625
Batch idx 735
Batch idx 736
Batch idx 737
Batch idx 738
Gradient norm: 0.2236328125
Batch idx 739
Batch idx 740
Batch idx 741
Batch idx 742
Gradient norm: 0.26171875
Batch idx 743
Batch idx 744
Batch idx 745
Batch idx 746
Gradient norm: 0.24609375
Batch idx 747
Batch idx 748
Batch idx 749
Batch idx 750
Gradient norm: 0.1953125
Batch idx 751
Batch idx 752
Batch idx 753
Batch idx 754
Gradient norm: 0.2578125
Batch idx 755
Batch idx 756
Batch idx 757
Batch idx 758
Gradient norm: 0.2138671875
Batch idx 759
Batch idx 760
Batch idx 761
Batch idx 762
Gradient norm: 0.2138671875
Batch idx 763
Batch idx 764
Batch idx 765
Batch idx 766
Gradient norm: 0.203125
Batch idx 767
Batch idx 768
Batch idx 769
Batch idx 770
Gradient norm: 0.2431640625
Batch idx 771
Batch idx 772
Batch idx 773
Batch idx 774
Gradient norm: 0.259765625
Batch idx 775
Batch idx 776
Batch idx 777
Batch idx 778
Gradient norm: 0.2255859375
Batch idx 779
Batch idx 780
Batch idx 781
Batch idx 782
Gradient norm: 0.2314453125
Batch idx 783
Batch idx 784
Batch idx 785
Batch idx 786
Gradient norm: 0.244140625
Batch idx 787
Batch idx 788
Batch idx 789
Batch idx 790
Gradient norm: 0.23828125
Batch idx 791
Batch idx 792
Batch idx 793
Batch idx 794
Gradient norm: 0.234375
Batch idx 795
Batch idx 796
Batch idx 797
Epoch 0, Loss 1.629, LR 1.33e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 200/267 [3:04:36<59:00, 52.85s/it]  Epoch 0, Loss 1.691, LR 1.30e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 200/267 [3:04:36<59:00, 52.85s/it]Epoch 0, Loss 1.691, LR 1.30e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 201/267 [3:05:32<59:20, 53.95s/it]Epoch 0, Loss 1.700, LR 1.28e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 201/267 [3:05:32<59:20, 53.95s/it]Epoch 0, Loss 1.700, LR 1.28e-05:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 202/267 [3:06:29<59:13, 54.67s/it]Epoch 0, Loss 1.692, LR 1.26e-05:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 202/267 [3:06:29<59:13, 54.67s/it]Epoch 0, Loss 1.692, LR 1.26e-05:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 203/267 [3:07:25<58:48, 55.14s/it]Epoch 0, Loss 1.613, LR 1.24e-05:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 203/267 [3:07:25<58:48, 55.14s/it]Epoch 0, Loss 1.613, LR 1.24e-05:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 204/267 [3:08:20<57:54, 55.15s/it]Epoch 0, Loss 1.742, LR 1.22e-05:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 204/267 [3:08:20<57:54, 55.15s/it]Epoch 0, Loss 1.742, LR 1.22e-05:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 205/267 [3:09:19<58:08, 56.26s/it]Epoch 0, Loss 1.773, LR 1.20e-05:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 205/267 [3:09:19<58:08, 56.26s/it]Epoch 0, Loss 1.773, LR 1.20e-05:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 206/267 [3:10:18<58:00, 57.05s/it]Epoch 0, Loss 1.611, LR 1.17e-05:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 206/267 [3:10:18<58:00, 57.05s/it]Epoch 0, Loss 1.611, LR 1.17e-05:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 207/267 [3:11:13<56:33, 56.56s/it]Epoch 0, Loss 1.752, LR 1.15e-05:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 207/267 [3:11:13<56:33, 56.56s/it]Epoch 0, Loss 1.752, LR 1.15e-05:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 208/267 [3:11:56<51:41, 52.56s/it]Epoch 0, Loss 1.669, LR 1.13e-05:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 208/267 [3:11:56<51:41, 52.56s/it]Epoch 0, Loss 1.669, LR 1.13e-05:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 209/267 [3:12:49<50:43, 52.47s/it]Epoch 0, Loss 1.746, LR 1.11e-05:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 209/267 [3:12:49<50:43, 52.47s/it]Epoch 0, Loss 1.746, LR 1.11e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 210/267 [3:13:46<51:18, 54.01s/it]Epoch 0, Loss 1.695, LR 1.09e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 210/267 [3:13:46<51:18, 54.01s/it]Epoch 0, Loss 1.695, LR 1.09e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 211/267 [3:14:43<51:10, 54.84s/it]Epoch 0, Loss 1.731, LR 1.07e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 211/267 [3:14:43<51:10, 54.84s/it]Epoch 0, Loss 1.731, LR 1.07e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 212/267 [3:15:40<50:50, 55.47s/it]Epoch 0, Loss 1.599, LR 1.05e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 212/267 [3:15:40<50:50, 55.47s/it]Epoch 0, Loss 1.599, LR 1.05e-05:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 213/267 [3:16:24<46:43, 51.91s/it]Epoch 0, Loss 1.740, LR 1.03e-05:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 213/267 [3:16:24<46:43, 51.91s/it]Epoch 0, Loss 1.740, LR 1.03e-05:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 214/267 [3:17:14<45:25, 51.42s/it]Epoch 0, Loss 1.717, LR 1.02e-05:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 214/267 [3:17:14<45:25, 51.42s/it]Epoch 0, Loss 1.717, LR 1.02e-05:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 215/267 [3:18:07<44:55, 51.84s/it]Epoch 0, Loss 1.751, LR 9.97e-06:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 215/267 [3:18:07<44:55, 51.84s/it]Epoch 0, Loss 1.751, LR 9.97e-06:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 216/267 [3:19:07<46:15, 54.43s/it]Epoch 0, Loss 1.705, LR 9.79e-06:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 216/267 [3:19:07<46:15, 54.43s/it]Epoch 0, Loss 1.705, LR 9.79e-06:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 217/267 [3:20:07<46:41, 56.02s/it]Epoch 0, Loss 1.711, LR 9.61e-06:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 217/267 [3:20:07<46:41, 56.02s/it]Epoch 0, Loss 1.711, LR 9.61e-06:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 218/267 [3:21:06<46:37, 57.10s/it]Epoch 0, Loss 1.737, LR 9.44e-06:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 218/267 [3:21:07<46:37, 57.10s/it]Epoch 0, Loss 1.737, LR 9.44e-06:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 219/267 [3:22:09<47:02, 58.80s/it]Epoch 0, Loss 1.758, LR 9.26e-06:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 219/267 [3:22:09<47:02, 58.80s/it]Epoch 0, Loss 1.758, LR 9.26e-06:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 220/267 [3:23:10<46:27, 59.31s/it]Epoch 0, Loss 1.485, LR 9.09e-06:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 220/267 [3:23:10<46:27, 59.31s/it]Epoch 0, Loss 1.485, LR 9.09e-06:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 221/267 [3:24:05<44:33, 58.12s/it]Epoch 0, Loss 1.692, LR 8.93e-06:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 221/267 [3:24:05<44:33, 58.12s/it]Epoch 0, Loss 1.692, LR 8.93e-06:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 222/267 [3:25:06<44:09, 58.87s/it]Epoch 0, Loss 1.637, LR 8.76e-06:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 222/267 [3:25:06<44:09, 58.87s/it]Epoch 0, Loss 1.637, LR 8.76e-06:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 223/267 [3:26:10<44:17, 60.39s/it]Epoch 0, Loss 1.731, LR 8.60e-06:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 223/267 [3:26:10<44:17, 60.39s/it]Epoch 0, Loss 1.731, LR 8.60e-06:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 224/267 [3:26:55<40:04, 55.91s/it]Epoch 0, Loss 1.743, LR 8.44e-06:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 224/267 [3:26:55<40:04, 55.91s/it]Epoch 0, Loss 1.743, LR 8.44e-06:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 225/267 [3:27:50<38:54, 55.57s/it]Epoch 0, Loss 1.715, LR 8.29e-06:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 225/267 [3:27:50<38:54, 55.57s/it]Epoch 0, Loss 1.715, LR 8.29e-06:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 226/267 [3:28:52<39:22, 57.63s/it]Epoch 0, Loss 1.732, LR 8.14e-06:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 226/267 [3:28:52<39:22, 57.63s/it]Epoch 0, Loss 1.732, LR 8.14e-06:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 227/267 [3:29:39<36:15, 54.39s/it]Epoch 0, Loss 1.725, LR 7.99e-06:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 227/267 [3:29:39<36:15, 54.39s/it]Epoch 0, Loss 1.725, LR 7.99e-06:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 228/267 [3:30:32<34:58, 53.81s/it]Epoch 0, Loss 1.643, LR 7.85e-06:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 228/267 [3:30:32<34:58, 53.81s/it]Epoch 0, Loss 1.643, LR 7.85e-06:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 229/267 [3:31:20<33:01, 52.15s/it]Epoch 0, Loss 1.634, LR 7.70e-06:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 229/267 [3:31:20<33:01, 52.15s/it]Epoch 0, Loss 1.634, LR 7.70e-06:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 230/267 [3:32:18<33:16, 53.95s/it]Epoch 0, Loss 1.647, LR 7.57e-06:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 230/267 [3:32:18<33:16, 53.95s/it]Epoch 0, Loss 1.647, LR 7.57e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 231/267 [3:33:09<31:54, 53.17s/it]Epoch 0, Loss 1.743, LR 7.43e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 231/267 [3:33:09<31:54, 53.17s/it]Epoch 0, Loss 1.743, LR 7.43e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 232/267 [3:34:01<30:43, 52.68s/it]Epoch 0, Loss 1.734, LR 7.30e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 232/267 [3:34:01<30:43, 52.68s/it]Epoch 0, Loss 1.734, LR 7.30e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 233/267 [3:34:57<30:28, 53.78s/it]Epoch 0, Loss 1.779, LR 7.17e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 233/267 [3:34:57<30:28, 53.78s/it]Epoch 0, Loss 1.779, LR 7.17e-06:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 234/267 [3:35:47<28:54, 52.56s/it]Epoch 0, Loss 1.741, LR 7.05e-06:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 234/267 [3:35:47<28:54, 52.56s/it]Epoch 0, Loss 1.741, LR 7.05e-06:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 235/267 [3:36:35<27:23, 51.36s/it]Epoch 0, Loss 1.754, LR 6.93e-06:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 235/267 [3:36:36<27:23, 51.36s/it]Epoch 0, Loss 1.754, LR 6.93e-06:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 236/267 [3:37:34<27:39, 53.52s/it]Epoch 0, Loss 1.732, LR 6.81e-06:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 236/267 [3:37:34<27:39, 53.52s/it]Epoch 0, Loss 1.732, LR 6.81e-06:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 237/267 [3:38:26<26:27, 52.93s/it]Epoch 0, Loss 1.794, LR 6.70e-06:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 237/267 [3:38:26<26:27, 52.93s/it]Epoch 0, Loss 1.794, LR 6.70e-06:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 238/267 [3:39:26<26:43, 55.29s/it]Epoch 0, Loss 1.705, LR 6.59e-06:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 238/267 [3:39:27<26:43, 55.29s/it]Epoch 0, Loss 1.705, LR 6.59e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 239/267 [3:40:22<25:53, 55.47s/it]Epoch 0, Loss 1.695, LR 6.48e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 239/267 [3:40:22<25:53, 55.47s/it]Epoch 0, Loss 1.695, LR 6.48e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 240/267 [3:41:11<24:01, 53.39s/it]Epoch 0, Loss 1.685, LR 6.38e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 240/267 [3:41:11<24:01, 53.39s/it]Epoch 0, Loss 1.685, LR 6.38e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 241/267 [3:42:13<24:20, 56.16s/it]Epoch 0, Loss 1.731, LR 6.28e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 241/267 [3:42:13<24:20, 56.16s/it]Epoch 0, Loss 1.731, LR 6.28e-06:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 242/267 [3:42:59<22:04, 53.00s/it]Epoch 0, Loss 1.752, LR 6.18e-06:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 242/267 [3:42:59<22:04, 53.00s/it]Epoch 0, Loss 1.752, LR 6.18e-06:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 243/267 [3:43:46<20:26, 51.09s/it]Epoch 0, Loss 1.771, LR 6.09e-06:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 243/267 [3:43:46<20:26, 51.09s/it]Epoch 0, Loss 1.771, LR 6.09e-06:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 244/267 [3:44:44<20:22, 53.14s/it]Epoch 0, Loss 1.743, LR 6.00e-06:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 244/267 [3:44:44<20:22, 53.14s/it]Epoch 0, Loss 1.743, LR 6.00e-06:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 245/267 [3:45:35<19:18, 52.68s/it]Epoch 0, Loss 1.770, LR 5.92e-06:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 245/267 [3:45:35<19:18, 52.68s/it]Epoch 0, Loss 1.770, LR 5.92e-06:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 246/267 [3:46:28<18:24, 52.61s/it]Epoch 0, Loss 1.698, LR 5.84e-06:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 246/267 [3:46:28<18:24, 52.61s/it]Epoch 0, Loss 1.698, LR 5.84e-06:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 247/267 [3:47:26<18:09, 54.47s/it]Epoch 0, Loss 1.681, LR 5.76e-06:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 247/267 [3:47:27<18:09, 54.47s/it]Epoch 0, Loss 1.681, LR 5.76e-06:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 248/267 [3:48:24<17:29, 55.23s/it]Epoch 0, Loss 1.727, LR 5.69e-06:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 248/267 [3:48:24<17:29, 55.23s/it]Epoch 0, Loss 1.727, LR 5.69e-06:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 249/267 [3:49:20<16:41, 55.66s/it]Epoch 0, Loss 1.728, LR 5.62e-06:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 249/267 [3:49:20<16:41, 55.66s/it]Epoch 0, Loss 1.728, LR 5.62e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 250/267 [3:50:16<15:45, 55.59s/it]Epoch 0, Loss 1.692, LR 5.55e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 250/267 [3:50:16<15:45, 55.59s/it]Epoch 0, Loss 1.692, LR 5.55e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 251/267 [3:51:01<14:00, 52.51s/it]Epoch 0, Loss 1.640, LR 5.49e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 251/267 [3:51:01<14:00, 52.51s/it]Epoch 0, Loss 1.640, LR 5.49e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 252/267 [3:52:00<13:38, 54.58s/it]Epoch 0, Loss 1.590, LR 5.43e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 252/267 [3:52:00<13:38, 54.58s/it]Epoch 0, Loss 1.590, LR 5.43e-06:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 253/267 [3:52:54<12:39, 54.24s/it]Epoch 0, Loss 1.641, LR 5.37e-06:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 253/267 [3:52:54<12:39, 54.24s/it]Epoch 0, Loss 1.641, LR 5.37e-06:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 254/267 [3:53:55<12:13, 56.39s/it]Epoch 0, Loss 1.623, LR 5.32e-06:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 254/267 [3:53:55<12:13, 56.39s/it]Epoch 0, Loss 1.623, LR 5.32e-06:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 255/267 [3:54:48<11:03, 55.32s/it]Epoch 0, Loss 1.711, LR 5.27e-06:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 255/267 [3:54:48<11:03, 55.32s/it]Epoch 0, Loss 1.711, LR 5.27e-06:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 256/267 [3:55:46<10:15, 55.99s/it]Epoch 0, Loss 1.792, LR 5.23e-06:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 256/267 [3:55:46<10:15, 55.99s/it]Epoch 0, Loss 1.792, LR 5.23e-06:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 257/267 [3:56:44<09:28, 56.81s/it]Epoch 0, Loss 1.799, LR 5.19e-06:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 257/267 [3:56:44<09:28, 56.81s/it]Epoch 0, Loss 1.799, LR 5.19e-06:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 258/267 [3:57:45<08:41, 57.97s/it]Epoch 0, Loss 1.646, LR 5.15e-06:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 258/267 [3:57:45<08:41, 57.97s/it]Epoch 0, Loss 1.646, LR 5.15e-06:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 259/267 [3:58:33<07:21, 55.13s/it]Epoch 0, Loss 1.715, LR 5.12e-06:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 259/267 [3:58:33<07:21, 55.13s/it]Epoch 0, Loss 1.715, LR 5.12e-06:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 260/267 [3:59:39<06:47, 58.20s/it]Epoch 0, Loss 1.727, LR 5.09e-06:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 260/267 [3:59:39<06:47, 58.20s/it]Epoch 0, Loss 1.727, LR 5.09e-06:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 261/267 [4:00:31<05:38, 56.43s/it]Epoch 0, Loss 1.647, LR 5.07e-06:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 261/267 [4:00:31<05:38, 56.43s/it]Epoch 0, Loss 1.647, LR 5.07e-06:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 262/267 [4:01:14<04:22, 52.46s/it]Epoch 0, Loss 1.624, LR 5.05e-06:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 262/267 [4:01:14<04:22, 52.46s/it]Epoch 0, Loss 1.624, LR 5.05e-06:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 263/267 [4:02:05<03:28, 52.04s/it]Epoch 0, Loss 1.654, LR 5.03e-06:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 263/267 [4:02:05<03:28, 52.04s/it]Epoch 0, Loss 1.654, LR 5.03e-06:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 264/267 [4:02:52<02:31, 50.43s/it]Epoch 0, Loss 1.735, LR 5.02e-06:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 264/267 [4:02:52<02:31, 50.43s/it]Epoch 0, Loss 1.735, LR 5.02e-06:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 265/267 [4:03:47<01:43, 51.93s/it]Epoch 0, Loss 1.580, LR 5.01e-06:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 265/267 [4:03:48<01:43, 51.93s/it]Epoch 0, Loss 1.580, LR 5.01e-06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 266/267 [4:04:48<00:54, 54.56s/it]Epoch 0, Loss 1.723, LR 5.00e-06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 266/267 [4:04:48<00:54, 54.56s/it]Epoch 0, Loss 1.723, LR 5.00e-06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 267/267 [4:05:45<00:00, 55.28s/it]Epoch 0, Loss 1.712, LR 5.00e-06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 267/267 [4:05:45<00:00, 55.28s/it]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.007 MB of 0.007 MB uploadedwandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.018 MB of 0.116 MB uploaded (0.004 MB deduped)wandb: | 0.018 MB of 0.116 MB uploaded (0.004 MB deduped)wandb: / 0.116 MB of 0.116 MB uploaded (0.004 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 3.5%             
wandb: 
wandb: Run history:
wandb:                            grad_norm ‚ñà‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                 loss ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb:                                   lr ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      memory/allocated_after_backward ‚ñÅ
wandb:       memory/allocated_after_forward ‚ñÅ
wandb: memory/allocated_after_model_created ‚ñÅ
wandb:    memory/allocated_after_model_wrap ‚ñÅ
wandb:      memory/allocated_before_forward ‚ñÅ
wandb:                memory/allocated_peak ‚ñÅ
wandb:       memory/reserved_after_backward ‚ñÅ
wandb:        memory/reserved_after_forward ‚ñÅ
wandb: memory/reserved_after_model_creation ‚ñÅ
wandb:     memory/reserved_after_model_wrap ‚ñÅ
wandb:       memory/reserved_before_forward ‚ñÅ
wandb:                 memory/reserved_peak ‚ñÅ
wandb:                           time_taken ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                            grad_norm 0.24316
wandb:                                 loss 1.7118
wandb:                                   lr 1e-05
wandb:      memory/allocated_after_backward 379359232
wandb:       memory/allocated_after_forward 1023601152
wandb: memory/allocated_after_model_created 8652800
wandb:    memory/allocated_after_model_wrap 111200256
wandb:      memory/allocated_before_forward 111201280
wandb:                memory/allocated_peak 3124532224
wandb:       memory/reserved_after_backward 3013607424
wandb:        memory/reserved_after_forward 2860515328
wandb: memory/reserved_after_model_creation 115343360
wandb:     memory/reserved_after_model_wrap 1958739968
wandb:       memory/reserved_before_forward 1958739968
wandb:                 memory/reserved_peak 4009754624
wandb:                           time_taken 14780.809
wandb: 
wandb: üöÄ View run soft-energy-55 at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/lgs1naa4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240826_034729-lgs1naa4/logs
Rank 1: Model created: 0.105 GiB
Wrapping model w/ FSDP 1
Rank 1: Wrapped model: 1.824 GiB
Applying activation checkpointing 1
Rank 1: Before forward: 1.82 GiB
Rank 1: After forward: 2.66 GiB
Rank 1: After backward: 2.81 GiB
Rank 1: Peak allocated memory: 2.91 GiB
Rank 1: Peak reserved memory:  3.70 GiB
Rank 3: Model created: 0.107 GiB
Wrapping model w/ FSDP 3
Rank 3: Wrapped model: 1.824 GiB
Applying activation checkpointing 3
Rank 3: Before forward: 1.82 GiB
Rank 3: After forward: 2.67 GiB
Rank 3: After backward: 2.81 GiB
Rank 3: Peak allocated memory: 2.91 GiB
Rank 3: Peak reserved memory:  3.68 GiB
Rank 2: Model created: 0.107 GiB
Wrapping model w/ FSDP 2
Rank 2: Wrapped model: 1.824 GiB
Applying activation checkpointing 2
Rank 2: Before forward: 1.82 GiB
Rank 2: After forward: 2.66 GiB
Rank 2: After backward: 2.81 GiB
Rank 2: Peak allocated memory: 2.91 GiB
Rank 2: Peak reserved memory:  3.90 GiB
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (lgs1naa4) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
Epoch 0, Loss 1.712, LR 5.00e-06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 267/267 [4:06:30<00:00, 55.40s/it]
Batch idx 798
Gradient norm: 0.2265625
Batch idx 799
Batch idx 800
Batch idx 801
Batch idx 802
Gradient norm: 0.265625
Batch idx 803
Batch idx 804
Batch idx 805
Batch idx 806
Gradient norm: 0.220703125
Batch idx 807
Batch idx 808
Batch idx 809
Batch idx 810
Gradient norm: 0.2275390625
Batch idx 811
Batch idx 812
Batch idx 813
Batch idx 814
Gradient norm: 0.2109375
Batch idx 815
Batch idx 816
Batch idx 817
Batch idx 818
Gradient norm: 0.2080078125
Batch idx 819
Batch idx 820
Batch idx 821
Batch idx 822
Gradient norm: 0.2412109375
Batch idx 823
Batch idx 824
Batch idx 825
Batch idx 826
Gradient norm: 0.2119140625
Batch idx 827
Batch idx 828
Batch idx 829
Batch idx 830
Gradient norm: 0.2275390625
Batch idx 831
Batch idx 832
Batch idx 833
Batch idx 834
Gradient norm: 0.23828125
Batch idx 835
Batch idx 836
Batch idx 837
Batch idx 838
Gradient norm: 0.244140625
Batch idx 839
Batch idx 840
Batch idx 841
Batch idx 842
Gradient norm: 0.2333984375
Batch idx 843
Batch idx 844
Batch idx 845
Batch idx 846
Gradient norm: 0.25
Batch idx 847
Batch idx 848
Batch idx 849
Batch idx 850
Gradient norm: 0.244140625
Batch idx 851
Batch idx 852
Batch idx 853
Batch idx 854
Gradient norm: 0.25
Batch idx 855
Batch idx 856
Batch idx 857
Batch idx 858
Gradient norm: 0.21875
Batch idx 859
Batch idx 860
Batch idx 861
Batch idx 862
Gradient norm: 0.236328125
Batch idx 863
Batch idx 864
Batch idx 865
Batch idx 866
Gradient norm: 0.259765625
Batch idx 867
Batch idx 868
Batch idx 869
Batch idx 870
Gradient norm: 0.23046875
Batch idx 871
Batch idx 872
Batch idx 873
Batch idx 874
Gradient norm: 0.2041015625
Batch idx 875
Batch idx 876
Batch idx 877
Batch idx 878
Gradient norm: 0.25390625
Batch idx 879
Batch idx 880
Batch idx 881
Batch idx 882
Gradient norm: 0.2373046875
Batch idx 883
Batch idx 884
Batch idx 885
Batch idx 886
Gradient norm: 0.201171875
Batch idx 887
Batch idx 888
Batch idx 889
Batch idx 890
Gradient norm: 0.251953125
Batch idx 891
Batch idx 892
Batch idx 893
Batch idx 894
Gradient norm: 0.251953125
Batch idx 895
Batch idx 896
Batch idx 897
Batch idx 898
Gradient norm: 0.2197265625
Batch idx 899
Batch idx 900
Batch idx 901
Batch idx 902
Gradient norm: 0.2177734375
Batch idx 903
Batch idx 904
Batch idx 905
Batch idx 906
Gradient norm: 0.25390625
Batch idx 907
Batch idx 908
Batch idx 909
Batch idx 910
Gradient norm: 0.23046875
Batch idx 911
Batch idx 912
Batch idx 913
Batch idx 914
Gradient norm: 0.26171875
Batch idx 915
Batch idx 916
Batch idx 917
Batch idx 918
Gradient norm: 0.2099609375
Batch idx 919
Batch idx 920
Batch idx 921
Batch idx 922
Gradient norm: 0.220703125
Batch idx 923
Batch idx 924
Batch idx 925
Batch idx 926
Gradient norm: 0.2333984375
Batch idx 927
Batch idx 928
Batch idx 929
Batch idx 930
Gradient norm: 0.2109375
Batch idx 931
Batch idx 932
Batch idx 933
Batch idx 934
Gradient norm: 0.212890625
Batch idx 935
Batch idx 936
Batch idx 937
Batch idx 938
Gradient norm: 0.298828125
Batch idx 939
Batch idx 940
Batch idx 941
Batch idx 942
Gradient norm: 0.21875
Batch idx 943
Batch idx 944
Batch idx 945
Batch idx 946
Gradient norm: 0.22265625
Batch idx 947
Batch idx 948
Batch idx 949
Batch idx 950
Gradient norm: 0.205078125
Batch idx 951
Batch idx 952
Batch idx 953
Batch idx 954
Gradient norm: 0.2294921875
Batch idx 955
Batch idx 956
Batch idx 957
Batch idx 958
Gradient norm: 0.2490234375
Batch idx 959
Batch idx 960
Batch idx 961
Batch idx 962
Gradient norm: 0.236328125
Batch idx 963
Batch idx 964
Batch idx 965
Batch idx 966
Gradient norm: 0.2490234375
Batch idx 967
Batch idx 968
Batch idx 969
Batch idx 970
Gradient norm: 0.2236328125
Batch idx 971
Batch idx 972
Batch idx 973
Batch idx 974
Gradient norm: 0.23046875
Batch idx 975
Batch idx 976
Batch idx 977
Batch idx 978
Gradient norm: 0.55078125
Batch idx 979
Batch idx 980
Batch idx 981
Batch idx 982
Gradient norm: 0.2333984375
Batch idx 983
Batch idx 984
Batch idx 985
Batch idx 986
Gradient norm: 0.2177734375
Batch idx 987
Batch idx 988
Batch idx 989
Batch idx 990
Gradient norm: 0.2451171875
Batch idx 991
Batch idx 992
Batch idx 993
Batch idx 994
Gradient norm: 0.2314453125
Batch idx 995
Batch idx 996
Batch idx 997
Batch idx 998
Gradient norm: 0.2177734375
Batch idx 999
Batch idx 1000
Batch idx 1001
Batch idx 1002
Gradient norm: 0.259765625
Batch idx 1003
Batch idx 1004
Batch idx 1005
Batch idx 1006
Gradient norm: 0.248046875
Batch idx 1007
Batch idx 1008
Batch idx 1009
Batch idx 1010
Gradient norm: 0.26953125
Batch idx 1011
Batch idx 1012
Batch idx 1013
Batch idx 1014
Gradient norm: 0.21484375
Batch idx 1015
Batch idx 1016
Batch idx 1017
Batch idx 1018
Gradient norm: 0.2490234375
Batch idx 1019
Batch idx 1020
Batch idx 1021
Batch idx 1022
Gradient norm: 0.2080078125
Batch idx 1023
Batch idx 1024
Batch idx 1025
Batch idx 1026
Gradient norm: 0.2373046875
Batch idx 1027
Batch idx 1028
Batch idx 1029
Batch idx 1030
Gradient norm: 0.294921875
Batch idx 1031
Batch idx 1032
Batch idx 1033
Batch idx 1034
Gradient norm: 0.2578125
Batch idx 1035
Batch idx 1036
Batch idx 1037
Batch idx 1038
Gradient norm: 0.21875
Batch idx 1039
Batch idx 1040
Batch idx 1041
Batch idx 1042
Gradient norm: 0.2421875
Batch idx 1043
Batch idx 1044
Batch idx 1045
Batch idx 1046
Gradient norm: 0.279296875
Batch idx 1047
Batch idx 1048
Batch idx 1049
Batch idx 1050
Gradient norm: 0.26171875
Batch idx 1051
Batch idx 1052
Batch idx 1053
Batch idx 1054
Gradient norm: 0.23046875
Batch idx 1055
Batch idx 1056
Batch idx 1057
Batch idx 1058
Gradient norm: 0.22265625
Batch idx 1059
Batch idx 1060
Batch idx 1061
Batch idx 1062
Gradient norm: 0.236328125
Batch idx 1063
Batch idx 1064
Batch idx 1065
Batch idx 1066
Gradient norm: 0.2431640625
Batch idx 1067
Batch idx 1068
Batch idx 1069
Saving full model weights.
Done 0
Finished training 0
CUDA event elapsed time: 14780.809 sec
Rank 0: Before forward: 1.82 GiB
Rank 0: After forward: 2.66 GiB
Rank 0: After backward: 2.81 GiB
Rank 0: Peak allocated memory: 2.91 GiB
Rank 0: Peak reserved memory:  3.73 GiB
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /scratch/tathagato/token
Login successful
World size: 4
{'world_size': 4, 'train_type': 'qlora', 'llama_pro_path': None, 'batch_size': 1, 'context_length': 2048, 'gradient_accumulation_steps': 4, 'num_epochs': 1, 'dataset': 'macsum', 'macsum_path': '/home2/tathagato/summarization/MACSUM/dataset/macdoc/train_dataset.json', 'dataset_samples': -1, 'sharding_strategy': 'full_shard', 'use_gradient_checkpointing': 1, 'reentrant_checkpointing': 1, 'use_cpu_offload': 1, 'use_activation_cpu_offload': False, 'low_memory': True, 'no_sync': False, 'precision': 'bf16', 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'save_model': 1, 'output_dir': '/scratch/tathagato/fsdp_qlora_experiments_25_August_test_mistral/topic', 'lora_rank': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'lora_target_modules': 'all', 'verbose': 1, 'lr': 5e-05, 'apply_gradient_clipping': 1, 'grad_norm': 1.0, 'wd': 0.1, 'profile_memory': False, 'optimizer': 'adamw', 'lr_scheduler': 'cosine', 'loading_workers': -1, 'log_to': 'wandb', 'master_addr': 'localhost', 'master_port': '12355', 'seed': 42, 'project_name': 'fsdp_qlora', 'name': None, 'group': None, 'entity': None, 'n_bits': 4, 'profile': False, 'profiling_output': 'profiles', 'overwrite_profiling_output': True, 'with_stack': False, 'with_shapes': False, 'export_trace': True, 'export_memory_timeline': False, 'wait_steps': 0, 'warmup_steps': 1, 'active_steps': 2, 'repeat': 0, 'profiling_frequency': 10, 'max_steps': -1, 'attribute': 'topic'}
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
Creating model 2
Loading model 2
Creating model 3
Loading model 3
Creating model 1
Loading model 1
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSUM/fsdp_lora/wandb/run-20240826_075611-ctwtoph5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-bush-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: üöÄ View run at https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/ctwtoph5
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
dataset chosen is :  macsum
Example input_ids shape:  torch.Size([1, 771])
Example labels shape:  torch.Size([1, 771])
example input 
<s>[INST] Write a summary of the source text. The summary should be focussed on the topic marquis de lafayette , france. The source text is given below.  (CNN)Lest we Americans forget that -- in the wellsprings of our nation -- France bore the torch of liberty alongside us, our old ally has launched a reminder from across the Atlantic's waves. L'Hermione, with three sail masts and bright royal blue and gold markings, is a painstaking replica of an 18th century French frigate that fought with the United States' founding fathers in the War of Independence. It set sail in France on Saturday for Virginia to retrace a journey through American history. In 1780, the original Hermione was assigned to a French nobleman, who fought as a general in George Washington's army against the British. His name: Marie-Joseph Paul Yves Roch Gilbert du Motier, Marquis de Lafayette. Lafayette carried prized cargo -- news from King Louis XVI that France was throwing men, guns and treasure behind the Colonies, according to a historical summary on L'Hermione's website. Lafayette, who had been wounded in the Revolutionary War had gone back to his homeland to lobby on behalf of Washington, who was also one of his close friends. Lafayette rejoined the fight on the front lines in Virginia, while L'Hermione did sea battle with its 32 guns against the English farther north. Its coppered bottom was an innovation that made it cut faster through the waters. In Chesapeake Bay, it joined the blockade that led the British to surrender. After the war, Lafayette returned to live in France. More than 200 years later, in 1997, a group of people came up with the idea of reconstructing the frigate using the same building methods applied in the original. They claim the replica is the "most authentically built Tall Ship in the last 150 years." After a year of testing, it set sail Saturday to retrace Lafayette's journey, 235 years after the original, and France gave it a sendoff with the trappings of an act of state. "L'Hermione is a luminous episode of our history. She is a champion of universal values, freedom, courage and of the friendship between France and the United States," French President Francois Hollande said in a speech. President Obama, in a letter to congratulate the launch, called France "our Nation's oldest ally." "For more than two centuries, the United States and France have stood united in the freedom we owe to one another," he wrote. L'Hermione will ply across the Atlantic for 27 days en route to Yorktown, Virginia, where it plans to arrive in early June. After that, it will show itself off in 12 ports along the East Coast. It should be in New York City for the Fourth of July, possibly sharing Independence Day fireworks with the Statue of Liberty. Since 1886, that gift from France -- also a reminder of our common bond -- has been America's quintessential national symbol of freedom. [/INST]General Lafayette carried news from King Louis XVI about France. After having been injured in the war, Lafayette came back to France. 235 years later, the replica of the famous ship set sail from France to retrace Lafayette's journey.</s>
tensor([[    1,     3, 12786,  1032, 14828,  1070,  1040,  3600,  3013, 29491,
          1183, 14828,  1791,  1115,  1053,  1170,  2117,  1054,  1124,  1040,
          9835,  2597, 11993,  1108,  1311, 29490,  1107,  3954,  1968,  1872,
          1385, 29491,  1183,  3600,  3013,  1117,  2846,  4392, 29491, 29473,
          1093, 29511, 12116, 29499, 29526,  1142,  1246,  8933,  7888,  1137,
          2707,  1065,  1040,  1930,  8326, 29481,  1070,  1581,  6646,  2707,
          5611, 23577,  1040, 13209,  1070, 27508, 14059,  1360, 29493,  1581,
          2339,  1157,  1114,  1427, 11855,  1032, 26331,  1245,  3441,  1040,
         17111, 29510, 29481, 14063, 29491,  1161, 29510, 29537,  1626,  3089,
         29493,  1163,  2480, 13200,  1058, 11385,  1072,  7601, 14521,  5813,
          1072,  5782,  2484,  1510, 29493,  1117,  1032,  4126,  1071,  2056,
          1080, 16122,  1070,  1164, 29473, 29508, 29551,  1130,  6213,  5717,
          1872, 12358,  1137, 13357,  1163,  1040,  3737,  4311, 29510, 25518,
          1053, 14645,  1065,  1040,  4041,  1070, 24739,  1404, 29491,  1429,
          1576, 13200,  1065,  5611,  1124,  9281,  1122, 10885,  1066,  1080,
          9981,  1032,  8891,  1827,  3324,  4108, 29491,  1328, 29473, 29508,
         29555, 29551, 29502, 29493,  1040,  4261, 15509,  3089,  1171, 12168,
          1066,  1032,  5717, 20111,  2062, 29493,  1461, 13357,  1158,  1032,
          3720,  1065,  5931,  6692, 29510, 29481,  8605,  2603,  1040,  5177,
         29491,  3122,  1909, 29515, 13731, 29501, 29566,  1413,  1489,  4688,
          1395,  2623,  1167,  4810, 27542,  2183,  9622,  1517, 29493,  2239,
         11993,  1108,  2468, 29490,  1107,  3954, 29491,  2468, 29490,  1107,
          3954,  7926,  3829, 13384, 17434,  2707,  4999,  1245,  4809,  7466,
         24296,  1137,  5611,  1171, 16600,  2451, 29493, 12228,  1072, 26264,
          3678,  1040,  2656,  1034,  1265, 29493,  5539,  1066,  1032, 11346,
         14828,  1124,  1161, 29510, 29537,  1626,  3089, 29510, 29481,  5168,
         29491,  2468, 29490,  1107,  3954, 29493,  1461,  1321,  1518, 17852,
          1065,  1040, 14618,  1396,  4041,  1321,  4982,  1620,  1066,  1284,
          3921, 14710,  1066, 19659,  1124, 16433,  1070,  6692, 29493,  1461,
          1171,  1603,  1392,  1070,  1284,  3616,  4050, 29491,  2468, 29490,
          1107,  3954, 27119,  2079,  1040,  4760,  1124,  1040,  3546,  5483,
          1065, 10885, 29493,  2080,  1161, 29510, 29537,  1626,  3089,  1631,
          6931,  7419,  1163,  1639, 29473, 29538, 29518, 12228,  2603,  1040,
          5068, 21704,  6888, 29491,  8035, 22337,  1054,  6627,  1171,  1164,
         17631,  1137,  2037,  1146,  3887, 10324,  1827,  1040, 16275, 29491,
          1328,  1102,  2821,  2516,  1389,  7412, 29493,  1146,  8066,  1040,
          3492,  1538,  1137,  4931,  1040,  5177,  1066, 26652, 29491,  3298,
          1040,  2264, 29493,  2468, 29490,  1107,  3954,  5021,  1066,  3711,
          1065,  5611, 29491,  4532,  1589, 29473, 29518, 29502, 29502,  2035,
          2830, 29493,  1065, 29473, 29508, 29542, 29542, 29555, 29493,  1032,
          2839,  1070,  1673,  2756,  1350,  1163,  1040,  3796,  1070,  9988,
          2124,  1056,  1040,  1872, 12358,  2181,  1040,  2116,  4435,  6330,
          8357,  1065,  1040,  4261, 29491,  2074,  4220,  1040,  1080, 16122,
          1117,  1040,  1113,  3052,  1974,  6345,  2712,  5197, 25669, 23936,
          1065,  1040,  2200, 29473, 29508, 29550, 29502,  2035,  1379,  3298,
          1032,  1647,  1070,  9028, 29493,  1146,  1576, 13200,  9281,  1066,
          1080,  9981,  2468, 29490,  1107,  3954, 29510, 29481,  8891, 29493,
         29473, 29518, 29538, 29550,  2035,  1792,  1040,  4261, 29493,  1072,
          5611,  4163,  1146,  1032,  4848,  2537,  1163,  1040,  1235, 21812,
          1070,  1164,  1728,  1070,  2433, 29491,  1113, 29526, 29510, 29537,
          1626,  3089,  1117,  1032,  1073, 10635,  1375, 10328,  1070,  1581,
          4108, 29491,  1753,  1117,  1032, 13032,  1070, 15366,  3837, 29493,
          9235, 29493, 16044,  1072,  1070,  1040, 18279,  2212,  5611,  1072,
          1040,  3737,  4311,  1630,  5717,  5888,  5464,  6396, 15535,  5992,
          1541,  1065,  1032,  9434, 29491,  5888, 12532, 29493,  1065,  1032,
          6266,  1066, 26454,  1038,  6980,  1040,  9585, 29493,  2755,  5611,
          1113,  1191, 15085, 29510, 29481, 18458,  1157,  1114,  1379,  1113,
          3333,  1448,  1589,  1757, 15765, 29493,  1040,  3737,  4311,  1072,
          5611,  1274,  5625, 28202,  1065,  1040,  9235,  1246, 27815,  1066,
          1392,  2466,  1630,  1168,  5445, 29491,  1161, 29510, 29537,  1626,
          3089,  1390,  1052,  1114,  3441,  1040, 17111,  1122, 29473, 29518,
         29555,  2970,  1249,  7871,  1066,  3494, 20437, 29493, 10885, 29493,
          1738,  1146,  7168,  1066, 13456,  1065,  3703,  4980, 29491,  3298,
          1137, 29493,  1146,  1390,  2115,  4605,  1573,  1065, 29473, 29508,
         29518, 21916,  3035,  1040,  6459, 14108, 29491,  1429,  1791,  1115,
          1065,  2218,  3494,  4573,  1122,  1040, 28953,  1070,  5166, 29493,
          8957, 11449, 24739,  1404,  6167,  4107, 17875,  1163,  1040, 10016,
          1209,  1070, 28138, 29491,  5345, 29473, 29508, 29551, 29551, 29552,
         29493,  1137,  8643,  1245,  5611,  2707,  1603,  1032, 26331,  1070,
          1581,  4066, 10178,  2707,  1427,  1518,  5120, 29510, 29481,  1294,
          1269,  1177,  3024,  5050,  6370,  1070,  9235, 29491, 29473,     4,
         16116,  2468, 29490,  1107,  3954,  7926,  4999,  1245,  4809,  7466,
         24296,  1452,  5611, 29491,  3298,  3229,  1518, 16482,  1065,  1040,
          2264, 29493,  2468, 29490,  1107,  3954,  2756,  1620,  1066,  5611,
         29491, 29473, 29518, 29538, 29550,  2035,  2830, 29493,  1040,  1080,
         16122,  1070,  1040,  9144,  5077,  1576, 13200,  1245,  5611,  1066,
          1080,  9981,  2468, 29490,  1107,  3954, 29510, 29481,  8891, 29491,
             2]])
Creating model 0
Loading model 0
Total model params: 7248023552
Using n_workers: 10 for loading
Loading & Quantizing Model Shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading & Quantizing Model Shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:10<00:20, 10.15s/it]Loading & Quantizing Model Shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:20<00:10, 10.21s/it]Loading & Quantizing Model Shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:31<00:00, 10.47s/it]Loading & Quantizing Model Shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:31<00:00, 10.39s/it]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py:491: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  return wrapper_cls(module, **kwargs)
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:855: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Loaded model weights in 31.182 seconds
Rank 0: Model created: 0.107 GiB
trainable params: 75,497,472 || all params: 7,323,521,024 || trainable%: 1.0308903566001424
Wrapping model w/ FSDP 0
Rank 0: Wrapped model: 1.824 GiB
Applying activation checkpointing 0
Config:
MistralConfig {
  "_name_or_path": "mistralai/Mistral-7B-Instruct-v0.3",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": false,
  "vocab_size": 32768
}

Model:
FullyShardedDataParallel(
  (_fsdp_wrapped_module): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): MistralForCausalLM(
        (model): MistralModel(
          (embed_tokens): Embedding(32768, 4096)
          (layers): ModuleList(
            (0-31): 32 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): MistralDecoderLayer(
                  (self_attn): MistralSdpaAttention(
                    (q_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (k_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (v_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=1024, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
                    (rotary_emb): MistralRotaryEmbedding()
                  )
                  (mlp): MistralMLP(
                    (gate_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (up_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=4096, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=14336, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (down_proj): lora.Linear4bit(
                      (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.1, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=14336, out_features=32, bias=False)
                        )
                      )
                      (lora_B): ModuleDict(
                        (default): FullyShardedDataParallel(
                          (_fsdp_wrapped_module): Linear(in_features=32, out_features=4096, bias=False)
                        )
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                    )
                    (act_fn): SiLU()
                  )
                  (input_layernorm): MistralRMSNorm()
                  (post_attention_layernorm): MistralRMSNorm()
                )
              )
            )
          )
          (norm): MistralRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32768, bias=False)
      )
    )
  )
)
Starting training
Optimizer params:
Shape: torch.Size([67109888]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:855: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:855: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([13633536]), Requires Grad: False, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([8192]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([114688]), Requires Grad: True, Dtype: torch.bfloat16
Shape: torch.Size([32768]), Requires Grad: True, Dtype: torch.bfloat16
Total Training Steps: 126
  0%|          | 0/126 [00:00<?, ?it/s]/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input_ids = [torch.tensor(item['input_ids']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attention_masks = [torch.tensor(item['attention_mask']) for item in batch]
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:855: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
/home2/tathagato/summarization/MACSUM/fsdp_lora/train.py:401: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = [torch.tensor(item['labels']) for item in batch]
Epoch 0, Loss 0.000:   0%|          | 0/126 [00:00<?, ?it/s]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Epoch 0, Loss 0.000:   1%|          | 1/126 [00:54<1:53:12, 54.34s/it]Epoch 0, Loss 2.168, LR 4.17e-06:   1%|          | 1/126 [00:54<1:53:12, 54.34s/it]Epoch 0, Loss 2.168, LR 4.17e-06:   2%|‚ñè         | 2/126 [01:54<1:59:00, 57.58s/it]Epoch 0, Loss 2.202, LR 8.33e-06:   2%|‚ñè         | 2/126 [01:54<1:59:00, 57.58s/it]Epoch 0, Loss 2.202, LR 8.33e-06:   2%|‚ñè         | 3/126 [02:48<1:55:23, 56.29s/it]Epoch 0, Loss 2.148, LR 1.25e-05:   2%|‚ñè         | 3/126 [02:49<1:55:23, 56.29s/it]Epoch 0, Loss 2.148, LR 1.25e-05:   3%|‚ñé         | 4/126 [03:50<1:58:18, 58.18s/it]Epoch 0, Loss 2.238, LR 1.67e-05:   3%|‚ñé         | 4/126 [03:50<1:58:18, 58.18s/it]Epoch 0, Loss 2.238, LR 1.67e-05:   4%|‚ñç         | 5/126 [04:52<2:00:30, 59.76s/it]Epoch 0, Loss 2.217, LR 2.08e-05:   4%|‚ñç         | 5/126 [04:52<2:00:30, 59.76s/it]Epoch 0, Loss 2.217, LR 2.08e-05:   5%|‚ñç         | 6/126 [05:57<2:02:48, 61.40s/it]Epoch 0, Loss 2.173, LR 2.50e-05:   5%|‚ñç         | 6/126 [05:57<2:02:48, 61.40s/it]Epoch 0, Loss 2.173, LR 2.50e-05:   6%|‚ñå         | 7/126 [06:51<1:56:59, 58.99s/it]Epoch 0, Loss 2.129, LR 2.92e-05:   6%|‚ñå         | 7/126 [06:51<1:56:59, 58.99s/it]Epoch 0, Loss 2.129, LR 2.92e-05:   6%|‚ñã         | 8/126 [07:47<1:54:13, 58.08s/it]Epoch 0, Loss 2.171, LR 3.33e-05:   6%|‚ñã         | 8/126 [07:47<1:54:13, 58.08s/it]Epoch 0, Loss 2.171, LR 3.33e-05:   7%|‚ñã         | 9/126 [08:38<1:49:15, 56.03s/it]Epoch 0, Loss 2.117, LR 3.75e-05:   7%|‚ñã         | 9/126 [08:38<1:49:15, 56.03s/it]Epoch 0, Loss 2.117, LR 3.75e-05:   8%|‚ñä         | 10/126 [09:35<1:48:24, 56.08s/it]Epoch 0, Loss 2.015, LR 4.17e-05:   8%|‚ñä         | 10/126 [09:35<1:48:24, 56.08s/it]Epoch 0, Loss 2.015, LR 4.17e-05:   9%|‚ñä         | 11/126 [10:34<1:49:32, 57.15s/it]Epoch 0, Loss 2.073, LR 4.58e-05:   9%|‚ñä         | 11/126 [10:34<1:49:32, 57.15s/it]Epoch 0, Loss 2.073, LR 4.58e-05:  10%|‚ñâ         | 12/126 [11:27<1:46:15, 55.92s/it]Epoch 0, Loss 2.158, LR 5.00e-05:  10%|‚ñâ         | 12/126 [11:27<1:46:15, 55.92s/it]Epoch 0, Loss 2.158, LR 5.00e-05:  10%|‚ñà         | 13/126 [12:27<1:47:21, 57.00s/it]Epoch 0, Loss 1.968, LR 5.00e-05:  10%|‚ñà         | 13/126 [12:27<1:47:21, 57.00s/it]Epoch 0, Loss 1.968, LR 5.00e-05:  11%|‚ñà         | 14/126 [13:19<1:43:28, 55.44s/it]Epoch 0, Loss 2.043, LR 5.00e-05:  11%|‚ñà         | 14/126 [13:19<1:43:28, 55.44s/it]Epoch 0, Loss 2.043, LR 5.00e-05:  12%|‚ñà‚ñè        | 15/126 [14:16<1:43:48, 56.11s/it]Epoch 0, Loss 1.890, LR 4.99e-05:  12%|‚ñà‚ñè        | 15/126 [14:16<1:43:48, 56.11s/it]Epoch 0, Loss 1.890, LR 4.99e-05:  13%|‚ñà‚ñé        | 16/126 [15:21<1:47:52, 58.84s/it]Epoch 0, Loss 1.944, LR 4.99e-05:  13%|‚ñà‚ñé        | 16/126 [15:21<1:47:52, 58.84s/it]Epoch 0, Loss 1.944, LR 4.99e-05:  13%|‚ñà‚ñé        | 17/126 [16:22<1:47:55, 59.41s/it]Epoch 0, Loss 1.882, LR 4.98e-05:  13%|‚ñà‚ñé        | 17/126 [16:22<1:47:55, 59.41s/it]Epoch 0, Loss 1.882, LR 4.98e-05:  14%|‚ñà‚ñç        | 18/126 [17:10<1:40:30, 55.84s/it]Epoch 0, Loss 1.911, LR 4.97e-05:  14%|‚ñà‚ñç        | 18/126 [17:10<1:40:30, 55.84s/it]Epoch 0, Loss 1.911, LR 4.97e-05:  15%|‚ñà‚ñå        | 19/126 [18:08<1:41:10, 56.73s/it]Epoch 0, Loss 1.883, LR 4.96e-05:  15%|‚ñà‚ñå        | 19/126 [18:09<1:41:10, 56.73s/it]Epoch 0, Loss 1.883, LR 4.96e-05:  16%|‚ñà‚ñå        | 20/126 [19:03<1:38:48, 55.93s/it]Epoch 0, Loss 1.953, LR 4.95e-05:  16%|‚ñà‚ñå        | 20/126 [19:03<1:38:48, 55.93s/it]Epoch 0, Loss 1.953, LR 4.95e-05:  17%|‚ñà‚ñã        | 21/126 [19:59<1:38:17, 56.17s/it]Epoch 0, Loss 1.970, LR 4.93e-05:  17%|‚ñà‚ñã        | 21/126 [19:59<1:38:17, 56.17s/it]Epoch 0, Loss 1.970, LR 4.93e-05:  17%|‚ñà‚ñã        | 22/126 [20:57<1:38:04, 56.58s/it]Epoch 0, Loss 1.860, LR 4.92e-05:  17%|‚ñà‚ñã        | 22/126 [20:57<1:38:04, 56.58s/it]Epoch 0, Loss 1.860, LR 4.92e-05:  18%|‚ñà‚ñä        | 23/126 [21:47<1:33:40, 54.56s/it]Epoch 0, Loss 1.919, LR 4.90e-05:  18%|‚ñà‚ñä        | 23/126 [21:47<1:33:40, 54.56s/it]Epoch 0, Loss 1.919, LR 4.90e-05:  19%|‚ñà‚ñâ        | 24/126 [22:52<1:38:10, 57.75s/it]Epoch 0, Loss 1.855, LR 4.88e-05:  19%|‚ñà‚ñâ        | 24/126 [22:52<1:38:10, 57.75s/it]Epoch 0, Loss 1.855, LR 4.88e-05:  20%|‚ñà‚ñâ        | 25/126 [23:43<1:33:49, 55.74s/it]Epoch 0, Loss 1.865, LR 4.86e-05:  20%|‚ñà‚ñâ        | 25/126 [23:43<1:33:49, 55.74s/it]Epoch 0, Loss 1.865, LR 4.86e-05:  21%|‚ñà‚ñà        | 26/126 [24:40<1:33:28, 56.09s/it]Epoch 0, Loss 1.870, LR 4.83e-05:  21%|‚ñà‚ñà        | 26/126 [24:40<1:33:28, 56.09s/it]Epoch 0, Loss 1.870, LR 4.83e-05:  21%|‚ñà‚ñà‚ñè       | 27/126 [25:34<1:31:36, 55.52s/it]Epoch 0, Loss 1.949, LR 4.81e-05:  21%|‚ñà‚ñà‚ñè       | 27/126 [25:34<1:31:36, 55.52s/it]Epoch 0, Loss 1.949, LR 4.81e-05:  22%|‚ñà‚ñà‚ñè       | 28/126 [26:39<1:35:29, 58.47s/it]Epoch 0, Loss 1.919, LR 4.78e-05:  22%|‚ñà‚ñà‚ñè       | 28/126 [26:39<1:35:29, 58.47s/it]Epoch 0, Loss 1.919, LR 4.78e-05:  23%|‚ñà‚ñà‚ñé       | 29/126 [27:32<1:31:55, 56.87s/it]Epoch 0, Loss 1.920, LR 4.76e-05:  23%|‚ñà‚ñà‚ñé       | 29/126 [27:33<1:31:55, 56.87s/it]Epoch 0, Loss 1.920, LR 4.76e-05:  24%|‚ñà‚ñà‚ñç       | 30/126 [28:20<1:26:31, 54.08s/it]Epoch 0, Loss 1.798, LR 4.73e-05:  24%|‚ñà‚ñà‚ñç       | 30/126 [28:20<1:26:31, 54.08s/it]Epoch 0, Loss 1.798, LR 4.73e-05:  25%|‚ñà‚ñà‚ñç       | 31/126 [29:21<1:28:50, 56.11s/it]Epoch 0, Loss 1.825, LR 4.70e-05:  25%|‚ñà‚ñà‚ñç       | 31/126 [29:21<1:28:50, 56.11s/it]Epoch 0, Loss 1.825, LR 4.70e-05:  25%|‚ñà‚ñà‚ñå       | 32/126 [30:21<1:29:44, 57.29s/it]Epoch 0, Loss 1.809, LR 4.67e-05:  25%|‚ñà‚ñà‚ñå       | 32/126 [30:21<1:29:44, 57.29s/it]Epoch 0, Loss 1.809, LR 4.67e-05:  26%|‚ñà‚ñà‚ñå       | 33/126 [31:14<1:26:42, 55.94s/it]Epoch 0, Loss 1.870, LR 4.63e-05:  26%|‚ñà‚ñà‚ñå       | 33/126 [31:14<1:26:42, 55.94s/it]Epoch 0, Loss 1.870, LR 4.63e-05:  27%|‚ñà‚ñà‚ñã       | 34/126 [32:10<1:26:04, 56.14s/it]Epoch 0, Loss 1.917, LR 4.60e-05:  27%|‚ñà‚ñà‚ñã       | 34/126 [32:10<1:26:04, 56.14s/it]Epoch 0, Loss 1.917, LR 4.60e-05:  28%|‚ñà‚ñà‚ñä       | 35/126 [33:09<1:26:09, 56.81s/it]Epoch 0, Loss 1.819, LR 4.56e-05:  28%|‚ñà‚ñà‚ñä       | 35/126 [33:09<1:26:09, 56.81s/it]Epoch 0, Loss 1.819, LR 4.56e-05:  29%|‚ñà‚ñà‚ñä       | 36/126 [34:14<1:28:59, 59.33s/it]Epoch 0, Loss 1.852, LR 4.53e-05:  29%|‚ñà‚ñà‚ñä       | 36/126 [34:14<1:28:59, 59.33s/it]Epoch 0, Loss 1.852, LR 4.53e-05:  29%|‚ñà‚ñà‚ñâ       | 37/126 [35:21<1:31:23, 61.62s/it]Epoch 0, Loss 1.864, LR 4.49e-05:  29%|‚ñà‚ñà‚ñâ       | 37/126 [35:21<1:31:23, 61.62s/it]Epoch 0, Loss 1.864, LR 4.49e-05:  30%|‚ñà‚ñà‚ñà       | 38/126 [36:20<1:29:13, 60.83s/it]Epoch 0, Loss 1.766, LR 4.45e-05:  30%|‚ñà‚ñà‚ñà       | 38/126 [36:20<1:29:13, 60.83s/it]Epoch 0, Loss 1.766, LR 4.45e-05:  31%|‚ñà‚ñà‚ñà       | 39/126 [37:10<1:23:34, 57.64s/it]Epoch 0, Loss 1.838, LR 4.41e-05:  31%|‚ñà‚ñà‚ñà       | 39/126 [37:10<1:23:34, 57.64s/it]Epoch 0, Loss 1.838, LR 4.41e-05:  32%|‚ñà‚ñà‚ñà‚ñè      | 40/126 [38:09<1:23:19, 58.13s/it]Epoch 0, Loss 1.849, LR 4.36e-05:  32%|‚ñà‚ñà‚ñà‚ñè      | 40/126 [38:09<1:23:19, 58.13s/it]Epoch 0, Loss 1.849, LR 4.36e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 41/126 [39:06<1:21:45, 57.71s/it]Epoch 0, Loss 1.900, LR 4.32e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 41/126 [39:06<1:21:45, 57.71s/it]Epoch 0, Loss 1.900, LR 4.32e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 42/126 [40:02<1:20:00, 57.15s/it]Epoch 0, Loss 1.863, LR 4.27e-05:  33%|‚ñà‚ñà‚ñà‚ñé      | 42/126 [40:02<1:20:00, 57.15s/it]Epoch 0, Loss 1.863, LR 4.27e-05:  34%|‚ñà‚ñà‚ñà‚ñç      | 43/126 [41:01<1:19:58, 57.81s/it]Epoch 0, Loss 1.900, LR 4.23e-05:  34%|‚ñà‚ñà‚ñà‚ñç      | 43/126 [41:01<1:19:58, 57.81s/it]Epoch 0, Loss 1.900, LR 4.23e-05:  35%|‚ñà‚ñà‚ñà‚ñç      | 44/126 [41:54<1:17:03, 56.39s/it]Epoch 0, Loss 1.790, LR 4.18e-05:  35%|‚ñà‚ñà‚ñà‚ñç      | 44/126 [41:54<1:17:03, 56.39s/it]Epoch 0, Loss 1.790, LR 4.18e-05:  36%|‚ñà‚ñà‚ñà‚ñå      | 45/126 [42:48<1:15:13, 55.72s/it]Epoch 0, Loss 1.978, LR 4.13e-05:  36%|‚ñà‚ñà‚ñà‚ñå      | 45/126 [42:48<1:15:13, 55.72s/it]Epoch 0, Loss 1.978, LR 4.13e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 46/126 [43:41<1:13:10, 54.89s/it]Epoch 0, Loss 1.877, LR 4.08e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 46/126 [43:41<1:13:10, 54.89s/it]Epoch 0, Loss 1.877, LR 4.08e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 47/126 [44:38<1:12:55, 55.39s/it]Epoch 0, Loss 1.776, LR 4.03e-05:  37%|‚ñà‚ñà‚ñà‚ñã      | 47/126 [44:38<1:12:55, 55.39s/it]Epoch 0, Loss 1.776, LR 4.03e-05:  38%|‚ñà‚ñà‚ñà‚ñä      | 48/126 [45:36<1:13:05, 56.22s/it]Epoch 0, Loss 1.724, LR 3.98e-05:  38%|‚ñà‚ñà‚ñà‚ñä      | 48/126 [45:36<1:13:05, 56.22s/it]Epoch 0, Loss 1.724, LR 3.98e-05:  39%|‚ñà‚ñà‚ñà‚ñâ      | 49/126 [46:40<1:15:12, 58.60s/it]Epoch 0, Loss 1.863, LR 3.93e-05:  39%|‚ñà‚ñà‚ñà‚ñâ      | 49/126 [46:40<1:15:12, 58.60s/it]Epoch 0, Loss 1.863, LR 3.93e-05:  40%|‚ñà‚ñà‚ñà‚ñâ      | 50/126 [47:33<1:11:52, 56.74s/it]Epoch 0, Loss 1.811, LR 3.87e-05:  40%|‚ñà‚ñà‚ñà‚ñâ      | 50/126 [47:33<1:11:52, 56.74s/it]Epoch 0, Loss 1.811, LR 3.87e-05:  40%|‚ñà‚ñà‚ñà‚ñà      | 51/126 [48:33<1:12:13, 57.78s/it]Epoch 0, Loss 1.913, LR 3.82e-05:  40%|‚ñà‚ñà‚ñà‚ñà      | 51/126 [48:33<1:12:13, 57.78s/it]Epoch 0, Loss 1.913, LR 3.82e-05:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 52/126 [49:31<1:11:23, 57.89s/it]Epoch 0, Loss 1.806, LR 3.77e-05:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 52/126 [49:31<1:11:23, 57.89s/it]Epoch 0, Loss 1.806, LR 3.77e-05:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53/126 [50:17<1:06:10, 54.39s/it]Epoch 0, Loss 1.863, LR 3.71e-05:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53/126 [50:17<1:06:10, 54.39s/it]Epoch 0, Loss 1.863, LR 3.71e-05:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 54/126 [51:08<1:03:46, 53.15s/it]Epoch 0, Loss 1.881, LR 3.65e-05:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 54/126 [51:08<1:03:46, 53.15s/it]Epoch 0, Loss 1.881, LR 3.65e-05:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 55/126 [52:03<1:03:52, 53.98s/it]Epoch 0, Loss 1.824, LR 3.60e-05:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 55/126 [52:04<1:03:52, 53.98s/it]Epoch 0, Loss 1.824, LR 3.60e-05:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56/126 [52:59<1:03:40, 54.58s/it]Epoch 0, Loss 1.746, LR 3.54e-05:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56/126 [53:00<1:03:40, 54.58s/it]Epoch 0, Loss 1.746, LR 3.54e-05:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 57/126 [53:59<1:04:39, 56.23s/it]Epoch 0, Loss 1.878, LR 3.48e-05:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 57/126 [54:00<1:04:39, 56.23s/it]Epoch 0, Loss 1.878, LR 3.48e-05:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 58/126 [54:56<1:03:43, 56.23s/it]Epoch 0, Loss 1.765, LR 3.42e-05:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 58/126 [54:56<1:03:43, 56.23s/it]Epoch 0, Loss 1.765, LR 3.42e-05:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 59/126 [55:57<1:04:36, 57.87s/it]Epoch 0, Loss 1.796, LR 3.36e-05:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 59/126 [55:58<1:04:36, 57.87s/it]Epoch 0, Loss 1.796, LR 3.36e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 60/126 [56:57<1:04:03, 58.24s/it]Epoch 0, Loss 1.779, LR 3.30e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 60/126 [56:57<1:04:03, 58.24s/it]Epoch 0, Loss 1.779, LR 3.30e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61/126 [57:44<59:31, 54.94s/it]  Epoch 0, Loss 1.824, LR 3.24e-05:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61/126 [57:44<59:31, 54.94s/it]Epoch 0, Loss 1.824, LR 3.24e-05:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 62/126 [58:48<1:01:39, 57.81s/it]Epoch 0, Loss 1.853, LR 3.18e-05:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 62/126 [58:48<1:01:39, 57.81s/it]Epoch 0, Loss 1.853, LR 3.18e-05:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 63/126 [59:41<59:09, 56.34s/it]  Epoch 0, Loss 1.795, LR 3.12e-05:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 63/126 [59:41<59:09, 56.34s/it]Epoch 0, Loss 1.795, LR 3.12e-05:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64/126 [1:00:37<58:04, 56.20s/it]Epoch 0, Loss 1.890, LR 3.06e-05:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64/126 [1:00:37<58:04, 56.20s/it]Epoch 0, Loss 1.890, LR 3.06e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 65/126 [1:01:33<57:05, 56.16s/it]Epoch 0, Loss 1.807, LR 3.00e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 65/126 [1:01:33<57:05, 56.16s/it]Epoch 0, Loss 1.807, LR 3.00e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 66/126 [1:02:30<56:23, 56.40s/it]Epoch 0, Loss 1.746, LR 2.94e-05:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 66/126 [1:02:30<56:23, 56.40s/it]Epoch 0, Loss 1.746, LR 2.94e-05:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 67/126 [1:03:18<52:53, 53.79s/it]Epoch 0, Loss 1.842, LR 2.87e-05:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 67/126 [1:03:18<52:53, 53.79s/it]Epoch 0, Loss 1.842, LR 2.87e-05:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 68/126 [1:04:16<53:14, 55.07s/it]Epoch 0, Loss 1.894, LR 2.81e-05:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 68/126 [1:04:16<53:14, 55.07s/it]Epoch 0, Loss 1.894, LR 2.81e-05:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 69/126 [1:05:06<51:03, 53.74s/it]Epoch 0, Loss 1.841, LR 2.75e-05:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 69/126 [1:05:06<51:03, 53.74s/it]Epoch 0, Loss 1.841, LR 2.75e-05:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 70/126 [1:06:02<50:34, 54.19s/it]Epoch 0, Loss 1.903, LR 2.69e-05:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 70/126 [1:06:02<50:34, 54.19s/it]Epoch 0, Loss 1.903, LR 2.69e-05:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 71/126 [1:07:03<51:38, 56.33s/it]Epoch 0, Loss 1.796, LR 2.63e-05:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 71/126 [1:07:03<51:38, 56.33s/it]Epoch 0, Loss 1.796, LR 2.63e-05:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72/126 [1:08:03<51:42, 57.45s/it]Epoch 0, Loss 1.860, LR 2.56e-05:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72/126 [1:08:03<51:42, 57.45s/it]Epoch 0, Loss 1.860, LR 2.56e-05:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 73/126 [1:08:57<49:49, 56.40s/it]Epoch 0, Loss 1.804, LR 2.50e-05:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 73/126 [1:08:57<49:49, 56.40s/it]Epoch 0, Loss 1.804, LR 2.50e-05:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 74/126 [1:09:51<48:16, 55.71s/it]Epoch 0, Loss 1.857, LR 2.44e-05:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 74/126 [1:09:51<48:16, 55.71s/it]Epoch 0, Loss 1.857, LR 2.44e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 75/126 [1:10:46<47:10, 55.50s/it]Epoch 0, Loss 1.691, LR 2.38e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 75/126 [1:10:46<47:10, 55.50s/it]Epoch 0, Loss 1.691, LR 2.38e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 76/126 [1:11:46<47:14, 56.69s/it]Epoch 0, Loss 1.996, LR 2.32e-05:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 76/126 [1:11:46<47:14, 56.69s/it]Epoch 0, Loss 1.996, LR 2.32e-05:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 77/126 [1:12:39<45:28, 55.68s/it]Epoch 0, Loss 1.781, LR 2.26e-05:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 77/126 [1:12:39<45:28, 55.68s/it]Epoch 0, Loss 1.781, LR 2.26e-05:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 78/126 [1:13:41<45:58, 57.47s/it]Epoch 0, Loss 1.808, LR 2.20e-05:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 78/126 [1:13:41<45:58, 57.47s/it]Epoch 0, Loss 1.808, LR 2.20e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 79/126 [1:14:44<46:19, 59.13s/it]Epoch 0, Loss 1.814, LR 2.14e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 79/126 [1:14:44<46:19, 59.13s/it]Epoch 0, Loss 1.814, LR 2.14e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/126 [1:15:37<44:02, 57.46s/it]Epoch 0, Loss 1.803, LR 2.08e-05:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/126 [1:15:37<44:02, 57.46s/it]Epoch 0, Loss 1.803, LR 2.08e-05:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 81/126 [1:16:36<43:17, 57.73s/it]Epoch 0, Loss 1.764, LR 2.02e-05:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 81/126 [1:16:36<43:17, 57.73s/it]Epoch 0, Loss 1.764, LR 2.02e-05:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 82/126 [1:17:34<42:33, 58.04s/it]Epoch 0, Loss 1.800, LR 1.96e-05:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 82/126 [1:17:34<42:33, 58.04s/it]Epoch 0, Loss 1.800, LR 1.96e-05:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 83/126 [1:18:29<40:48, 56.94s/it]Epoch 0, Loss 1.801, LR 1.90e-05:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 83/126 [1:18:29<40:48, 56.94s/it]Epoch 0, Loss 1.801, LR 1.90e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 84/126 [1:19:20<38:46, 55.40s/it]Epoch 0, Loss 1.755, LR 1.85e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 84/126 [1:19:21<38:46, 55.40s/it]Epoch 0, Loss 1.755, LR 1.85e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 85/126 [1:20:13<37:13, 54.47s/it]Epoch 0, Loss 1.817, LR 1.79e-05:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 85/126 [1:20:13<37:13, 54.47s/it]Epoch 0, Loss 1.817, LR 1.79e-05:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 86/126 [1:21:18<38:28, 57.72s/it]Epoch 0, Loss 1.677, LR 1.73e-05:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 86/126 [1:21:18<38:28, 57.72s/it]Epoch 0, Loss 1.677, LR 1.73e-05:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 87/126 [1:22:14<37:13, 57.26s/it]Epoch 0, Loss 1.820, LR 1.68e-05:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 87/126 [1:22:14<37:13, 57.26s/it]Epoch 0, Loss 1.820, LR 1.68e-05:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 88/126 [1:23:13<36:32, 57.71s/it]Epoch 0, Loss 1.865, LR 1.63e-05:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 88/126 [1:23:13<36:32, 57.71s/it]Epoch 0, Loss 1.865, LR 1.63e-05:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 89/126 [1:24:11<35:35, 57.71s/it]Epoch 0, Loss 1.884, LR 1.57e-05:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 89/126 [1:24:11<35:35, 57.71s/it]Epoch 0, Loss 1.884, LR 1.57e-05:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 90/126 [1:25:09<34:39, 57.76s/it]Epoch 0, Loss 1.809, LR 1.52e-05:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 90/126 [1:25:09<34:39, 57.76s/it]Epoch 0, Loss 1.809, LR 1.52e-05:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 91/126 [1:26:07<33:50, 58.01s/it]Epoch 0, Loss 1.839, LR 1.47e-05:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 91/126 [1:26:07<33:50, 58.01s/it]Epoch 0, Loss 1.839, LR 1.47e-05:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 92/126 [1:27:00<32:00, 56.48s/it]Epoch 0, Loss 1.919, LR 1.42e-05:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 92/126 [1:27:00<32:00, 56.48s/it]Epoch 0, Loss 1.919, LR 1.42e-05:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 93/126 [1:27:51<30:10, 54.87s/it]Epoch 0, Loss 1.818, LR 1.37e-05:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 93/126 [1:27:51<30:10, 54.87s/it]Epoch 0, Loss 1.818, LR 1.37e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 94/126 [1:28:45<29:07, 54.62s/it]Epoch 0, Loss 1.727, LR 1.32e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 94/126 [1:28:45<29:07, 54.62s/it]Epoch 0, Loss 1.727, LR 1.32e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 95/126 [1:29:48<29:31, 57.15s/it]Epoch 0, Loss 1.733, LR 1.27e-05:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 95/126 [1:29:48<29:31, 57.15s/it]Epoch 0, Loss 1.733, LR 1.27e-05:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 96/126 [1:30:38<27:31, 55.06s/it]Epoch 0, Loss 1.859, LR 1.23e-05:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 96/126 [1:30:39<27:31, 55.06s/it]Epoch 0, Loss 1.859, LR 1.23e-05:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 97/126 [1:31:36<27:01, 55.90s/it]Epoch 0, Loss 1.834, LR 1.18e-05:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 97/126 [1:31:36<27:01, 55.90s/it]Epoch 0, Loss 1.834, LR 1.18e-05:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 98/126 [1:32:33<26:07, 55.98s/it]Epoch 0, Loss 1.796, LR 1.14e-05:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 98/126 [1:32:33<26:07, 55.98s/it]Epoch 0, Loss 1.796, LR 1.14e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 99/126 [1:33:36<26:15, 58.34s/it]Epoch 0, Loss 1.863, LR 1.09e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 99/126 [1:33:36<26:15, 58.34s/it]Epoch 0, Loss 1.863, LR 1.09e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 100/126 [1:34:39<25:52, 59.73s/it]Epoch 0, Loss 1.806, LR 1.05e-05:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 100/126 [1:34:39<25:52, 59.73s/it]Batch idx 0
Batch idx 1
Batch idx 2
Gradient norm: 1.0234375
Batch idx 3
Batch idx 4
Batch idx 5
Batch idx 6
Gradient norm: 0.94921875
Batch idx 7
Batch idx 8
Batch idx 9
Batch idx 10
Gradient norm: 0.90625
Batch idx 11
Batch idx 12
Batch idx 13
Batch idx 14
Gradient norm: 0.9765625
Batch idx 15
Batch idx 16
Batch idx 17
Batch idx 18
Gradient norm: 0.9296875
Batch idx 19
Batch idx 20
Batch idx 21
Batch idx 22
Gradient norm: 0.77734375
Batch idx 23
Batch idx 24
Batch idx 25
Batch idx 26
Gradient norm: 0.83984375
Batch idx 27
Batch idx 28
Batch idx 29
Batch idx 30
Gradient norm: 0.8203125
Batch idx 31
Batch idx 32
Batch idx 33
Batch idx 34
Gradient norm: 0.71875
Batch idx 35
Batch idx 36
Batch idx 37
Batch idx 38
Gradient norm: 0.765625
Batch idx 39
Batch idx 40
Batch idx 41
Batch idx 42
Gradient norm: 0.73046875
Batch idx 43
Batch idx 44
Batch idx 45
Batch idx 46
Gradient norm: 0.67578125
Batch idx 47
Batch idx 48
Batch idx 49
Batch idx 50
Gradient norm: 0.478515625
Batch idx 51
Batch idx 52
Batch idx 53
Batch idx 54
Gradient norm: 0.5078125
Batch idx 55
Batch idx 56
Batch idx 57
Batch idx 58
Gradient norm: 0.40625
Batch idx 59
Batch idx 60
Batch idx 61
Batch idx 62
Gradient norm: 0.30859375
Batch idx 63
Batch idx 64
Batch idx 65
Batch idx 66
Gradient norm: 0.3046875
Batch idx 67
Batch idx 68
Batch idx 69
Batch idx 70
Gradient norm: 0.322265625
Batch idx 71
Batch idx 72
Batch idx 73
Batch idx 74
Gradient norm: 0.25
Batch idx 75
Batch idx 76
Batch idx 77
Batch idx 78
Gradient norm: 0.330078125
Batch idx 79
Batch idx 80
Batch idx 81
Batch idx 82
Gradient norm: 0.30078125
Batch idx 83
Batch idx 84
Batch idx 85
Batch idx 86
Gradient norm: 0.33203125
Batch idx 87
Batch idx 88
Batch idx 89
Batch idx 90
Gradient norm: 0.27734375
Batch idx 91
Batch idx 92
Batch idx 93
Batch idx 94
Gradient norm: 0.306640625
Batch idx 95
Batch idx 96
Batch idx 97
Batch idx 98
Gradient norm: 0.296875
Batch idx 99
Batch idx 100
Batch idx 101
Batch idx 102
Gradient norm: 0.318359375
Batch idx 103
Batch idx 104
Batch idx 105
Batch idx 106
Gradient norm: 0.263671875
Batch idx 107
Batch idx 108
Batch idx 109
Batch idx 110
Gradient norm: 0.1962890625
Batch idx 111
Batch idx 112
Batch idx 113
Batch idx 114
Gradient norm: 0.294921875
Batch idx 115
Batch idx 116
Batch idx 117
Batch idx 118
Gradient norm: 0.3125
Batch idx 119
Batch idx 120
Batch idx 121
Batch idx 122
Gradient norm: 0.21484375
Batch idx 123
Batch idx 124
Batch idx 125
Batch idx 126
Gradient norm: 0.193359375
Batch idx 127
Batch idx 128
Batch idx 129
Batch idx 130
Gradient norm: 0.216796875
Batch idx 131
Batch idx 132
Batch idx 133
Batch idx 134
Gradient norm: 0.2119140625
Batch idx 135
Batch idx 136
Batch idx 137
Batch idx 138
Gradient norm: 0.205078125
Batch idx 139
Batch idx 140
Batch idx 141
Batch idx 142
Gradient norm: 0.171875
Batch idx 143
Batch idx 144
Batch idx 145
Batch idx 146
Gradient norm: 0.193359375
Batch idx 147
Batch idx 148
Batch idx 149
Batch idx 150
Gradient norm: 0.203125
Batch idx 151
Batch idx 152
Batch idx 153
Batch idx 154
Gradient norm: 0.251953125
Batch idx 155
Batch idx 156
Batch idx 157
Batch idx 158
Gradient norm: 0.208984375
Batch idx 159
Batch idx 160
Batch idx 161
Batch idx 162
Gradient norm: 0.1826171875
Batch idx 163
Batch idx 164
Batch idx 165
Batch idx 166
Gradient norm: 0.197265625
Batch idx 167
Batch idx 168
Batch idx 169
Batch idx 170
Gradient norm: 0.197265625
Batch idx 171
Batch idx 172
Batch idx 173
Batch idx 174
Gradient norm: 0.2041015625
Batch idx 175
Batch idx 176
Batch idx 177
Batch idx 178
Gradient norm: 0.1826171875
Batch idx 179
Batch idx 180
Batch idx 181
Batch idx 182
Gradient norm: 0.19140625
Batch idx 183
Batch idx 184
Batch idx 185
Batch idx 186
Gradient norm: 0.21484375
Batch idx 187
Batch idx 188
Batch idx 189
Batch idx 190
Gradient norm: 0.1962890625
Batch idx 191
Batch idx 192
Batch idx 193
Batch idx 194
Gradient norm: 0.1845703125
Batch idx 195
Batch idx 196
Batch idx 197
Batch idx 198
Gradient norm: 0.248046875
Batch idx 199
Batch idx 200
Batch idx 201
Batch idx 202
Gradient norm: 0.1787109375
Batch idx 203
Batch idx 204
Batch idx 205
Batch idx 206
Gradient norm: 0.234375
Batch idx 207
Batch idx 208
Batch idx 209
Batch idx 210
Gradient norm: 0.234375
Batch idx 211
Batch idx 212
Batch idx 213
Batch idx 214
Gradient norm: 0.1982421875
Batch idx 215
Batch idx 216
Batch idx 217
Batch idx 218
Gradient norm: 0.1982421875
Batch idx 219
Batch idx 220
Batch idx 221
Batch idx 222
Gradient norm: 0.1962890625
Batch idx 223
Batch idx 224
Batch idx 225
Batch idx 226
Gradient norm: 0.1884765625
Batch idx 227
Batch idx 228
Batch idx 229
Batch idx 230
Gradient norm: 0.2255859375
Batch idx 231
Batch idx 232
Batch idx 233
Batch idx 234
Gradient norm: 0.1728515625
Batch idx 235
Batch idx 236
Batch idx 237
Batch idx 238
Gradient norm: 0.2021484375
Batch idx 239
Batch idx 240
Batch idx 241
Batch idx 242
Gradient norm: 0.2177734375
Batch idx 243
Batch idx 244
Batch idx 245
Batch idx 246
Gradient norm: 0.1865234375
Batch idx 247
Batch idx 248
Batch idx 249
Batch idx 250
Gradient norm: 0.208984375
Batch idx 251
Batch idx 252
Batch idx 253
Batch idx 254
Gradient norm: 0.1767578125
Batch idx 255
Batch idx 256
Batch idx 257
Batch idx 258
Gradient norm: 0.2216796875
Batch idx 259
Batch idx 260
Batch idx 261
Batch idx 262
Gradient norm: 0.2001953125
Batch idx 263
Batch idx 264
Batch idx 265
Batch idx 266
Gradient norm: 0.2158203125
Batch idx 267
Batch idx 268
Batch idx 269
Batch idx 270
Gradient norm: 0.1953125
Batch idx 271
Batch idx 272
Batch idx 273
Batch idx 274
Gradient norm: 0.212890625
Batch idx 275
Batch idx 276
Batch idx 277
Batch idx 278
Gradient norm: 0.1826171875
Batch idx 279
Batch idx 280
Batch idx 281
Batch idx 282
Gradient norm: 0.193359375
Batch idx 283
Batch idx 284
Batch idx 285
Batch idx 286
Gradient norm: 0.201171875
Batch idx 287
Batch idx 288
Batch idx 289
Batch idx 290
Gradient norm: 0.1953125
Batch idx 291
Batch idx 292
Batch idx 293
Batch idx 294
Gradient norm: 0.1923828125
Batch idx 295
Batch idx 296
Batch idx 297
Batch idx 298
Gradient norm: 0.21875
Batch idx 299
Batch idx 300
Batch idx 301
Batch idx 302
Gradient norm: 0.1923828125
Batch idx 303
Batch idx 304
Batch idx 305
Batch idx 306
Gradient norm: 0.197265625
Batch idx 307
Batch idx 308
Batch idx 309
Batch idx 310
Gradient norm: 0.197265625
Batch idx 311
Batch idx 312
Batch idx 313
Batch idx 314
Gradient norm: 0.1787109375
Batch idx 315
Batch idx 316
Batch idx 317
Batch idx 318
Gradient norm: 0.22265625
Batch idx 319
Batch idx 320
Batch idx 321
Batch idx 322
Gradient norm: 0.1982421875
Batch idx 323
Batch idx 324
Batch idx 325
Batch idx 326
Gradient norm: 0.203125
Batch idx 327
Batch idx 328
Batch idx 329
Batch idx 330
Gradient norm: 0.24609375
Batch idx 331
Batch idx 332
Batch idx 333
Batch idx 334
Gradient norm: 0.181640625
Batch idx 335
Batch idx 336
Batch idx 337
Batch idx 338
Gradient norm: 0.1904296875
Batch idx 339
Batch idx 340
Batch idx 341
Batch idx 342
Gradient norm: 0.2080078125
Batch idx 343
Batch idx 344
Batch idx 345
Batch idx 346
Gradient norm: 0.185546875
Batch idx 347
Batch idx 348
Batch idx 349
Batch idx 350
Gradient norm: 0.1806640625
Batch idx 351
Batch idx 352
Batch idx 353
Batch idx 354
Gradient norm: 0.171875
Batch idx 355
Batch idx 356
Batch idx 357
Batch idx 358
Gradient norm: 0.22265625
Batch idx 359
Batch idx 360
Batch idx 361
Batch idx 362
Gradient norm: 0.181640625
Batch idx 363
Batch idx 364
Batch idx 365
Batch idx 366
Gradient norm: 0.1904296875
Batch idx 367
Batch idx 368
Batch idx 369
Batch idx 370
Gradient norm: 0.2001953125
Batch idx 371
Batch idx 372
Batch idx 373
Batch idx 374
Gradient norm: 0.2080078125
Batch idx 375
Batch idx 376
Batch idx 377
Batch idx 378
Gradient norm: 0.1728515625
Batch idx 379
Batch idx 380
Batch idx 381
Batch idx 382
Gradient norm: 0.197265625
Batch idx 383
Batch idx 384
Batch idx 385
Batch idx 386
Gradient norm: 0.1943359375
Batch idx 387
Batch idx 388
Batch idx 389
Batch idx 390
Gradient norm: 0.203125
Batch idx 391
Batch idx 392
Batch idx 393
Batch idx 394
Gradient norm: 0.1748046875
Batch idx 395
Batch idx 396
Batch idx 397
Batch idx 398
Gradient norm: 0.1865234375
Batch idx 399
Batch idx 400
Batch idx 401
Batch idx 402
Epoch 0, Loss 1.806, LR 1.05e-05:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 101/126 [1:35:26<23:12, 55.71s/it]Epoch 0, Loss 1.752, LR 1.01e-05:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 101/126 [1:35:26<23:12, 55.71s/it]Epoch 0, Loss 1.752, LR 1.01e-05:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 102/126 [1:36:21<22:15, 55.64s/it]Epoch 0, Loss 1.747, LR 9.74e-06:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 102/126 [1:36:21<22:15, 55.64s/it]Epoch 0, Loss 1.747, LR 9.74e-06:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 103/126 [1:37:19<21:33, 56.24s/it]Epoch 0, Loss 1.890, LR 9.37e-06:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 103/126 [1:37:19<21:33, 56.24s/it]Epoch 0, Loss 1.890, LR 9.37e-06:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 104/126 [1:38:13<20:22, 55.59s/it]Epoch 0, Loss 1.871, LR 9.01e-06:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 104/126 [1:38:13<20:22, 55.59s/it]Epoch 0, Loss 1.871, LR 9.01e-06:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 105/126 [1:39:11<19:42, 56.33s/it]Epoch 0, Loss 1.753, LR 8.66e-06:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 105/126 [1:39:11<19:42, 56.33s/it]Epoch 0, Loss 1.753, LR 8.66e-06:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 106/126 [1:40:13<19:20, 58.05s/it]Epoch 0, Loss 1.754, LR 8.33e-06:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 106/126 [1:40:13<19:20, 58.05s/it]Epoch 0, Loss 1.754, LR 8.33e-06:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 107/126 [1:41:07<18:01, 56.95s/it]Epoch 0, Loss 1.857, LR 8.01e-06:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 107/126 [1:41:07<18:01, 56.95s/it]Epoch 0, Loss 1.857, LR 8.01e-06:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 108/126 [1:42:11<17:39, 58.87s/it]Epoch 0, Loss 1.802, LR 7.71e-06:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 108/126 [1:42:11<17:39, 58.87s/it]Epoch 0, Loss 1.802, LR 7.71e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 109/126 [1:43:07<16:25, 57.97s/it]Epoch 0, Loss 1.766, LR 7.42e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 109/126 [1:43:07<16:25, 57.97s/it]Epoch 0, Loss 1.766, LR 7.42e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 110/126 [1:44:03<15:19, 57.45s/it]Epoch 0, Loss 1.826, LR 7.15e-06:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 110/126 [1:44:03<15:19, 57.45s/it]Epoch 0, Loss 1.826, LR 7.15e-06:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 111/126 [1:45:05<14:42, 58.82s/it]Epoch 0, Loss 1.830, LR 6.90e-06:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 111/126 [1:45:05<14:42, 58.82s/it]Epoch 0, Loss 1.830, LR 6.90e-06:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 112/126 [1:45:53<12:57, 55.55s/it]Epoch 0, Loss 1.814, LR 6.65e-06:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 112/126 [1:45:53<12:57, 55.55s/it]Epoch 0, Loss 1.814, LR 6.65e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 113/126 [1:46:47<11:58, 55.26s/it]Epoch 0, Loss 1.854, LR 6.43e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 113/126 [1:46:47<11:58, 55.26s/it]Epoch 0, Loss 1.854, LR 6.43e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 114/126 [1:47:50<11:28, 57.35s/it]Epoch 0, Loss 1.741, LR 6.22e-06:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 114/126 [1:47:50<11:28, 57.35s/it]Epoch 0, Loss 1.741, LR 6.22e-06:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 115/126 [1:48:49<10:37, 57.94s/it]Epoch 0, Loss 1.658, LR 6.03e-06:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 115/126 [1:48:49<10:37, 57.94s/it]Epoch 0, Loss 1.658, LR 6.03e-06:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 116/126 [1:49:44<09:30, 57.01s/it]Epoch 0, Loss 1.783, LR 5.85e-06:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 116/126 [1:49:44<09:30, 57.01s/it]Epoch 0, Loss 1.783, LR 5.85e-06:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 117/126 [1:50:33<08:13, 54.84s/it]Epoch 0, Loss 1.766, LR 5.69e-06:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 117/126 [1:50:33<08:13, 54.84s/it]Epoch 0, Loss 1.766, LR 5.69e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 118/126 [1:51:34<07:33, 56.67s/it]Epoch 0, Loss 1.739, LR 5.54e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 118/126 [1:51:35<07:33, 56.67s/it]Epoch 0, Loss 1.739, LR 5.54e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 119/126 [1:52:32<06:37, 56.80s/it]Epoch 0, Loss 1.733, LR 5.42e-06:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 119/126 [1:52:32<06:37, 56.80s/it]Epoch 0, Loss 1.733, LR 5.42e-06:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 120/126 [1:53:30<05:44, 57.38s/it]Epoch 0, Loss 1.833, LR 5.31e-06:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 120/126 [1:53:30<05:44, 57.38s/it]Epoch 0, Loss 1.833, LR 5.31e-06:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 121/126 [1:54:21<04:36, 55.28s/it]Epoch 0, Loss 1.797, LR 5.21e-06:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 121/126 [1:54:21<04:36, 55.28s/it]Epoch 0, Loss 1.797, LR 5.21e-06:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 122/126 [1:55:20<03:45, 56.44s/it]Epoch 0, Loss 1.839, LR 5.14e-06:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 122/126 [1:55:20<03:45, 56.44s/it]Epoch 0, Loss 1.839, LR 5.14e-06:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 123/126 [1:56:06<02:40, 53.38s/it]Epoch 0, Loss 1.703, LR 5.08e-06:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 123/126 [1:56:06<02:40, 53.38s/it]Epoch 0, Loss 1.703, LR 5.08e-06:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 124/126 [1:57:09<01:52, 56.18s/it]Epoch 0, Loss 1.756, LR 5.03e-06:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 124/126 [1:57:09<01:52, 56.18s/it]Epoch 0, Loss 1.756, LR 5.03e-06:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 125/126 [1:58:01<00:55, 55.09s/it]Epoch 0, Loss 1.748, LR 5.01e-06:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 125/126 [1:58:01<00:55, 55.09s/it]Epoch 0, Loss 1.748, LR 5.01e-06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [1:58:50<00:00, 53.31s/it]Epoch 0, Loss 1.793, LR 5.00e-06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [1:58:51<00:00, 53.31s/it]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.007 MB of 0.082 MB uploadedwandb: - 0.011 MB of 0.088 MB uploadedwandb: \ 0.088 MB of 0.088 MB uploadedwandb: | 0.088 MB of 0.088 MB uploadedwandb: / 0.088 MB of 0.088 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                            grad_norm ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                 loss ‚ñá‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:                                   lr ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      memory/allocated_after_backward ‚ñÅ
wandb:       memory/allocated_after_forward ‚ñÅ
wandb: memory/allocated_after_model_created ‚ñÅ
wandb:    memory/allocated_after_model_wrap ‚ñÅ
wandb:      memory/allocated_before_forward ‚ñÅ
wandb:                memory/allocated_peak ‚ñÅ
wandb:       memory/reserved_after_backward ‚ñÅ
wandb:        memory/reserved_after_forward ‚ñÅ
wandb: memory/reserved_after_model_creation ‚ñÅ
wandb:     memory/reserved_after_model_wrap ‚ñÅ
wandb:       memory/reserved_before_forward ‚ñÅ
wandb:                 memory/reserved_peak ‚ñÅ
wandb:                           time_taken ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                            grad_norm 0.19238
wandb:                                 loss 1.79345
wandb:                                   lr 1e-05
wandb:      memory/allocated_after_backward 392925184
wandb:       memory/allocated_after_forward 1073345536
wandb: memory/allocated_after_model_created 8652800
wandb:    memory/allocated_after_model_wrap 111200256
wandb:      memory/allocated_before_forward 111201280
wandb:                memory/allocated_peak 3124925440
wandb:       memory/reserved_after_backward 3013607424
wandb:        memory/reserved_after_forward 2860515328
wandb: memory/reserved_after_model_creation 115343360
wandb:     memory/reserved_after_model_wrap 1958739968
wandb:       memory/reserved_before_forward 1958739968
wandb:                 memory/reserved_peak 4068474880
wandb:                           time_taken 7143.11
wandb: 
wandb: üöÄ View run dulcet-bush-56 at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora/runs/ctwtoph5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ihub-drug-discovery/fsdp_qlora
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240826_075611-ctwtoph5/logs
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (ctwtoph5) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
Epoch 0, Loss 1.793, LR 5.00e-06: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [1:59:13<00:00, 56.78s/it]
Rank 1: Model created: 0.105 GiB
Wrapping model w/ FSDP 1
Rank 1: Wrapped model: 1.824 GiB
Applying activation checkpointing 1
Rank 1: Before forward: 1.82 GiB
Rank 1: After forward: 2.67 GiB
Rank 1: After backward: 2.81 GiB
Rank 1: Peak allocated memory: 2.91 GiB
Rank 1: Peak reserved memory:  3.80 GiB
Rank 3: Model created: 0.107 GiB
Wrapping model w/ FSDP 3
Rank 3: Wrapped model: 1.824 GiB
Applying activation checkpointing 3
Rank 3: Before forward: 1.82 GiB
Rank 3: After forward: 2.66 GiB
Rank 3: After backward: 2.81 GiB
Rank 3: Peak allocated memory: 2.91 GiB
Rank 3: Peak reserved memory:  3.90 GiB
Rank 2: Model created: 0.107 GiB
Wrapping model w/ FSDP 2
Rank 2: Wrapped model: 1.824 GiB
Applying activation checkpointing 2
Rank 2: Before forward: 1.82 GiB
Rank 2: After forward: 2.66 GiB
Rank 2: After backward: 2.81 GiB
Rank 2: Peak allocated memory: 2.91 GiB
Rank 2: Peak reserved memory:  3.73 GiB
Gradient norm: 0.1943359375
Batch idx 403
Batch idx 404
Batch idx 405
Batch idx 406
Gradient norm: 0.1982421875
Batch idx 407
Batch idx 408
Batch idx 409
Batch idx 410
Gradient norm: 0.1796875
Batch idx 411
Batch idx 412
Batch idx 413
Batch idx 414
Gradient norm: 0.1884765625
Batch idx 415
Batch idx 416
Batch idx 417
Batch idx 418
Gradient norm: 0.1708984375
Batch idx 419
Batch idx 420
Batch idx 421
Batch idx 422
Gradient norm: 0.2314453125
Batch idx 423
Batch idx 424
Batch idx 425
Batch idx 426
Gradient norm: 0.21484375
Batch idx 427
Batch idx 428
Batch idx 429
Batch idx 430
Gradient norm: 0.177734375
Batch idx 431
Batch idx 432
Batch idx 433
Batch idx 434
Gradient norm: 0.1884765625
Batch idx 435
Batch idx 436
Batch idx 437
Batch idx 438
Gradient norm: 0.1953125
Batch idx 439
Batch idx 440
Batch idx 441
Batch idx 442
Gradient norm: 0.1845703125
Batch idx 443
Batch idx 444
Batch idx 445
Batch idx 446
Gradient norm: 0.1875
Batch idx 447
Batch idx 448
Batch idx 449
Batch idx 450
Gradient norm: 0.1787109375
Batch idx 451
Batch idx 452
Batch idx 453
Batch idx 454
Gradient norm: 0.1806640625
Batch idx 455
Batch idx 456
Batch idx 457
Batch idx 458
Gradient norm: 0.1904296875
Batch idx 459
Batch idx 460
Batch idx 461
Batch idx 462
Gradient norm: 0.197265625
Batch idx 463
Batch idx 464
Batch idx 465
Batch idx 466
Gradient norm: 0.216796875
Batch idx 467
Batch idx 468
Batch idx 469
Batch idx 470
Gradient norm: 0.2138671875
Batch idx 471
Batch idx 472
Batch idx 473
Batch idx 474
Gradient norm: 0.23046875
Batch idx 475
Batch idx 476
Batch idx 477
Batch idx 478
Gradient norm: 0.2041015625
Batch idx 479
Batch idx 480
Batch idx 481
Batch idx 482
Gradient norm: 0.2041015625
Batch idx 483
Batch idx 484
Batch idx 485
Batch idx 486
Gradient norm: 0.1767578125
Batch idx 487
Batch idx 488
Batch idx 489
Batch idx 490
Gradient norm: 0.2119140625
Batch idx 491
Batch idx 492
Batch idx 493
Batch idx 494
Gradient norm: 0.2109375
Batch idx 495
Batch idx 496
Batch idx 497
Batch idx 498
Gradient norm: 0.2041015625
Batch idx 499
Batch idx 500
Batch idx 501
Batch idx 502
Gradient norm: 0.1923828125
Batch idx 503
Saving full model weights.
Done 0
Finished training 0
CUDA event elapsed time: 7143.11 sec
Rank 0: Before forward: 1.82 GiB
Rank 0: After forward: 2.66 GiB
Rank 0: After backward: 2.81 GiB
Rank 0: Peak allocated memory: 2.91 GiB
Rank 0: Peak reserved memory:  3.79 GiB
Completed
