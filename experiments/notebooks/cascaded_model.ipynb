{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.nn import Linear4bit\n",
    "import bitsandbytes as bnb\n",
    "import tqdm\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig,PeftConfig, PeftModel, PeftModelForCausalLM\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "import safetensors\n",
    "import torch.nn as nn\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "safetensor_path = \"/scratch/tathagato/redo_adapter_experiments/length/length/adapter_model.safetensors\"\n",
    "adapter_path = \"/scratch/tathagato/redo_adapter_experiments/length/length/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "model_kwargs = dict(\n",
    "        use_cache=False,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=None,\n",
    "        cache_dir = \"/scratch/tathagato\",\n",
    "        attn_implementation = \"eager\",\n",
    "        quantization_config = nf4_config, \n",
    "\n",
    "    )\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model,cache_dir = \"/scratch/tathagato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n",
      "torch.Size([256, 2048])\n"
     ]
    }
   ],
   "source": [
    "non_quantized_model = AutoModelForCausalLM.from_pretrained(base_model, trust_remote_code=True, use_cache=False, cache_dir = \"/scratch/tathagato\")\n",
    "print(non_quantized_model)\n",
    "print(non_quantized_model.model.layers[0].self_attn.k_proj.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CascadedLoRALinear4bit(torch.nn.Module):\n",
    "    def __init__(self, linear, in_dim, out_dim, rank_1 = 64, rank_2 = 32, alpha_1 = 16, alpha_2 = 16, adapter_name = \"default\" , dropout = None):\n",
    "        super().__init__()\n",
    "        self.base_layer = linear\n",
    "        std_dev_1 = 1 / torch.sqrt(torch.tensor(rank_1).float())\n",
    "        std_dev_2 = 1 / torch.sqrt(torch.tensor(rank_2).float())\n",
    "        if dropout is not None:\n",
    "            self.lora_dropout = nn.ModuleDict(\n",
    "                {\n",
    "                    adapter_name : torch.nn.Dropout(dropout)\n",
    "                }\n",
    "            )\n",
    "        #first dimension\n",
    "        self.lora_A = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(in_dim, rank_1, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_B = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(rank_1, out_dim, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_A[adapter_name].weight = torch.nn.Parameter(torch.randn(rank_1, in_dim) * std_dev_1)\n",
    "        self.lora_B[adapter_name].weight = torch.nn.Parameter(torch.zeros(out_dim, rank_1))  \n",
    "\n",
    "        self.lora_A1 = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(in_dim, rank_2, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_A2 = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(rank_2, rank_1, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_B1 = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(rank_1, rank_2, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_B2 = nn.ModuleDict(\n",
    "            {\n",
    "                adapter_name : torch.nn.Linear(rank_2, out_dim, bias = False)\n",
    "            }\n",
    "        )\n",
    "        self.lora_A1[adapter_name].weight = torch.nn.Parameter(torch.randn(rank_2, in_dim) * std_dev_2)\n",
    "        self.lora_A2[adapter_name].weight = torch.nn.Parameter(torch.zeros(rank_1, rank_2))\n",
    "        self.lora_B1[adapter_name].weight = torch.nn.Parameter(torch.zeros(rank_2, rank_1))\n",
    "        self.lora_B2[adapter_name].weight = torch.nn.Parameter(torch.zeros(out_dim, rank_2) * std_dev_2)  \n",
    "        self.alpha_1 = alpha_1\n",
    "        self.alpha_2 = alpha_2\n",
    "        self.rank_1 = rank_1\n",
    "        self.rank_2 = rank_2\n",
    "        self.is_second_layar_being_trained = False\n",
    "        self.is_first_layer_being_trained = False\n",
    "        self.is_first_layer_being_used_for_inference = True\n",
    "        self.is_first_layer_being_used_for_inference = True\n",
    "        self.scaling_1 = self.rank_1 / self.alpha_1\n",
    "        self.scaling_2 = self.rank_2 / self.alpha_1\n",
    "        self.adapter_name = adapter_name\n",
    "\n",
    "\n",
    "\n",
    "    def set_gradients_for_all_layer(self):\n",
    "        if self.is_second_layar_being_trained:\n",
    "            self.lora_A1[self.adapter_name].requires_grad = True\n",
    "            self.lora_A2[self.adapter_name].requires_grad = True\n",
    "            self.lora_B1[self.adapter_name].requires_grad = True\n",
    "            self.lora_B2[self.adapter_name].requires_grad = True\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.lora_A1[self.adapter_name].requires_grad = False\n",
    "            self.lora_A2[self.adapter_name].requires_grad = False\n",
    "            self.lora_B1[self.adapter_name].requires_grad = False\n",
    "            self.lora_B2[self.adapter_name].requires_grad = False\n",
    "            \n",
    "        if self.is_first_layer_being_trained:\n",
    "            self.lora_A[self.adapter_name].requires_grad = True\n",
    "            self.lora_B[self.adapter_name].requires_grad = True\n",
    "        else:\n",
    "            self.lora_A[self.adapter_name].requires_grad = False\n",
    "            self.lora_B[self.adapter_name].requires_grad = False  \n",
    "    \n",
    "    def tune_the_first_adapter(self):\n",
    "        self.is_first_layer_being_trained = True\n",
    "    \n",
    "    def freeze_the_first_adapter(self):\n",
    "        self.is_first_layer_being_trained = False\n",
    "    \n",
    "    def tune_the_second_adapter(self):\n",
    "        self.is_second_layar_being_trained = True\n",
    "    \n",
    "    def freeze_the_second_adapter(self):\n",
    "        self.is_second_layar_being_trained = False\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.set_gradients_for_all_layer()\n",
    "        if self.is_first_layer_being_used_for_inference and self.is_second_layer_being_used_for_inference:\n",
    "            #x = self.scaling_1 * (x @ self.W1_a @ self.W1_b) + self.scaling_2 * (x @ self.W2_a1 @ self.W2_a2 @ self.W2_b1 @ self.W2_b2)\n",
    "            output  = self.linear(x) + self.scaling_1 * (self.W1['A'](self.W1['B'](x))) + self.scaling_2 * (self.W2['B2'](self.W2['A2'](self.W2['B1'](self.W2['A1'](x)))))\n",
    "        if self.is_first_layer_being_used_for_inference and not self.is_second_layer_being_used_for_inference:\n",
    "            #x = self.scaling_2 * (x @ self.W2_a1 @ self.W2_a2) \n",
    "            output  =  self.linear(x)  + self.scaling_1 * (self.W1['A'](self.W1['B'](x))) \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rank_1 = 64\n",
    "rank_2 = 32\n",
    "alpha_1 = 16\n",
    "alpha_2 = 16\n",
    "adapter_name = \"test\"\n",
    "dropout = 0.05\n",
    "target_modules = [\n",
    "                    'q_proj',\n",
    "                    'k_proj',\n",
    "                    'v_proj',\n",
    "                    'o_proj',\n",
    "                    'gate_proj',\n",
    "                    'up_proj',\n",
    "                    'down_proj'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_cascaded_lora(module, target_modules = target_modules, rank_1 = 64, rank_2 = 32, alpha_1 = 16 , alpha_2 = 16 , adapter_name = \"default\" , dropout = None):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, bnb.nn.Linear4bit) and name in target_modules:\n",
    "            #setattr(module, name, CascadedLoRALinear4bit(child, in_dim, out_dim, **kwargs))\n",
    "            #print(name)\n",
    "            #print(child.in_features, child.out_features)\n",
    "            #get the device of the child\n",
    "            setattr(module, name, CascadedLoRALinear4bit(child, child.in_features, child.out_features, rank_1, rank_2, alpha_1, alpha_2, adapter_name , dropout = dropout))\n",
    "            #put everything in device \n",
    "        else:\n",
    "            replace_with_cascaded_lora(child, target_modules, rank_1, rank_2, alpha_1, alpha_2, adapter_name , dropout = None)\n",
    "def print_device_and_dtype(model, file = sys.stdout):\n",
    "    if file == sys.stdout:\n",
    "            for name, module in model.named_modules():\n",
    "            # Get the device and dtype of the module's parameters\n",
    "            #file = open(file, \"a\")\n",
    "                try:\n",
    "                    param = next(module.parameters())\n",
    "                    device = param.device\n",
    "                    dtype = param.dtype\n",
    "                    type = param.type()\n",
    "                except StopIteration:\n",
    "                    device = 'No parameters'\n",
    "                    dtype = 'No parameters'\n",
    "                    type = 'No parameters'\n",
    "\n",
    "                \n",
    "                # Print the name, device, and dtype of the module\n",
    "                print(f\"Module: {name}\", file = file)\n",
    "                print(f\"  Device: {device}\", file = file)\n",
    "                print(f\"  Dtype: {dtype}\", file = file)\n",
    "                print(f\"  Type: {type}\", file = file)\n",
    "                print(\" \",file = file )\n",
    "            return \n",
    "\n",
    "    with open(file, \"w\") as file:\n",
    "\n",
    "        for name, module in model.named_modules():\n",
    "            # Get the device and dtype of the module's parameters\n",
    "            #file = open(file, \"a\")\n",
    "            try:\n",
    "                param = next(module.parameters())\n",
    "                device = param.device\n",
    "                dtype = param.dtype\n",
    "                type = param.type()\n",
    "            except StopIteration:\n",
    "                device = 'No parameters'\n",
    "                dtype = 'No parameters'\n",
    "                type = 'No parameters'\n",
    "\n",
    "            \n",
    "            # Print the name, device, and dtype of the module\n",
    "            print(f\"Module: {name}\", file = file)\n",
    "            print(f\"  Device: {device}\", file = file)\n",
    "            print(f\"  Dtype: {dtype}\", file = file)\n",
    "            print(f\"  Type: {type}\", file = file)\n",
    "            print(\" \",file = file )\n",
    "# Function to ensure all submodules are on GPU\n",
    "def move_to_device(model, device):\n",
    "    for name, module in model.named_modules():\n",
    "        try:\n",
    "            # Check if the module is already on the device\n",
    "            param = next(module.parameters())\n",
    "            if param.device != device:\n",
    "                # Move the module to the specified device\n",
    "                module.to(device)\n",
    "                #print(f\"Moved module: {name} to {device}\")\n",
    "        except StopIteration:\n",
    "            # No parameters in the module\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "replace_with_cascaded_lora(quantized_model)\n",
    "#print(quantized_model)\n",
    "move_to_device(quantized_model, torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print_device_and_dtype(quantized_model, file = \"cascaded_lora_structure.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cascaded_lora.ipynb          extract_outputs.ipynb       test_model.ipynb\n",
      "cascaded_lora_structure.txt  peft_adapter_structure.txt\n",
      "cascaded_model.ipynb         test_dataset.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "base_model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "base_model_quantized = AutoModelForCausalLM.from_pretrained(base_model_path, **model_kwargs)\n",
    "adapter_model = PeftModelForCausalLM.from_pretrained(base_model_quantized, adapter_path, \"test\")\n",
    "#print(adapter_model)\n",
    "print_device_and_dtype(adapter_model, file = \"./peft_adapter_structure.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
