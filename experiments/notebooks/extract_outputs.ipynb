{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home2/tathagato/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_attribute(filepath):\n",
    "    basename = os.path.basename(filepath)\n",
    "    filename = os.path.splitext(basename)[0]\n",
    "    attribute = filename.split(\"_\")[-1]\n",
    "    return attribute\n",
    "def get_fragment_density(article, summary):\n",
    "    \"\"\"\n",
    "    Calculates the fragment density of a summary on an article.\n",
    "\n",
    "    Density is defined as the average squared length of extracted fragments.\n",
    "\n",
    "    Args:\n",
    "        article (str): The article text.\n",
    "        summary (str): The summary text.\n",
    "\n",
    "    Returns:\n",
    "        float: The fragment density of the summary on the article.\n",
    "    \"\"\"\n",
    "\n",
    "    frags, article_tokens, summary_tokens = get_extractive_fragments(article, summary)\n",
    "    if len(summary_tokens) == 0:\n",
    "        print(\"fragment density sumary_tokens is zero\")\n",
    "        print(article)\n",
    "        print(summary)\n",
    "        return 0\n",
    "    density = float(sum([len(f)**2 for f in frags])) / float(len(summary_tokens))\n",
    "    return density\n",
    "def get_overlap(inp, out, ngram = 2):\n",
    "    grams_inp = set(ngrams(word_tokenize(inp.lower()), ngram))\n",
    "    grams_out = set(ngrams(word_tokenize(out.lower()), ngram))\n",
    "\n",
    "    total = len(grams_out)\n",
    "    common = len(grams_inp.intersection(grams_out))\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return float(common) / float(total)\n",
    "def get_extractive_fragments(article, summary):\n",
    "    \"\"\"\n",
    "    Extracts fragments from an article that match sequences of words in a summary.\n",
    "\n",
    "    Args:\n",
    "        article (str): The article text.\n",
    "        summary (str): The summary text.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist represents a sequence of word indexes\n",
    "            in the article that match a sequence in the summary.\n",
    "        list: The tokenized article.\n",
    "        list: The tokenized summary.\n",
    "    \"\"\"\n",
    "\n",
    "    article_tokens = word_tokenize(article.lower())\n",
    "    summary_tokens = word_tokenize(summary.lower())\n",
    "\n",
    "    F = []  # List to store the extracted fragments\n",
    "    i, j = 0, 0  # Indexes for iterating over article and summary tokens, respectively\n",
    "\n",
    "    while i < len(summary_tokens):\n",
    "        f = []  # List to store the current fragment\n",
    "        while j < len(article_tokens):\n",
    "            if summary_tokens[i] == article_tokens[j]:\n",
    "                i_, j_ = i, j  # Store starting indexes of potential fragment\n",
    "                #print(len(summary_tokens), len(article_tokens), i, j, i_, j_, summary_tokens[i_], article_tokens[j_])\n",
    "                while (i_ < len(summary_tokens) and j_ < len(article_tokens)) and summary_tokens[i_] == article_tokens[j_]:\n",
    "                    i_, j_ = i_ + 1, j_ + 1  # Update indexes while words match\n",
    "                if len(f) < (i_ - i):  # Update fragment if a longer match is found\n",
    "                    f = list(range(i, i_))\n",
    "                j = j_  # Set j to the next position after the matched sequence\n",
    "            else:\n",
    "                j += 1  # Move to the next article token if no match found\n",
    "        i += max(len(f), 1)  # Update i by the length of the extracted fragment or 1\n",
    "        j = 1  # Reset j for the next iteration\n",
    "\n",
    "        F.append(f)  # Append the extracted fragment to the list\n",
    "\n",
    "    return F, article_tokens, summary_tokens\n",
    "\n",
    "\n",
    "\n",
    "def get_extractive_coverage(article, summary):\n",
    "    \"\"\"\n",
    "    Calculates the extractive coverage of a summary on an article.\n",
    "\n",
    "    Coverage is defined as the ratio of words in the summary covered by fragments\n",
    "    extracted from the article.\n",
    "\n",
    "    Args:\n",
    "        article (str): The article text.\n",
    "        summary (str): The summary text.\n",
    "\n",
    "    Returns:\n",
    "        float: The extractive coverage of the summary on the article.\n",
    "    \"\"\"\n",
    "\n",
    "    frags, article_tokens, summary_tokens = get_extractive_fragments(article, summary)\n",
    "    if len(summary_tokens) == 0:\n",
    "        print(\"sumary_tokens is zero\")\n",
    "        print(article)\n",
    "        print(summary)\n",
    "        return 0\n",
    "\n",
    "    coverage = float(sum([len(f) for f in frags])) / float(len(summary_tokens))\n",
    "    return coverage\n",
    "\n",
    "def clean_and_process_data(file):\n",
    "    data = json.load(open(file,\"r\"))\n",
    "    keys = list(data.keys())\n",
    "    for key in tqdm.tqdm(keys):\n",
    "        #print(data[key].keys())\n",
    "        if 'generated_text' not in data[key]:\n",
    "            #remove the element from dict\n",
    "            print(\"popping key\", key)\n",
    "            data.pop(key)\n",
    "            continue\n",
    "        summary = data[key]['generated_text'].split(\"\\n\")[-1]\n",
    "        data[key]['predicted_summary'] = summary\n",
    "    with open(file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "    return data\n",
    "\n",
    "def get_length_stats(data):\n",
    "    output_dict = {}\n",
    "    controlled_attribute = \"length\"\n",
    "    keys = list(data.keys())\n",
    "    for key in keys:\n",
    "        control_value = data[key]['control_value']\n",
    "        article = data[key]['input']\n",
    "        summary = data[key]['predicted_summary']\n",
    "        reference = data[key]['output']\n",
    "        if control_value not in output_dict:\n",
    "            output_dict[control_value] = {\"article\":[], \"summary\":[], \"reference\":[]}\n",
    "        output_dict[control_value]['article'].append(article)\n",
    "        output_dict[control_value]['summary'].append(summary)\n",
    "        output_dict[control_value]['reference'].append(reference)\n",
    "    for key in output_dict:\n",
    "        output_dict[key]['prediction_summary_length'] = [get_summary_length(summary) for summary in output_dict[key]['summary']]\n",
    "        output_dict[key]['prediction_compression_ratio'] = [get_compression_ratio(article, summary) for article, summary in zip(output_dict[key]['article'], output_dict[key]['summary'])]\n",
    "        output_dict[key]['reference_summary_length'] = [get_summary_length(summary) for summary in output_dict[key]['reference']]\n",
    "        output_dict[key]['reference_compression_ratio'] = [get_compression_ratio(article, summary) for article, summary in zip(output_dict[key]['article'], output_dict[key]['reference'])]\n",
    "        print(\"control_value\", key, len(output_dict[key]['article']), len(output_dict[key]['summary']))\n",
    "        print(\"Prediction Summary Length\", sum(output_dict[key]['prediction_summary_length'])/ len(output_dict[key]['prediction_summary_length']))\n",
    "        print(\"Reference Summary Length\", sum(output_dict[key]['reference_summary_length'])/ len(output_dict[key]['reference_summary_length']))\n",
    "        print(\"Prediction Compression Ratio\", sum(output_dict[key]['prediction_compression_ratio'])/ len(output_dict[key]['prediction_compression_ratio']))\n",
    "        print(\"Reference Compression Ratio\", sum(output_dict[key]['reference_compression_ratio'])/ len(output_dict[key]['reference_compression_ratio']))\n",
    "    \n",
    "    print(\"------------------------------\")\n",
    "    #print(output_dict.keys())\n",
    "\n",
    "def get_summary_length(summary):\n",
    "    return len(word_tokenize(summary.lower()))\n",
    "\n",
    "def get_compression_ratio(article, summary):\n",
    "    if len(word_tokenize(article.lower())) == 0:\n",
    "        print(\"article_tokens is zero\")\n",
    "        print(article)\n",
    "        print(summary)\n",
    "        return 0\n",
    "    return float(len(word_tokenize(summary.lower()))) / float(len(word_tokenize(article.lower())))\n",
    "\n",
    "def get_abstractive_data(data):\n",
    "    keys = list(data.keys())\n",
    "    print(data[keys[0]].keys())\n",
    "    output_dict = {}\n",
    "    controlled_attribute = \"extractiveness\"\n",
    "    for key in tqdm.tqdm(keys):\n",
    "        control_value = data[key]['control_value']\n",
    "        article = data[key]['input']\n",
    "        summary = data[key]['predicted_summary']\n",
    "        reference = data[key]['output']\n",
    "        if control_value not in output_dict:\n",
    "            output_dict[control_value] = {\"article\":[], \"summary\":[], \"reference\":[]}\n",
    "        output_dict[control_value]['article'].append(article)\n",
    "        output_dict[control_value]['summary'].append(summary)\n",
    "        output_dict[control_value]['reference'].append(reference)\n",
    "    #print(output_dict.keys())\n",
    "    \n",
    "\n",
    "    for key in tqdm.tqdm(output_dict):\n",
    "        print(key, len(output_dict[key]['article']), len(output_dict[key]['summary']), len(output_dict[key]['reference']))\n",
    "        output_dict[key]['prediction_density'] = [get_fragment_density(article, summary) for article, summary in zip(output_dict[key]['article'], output_dict[key]['summary'])]\n",
    "        output_dict[key]['prediction_coverage'] = [get_extractive_coverage(article, summary) for article, summary in zip(output_dict[key]['article'], output_dict[key]['summary'])]\n",
    "        output_dict[key]['prediction_overlap'] = [get_overlap(article, summary) for article, summary in zip(output_dict[key]['article'], output_dict[key]['summary'])]\n",
    "        output_dict[key]['reference_density'] = [get_fragment_density(article, summary) for article, summary in zip(output_dict[key]['article'], output_dict[key]['reference'])]\n",
    "        output_dict[key]['reference_coverage'] = [get_extractive_coverage(article, summary) for article, summary in zip(output_dict[key]['article'], output_dict[key]['reference'])]\n",
    "        output_dict[key]['reference_overlap'] = [get_overlap(article, summary) for article, summary in zip(output_dict[key]['article'], output_dict[key]['reference'])]\n",
    "\n",
    "        print(\"control_value\", key, len(output_dict[key]['article']), len(output_dict[key]['summary']))\n",
    "        print(\"Prediction Density\", sum(output_dict[key]['prediction_density'])/ len(output_dict[key]['prediction_density']))\n",
    "        print(\"Prediction Coverage\", sum(output_dict[key]['prediction_coverage'])/ len(output_dict[key]['prediction_coverage']))\n",
    "        print(\"Prediction Overlap\", sum(output_dict[key]['prediction_overlap'])/ len(output_dict[key]['prediction_overlap']))\n",
    "\n",
    "\n",
    "        print(\"Reference Density\", sum(output_dict[key]['reference_density'])/ len(output_dict[key]['reference_density']))\n",
    "        print(\"Reference Coverage\", sum(output_dict[key]['reference_coverage'])/ len(output_dict[key]['reference_coverage']))\n",
    "        print(\"Reference Overlap\", sum(output_dict[key]['reference_overlap'])/ len(output_dict[key]['reference_overlap']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def output_metrics(file, attribute):\n",
    "    if attribute == \"length\":\n",
    "        data = clean_and_process_data(file)\n",
    "        get_length_stats(data)\n",
    "    # elif attribute == \"extractiveness\":\n",
    "    #     data = clean_and_process_data(file)\n",
    "    #     get_abstractive_data(data)\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../results/extractiveness_on_extractiveness.json\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/extractiveness_then_length_evaluate_length.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 208165.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_value short 40 40\n",
      "Prediction Summary Length 60.65\n",
      "Reference Summary Length 35.6\n",
      "Prediction Compression Ratio 0.09896401818633468\n",
      "Reference Compression Ratio 0.0454279694065336\n",
      "control_value normal 107 107\n",
      "Prediction Summary Length 49.0\n",
      "Reference Summary Length 45.81308411214953\n",
      "Prediction Compression Ratio 0.0657310598337557\n",
      "Reference Compression Ratio 0.05628746237555814\n",
      "control_value long 41 41\n",
      "Prediction Summary Length 54.02439024390244\n",
      "Reference Summary Length 103.7560975609756\n",
      "Prediction Compression Ratio 0.08622761825649405\n",
      "Reference Compression Ratio 0.12692456629661195\n",
      "------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/topic_then_extractiveness_evaluate_extractiveness.json\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/zero_shot_length.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 285285.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_value short 40 40\n",
      "Prediction Summary Length 157.9\n",
      "Reference Summary Length 35.6\n",
      "Prediction Compression Ratio 0.23116922096033563\n",
      "Reference Compression Ratio 0.0454279694065336\n",
      "control_value normal 107 107\n",
      "Prediction Summary Length 159.34579439252337\n",
      "Reference Summary Length 45.81308411214953\n",
      "Prediction Compression Ratio 0.20411269441461977\n",
      "Reference Compression Ratio 0.05628746237555814\n",
      "control_value long 41 41\n",
      "Prediction Summary Length 167.0\n",
      "Reference Summary Length 103.7560975609756\n",
      "Prediction Compression Ratio 0.23625447832617838\n",
      "Reference Compression Ratio 0.12692456629661195\n",
      "------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/length_on_length.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 299252.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_value short 40 40\n",
      "Prediction Summary Length 35.85\n",
      "Reference Summary Length 35.6\n",
      "Prediction Compression Ratio 0.04793024624803299\n",
      "Reference Compression Ratio 0.0454279694065336\n",
      "control_value normal 107 107\n",
      "Prediction Summary Length 44.71028037383178\n",
      "Reference Summary Length 45.81308411214953\n",
      "Prediction Compression Ratio 0.05334883446187778\n",
      "Reference Compression Ratio 0.05628746237555814\n",
      "control_value long 41 41\n",
      "Prediction Summary Length 60.26829268292683\n",
      "Reference Summary Length 103.7560975609756\n",
      "Prediction Compression Ratio 0.07188497115273834\n",
      "Reference Compression Ratio 0.12692456629661195\n",
      "------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/extractiveness_then_length_evaluate_extractiveness.json\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/zero_shot_extractiveness.json\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/topic_then_length_evaluate_length.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 253954.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_value short 40 40\n",
      "Prediction Summary Length 58.8\n",
      "Reference Summary Length 35.6\n",
      "Prediction Compression Ratio 0.1202781560981004\n",
      "Reference Compression Ratio 0.0454279694065336\n",
      "control_value normal 107 107\n",
      "Prediction Summary Length 50.0\n",
      "Reference Summary Length 45.81308411214953\n",
      "Prediction Compression Ratio 0.09162414569641121\n",
      "Reference Compression Ratio 0.05628746237555814\n",
      "control_value long 41 41\n",
      "Prediction Summary Length 66.07317073170732\n",
      "Reference Summary Length 103.7560975609756\n",
      "Prediction Compression Ratio 0.1319415878932052\n",
      "Reference Compression Ratio 0.12692456629661195\n",
      "------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/length_then_extractiveness_evaluate_extractiveness.json\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/length_then_topic_evaluate_length.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 211628.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_value short 40 40\n",
      "Prediction Summary Length 73.3\n",
      "Reference Summary Length 35.6\n",
      "Prediction Compression Ratio 0.10781389132726382\n",
      "Reference Compression Ratio 0.0454279694065336\n",
      "control_value normal 107 107\n",
      "Prediction Summary Length 62.691588785046726\n",
      "Reference Summary Length 45.81308411214953\n",
      "Prediction Compression Ratio 0.07566608351673165\n",
      "Reference Compression Ratio 0.05628746237555814\n",
      "control_value long 41 41\n",
      "Prediction Summary Length 71.78048780487805\n",
      "Reference Summary Length 103.7560975609756\n",
      "Prediction Compression Ratio 0.10132615716666588\n",
      "Reference Compression Ratio 0.12692456629661195\n",
      "------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/extractiveness_then_topic_evaluate_extractiveness.json\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/extractiveness_and_length_evaluate_length.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 258026.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_value short 40 40\n",
      "Prediction Summary Length 60.65\n",
      "Reference Summary Length 35.6\n",
      "Prediction Compression Ratio 0.09896401818633468\n",
      "Reference Compression Ratio 0.0454279694065336\n",
      "control_value normal 107 107\n",
      "Prediction Summary Length 49.0\n",
      "Reference Summary Length 45.81308411214953\n",
      "Prediction Compression Ratio 0.0657310598337557\n",
      "Reference Compression Ratio 0.05628746237555814\n",
      "control_value long 41 41\n",
      "Prediction Summary Length 54.02439024390244\n",
      "Reference Summary Length 103.7560975609756\n",
      "Prediction Compression Ratio 0.08622761825649405\n",
      "Reference Compression Ratio 0.12692456629661195\n",
      "------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/length_then_extractiveness_evaluate_length.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 318726.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_value short 40 40\n",
      "Prediction Summary Length 73.3\n",
      "Reference Summary Length 35.6\n",
      "Prediction Compression Ratio 0.10781389132726382\n",
      "Reference Compression Ratio 0.0454279694065336\n",
      "control_value normal 107 107\n",
      "Prediction Summary Length 62.691588785046726\n",
      "Reference Summary Length 45.81308411214953\n",
      "Prediction Compression Ratio 0.07566608351673165\n",
      "Reference Compression Ratio 0.05628746237555814\n",
      "control_value long 41 41\n",
      "Prediction Summary Length 71.78048780487805\n",
      "Reference Summary Length 103.7560975609756\n",
      "Prediction Compression Ratio 0.10132615716666588\n",
      "Reference Compression Ratio 0.12692456629661195\n",
      "------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n",
      "./../results/extractiveness_and_length_evaluate_extractiveness.json\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dirname = \"./../results/\"\n",
    "files = [os.path.join(dirname, f) for f in os.listdir(dirname) if os.path.isfile(os.path.join(dirname, f))]\n",
    "files = [(file, get_evaluation_attribute(file)) for file in files]\n",
    "for file, attribute in files:\n",
    "    if attribute != \"topic\":\n",
    "        print(file)\n",
    "        output_metrics(file, attribute)\n",
    "        print(\"\\n---------------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
