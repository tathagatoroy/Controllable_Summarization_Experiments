Activating Conda Environment Virtual Environment
running script
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-05-25 03:03:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 03:03:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch/tathagato/adapter_experiments/length_then_topic/runs/May25_03-03-38_gnode081,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=20,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/scratch/tathagato/adapter_experiments/length_then_topic,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch/tathagato/adapter_experiments/length_then_topic,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=400,
save_strategy=steps,
save_total_limit=400,
seed=0,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
2024-05-25 03:03:38 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'q_proj', 'o_proj', 'k_proj', 'v_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
2024-05-25 03:03:38 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 03:03:38 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 03:03:38 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1 distributed training: True, 16-bits training: True
[WARNING|modeling_utils.py:3058] 2024-05-25 03:03:39,362 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|configuration_utils.py:726] 2024-05-25 03:03:39,433 >> loading configuration file config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 03:03:39,437 >> Model config LlamaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": false,
  "vocab_size": 32000
}

[INFO|quantizer_bnb_4bit.py:247] 2024-05-25 03:03:39,537 >> The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' 
[WARNING|modeling_utils.py:3058] 2024-05-25 03:03:39,537 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:3283] 2024-05-25 03:03:39,537 >> loading weights file model.safetensors from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors
[INFO|modeling_utils.py:1417] 2024-05-25 03:03:39,555 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:928] 2024-05-25 03:03:39,557 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

[WARNING|modeling_utils.py:3058] 2024-05-25 03:03:39,659 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 03:03:39,683 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:4024] 2024-05-25 03:03:42,390 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-25 03:03:42,391 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-05-25 03:03:42,634 >> loading configuration file generation_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-25 03:03:42,635 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 2048,
  "pad_token_id": 0
}

[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:03:43,060 >> loading file tokenizer.model from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:03:43,060 >> loading file tokenizer.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:03:43,060 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:03:43,060 >> loading file special_tokens_map.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:03:43,060 >> loading file tokenizer_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json
loading model from : /scratch/tathagato/adapter_experiments/length/length
loading model from : /scratch/tathagato/adapter_experiments/length/length
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
loading model from : /scratch/tathagato/adapter_experiments/length/length
loading model from : /scratch/tathagato/adapter_experiments/length/length
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
train dataset size 2013
test dataset size 272
2013
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
Spawning 10 processes
2024-05-25 03:03:45 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to train_sft (num_proc=10):   0%|          | 0/2013 [00:00<?, ? examples/s]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
Applying chat template to train_sft (num_proc=10):   0%|          | 1/2013 [00:00<22:30,  1.49 examples/s]trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Applying chat template to train_sft (num_proc=10):  10%|█         | 203/2013 [00:00<00:06, 274.56 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 405/2013 [00:01<00:03, 451.15 examples/s]train dataset size 2013
test dataset size 272
2013
Applying chat template to train_sft (num_proc=10):  30%|███       | 607/2013 [00:01<00:02, 580.41 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 808/2013 [00:01<00:01, 642.36 examples/s]trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Applying chat template to train_sft (num_proc=10):   0%|          | 0/2013 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 1009/2013 [00:01<00:01, 704.47 examples/s]train dataset size 2013
test dataset size 272
2013
Applying chat template to train_sft (num_proc=10):  60%|██████    | 1210/2013 [00:02<00:01, 777.56 examples/s]train dataset size 2013
test dataset size 272
2013
Applying chat template to train_sft (num_proc=10):  70%|███████   | 1411/2013 [00:02<00:00, 767.35 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 1612/2013 [00:02<00:00, 894.77 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/2013 [00:00<24:40,  1.36 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 1813/2013 [00:02<00:00, 886.98 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/2013 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 203/2013 [00:00<00:06, 261.40 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/2013 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 405/2013 [00:01<00:03, 478.70 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 2013/2013 [00:03<00:00, 657.02 examples/s]
Concatenating 10 shards
2024-05-25 03:03:48 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Applying chat template to train_sft (num_proc=10):  30%|███       | 607/2013 [00:01<00:02, 542.16 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 808/2013 [00:01<00:01, 651.83 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/2013 [00:00<26:37,  1.26 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/2013 [00:00<27:24,  1.22 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 1009/2013 [00:01<00:01, 701.19 examples/s]Spawning 10 processes
2024-05-25 03:03:49 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to test_sft (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 203/2013 [00:01<00:07, 237.67 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 203/2013 [00:01<00:07, 239.22 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 1210/2013 [00:02<00:01, 706.17 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 405/2013 [00:01<00:03, 424.89 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 405/2013 [00:01<00:03, 432.52 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/272 [00:00<01:47,  2.52 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 607/2013 [00:01<00:02, 562.69 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 1411/2013 [00:02<00:00, 735.91 examples/s]Applying chat template to test_sft (num_proc=10):  21%|██        | 57/272 [00:00<00:01, 120.20 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 1612/2013 [00:02<00:00, 780.94 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 607/2013 [00:01<00:02, 494.26 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 111/272 [00:00<00:00, 191.80 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 808/2013 [00:01<00:01, 610.05 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 808/2013 [00:01<00:01, 630.48 examples/s]Applying chat template to test_sft (num_proc=10):  61%|██████    | 165/272 [00:00<00:00, 229.08 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 1813/2013 [00:02<00:00, 770.35 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 1009/2013 [00:02<00:01, 687.75 examples/s]Applying chat template to test_sft (num_proc=10):  81%|████████  | 219/272 [00:01<00:00, 263.92 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 1009/2013 [00:02<00:01, 649.56 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 2013/2013 [00:03<00:00, 623.01 examples/s]
Applying chat template to train_sft (num_proc=10):  60%|██████    | 1210/2013 [00:02<00:01, 694.98 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 204.00 examples/s]
Concatenating 10 shards
2024-05-25 03:03:50 - INFO - datasets.arrow_dataset - Concatenating 10 shards
tokenizer padding side left
Applying chat template to train_sft (num_proc=10):  60%|██████    | 1210/2013 [00:02<00:01, 645.93 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 1411/2013 [00:02<00:00, 704.02 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 1612/2013 [00:02<00:00, 749.93 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 1411/2013 [00:02<00:00, 664.78 examples/s]Using custom data configuration default-6ab037a909b14552
2024-05-25 03:03:51 - INFO - datasets.builder - Using custom data configuration default-6ab037a909b14552
Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
2024-05-25 03:03:51 - INFO - datasets.info - Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
Overwrite dataset info from restored data version if exists.
2024-05-25 03:03:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0
2024-05-25 03:03:51 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0
Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0)
2024-05-25 03:03:51 - INFO - datasets.builder - Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0)
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0
2024-05-25 03:03:51 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0
Applying chat template to train_sft (num_proc=10):  80%|████████  | 1612/2013 [00:02<00:00, 765.80 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 1813/2013 [00:03<00:00, 803.02 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 1813/2013 [00:03<00:00, 796.92 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 2013/2013 [00:03<00:00, 600.43 examples/s]
Applying chat template to train_sft (num_proc=10): 100%|██████████| 2013/2013 [00:03<00:00, 594.97 examples/s]
Applying chat template to test_sft (num_proc=10):   0%|          | 1/272 [00:00<01:52,  2.40 examples/s]Applying chat template to test_sft (num_proc=10):  21%|██        | 57/272 [00:00<00:01, 123.13 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 111/272 [00:00<00:00, 193.27 examples/s]Applying chat template to test_sft (num_proc=10):  61%|██████    | 165/272 [00:00<00:00, 268.99 examples/s]Applying chat template to test_sft (num_proc=10):  81%|████████  | 219/272 [00:00<00:00, 321.33 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 356.11 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 214.44 examples/s]
tokenizer padding side left
Applying chat template to test_sft (num_proc=10):   0%|          | 1/272 [00:00<02:18,  1.95 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/272 [00:00<01:54,  2.36 examples/s]Applying chat template to test_sft (num_proc=10):  21%|██        | 57/272 [00:00<00:02, 100.21 examples/s]Applying chat template to test_sft (num_proc=10):  21%|██        | 56/272 [00:00<00:01, 129.70 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 111/272 [00:00<00:00, 164.76 examples/s]Applying chat template to test_sft (num_proc=10):  31%|███       | 84/272 [00:00<00:01, 143.92 examples/s]Applying chat template to test_sft (num_proc=10):  61%|██████    | 165/272 [00:01<00:00, 212.48 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 138/272 [00:00<00:00, 206.64 examples/s]Applying chat template to test_sft (num_proc=10):  81%|████████  | 219/272 [00:01<00:00, 240.46 examples/s]Applying chat template to test_sft (num_proc=10):  81%|████████  | 219/272 [00:01<00:00, 309.52 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 181.39 examples/s]
Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 209.01 examples/s]
tokenizer padding side left
tokenizer padding side left
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
2024-05-25 03:03:54 - WARNING - accelerate.utils.other - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:607] 2024-05-25 03:03:55,463 >> Using auto half precision backend
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
[INFO|trainer.py:1969] 2024-05-25 03:03:56,056 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-25 03:03:56,056 >>   Num examples = 1,279
[INFO|trainer.py:1971] 2024-05-25 03:03:56,056 >>   Num Epochs = 5
[INFO|trainer.py:1972] 2024-05-25 03:03:56,056 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-25 03:03:56,056 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-05-25 03:03:56,056 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-05-25 03:03:56,057 >>   Total optimization steps = 800
[INFO|trainer.py:1978] 2024-05-25 03:03:56,059 >>   Number of trainable parameters = 4,505,600
[INFO|integration_utils.py:723] 2024-05-25 03:03:56,122 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSum/experiments/wandb/run-20240525_030358-ju0x0lyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-salad-98
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/huggingface
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/huggingface/runs/ju0x0lyr
  0%|          | 0/800 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/800 [00:02<32:35,  2.45s/it]  0%|          | 2/800 [00:04<32:08,  2.42s/it]  0%|          | 3/800 [00:07<31:58,  2.41s/it]  0%|          | 4/800 [00:09<31:53,  2.40s/it]  1%|          | 5/800 [00:12<31:50,  2.40s/it]  1%|          | 6/800 [00:14<31:46,  2.40s/it]  1%|          | 7/800 [00:16<31:42,  2.40s/it]  1%|          | 8/800 [00:19<31:40,  2.40s/it]  1%|          | 9/800 [00:21<31:37,  2.40s/it]  1%|▏         | 10/800 [00:24<31:36,  2.40s/it]  1%|▏         | 11/800 [00:26<31:35,  2.40s/it]  2%|▏         | 12/800 [00:28<31:56,  2.43s/it]  2%|▏         | 13/800 [00:31<31:57,  2.44s/it]  2%|▏         | 14/800 [00:33<31:47,  2.43s/it]  2%|▏         | 15/800 [00:36<31:40,  2.42s/it]  2%|▏         | 16/800 [00:38<31:34,  2.42s/it]  2%|▏         | 17/800 [00:41<31:29,  2.41s/it]  2%|▏         | 18/800 [00:43<31:25,  2.41s/it]  2%|▏         | 19/800 [00:45<31:22,  2.41s/it]  2%|▎         | 20/800 [00:48<31:19,  2.41s/it]                                                {'loss': 0.7218, 'grad_norm': 0.5586788654327393, 'learning_rate': 6.25e-05, 'epoch': 0.12}
  2%|▎         | 20/800 [00:48<31:19,  2.41s/it]  3%|▎         | 21/800 [00:50<31:17,  2.41s/it]  3%|▎         | 22/800 [00:53<31:14,  2.41s/it]  3%|▎         | 23/800 [00:55<31:11,  2.41s/it]  3%|▎         | 24/800 [00:57<31:09,  2.41s/it]  3%|▎         | 25/800 [01:00<31:06,  2.41s/it]  3%|▎         | 26/800 [01:02<31:10,  2.42s/it]  3%|▎         | 27/800 [01:05<31:06,  2.41s/it]  4%|▎         | 28/800 [01:07<31:03,  2.41s/it]  4%|▎         | 29/800 [01:09<31:08,  2.42s/it]  4%|▍         | 30/800 [01:12<31:03,  2.42s/it]  4%|▍         | 31/800 [01:14<30:58,  2.42s/it]  4%|▍         | 32/800 [01:17<30:55,  2.42s/it]  4%|▍         | 33/800 [01:19<30:51,  2.41s/it]  4%|▍         | 34/800 [01:22<30:48,  2.41s/it]  4%|▍         | 35/800 [01:24<30:46,  2.41s/it]  4%|▍         | 36/800 [01:26<30:45,  2.42s/it]  5%|▍         | 37/800 [01:29<30:43,  2.42s/it]  5%|▍         | 38/800 [01:31<30:40,  2.41s/it]  5%|▍         | 39/800 [01:34<30:39,  2.42s/it]  5%|▌         | 40/800 [01:36<30:36,  2.42s/it]                                                {'loss': 0.6481, 'grad_norm': 0.4677649438381195, 'learning_rate': 0.000121875, 'epoch': 0.25}
  5%|▌         | 40/800 [01:36<30:36,  2.42s/it]  5%|▌         | 41/800 [01:38<30:34,  2.42s/it]  5%|▌         | 42/800 [01:41<30:37,  2.42s/it]  5%|▌         | 43/800 [01:43<30:32,  2.42s/it]  6%|▌         | 44/800 [01:46<30:28,  2.42s/it]  6%|▌         | 45/800 [01:48<30:24,  2.42s/it]  6%|▌         | 46/800 [01:51<30:21,  2.42s/it]  6%|▌         | 47/800 [01:53<30:19,  2.42s/it]  6%|▌         | 48/800 [01:55<30:17,  2.42s/it]  6%|▌         | 49/800 [01:58<30:14,  2.42s/it]  6%|▋         | 50/800 [02:00<30:11,  2.42s/it]  6%|▋         | 51/800 [02:03<30:08,  2.41s/it]  6%|▋         | 52/800 [02:05<30:05,  2.41s/it]  7%|▋         | 53/800 [02:07<30:04,  2.42s/it]  7%|▋         | 54/800 [02:10<30:10,  2.43s/it]  7%|▋         | 55/800 [02:12<30:05,  2.42s/it]  7%|▋         | 56/800 [02:15<30:16,  2.44s/it]  7%|▋         | 57/800 [02:17<30:08,  2.43s/it]  7%|▋         | 58/800 [02:20<30:01,  2.43s/it]  7%|▋         | 59/800 [02:22<29:56,  2.43s/it]  8%|▊         | 60/800 [02:24<29:52,  2.42s/it]                                                {'loss': 0.6645, 'grad_norm': 0.4793098568916321, 'learning_rate': 0.00018437500000000002, 'epoch': 0.38}
  8%|▊         | 60/800 [02:24<29:52,  2.42s/it]  8%|▊         | 61/800 [02:27<29:49,  2.42s/it]  8%|▊         | 62/800 [02:29<29:47,  2.42s/it]  8%|▊         | 63/800 [02:32<29:43,  2.42s/it]  8%|▊         | 64/800 [02:34<29:40,  2.42s/it]  8%|▊         | 65/800 [02:37<29:37,  2.42s/it]  8%|▊         | 66/800 [02:39<29:34,  2.42s/it]  8%|▊         | 67/800 [02:41<29:32,  2.42s/it]  8%|▊         | 68/800 [02:44<29:30,  2.42s/it]  9%|▊         | 69/800 [02:46<29:27,  2.42s/it]  9%|▉         | 70/800 [02:49<29:30,  2.43s/it]  9%|▉         | 71/800 [02:51<29:26,  2.42s/it]  9%|▉         | 72/800 [02:54<29:23,  2.42s/it]  9%|▉         | 73/800 [02:56<29:19,  2.42s/it]  9%|▉         | 74/800 [02:58<29:16,  2.42s/it]  9%|▉         | 75/800 [03:01<29:14,  2.42s/it] 10%|▉         | 76/800 [03:03<29:11,  2.42s/it] 10%|▉         | 77/800 [03:06<29:08,  2.42s/it] 10%|▉         | 78/800 [03:08<29:05,  2.42s/it] 10%|▉         | 79/800 [03:10<29:03,  2.42s/it] 10%|█         | 80/800 [03:13<29:00,  2.42s/it]                                                {'loss': 0.6276, 'grad_norm': 0.44130441546440125, 'learning_rate': 0.00024375, 'epoch': 0.5}
 10%|█         | 80/800 [03:13<29:00,  2.42s/it] 10%|█         | 81/800 [03:15<28:59,  2.42s/it] 10%|█         | 82/800 [03:18<28:57,  2.42s/it] 10%|█         | 83/800 [03:20<29:03,  2.43s/it] 10%|█         | 84/800 [03:23<28:58,  2.43s/it] 11%|█         | 85/800 [03:25<28:53,  2.42s/it] 11%|█         | 86/800 [03:27<28:50,  2.42s/it] 11%|█         | 87/800 [03:30<28:46,  2.42s/it] 11%|█         | 88/800 [03:32<28:43,  2.42s/it] 11%|█         | 89/800 [03:35<28:41,  2.42s/it] 11%|█▏        | 90/800 [03:37<28:38,  2.42s/it] 11%|█▏        | 91/800 [03:40<28:35,  2.42s/it] 12%|█▏        | 92/800 [03:42<28:32,  2.42s/it] 12%|█▏        | 93/800 [03:44<28:30,  2.42s/it] 12%|█▏        | 94/800 [03:47<28:27,  2.42s/it] 12%|█▏        | 95/800 [03:49<28:24,  2.42s/it] 12%|█▏        | 96/800 [03:52<28:22,  2.42s/it] 12%|█▏        | 97/800 [03:54<28:19,  2.42s/it] 12%|█▏        | 98/800 [03:56<28:26,  2.43s/it] 12%|█▏        | 99/800 [03:59<28:21,  2.43s/it] 12%|█▎        | 100/800 [04:01<28:17,  2.42s/it]                                                 {'loss': 0.6204, 'grad_norm': 0.4423437714576721, 'learning_rate': 0.00030625000000000004, 'epoch': 0.62}
 12%|█▎        | 100/800 [04:01<28:17,  2.42s/it] 13%|█▎        | 101/800 [04:04<28:13,  2.42s/it] 13%|█▎        | 102/800 [04:06<28:10,  2.42s/it] 13%|█▎        | 103/800 [04:09<28:06,  2.42s/it] 13%|█▎        | 104/800 [04:11<28:03,  2.42s/it] 13%|█▎        | 105/800 [04:13<28:00,  2.42s/it] 13%|█▎        | 106/800 [04:16<27:58,  2.42s/it] 13%|█▎        | 107/800 [04:18<27:55,  2.42s/it] 14%|█▎        | 108/800 [04:21<27:53,  2.42s/it] 14%|█▎        | 109/800 [04:23<27:50,  2.42s/it] 14%|█▍        | 110/800 [04:26<27:48,  2.42s/it] 14%|█▍        | 111/800 [04:28<27:55,  2.43s/it] 14%|█▍        | 112/800 [04:30<27:54,  2.43s/it] 14%|█▍        | 113/800 [04:33<27:48,  2.43s/it] 14%|█▍        | 114/800 [04:35<27:44,  2.43s/it] 14%|█▍        | 115/800 [04:38<27:40,  2.42s/it] 14%|█▍        | 116/800 [04:40<27:36,  2.42s/it] 15%|█▍        | 117/800 [04:42<27:33,  2.42s/it] 15%|█▍        | 118/800 [04:45<27:30,  2.42s/it] 15%|█▍        | 119/800 [04:47<27:27,  2.42s/it] 15%|█▌        | 120/800 [04:50<27:25,  2.42s/it]                                                 {'loss': 0.6466, 'grad_norm': 0.3271624445915222, 'learning_rate': 0.00036875000000000005, 'epoch': 0.75}
 15%|█▌        | 120/800 [04:50<27:25,  2.42s/it] 15%|█▌        | 121/800 [04:52<27:22,  2.42s/it] 15%|█▌        | 122/800 [04:55<27:20,  2.42s/it] 15%|█▌        | 123/800 [04:57<27:17,  2.42s/it] 16%|█▌        | 124/800 [04:59<27:15,  2.42s/it] 16%|█▌        | 125/800 [05:02<27:13,  2.42s/it] 16%|█▌        | 126/800 [05:04<27:27,  2.44s/it] 16%|█▌        | 127/800 [05:07<27:19,  2.44s/it] 16%|█▌        | 128/800 [05:09<27:13,  2.43s/it] 16%|█▌        | 129/800 [05:12<27:08,  2.43s/it] 16%|█▋        | 130/800 [05:14<27:04,  2.43s/it] 16%|█▋        | 131/800 [05:16<27:01,  2.42s/it] 16%|█▋        | 132/800 [05:19<26:57,  2.42s/it] 17%|█▋        | 133/800 [05:21<26:54,  2.42s/it] 17%|█▋        | 134/800 [05:24<26:51,  2.42s/it] 17%|█▋        | 135/800 [05:26<26:48,  2.42s/it] 17%|█▋        | 136/800 [05:29<26:46,  2.42s/it] 17%|█▋        | 137/800 [05:31<26:43,  2.42s/it] 17%|█▋        | 138/800 [05:33<26:41,  2.42s/it] 17%|█▋        | 139/800 [05:36<26:45,  2.43s/it] 18%|█▊        | 140/800 [05:38<26:40,  2.43s/it]                                                 {'loss': 0.6177, 'grad_norm': 0.31491976976394653, 'learning_rate': 0.00043125000000000005, 'epoch': 0.88}
 18%|█▊        | 140/800 [05:38<26:40,  2.43s/it] 18%|█▊        | 141/800 [05:41<26:37,  2.42s/it] 18%|█▊        | 142/800 [05:43<26:33,  2.42s/it] 18%|█▊        | 143/800 [05:45<26:30,  2.42s/it] 18%|█▊        | 144/800 [05:48<26:27,  2.42s/it] 18%|█▊        | 145/800 [05:50<26:24,  2.42s/it] 18%|█▊        | 146/800 [05:53<26:22,  2.42s/it] 18%|█▊        | 147/800 [05:55<26:19,  2.42s/it] 18%|█▊        | 148/800 [05:58<26:17,  2.42s/it] 19%|█▊        | 149/800 [06:00<26:14,  2.42s/it] 19%|█▉        | 150/800 [06:02<26:12,  2.42s/it] 19%|█▉        | 151/800 [06:05<26:10,  2.42s/it] 19%|█▉        | 152/800 [06:07<26:10,  2.42s/it] 19%|█▉        | 153/800 [06:10<26:06,  2.42s/it] 19%|█▉        | 154/800 [06:12<26:03,  2.42s/it] 19%|█▉        | 155/800 [06:15<26:05,  2.43s/it] 20%|█▉        | 156/800 [06:17<26:01,  2.43s/it] 20%|█▉        | 157/800 [06:19<25:58,  2.42s/it] 20%|█▉        | 158/800 [06:22<25:54,  2.42s/it] 20%|█▉        | 159/800 [06:24<25:51,  2.42s/it] 20%|██        | 160/800 [06:27<25:48,  2.42s/it]                                                 {'loss': 0.5436, 'grad_norm': 0.4272554814815521, 'learning_rate': 0.00049375, 'epoch': 1.0}
 20%|██        | 160/800 [06:27<25:48,  2.42s/it] 20%|██        | 161/800 [06:29<25:46,  2.42s/it] 20%|██        | 162/800 [06:31<25:44,  2.42s/it] 20%|██        | 163/800 [06:34<25:41,  2.42s/it] 20%|██        | 164/800 [06:36<25:38,  2.42s/it] 21%|██        | 165/800 [06:39<25:36,  2.42s/it] 21%|██        | 166/800 [06:41<25:33,  2.42s/it] 21%|██        | 167/800 [06:44<25:30,  2.42s/it] 21%|██        | 168/800 [06:46<25:34,  2.43s/it] 21%|██        | 169/800 [06:48<25:30,  2.43s/it] 21%|██▏       | 170/800 [06:51<25:26,  2.42s/it] 21%|██▏       | 171/800 [06:53<25:22,  2.42s/it] 22%|██▏       | 172/800 [06:56<25:20,  2.42s/it] 22%|██▏       | 173/800 [06:58<25:17,  2.42s/it] 22%|██▏       | 174/800 [07:01<25:14,  2.42s/it] 22%|██▏       | 175/800 [07:03<25:12,  2.42s/it] 22%|██▏       | 176/800 [07:05<25:09,  2.42s/it] 22%|██▏       | 177/800 [07:08<25:06,  2.42s/it] 22%|██▏       | 178/800 [07:10<25:04,  2.42s/it] 22%|██▏       | 179/800 [07:13<25:02,  2.42s/it] 22%|██▎       | 180/800 [07:15<24:59,  2.42s/it]                                                 {'loss': 0.5279, 'grad_norm': 0.29193350672721863, 'learning_rate': 0.0004990247583129218, 'epoch': 1.12}
 22%|██▎       | 180/800 [07:15<24:59,  2.42s/it] 23%|██▎       | 181/800 [07:18<25:05,  2.43s/it] 23%|██▎       | 182/800 [07:20<25:00,  2.43s/it] 23%|██▎       | 183/800 [07:22<24:57,  2.43s/it] 23%|██▎       | 184/800 [07:25<24:53,  2.42s/it] 23%|██▎       | 185/800 [07:27<24:49,  2.42s/it] 23%|██▎       | 186/800 [07:30<24:47,  2.42s/it] 23%|██▎       | 187/800 [07:32<24:44,  2.42s/it] 24%|██▎       | 188/800 [07:34<24:41,  2.42s/it] 24%|██▎       | 189/800 [07:37<24:38,  2.42s/it] 24%|██▍       | 190/800 [07:39<24:36,  2.42s/it] 24%|██▍       | 191/800 [07:42<24:33,  2.42s/it] 24%|██▍       | 192/800 [07:44<24:30,  2.42s/it] 24%|██▍       | 193/800 [07:47<24:28,  2.42s/it] 24%|██▍       | 194/800 [07:49<24:26,  2.42s/it] 24%|██▍       | 195/800 [07:51<24:36,  2.44s/it] 24%|██▍       | 196/800 [07:54<24:30,  2.43s/it] 25%|██▍       | 197/800 [07:56<24:25,  2.43s/it] 25%|██▍       | 198/800 [07:59<24:20,  2.43s/it] 25%|██▍       | 199/800 [08:01<24:21,  2.43s/it] 25%|██▌       | 200/800 [08:04<24:16,  2.43s/it]                                                 {'loss': 0.564, 'grad_norm': 0.32290545105934143, 'learning_rate': 0.000495663319832678, 'epoch': 1.25}
 25%|██▌       | 200/800 [08:04<24:16,  2.43s/it] 25%|██▌       | 201/800 [08:06<24:13,  2.43s/it] 25%|██▌       | 202/800 [08:08<24:09,  2.42s/it] 25%|██▌       | 203/800 [08:11<24:06,  2.42s/it] 26%|██▌       | 204/800 [08:13<24:03,  2.42s/it] 26%|██▌       | 205/800 [08:16<24:00,  2.42s/it] 26%|██▌       | 206/800 [08:18<23:57,  2.42s/it] 26%|██▌       | 207/800 [08:21<23:54,  2.42s/it] 26%|██▌       | 208/800 [08:23<23:52,  2.42s/it] 26%|██▌       | 209/800 [08:25<23:51,  2.42s/it] 26%|██▋       | 210/800 [08:28<23:48,  2.42s/it] 26%|██▋       | 211/800 [08:30<23:46,  2.42s/it] 26%|██▋       | 212/800 [08:33<23:43,  2.42s/it] 27%|██▋       | 213/800 [08:35<23:40,  2.42s/it] 27%|██▋       | 214/800 [08:37<23:37,  2.42s/it] 27%|██▋       | 215/800 [08:40<23:35,  2.42s/it] 27%|██▋       | 216/800 [08:42<23:32,  2.42s/it] 27%|██▋       | 217/800 [08:45<23:29,  2.42s/it] 27%|██▋       | 218/800 [08:47<23:27,  2.42s/it] 27%|██▋       | 219/800 [08:50<23:25,  2.42s/it] 28%|██▊       | 220/800 [08:52<23:22,  2.42s/it]                                                 {'loss': 0.5746, 'grad_norm': 0.38193926215171814, 'learning_rate': 0.000490277804028108, 'epoch': 1.38}
 28%|██▊       | 220/800 [08:52<23:22,  2.42s/it] 28%|██▊       | 221/800 [08:54<23:20,  2.42s/it] 28%|██▊       | 222/800 [08:57<23:25,  2.43s/it] 28%|██▊       | 223/800 [08:59<23:20,  2.43s/it] 28%|██▊       | 224/800 [09:02<23:16,  2.42s/it] 28%|██▊       | 225/800 [09:04<23:13,  2.42s/it] 28%|██▊       | 226/800 [09:07<23:10,  2.42s/it] 28%|██▊       | 227/800 [09:09<23:07,  2.42s/it] 28%|██▊       | 228/800 [09:11<23:04,  2.42s/it] 29%|██▊       | 229/800 [09:14<23:01,  2.42s/it] 29%|██▉       | 230/800 [09:16<22:58,  2.42s/it] 29%|██▉       | 231/800 [09:19<22:56,  2.42s/it] 29%|██▉       | 232/800 [09:21<22:53,  2.42s/it] 29%|██▉       | 233/800 [09:23<22:51,  2.42s/it] 29%|██▉       | 234/800 [09:26<22:48,  2.42s/it] 29%|██▉       | 235/800 [09:28<22:46,  2.42s/it] 30%|██▉       | 236/800 [09:31<22:44,  2.42s/it] 30%|██▉       | 237/800 [09:33<22:43,  2.42s/it] 30%|██▉       | 238/800 [09:36<22:40,  2.42s/it] 30%|██▉       | 239/800 [09:38<22:37,  2.42s/it] 30%|███       | 240/800 [09:40<22:35,  2.42s/it]                                                 {'loss': 0.5613, 'grad_norm': 0.38698992133140564, 'learning_rate': 0.0004823536581098261, 'epoch': 1.5}
 30%|███       | 240/800 [09:40<22:35,  2.42s/it] 30%|███       | 241/800 [09:43<22:33,  2.42s/it] 30%|███       | 242/800 [09:45<22:31,  2.42s/it] 30%|███       | 243/800 [09:48<22:27,  2.42s/it] 30%|███       | 244/800 [09:50<22:25,  2.42s/it] 31%|███       | 245/800 [09:53<22:22,  2.42s/it] 31%|███       | 246/800 [09:55<22:20,  2.42s/it] 31%|███       | 247/800 [09:57<22:17,  2.42s/it] 31%|███       | 248/800 [10:00<22:14,  2.42s/it] 31%|███       | 249/800 [10:02<22:12,  2.42s/it] 31%|███▏      | 250/800 [10:05<22:15,  2.43s/it] 31%|███▏      | 251/800 [10:07<22:11,  2.43s/it] 32%|███▏      | 252/800 [10:09<22:07,  2.42s/it] 32%|███▏      | 253/800 [10:12<22:05,  2.42s/it] 32%|███▏      | 254/800 [10:14<22:02,  2.42s/it] 32%|███▏      | 255/800 [10:17<21:59,  2.42s/it] 32%|███▏      | 256/800 [10:19<21:56,  2.42s/it] 32%|███▏      | 257/800 [10:22<21:53,  2.42s/it] 32%|███▏      | 258/800 [10:24<21:50,  2.42s/it] 32%|███▏      | 259/800 [10:26<21:48,  2.42s/it] 32%|███▎      | 260/800 [10:29<21:46,  2.42s/it]                                                 {'loss': 0.5697, 'grad_norm': 0.39490318298339844, 'learning_rate': 0.0004721918194465169, 'epoch': 1.62}
 32%|███▎      | 260/800 [10:29<21:46,  2.42s/it] 33%|███▎      | 261/800 [10:31<21:44,  2.42s/it] 33%|███▎      | 262/800 [10:34<21:42,  2.42s/it] 33%|███▎      | 263/800 [10:36<21:39,  2.42s/it] 33%|███▎      | 264/800 [10:39<21:36,  2.42s/it] 33%|███▎      | 265/800 [10:41<21:43,  2.44s/it] 33%|███▎      | 266/800 [10:43<21:37,  2.43s/it] 33%|███▎      | 267/800 [10:46<21:33,  2.43s/it] 34%|███▎      | 268/800 [10:48<21:29,  2.42s/it] 34%|███▎      | 269/800 [10:51<21:26,  2.42s/it] 34%|███▍      | 270/800 [10:53<21:23,  2.42s/it] 34%|███▍      | 271/800 [10:55<21:20,  2.42s/it] 34%|███▍      | 272/800 [10:58<21:18,  2.42s/it] 34%|███▍      | 273/800 [11:00<21:15,  2.42s/it] 34%|███▍      | 274/800 [11:03<21:12,  2.42s/it] 34%|███▍      | 275/800 [11:05<21:10,  2.42s/it] 34%|███▍      | 276/800 [11:08<21:07,  2.42s/it] 35%|███▍      | 277/800 [11:10<21:04,  2.42s/it] 35%|███▍      | 278/800 [11:12<21:08,  2.43s/it] 35%|███▍      | 279/800 [11:15<21:04,  2.43s/it] 35%|███▌      | 280/800 [11:17<21:00,  2.42s/it]                                                 {'loss': 0.5255, 'grad_norm': 0.3833886384963989, 'learning_rate': 0.00045989015209953394, 'epoch': 1.75}
 35%|███▌      | 280/800 [11:17<21:00,  2.42s/it] 35%|███▌      | 281/800 [11:20<20:57,  2.42s/it] 35%|███▌      | 282/800 [11:22<20:54,  2.42s/it] 35%|███▌      | 283/800 [11:25<20:51,  2.42s/it] 36%|███▌      | 284/800 [11:27<20:49,  2.42s/it] 36%|███▌      | 285/800 [11:29<20:46,  2.42s/it] 36%|███▌      | 286/800 [11:32<20:47,  2.43s/it] 36%|███▌      | 287/800 [11:34<20:43,  2.42s/it] 36%|███▌      | 288/800 [11:37<20:40,  2.42s/it] 36%|███▌      | 289/800 [11:39<20:37,  2.42s/it] 36%|███▋      | 290/800 [11:42<20:35,  2.42s/it] 36%|███▋      | 291/800 [11:44<20:34,  2.43s/it] 36%|███▋      | 292/800 [11:46<20:31,  2.42s/it] 37%|███▋      | 293/800 [11:49<20:27,  2.42s/it] 37%|███▋      | 294/800 [11:51<20:25,  2.42s/it] 37%|███▋      | 295/800 [11:54<20:22,  2.42s/it] 37%|███▋      | 296/800 [11:56<20:19,  2.42s/it] 37%|███▋      | 297/800 [11:58<20:17,  2.42s/it] 37%|███▋      | 298/800 [12:01<20:14,  2.42s/it] 37%|███▋      | 299/800 [12:03<20:12,  2.42s/it] 38%|███▊      | 300/800 [12:06<20:09,  2.42s/it]                                                 {'loss': 0.5085, 'grad_norm': 0.35175561904907227, 'learning_rate': 0.0004455671278502041, 'epoch': 1.88}
 38%|███▊      | 300/800 [12:06<20:09,  2.42s/it] 38%|███▊      | 301/800 [12:08<20:08,  2.42s/it] 38%|███▊      | 302/800 [12:11<20:05,  2.42s/it] 38%|███▊      | 303/800 [12:13<20:02,  2.42s/it] 38%|███▊      | 304/800 [12:15<20:00,  2.42s/it] 38%|███▊      | 305/800 [12:18<19:57,  2.42s/it] 38%|███▊      | 306/800 [12:20<19:54,  2.42s/it] 38%|███▊      | 307/800 [12:23<19:57,  2.43s/it] 38%|███▊      | 308/800 [12:25<19:53,  2.43s/it] 39%|███▊      | 309/800 [12:28<19:49,  2.42s/it] 39%|███▉      | 310/800 [12:30<19:47,  2.42s/it] 39%|███▉      | 311/800 [12:32<19:43,  2.42s/it] 39%|███▉      | 312/800 [12:35<19:41,  2.42s/it] 39%|███▉      | 313/800 [12:37<19:38,  2.42s/it] 39%|███▉      | 314/800 [12:40<19:35,  2.42s/it] 39%|███▉      | 315/800 [12:42<19:35,  2.42s/it] 40%|███▉      | 316/800 [12:44<19:32,  2.42s/it] 40%|███▉      | 317/800 [12:47<19:29,  2.42s/it] 40%|███▉      | 318/800 [12:49<19:26,  2.42s/it] 40%|███▉      | 319/800 [12:52<19:23,  2.42s/it] 40%|████      | 320/800 [12:54<19:23,  2.42s/it]                                                 {'loss': 0.5476, 'grad_norm': 0.35150063037872314, 'learning_rate': 0.00042936068525181004, 'epoch': 2.0}
 40%|████      | 320/800 [12:54<19:23,  2.42s/it] 40%|████      | 321/800 [12:57<19:20,  2.42s/it] 40%|████      | 322/800 [12:59<19:17,  2.42s/it] 40%|████      | 323/800 [13:01<19:14,  2.42s/it] 40%|████      | 324/800 [13:04<19:11,  2.42s/it] 41%|████      | 325/800 [13:06<19:08,  2.42s/it] 41%|████      | 326/800 [13:09<19:06,  2.42s/it] 41%|████      | 327/800 [13:11<19:04,  2.42s/it] 41%|████      | 328/800 [13:14<19:01,  2.42s/it] 41%|████      | 329/800 [13:16<18:58,  2.42s/it] 41%|████▏     | 330/800 [13:18<18:56,  2.42s/it] 41%|████▏     | 331/800 [13:21<18:54,  2.42s/it] 42%|████▏     | 332/800 [13:23<18:51,  2.42s/it] 42%|████▏     | 333/800 [13:26<18:49,  2.42s/it] 42%|████▏     | 334/800 [13:28<18:56,  2.44s/it] 42%|████▏     | 335/800 [13:30<18:51,  2.43s/it] 42%|████▏     | 336/800 [13:33<18:46,  2.43s/it] 42%|████▏     | 337/800 [13:35<18:42,  2.43s/it] 42%|████▏     | 338/800 [13:38<18:39,  2.42s/it] 42%|████▏     | 339/800 [13:40<18:36,  2.42s/it] 42%|████▎     | 340/800 [13:43<18:33,  2.42s/it]                                                 {'loss': 0.426, 'grad_norm': 0.31090718507766724, 'learning_rate': 0.00041142690120591686, 'epoch': 2.12}
 42%|████▎     | 340/800 [13:43<18:33,  2.42s/it] 43%|████▎     | 341/800 [13:45<18:31,  2.42s/it] 43%|████▎     | 342/800 [13:47<18:28,  2.42s/it] 43%|████▎     | 343/800 [13:50<18:25,  2.42s/it] 43%|████▎     | 344/800 [13:52<18:23,  2.42s/it] 43%|████▎     | 345/800 [13:55<18:20,  2.42s/it] 43%|████▎     | 346/800 [13:57<18:17,  2.42s/it] 43%|████▎     | 347/800 [14:00<18:15,  2.42s/it] 44%|████▎     | 348/800 [14:02<18:16,  2.43s/it] 44%|████▎     | 349/800 [14:04<18:12,  2.42s/it] 44%|████▍     | 350/800 [14:07<18:09,  2.42s/it] 44%|████▍     | 351/800 [14:09<18:07,  2.42s/it] 44%|████▍     | 352/800 [14:12<18:04,  2.42s/it] 44%|████▍     | 353/800 [14:14<18:01,  2.42s/it] 44%|████▍     | 354/800 [14:16<17:58,  2.42s/it] 44%|████▍     | 355/800 [14:19<17:56,  2.42s/it] 44%|████▍     | 356/800 [14:21<17:54,  2.42s/it] 45%|████▍     | 357/800 [14:24<17:51,  2.42s/it] 45%|████▍     | 358/800 [14:26<17:51,  2.42s/it] 45%|████▍     | 359/800 [14:29<17:48,  2.42s/it] 45%|████▌     | 360/800 [14:31<17:45,  2.42s/it]                                                 {'loss': 0.4652, 'grad_norm': 0.37678587436676025, 'learning_rate': 0.00039193848785649016, 'epoch': 2.25}
 45%|████▌     | 360/800 [14:31<17:45,  2.42s/it] 45%|████▌     | 361/800 [14:33<17:43,  2.42s/it] 45%|████▌     | 362/800 [14:36<17:40,  2.42s/it] 45%|████▌     | 363/800 [14:38<17:38,  2.42s/it] 46%|████▌     | 364/800 [14:41<17:35,  2.42s/it] 46%|████▌     | 365/800 [14:43<17:32,  2.42s/it] 46%|████▌     | 366/800 [14:46<17:30,  2.42s/it] 46%|████▌     | 367/800 [14:48<17:27,  2.42s/it] 46%|████▌     | 368/800 [14:50<17:25,  2.42s/it] 46%|████▌     | 369/800 [14:53<17:22,  2.42s/it] 46%|████▋     | 370/800 [14:55<17:20,  2.42s/it] 46%|████▋     | 371/800 [14:58<17:17,  2.42s/it] 46%|████▋     | 372/800 [15:00<17:15,  2.42s/it] 47%|████▋     | 373/800 [15:02<17:12,  2.42s/it] 47%|████▋     | 374/800 [15:05<17:10,  2.42s/it] 47%|████▋     | 375/800 [15:07<17:07,  2.42s/it] 47%|████▋     | 376/800 [15:10<17:08,  2.42s/it] 47%|████▋     | 377/800 [15:12<17:04,  2.42s/it] 47%|████▋     | 378/800 [15:15<17:01,  2.42s/it] 47%|████▋     | 379/800 [15:17<16:59,  2.42s/it] 48%|████▊     | 380/800 [15:19<16:56,  2.42s/it]                                                 {'loss': 0.4693, 'grad_norm': 0.338687926530838, 'learning_rate': 0.0003710831292775353, 'epoch': 2.38}
 48%|████▊     | 380/800 [15:19<16:56,  2.42s/it] 48%|████▊     | 381/800 [15:22<16:54,  2.42s/it] 48%|████▊     | 382/800 [15:24<16:51,  2.42s/it] 48%|████▊     | 383/800 [15:27<16:49,  2.42s/it] 48%|████▊     | 384/800 [15:29<16:46,  2.42s/it] 48%|████▊     | 385/800 [15:32<16:43,  2.42s/it] 48%|████▊     | 386/800 [15:34<16:41,  2.42s/it] 48%|████▊     | 387/800 [15:36<16:39,  2.42s/it] 48%|████▊     | 388/800 [15:39<16:36,  2.42s/it] 49%|████▊     | 389/800 [15:41<16:36,  2.43s/it] 49%|████▉     | 390/800 [15:44<16:33,  2.42s/it] 49%|████▉     | 391/800 [15:46<16:30,  2.42s/it] 49%|████▉     | 392/800 [15:48<16:27,  2.42s/it] 49%|████▉     | 393/800 [15:51<16:24,  2.42s/it] 49%|████▉     | 394/800 [15:53<16:22,  2.42s/it] 49%|████▉     | 395/800 [15:56<16:19,  2.42s/it] 50%|████▉     | 396/800 [15:58<16:17,  2.42s/it] 50%|████▉     | 397/800 [16:01<16:14,  2.42s/it] 50%|████▉     | 398/800 [16:03<16:11,  2.42s/it] 50%|████▉     | 399/800 [16:05<16:09,  2.42s/it] 50%|█████     | 400/800 [16:08<16:07,  2.42s/it]                                                 {'loss': 0.4531, 'grad_norm': 0.34929966926574707, 'learning_rate': 0.0003490616739728664, 'epoch': 2.5}
 50%|█████     | 400/800 [16:08<16:07,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 03:20:13,767 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/length_then_topic/checkpoint-400
[INFO|configuration_utils.py:726] 2024-05-25 03:20:14,973 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 03:20:14,976 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 03:20:15,066 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/length_then_topic/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 03:20:15,067 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/length_then_topic/checkpoint-400/special_tokens_map.json
 50%|█████     | 401/800 [16:12<18:57,  2.85s/it] 50%|█████     | 402/800 [16:14<18:02,  2.72s/it] 50%|█████     | 403/800 [16:16<17:24,  2.63s/it] 50%|█████     | 404/800 [16:19<16:59,  2.58s/it] 51%|█████     | 405/800 [16:21<16:38,  2.53s/it] 51%|█████     | 406/800 [16:24<16:23,  2.50s/it] 51%|█████     | 407/800 [16:26<16:12,  2.47s/it] 51%|█████     | 408/800 [16:29<16:03,  2.46s/it] 51%|█████     | 409/800 [16:31<15:56,  2.45s/it] 51%|█████▏    | 410/800 [16:33<15:51,  2.44s/it] 51%|█████▏    | 411/800 [16:36<15:46,  2.43s/it] 52%|█████▏    | 412/800 [16:38<15:42,  2.43s/it] 52%|█████▏    | 413/800 [16:41<15:39,  2.43s/it] 52%|█████▏    | 414/800 [16:43<15:35,  2.42s/it] 52%|█████▏    | 415/800 [16:46<15:43,  2.45s/it] 52%|█████▏    | 416/800 [16:48<15:37,  2.44s/it] 52%|█████▏    | 417/800 [16:51<15:33,  2.44s/it] 52%|█████▏    | 418/800 [16:53<15:28,  2.43s/it] 52%|█████▏    | 419/800 [16:55<15:25,  2.43s/it] 52%|█████▎    | 420/800 [16:58<15:21,  2.43s/it]                                                 {'loss': 0.4638, 'grad_norm': 0.41924846172332764, 'learning_rate': 0.0003260862005952193, 'epoch': 2.62}
 52%|█████▎    | 420/800 [16:58<15:21,  2.43s/it] 53%|█████▎    | 421/800 [17:00<15:18,  2.42s/it] 53%|█████▎    | 422/800 [17:03<15:15,  2.42s/it] 53%|█████▎    | 423/800 [17:05<15:13,  2.42s/it] 53%|█████▎    | 424/800 [17:07<15:10,  2.42s/it] 53%|█████▎    | 425/800 [17:10<15:08,  2.42s/it] 53%|█████▎    | 426/800 [17:12<15:05,  2.42s/it] 53%|█████▎    | 427/800 [17:15<15:02,  2.42s/it] 54%|█████▎    | 428/800 [17:17<15:00,  2.42s/it] 54%|█████▎    | 429/800 [17:20<14:57,  2.42s/it] 54%|█████▍    | 430/800 [17:22<14:57,  2.42s/it] 54%|█████▍    | 431/800 [17:24<14:54,  2.42s/it] 54%|█████▍    | 432/800 [17:27<14:51,  2.42s/it] 54%|█████▍    | 433/800 [17:29<14:48,  2.42s/it] 54%|█████▍    | 434/800 [17:32<14:46,  2.42s/it] 54%|█████▍    | 435/800 [17:34<14:43,  2.42s/it] 55%|█████▍    | 436/800 [17:37<14:41,  2.42s/it] 55%|█████▍    | 437/800 [17:39<14:38,  2.42s/it] 55%|█████▍    | 438/800 [17:41<14:36,  2.42s/it] 55%|█████▍    | 439/800 [17:44<14:33,  2.42s/it] 55%|█████▌    | 440/800 [17:46<14:31,  2.42s/it]                                                 {'loss': 0.4678, 'grad_norm': 0.38859426975250244, 'learning_rate': 0.00030237797551289225, 'epoch': 2.75}
 55%|█████▌    | 440/800 [17:46<14:31,  2.42s/it] 55%|█████▌    | 441/800 [17:49<14:28,  2.42s/it] 55%|█████▌    | 442/800 [17:51<14:26,  2.42s/it] 55%|█████▌    | 443/800 [17:53<14:26,  2.43s/it] 56%|█████▌    | 444/800 [17:56<14:22,  2.42s/it] 56%|█████▌    | 445/800 [17:58<14:20,  2.42s/it] 56%|█████▌    | 446/800 [18:01<14:19,  2.43s/it] 56%|█████▌    | 447/800 [18:03<14:16,  2.43s/it] 56%|█████▌    | 448/800 [18:06<14:13,  2.42s/it] 56%|█████▌    | 449/800 [18:08<14:10,  2.42s/it] 56%|█████▋    | 450/800 [18:10<14:07,  2.42s/it] 56%|█████▋    | 451/800 [18:13<14:04,  2.42s/it] 56%|█████▋    | 452/800 [18:15<14:02,  2.42s/it] 57%|█████▋    | 453/800 [18:18<14:00,  2.42s/it] 57%|█████▋    | 454/800 [18:20<13:57,  2.42s/it] 57%|█████▋    | 455/800 [18:23<13:54,  2.42s/it] 57%|█████▋    | 456/800 [18:25<13:52,  2.42s/it] 57%|█████▋    | 457/800 [18:27<13:49,  2.42s/it] 57%|█████▋    | 458/800 [18:30<13:47,  2.42s/it] 57%|█████▋    | 459/800 [18:32<13:47,  2.43s/it] 57%|█████▊    | 460/800 [18:35<13:44,  2.43s/it]                                                 {'loss': 0.4156, 'grad_norm': 0.37603145837783813, 'learning_rate': 0.00027816532189366193, 'epoch': 2.88}
 57%|█████▊    | 460/800 [18:35<13:44,  2.43s/it] 58%|█████▊    | 461/800 [18:37<13:41,  2.42s/it] 58%|█████▊    | 462/800 [18:39<13:38,  2.42s/it] 58%|█████▊    | 463/800 [18:42<13:36,  2.42s/it] 58%|█████▊    | 464/800 [18:44<13:33,  2.42s/it] 58%|█████▊    | 465/800 [18:47<13:31,  2.42s/it] 58%|█████▊    | 466/800 [18:49<13:28,  2.42s/it] 58%|█████▊    | 467/800 [18:52<13:26,  2.42s/it] 58%|█████▊    | 468/800 [18:54<13:23,  2.42s/it] 59%|█████▊    | 469/800 [18:56<13:21,  2.42s/it] 59%|█████▉    | 470/800 [18:59<13:18,  2.42s/it] 59%|█████▉    | 471/800 [19:01<13:16,  2.42s/it] 59%|█████▉    | 472/800 [19:04<13:14,  2.42s/it] 59%|█████▉    | 473/800 [19:06<13:16,  2.43s/it] 59%|█████▉    | 474/800 [19:09<13:12,  2.43s/it] 59%|█████▉    | 475/800 [19:11<13:08,  2.43s/it] 60%|█████▉    | 476/800 [19:13<13:05,  2.42s/it] 60%|█████▉    | 477/800 [19:16<13:02,  2.42s/it] 60%|█████▉    | 478/800 [19:18<12:59,  2.42s/it] 60%|█████▉    | 479/800 [19:21<12:57,  2.42s/it] 60%|██████    | 480/800 [19:23<12:54,  2.42s/it]                                                 {'loss': 0.4252, 'grad_norm': 0.3401625156402588, 'learning_rate': 0.00025368142082786465, 'epoch': 3.0}
 60%|██████    | 480/800 [19:23<12:54,  2.42s/it] 60%|██████    | 481/800 [19:26<12:52,  2.42s/it] 60%|██████    | 482/800 [19:28<12:49,  2.42s/it] 60%|██████    | 483/800 [19:30<12:47,  2.42s/it] 60%|██████    | 484/800 [19:33<12:44,  2.42s/it] 61%|██████    | 485/800 [19:35<12:42,  2.42s/it] 61%|██████    | 486/800 [19:38<12:39,  2.42s/it] 61%|██████    | 487/800 [19:40<12:37,  2.42s/it] 61%|██████    | 488/800 [19:42<12:34,  2.42s/it] 61%|██████    | 489/800 [19:45<12:32,  2.42s/it] 61%|██████▏   | 490/800 [19:47<12:30,  2.42s/it] 61%|██████▏   | 491/800 [19:50<12:27,  2.42s/it] 62%|██████▏   | 492/800 [19:52<12:25,  2.42s/it] 62%|██████▏   | 493/800 [19:55<12:22,  2.42s/it] 62%|██████▏   | 494/800 [19:57<12:20,  2.42s/it] 62%|██████▏   | 495/800 [19:59<12:17,  2.42s/it] 62%|██████▏   | 496/800 [20:02<12:15,  2.42s/it] 62%|██████▏   | 497/800 [20:04<12:12,  2.42s/it] 62%|██████▏   | 498/800 [20:07<12:10,  2.42s/it] 62%|██████▏   | 499/800 [20:09<12:08,  2.42s/it] 62%|██████▎   | 500/800 [20:12<12:07,  2.42s/it]                                                 {'loss': 0.3615, 'grad_norm': 0.2742671072483063, 'learning_rate': 0.0002291620656670256, 'epoch': 3.12}
 62%|██████▎   | 500/800 [20:12<12:07,  2.42s/it] 63%|██████▎   | 501/800 [20:14<12:04,  2.42s/it] 63%|██████▎   | 502/800 [20:16<12:02,  2.42s/it] 63%|██████▎   | 503/800 [20:19<11:59,  2.42s/it] 63%|██████▎   | 504/800 [20:21<11:56,  2.42s/it] 63%|██████▎   | 505/800 [20:24<11:54,  2.42s/it] 63%|██████▎   | 506/800 [20:26<11:51,  2.42s/it] 63%|██████▎   | 507/800 [20:28<11:49,  2.42s/it] 64%|██████▎   | 508/800 [20:31<11:46,  2.42s/it] 64%|██████▎   | 509/800 [20:33<11:44,  2.42s/it] 64%|██████▍   | 510/800 [20:36<11:41,  2.42s/it] 64%|██████▍   | 511/800 [20:38<11:39,  2.42s/it] 64%|██████▍   | 512/800 [20:41<11:36,  2.42s/it] 64%|██████▍   | 513/800 [20:43<11:34,  2.42s/it] 64%|██████▍   | 514/800 [20:45<11:31,  2.42s/it] 64%|██████▍   | 515/800 [20:48<11:31,  2.42s/it] 64%|██████▍   | 516/800 [20:50<11:28,  2.42s/it] 65%|██████▍   | 517/800 [20:53<11:27,  2.43s/it] 65%|██████▍   | 518/800 [20:55<11:24,  2.43s/it] 65%|██████▍   | 519/800 [20:58<11:21,  2.42s/it] 65%|██████▌   | 520/800 [21:00<11:18,  2.42s/it]                                                 {'loss': 0.3544, 'grad_norm': 0.3146759569644928, 'learning_rate': 0.0002048433912049868, 'epoch': 3.25}
 65%|██████▌   | 520/800 [21:00<11:18,  2.42s/it] 65%|██████▌   | 521/800 [21:02<11:15,  2.42s/it] 65%|██████▌   | 522/800 [21:05<11:13,  2.42s/it] 65%|██████▌   | 523/800 [21:07<11:10,  2.42s/it] 66%|██████▌   | 524/800 [21:10<11:07,  2.42s/it] 66%|██████▌   | 525/800 [21:12<11:05,  2.42s/it] 66%|██████▌   | 526/800 [21:14<11:02,  2.42s/it] 66%|██████▌   | 527/800 [21:17<11:00,  2.42s/it] 66%|██████▌   | 528/800 [21:19<10:58,  2.42s/it] 66%|██████▌   | 529/800 [21:22<10:55,  2.42s/it] 66%|██████▋   | 530/800 [21:24<10:53,  2.42s/it] 66%|██████▋   | 531/800 [21:27<10:50,  2.42s/it] 66%|██████▋   | 532/800 [21:29<10:48,  2.42s/it] 67%|██████▋   | 533/800 [21:31<10:45,  2.42s/it] 67%|██████▋   | 534/800 [21:34<10:43,  2.42s/it] 67%|██████▋   | 535/800 [21:36<10:41,  2.42s/it] 67%|██████▋   | 536/800 [21:39<10:38,  2.42s/it] 67%|██████▋   | 537/800 [21:41<10:36,  2.42s/it] 67%|██████▋   | 538/800 [21:43<10:33,  2.42s/it] 67%|██████▋   | 539/800 [21:46<10:31,  2.42s/it] 68%|██████▊   | 540/800 [21:48<10:29,  2.42s/it]                                                 {'loss': 0.3607, 'grad_norm': 0.2978363633155823, 'learning_rate': 0.0001809595995707573, 'epoch': 3.38}
 68%|██████▊   | 540/800 [21:48<10:29,  2.42s/it] 68%|██████▊   | 541/800 [21:51<10:26,  2.42s/it] 68%|██████▊   | 542/800 [21:53<10:24,  2.42s/it] 68%|██████▊   | 543/800 [21:56<10:27,  2.44s/it] 68%|██████▊   | 544/800 [21:58<10:23,  2.43s/it] 68%|██████▊   | 545/800 [22:00<10:19,  2.43s/it] 68%|██████▊   | 546/800 [22:03<10:16,  2.43s/it] 68%|██████▊   | 547/800 [22:05<10:13,  2.42s/it] 68%|██████▊   | 548/800 [22:08<10:10,  2.42s/it] 69%|██████▊   | 549/800 [22:10<10:07,  2.42s/it] 69%|██████▉   | 550/800 [22:13<10:05,  2.42s/it] 69%|██████▉   | 551/800 [22:15<10:02,  2.42s/it] 69%|██████▉   | 552/800 [22:17<10:00,  2.42s/it] 69%|██████▉   | 553/800 [22:20<09:57,  2.42s/it] 69%|██████▉   | 554/800 [22:22<09:55,  2.42s/it] 69%|██████▉   | 555/800 [22:25<09:52,  2.42s/it] 70%|██████▉   | 556/800 [22:27<09:52,  2.43s/it] 70%|██████▉   | 557/800 [22:30<09:49,  2.42s/it] 70%|██████▉   | 558/800 [22:32<09:46,  2.42s/it] 70%|██████▉   | 559/800 [22:34<09:45,  2.43s/it] 70%|███████   | 560/800 [22:37<09:42,  2.43s/it]                                                 {'loss': 0.3644, 'grad_norm': 0.29287776350975037, 'learning_rate': 0.0001577407047339834, 'epoch': 3.5}
 70%|███████   | 560/800 [22:37<09:42,  2.43s/it] 70%|███████   | 561/800 [22:39<09:39,  2.42s/it] 70%|███████   | 562/800 [22:42<09:36,  2.42s/it] 70%|███████   | 563/800 [22:44<09:33,  2.42s/it] 70%|███████   | 564/800 [22:46<09:31,  2.42s/it] 71%|███████   | 565/800 [22:49<09:28,  2.42s/it] 71%|███████   | 566/800 [22:51<09:26,  2.42s/it] 71%|███████   | 567/800 [22:54<09:23,  2.42s/it] 71%|███████   | 568/800 [22:56<09:21,  2.42s/it] 71%|███████   | 569/800 [22:59<09:18,  2.42s/it] 71%|███████▏  | 570/800 [23:01<09:16,  2.42s/it] 71%|███████▏  | 571/800 [23:03<09:13,  2.42s/it] 72%|███████▏  | 572/800 [23:06<09:11,  2.42s/it] 72%|███████▏  | 573/800 [23:08<09:09,  2.42s/it] 72%|███████▏  | 574/800 [23:11<09:06,  2.42s/it] 72%|███████▏  | 575/800 [23:13<09:04,  2.42s/it] 72%|███████▏  | 576/800 [23:16<09:01,  2.42s/it] 72%|███████▏  | 577/800 [23:18<08:59,  2.42s/it] 72%|███████▏  | 578/800 [23:20<08:57,  2.42s/it] 72%|███████▏  | 579/800 [23:23<08:54,  2.42s/it] 72%|███████▎  | 580/800 [23:25<08:52,  2.42s/it]                                                 {'loss': 0.3875, 'grad_norm': 0.3090277314186096, 'learning_rate': 0.00013541031734468211, 'epoch': 3.62}
 72%|███████▎  | 580/800 [23:25<08:52,  2.42s/it] 73%|███████▎  | 581/800 [23:28<08:49,  2.42s/it] 73%|███████▎  | 582/800 [23:30<08:47,  2.42s/it] 73%|███████▎  | 583/800 [23:32<08:45,  2.42s/it] 73%|███████▎  | 584/800 [23:35<08:42,  2.42s/it] 73%|███████▎  | 585/800 [23:37<08:41,  2.43s/it] 73%|███████▎  | 586/800 [23:40<08:38,  2.42s/it] 73%|███████▎  | 587/800 [23:42<08:36,  2.42s/it] 74%|███████▎  | 588/800 [23:45<08:33,  2.42s/it] 74%|███████▎  | 589/800 [23:47<08:31,  2.42s/it] 74%|███████▍  | 590/800 [23:49<08:28,  2.42s/it] 74%|███████▍  | 591/800 [23:52<08:25,  2.42s/it] 74%|███████▍  | 592/800 [23:54<08:23,  2.42s/it] 74%|███████▍  | 593/800 [23:57<08:20,  2.42s/it] 74%|███████▍  | 594/800 [23:59<08:18,  2.42s/it] 74%|███████▍  | 595/800 [24:02<08:15,  2.42s/it] 74%|███████▍  | 596/800 [24:04<08:13,  2.42s/it] 75%|███████▍  | 597/800 [24:06<08:10,  2.42s/it] 75%|███████▍  | 598/800 [24:09<08:10,  2.43s/it] 75%|███████▍  | 599/800 [24:11<08:07,  2.43s/it] 75%|███████▌  | 600/800 [24:14<08:04,  2.42s/it]                                                 {'loss': 0.3793, 'grad_norm': 0.3624802231788635, 'learning_rate': 0.00011418349124044405, 'epoch': 3.75}
 75%|███████▌  | 600/800 [24:14<08:04,  2.42s/it] 75%|███████▌  | 601/800 [24:16<08:02,  2.42s/it] 75%|███████▌  | 602/800 [24:18<07:59,  2.42s/it] 75%|███████▌  | 603/800 [24:21<07:56,  2.42s/it] 76%|███████▌  | 604/800 [24:23<07:54,  2.42s/it] 76%|███████▌  | 605/800 [24:26<07:52,  2.42s/it] 76%|███████▌  | 606/800 [24:28<07:49,  2.42s/it] 76%|███████▌  | 607/800 [24:31<07:47,  2.42s/it] 76%|███████▌  | 608/800 [24:33<07:44,  2.42s/it] 76%|███████▌  | 609/800 [24:35<07:42,  2.42s/it] 76%|███████▋  | 610/800 [24:38<07:39,  2.42s/it] 76%|███████▋  | 611/800 [24:40<07:37,  2.42s/it] 76%|███████▋  | 612/800 [24:43<07:36,  2.43s/it] 77%|███████▋  | 613/800 [24:45<07:33,  2.43s/it] 77%|███████▋  | 614/800 [24:48<07:30,  2.42s/it] 77%|███████▋  | 615/800 [24:50<07:28,  2.42s/it] 77%|███████▋  | 616/800 [24:52<07:25,  2.42s/it] 77%|███████▋  | 617/800 [24:55<07:22,  2.42s/it] 77%|███████▋  | 618/800 [24:57<07:20,  2.42s/it] 77%|███████▋  | 619/800 [25:00<07:17,  2.42s/it] 78%|███████▊  | 620/800 [25:02<07:15,  2.42s/it]                                                 {'loss': 0.3992, 'grad_norm': 0.3601585626602173, 'learning_rate': 9.42646523604165e-05, 'epoch': 3.88}
 78%|███████▊  | 620/800 [25:02<07:15,  2.42s/it] 78%|███████▊  | 621/800 [25:04<07:13,  2.42s/it] 78%|███████▊  | 622/800 [25:07<07:10,  2.42s/it] 78%|███████▊  | 623/800 [25:09<07:08,  2.42s/it] 78%|███████▊  | 624/800 [25:12<07:05,  2.42s/it] 78%|███████▊  | 625/800 [25:14<07:03,  2.42s/it] 78%|███████▊  | 626/800 [25:17<07:00,  2.42s/it] 78%|███████▊  | 627/800 [25:19<06:58,  2.42s/it] 78%|███████▊  | 628/800 [25:21<06:56,  2.42s/it] 79%|███████▊  | 629/800 [25:24<06:53,  2.42s/it] 79%|███████▉  | 630/800 [25:26<06:51,  2.42s/it] 79%|███████▉  | 631/800 [25:29<06:48,  2.42s/it] 79%|███████▉  | 632/800 [25:31<06:46,  2.42s/it] 79%|███████▉  | 633/800 [25:34<06:44,  2.42s/it] 79%|███████▉  | 634/800 [25:36<06:42,  2.42s/it] 79%|███████▉  | 635/800 [25:38<06:39,  2.42s/it] 80%|███████▉  | 636/800 [25:41<06:37,  2.42s/it] 80%|███████▉  | 637/800 [25:43<06:34,  2.42s/it] 80%|███████▉  | 638/800 [25:46<06:32,  2.42s/it] 80%|███████▉  | 639/800 [25:48<06:30,  2.43s/it] 80%|████████  | 640/800 [25:50<06:27,  2.42s/it]                                                 {'loss': 0.3864, 'grad_norm': 0.366159588098526, 'learning_rate': 7.584563001175895e-05, 'epoch': 4.0}
 80%|████████  | 640/800 [25:50<06:27,  2.42s/it] 80%|████████  | 641/800 [25:53<06:25,  2.42s/it] 80%|████████  | 642/800 [25:55<06:22,  2.42s/it] 80%|████████  | 643/800 [25:58<06:20,  2.42s/it] 80%|████████  | 644/800 [26:00<06:17,  2.42s/it] 81%|████████  | 645/800 [26:03<06:15,  2.42s/it] 81%|████████  | 646/800 [26:05<06:12,  2.42s/it] 81%|████████  | 647/800 [26:07<06:10,  2.42s/it] 81%|████████  | 648/800 [26:10<06:07,  2.42s/it] 81%|████████  | 649/800 [26:12<06:05,  2.42s/it] 81%|████████▏ | 650/800 [26:15<06:02,  2.42s/it] 81%|████████▏ | 651/800 [26:17<06:00,  2.42s/it] 82%|████████▏ | 652/800 [26:20<05:58,  2.42s/it] 82%|████████▏ | 653/800 [26:22<05:55,  2.42s/it] 82%|████████▏ | 654/800 [26:24<05:53,  2.42s/it] 82%|████████▏ | 655/800 [26:27<05:50,  2.42s/it] 82%|████████▏ | 656/800 [26:29<05:48,  2.42s/it] 82%|████████▏ | 657/800 [26:32<05:45,  2.42s/it] 82%|████████▏ | 658/800 [26:34<05:43,  2.42s/it] 82%|████████▏ | 659/800 [26:36<05:41,  2.42s/it] 82%|████████▎ | 660/800 [26:39<05:38,  2.42s/it]                                                 {'loss': 0.3296, 'grad_norm': 0.2865164279937744, 'learning_rate': 5.910380944855087e-05, 'epoch': 4.12}
 82%|████████▎ | 660/800 [26:39<05:38,  2.42s/it] 83%|████████▎ | 661/800 [26:41<05:36,  2.42s/it] 83%|████████▎ | 662/800 [26:44<05:33,  2.42s/it] 83%|████████▎ | 663/800 [26:46<05:31,  2.42s/it] 83%|████████▎ | 664/800 [26:49<05:29,  2.42s/it] 83%|████████▎ | 665/800 [26:51<05:26,  2.42s/it] 83%|████████▎ | 666/800 [26:53<05:24,  2.42s/it] 83%|████████▎ | 667/800 [26:56<05:21,  2.42s/it] 84%|████████▎ | 668/800 [26:58<05:19,  2.42s/it] 84%|████████▎ | 669/800 [27:01<05:16,  2.42s/it] 84%|████████▍ | 670/800 [27:03<05:14,  2.42s/it] 84%|████████▍ | 671/800 [27:05<05:12,  2.42s/it] 84%|████████▍ | 672/800 [27:08<05:09,  2.42s/it] 84%|████████▍ | 673/800 [27:10<05:07,  2.42s/it] 84%|████████▍ | 674/800 [27:13<05:04,  2.42s/it] 84%|████████▍ | 675/800 [27:15<05:03,  2.43s/it] 84%|████████▍ | 676/800 [27:18<05:00,  2.42s/it] 85%|████████▍ | 677/800 [27:20<04:57,  2.42s/it] 85%|████████▍ | 678/800 [27:22<04:55,  2.42s/it] 85%|████████▍ | 679/800 [27:25<04:52,  2.42s/it] 85%|████████▌ | 680/800 [27:27<04:50,  2.42s/it]                                                 {'loss': 0.3204, 'grad_norm': 0.3288865387439728, 'learning_rate': 4.420042355482601e-05, 'epoch': 4.25}
 85%|████████▌ | 680/800 [27:27<04:50,  2.42s/it] 85%|████████▌ | 681/800 [27:30<04:48,  2.42s/it] 85%|████████▌ | 682/800 [27:32<04:46,  2.43s/it] 85%|████████▌ | 683/800 [27:35<04:44,  2.43s/it] 86%|████████▌ | 684/800 [27:37<04:41,  2.42s/it] 86%|████████▌ | 685/800 [27:39<04:38,  2.42s/it] 86%|████████▌ | 686/800 [27:42<04:36,  2.42s/it] 86%|████████▌ | 687/800 [27:44<04:33,  2.42s/it] 86%|████████▌ | 688/800 [27:47<04:31,  2.42s/it] 86%|████████▌ | 689/800 [27:49<04:28,  2.42s/it] 86%|████████▋ | 690/800 [27:52<04:26,  2.42s/it] 86%|████████▋ | 691/800 [27:54<04:23,  2.42s/it] 86%|████████▋ | 692/800 [27:56<04:21,  2.42s/it] 87%|████████▋ | 693/800 [27:59<04:18,  2.42s/it] 87%|████████▋ | 694/800 [28:01<04:16,  2.42s/it] 87%|████████▋ | 695/800 [28:04<04:14,  2.42s/it] 87%|████████▋ | 696/800 [28:06<04:11,  2.42s/it] 87%|████████▋ | 697/800 [28:08<04:09,  2.42s/it] 87%|████████▋ | 698/800 [28:11<04:06,  2.42s/it] 87%|████████▋ | 699/800 [28:13<04:04,  2.42s/it] 88%|████████▊ | 700/800 [28:16<04:02,  2.42s/it]                                                 {'loss': 0.3415, 'grad_norm': 0.27765020728111267, 'learning_rate': 3.127900008376044e-05, 'epoch': 4.38}
 88%|████████▊ | 700/800 [28:16<04:02,  2.42s/it] 88%|████████▊ | 701/800 [28:18<03:59,  2.42s/it] 88%|████████▊ | 702/800 [28:21<03:57,  2.42s/it] 88%|████████▊ | 703/800 [28:23<03:54,  2.42s/it] 88%|████████▊ | 704/800 [28:25<03:52,  2.42s/it] 88%|████████▊ | 705/800 [28:28<03:49,  2.42s/it] 88%|████████▊ | 706/800 [28:30<03:47,  2.42s/it] 88%|████████▊ | 707/800 [28:33<03:45,  2.42s/it] 88%|████████▊ | 708/800 [28:35<03:43,  2.43s/it] 89%|████████▊ | 709/800 [28:38<03:40,  2.42s/it] 89%|████████▉ | 710/800 [28:40<03:38,  2.42s/it] 89%|████████▉ | 711/800 [28:42<03:35,  2.42s/it] 89%|████████▉ | 712/800 [28:45<03:33,  2.42s/it] 89%|████████▉ | 713/800 [28:47<03:30,  2.42s/it] 89%|████████▉ | 714/800 [28:50<03:28,  2.42s/it] 89%|████████▉ | 715/800 [28:52<03:25,  2.42s/it] 90%|████████▉ | 716/800 [28:54<03:23,  2.42s/it] 90%|████████▉ | 717/800 [28:57<03:20,  2.42s/it] 90%|████████▉ | 718/800 [28:59<03:18,  2.42s/it] 90%|████████▉ | 719/800 [29:02<03:15,  2.42s/it] 90%|█████████ | 720/800 [29:04<03:14,  2.43s/it]                                                 {'loss': 0.3472, 'grad_norm': 0.3174089789390564, 'learning_rate': 2.0463979406949023e-05, 'epoch': 4.5}
 90%|█████████ | 720/800 [29:04<03:14,  2.43s/it] 90%|█████████ | 721/800 [29:07<03:11,  2.43s/it] 90%|█████████ | 722/800 [29:09<03:09,  2.42s/it] 90%|█████████ | 723/800 [29:11<03:06,  2.42s/it] 90%|█████████ | 724/800 [29:14<03:03,  2.42s/it] 91%|█████████ | 725/800 [29:16<03:01,  2.42s/it] 91%|█████████ | 726/800 [29:19<02:59,  2.42s/it] 91%|█████████ | 727/800 [29:21<02:56,  2.42s/it] 91%|█████████ | 728/800 [29:24<02:54,  2.42s/it] 91%|█████████ | 729/800 [29:26<02:51,  2.42s/it] 91%|█████████▏| 730/800 [29:28<02:49,  2.42s/it] 91%|█████████▏| 731/800 [29:31<02:46,  2.42s/it] 92%|█████████▏| 732/800 [29:33<02:44,  2.42s/it] 92%|█████████▏| 733/800 [29:36<02:42,  2.42s/it] 92%|█████████▏| 734/800 [29:38<02:39,  2.42s/it] 92%|█████████▏| 735/800 [29:40<02:37,  2.42s/it] 92%|█████████▏| 736/800 [29:43<02:34,  2.42s/it] 92%|█████████▏| 737/800 [29:45<02:32,  2.43s/it] 92%|█████████▏| 738/800 [29:48<02:30,  2.42s/it] 92%|█████████▏| 739/800 [29:50<02:27,  2.42s/it] 92%|█████████▎| 740/800 [29:53<02:25,  2.42s/it]                                                 {'loss': 0.3281, 'grad_norm': 0.25584128499031067, 'learning_rate': 1.185951608560118e-05, 'epoch': 4.62}
 92%|█████████▎| 740/800 [29:53<02:25,  2.42s/it] 93%|█████████▎| 741/800 [29:55<02:22,  2.42s/it] 93%|█████████▎| 742/800 [29:57<02:20,  2.42s/it] 93%|█████████▎| 743/800 [30:00<02:17,  2.42s/it] 93%|█████████▎| 744/800 [30:02<02:15,  2.42s/it] 93%|█████████▎| 745/800 [30:05<02:13,  2.42s/it] 93%|█████████▎| 746/800 [30:07<02:10,  2.42s/it] 93%|█████████▎| 747/800 [30:10<02:08,  2.42s/it] 94%|█████████▎| 748/800 [30:12<02:05,  2.42s/it] 94%|█████████▎| 749/800 [30:14<02:03,  2.42s/it] 94%|█████████▍| 750/800 [30:17<02:01,  2.42s/it] 94%|█████████▍| 751/800 [30:19<01:59,  2.44s/it] 94%|█████████▍| 752/800 [30:22<01:56,  2.43s/it] 94%|█████████▍| 753/800 [30:24<01:54,  2.43s/it] 94%|█████████▍| 754/800 [30:27<01:51,  2.43s/it] 94%|█████████▍| 755/800 [30:29<01:49,  2.42s/it] 94%|█████████▍| 756/800 [30:31<01:46,  2.42s/it] 95%|█████████▍| 757/800 [30:34<01:44,  2.42s/it] 95%|█████████▍| 758/800 [30:36<01:41,  2.42s/it] 95%|█████████▍| 759/800 [30:39<01:39,  2.42s/it] 95%|█████████▌| 760/800 [30:41<01:36,  2.42s/it]                                                 {'loss': 0.3467, 'grad_norm': 0.2788195312023163, 'learning_rate': 5.548475805179587e-06, 'epoch': 4.75}
 95%|█████████▌| 760/800 [30:41<01:36,  2.42s/it] 95%|█████████▌| 761/800 [30:43<01:34,  2.42s/it] 95%|█████████▌| 762/800 [30:46<01:31,  2.42s/it] 95%|█████████▌| 763/800 [30:48<01:29,  2.42s/it] 96%|█████████▌| 764/800 [30:51<01:27,  2.42s/it] 96%|█████████▌| 765/800 [30:53<01:25,  2.43s/it] 96%|█████████▌| 766/800 [30:56<01:22,  2.43s/it] 96%|█████████▌| 767/800 [30:58<01:20,  2.43s/it] 96%|█████████▌| 768/800 [31:00<01:17,  2.42s/it] 96%|█████████▌| 769/800 [31:03<01:15,  2.42s/it] 96%|█████████▋| 770/800 [31:05<01:12,  2.42s/it] 96%|█████████▋| 771/800 [31:08<01:10,  2.42s/it] 96%|█████████▋| 772/800 [31:10<01:07,  2.42s/it] 97%|█████████▋| 773/800 [31:13<01:05,  2.42s/it] 97%|█████████▋| 774/800 [31:15<01:02,  2.42s/it] 97%|█████████▋| 775/800 [31:17<01:00,  2.42s/it] 97%|█████████▋| 776/800 [31:20<00:58,  2.42s/it] 97%|█████████▋| 777/800 [31:22<00:55,  2.42s/it] 97%|█████████▋| 778/800 [31:25<00:53,  2.42s/it] 97%|█████████▋| 779/800 [31:27<00:50,  2.42s/it] 98%|█████████▊| 780/800 [31:29<00:48,  2.42s/it]                                                 {'loss': 0.3464, 'grad_norm': 0.3104519844055176, 'learning_rate': 1.5916373335503054e-06, 'epoch': 4.88}
 98%|█████████▊| 780/800 [31:29<00:48,  2.42s/it] 98%|█████████▊| 781/800 [31:32<00:45,  2.42s/it] 98%|█████████▊| 782/800 [31:34<00:43,  2.42s/it] 98%|█████████▊| 783/800 [31:37<00:41,  2.42s/it] 98%|█████████▊| 784/800 [31:39<00:38,  2.42s/it] 98%|█████████▊| 785/800 [31:42<00:36,  2.42s/it] 98%|█████████▊| 786/800 [31:44<00:33,  2.42s/it] 98%|█████████▊| 787/800 [31:46<00:31,  2.42s/it] 98%|█████████▊| 788/800 [31:49<00:29,  2.42s/it] 99%|█████████▊| 789/800 [31:51<00:26,  2.42s/it] 99%|█████████▉| 790/800 [31:54<00:24,  2.42s/it] 99%|█████████▉| 791/800 [31:56<00:21,  2.42s/it] 99%|█████████▉| 792/800 [31:59<00:19,  2.42s/it] 99%|█████████▉| 793/800 [32:01<00:16,  2.42s/it] 99%|█████████▉| 794/800 [32:03<00:14,  2.42s/it] 99%|█████████▉| 795/800 [32:06<00:12,  2.42s/it]100%|█████████▉| 796/800 [32:08<00:09,  2.42s/it]100%|█████████▉| 797/800 [32:11<00:07,  2.42s/it]100%|█████████▉| 798/800 [32:13<00:04,  2.42s/it]100%|█████████▉| 799/800 [32:15<00:02,  2.42s/it]100%|██████████| 800/800 [32:18<00:00,  2.42s/it]                                                 {'loss': 0.3364, 'grad_norm': 0.23565912246704102, 'learning_rate': 2.7107188222991187e-08, 'epoch': 5.0}
100%|██████████| 800/800 [32:18<00:00,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 03:36:23,815 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/length_then_topic/checkpoint-800
[INFO|configuration_utils.py:726] 2024-05-25 03:36:25,180 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 03:36:25,182 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 03:36:26,427 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 03:36:26,429 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 03:36:26,481 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/length_then_topic/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 03:36:26,482 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/length_then_topic/checkpoint-800/special_tokens_map.json
[INFO|trainer.py:2231] 2024-05-25 03:36:26,609 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 1950.551, 'train_samples_per_second': 3.279, 'train_steps_per_second': 0.41, 'train_loss': 0.46862020671367643, 'epoch': 5.0}
100%|██████████| 800/800 [32:21<00:00,  2.42s/it]100%|██████████| 800/800 [32:21<00:00,  2.43s/it]
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4686
  train_runtime            = 0:32:30.55
  train_samples_per_second =      3.279
  train_steps_per_second   =       0.41
[INFO|trainer.py:3203] 2024-05-25 03:36:26,615 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/length_then_topic
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-05-25 03:36:27,545 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 03:36:27,547 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 03:36:28,035 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 03:36:28,037 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 03:36:28,078 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/length_then_topic/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 03:36:28,079 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/length_then_topic/special_tokens_map.json
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:471] 2024-05-25 03:36:28,461 >> Configuration saved in /scratch/tathagato/adapter_experiments/length_then_topic/final_merged_model/config.json
[INFO|configuration_utils.py:697] 2024-05-25 03:36:28,463 >> Configuration saved in /scratch/tathagato/adapter_experiments/length_then_topic/final_merged_model/generation_config.json
[INFO|modeling_utils.py:2474] 2024-05-25 03:36:34,839 >> Model weights saved in /scratch/tathagato/adapter_experiments/length_then_topic/final_merged_model/model.safetensors
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.006 MB uploadedwandb: | 0.006 MB of 0.029 MB uploadedwandb: / 0.008 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: \ 0.032 MB of 0.032 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:   train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:     train/grad_norm █▆▆▅▅▃▃▅▂▃▄▄▄▄▄▄▃▄▃▃▅▄▄▃▂▃▂▂▃▄▄▄▂▃▂▃▁▂▃▁
wandb: train/learning_rate ▂▃▄▄▅▆▇██████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:          train/loss █▇▇▆▆▇▆▅▅▅▅▅▅▅▄▅▃▄▄▃▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 8.17116991193088e+16
wandb:              train/epoch 5.0
wandb:        train/global_step 800
wandb:          train/grad_norm 0.23566
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.3364
wandb:               train_loss 0.46862
wandb:            train_runtime 1950.551
wandb: train_samples_per_second 3.279
wandb:   train_steps_per_second 0.41
wandb: 
wandb: 🚀 View run trim-salad-98 at: https://wandb.ai/ihub-drug-discovery/huggingface/runs/ju0x0lyr
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240525_030358-ju0x0lyr/logs
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-05-25 03:37:05 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 03:37:05 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 03:37:05 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 03:37:05 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 03:37:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch/tathagato/adapter_experiments/length_then_extractiveness/runs/May25_03-37-05_gnode081,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=20,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/scratch/tathagato/adapter_experiments/length_then_extractiveness,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch/tathagato/adapter_experiments/length_then_extractiveness,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=400,
save_strategy=steps,
save_total_limit=400,
seed=0,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
2024-05-25 03:37:05 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'o_proj', 'q_proj', 'v_proj', 'k_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[INFO|configuration_utils.py:726] 2024-05-25 03:37:06,444 >> loading configuration file config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 03:37:06,448 >> Model config LlamaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": false,
  "vocab_size": 32000
}

[WARNING|modeling_utils.py:3058] 2024-05-25 03:37:06,509 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 03:37:06,509 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 03:37:06,522 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|quantizer_bnb_4bit.py:247] 2024-05-25 03:37:06,542 >> The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' 
[WARNING|modeling_utils.py:3058] 2024-05-25 03:37:06,542 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:3283] 2024-05-25 03:37:06,543 >> loading weights file model.safetensors from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors
[INFO|modeling_utils.py:1417] 2024-05-25 03:37:06,559 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:928] 2024-05-25 03:37:06,561 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

[INFO|modeling_utils.py:4024] 2024-05-25 03:37:09,555 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-25 03:37:09,555 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-05-25 03:37:09,789 >> loading configuration file generation_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-25 03:37:09,789 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 2048,
  "pad_token_id": 0
}

loading model from : /scratch/tathagato/adapter_experiments/length/length
loading model from : /scratch/tathagato/adapter_experiments/length/length
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:37:10,555 >> loading file tokenizer.model from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:37:10,556 >> loading file tokenizer.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:37:10,556 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:37:10,556 >> loading file special_tokens_map.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 03:37:10,556 >> loading file tokenizer_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json
loading model from : /scratch/tathagato/adapter_experiments/length/length
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
loading model from : /scratch/tathagato/adapter_experiments/length/length
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
train dataset size 4278
test dataset size 554
4278
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Spawning 10 processes
2024-05-25 03:37:13 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]train dataset size 4278
test dataset size 554
4278
train dataset size 4278
test dataset size 554
4278
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:13:29,  1.03s/ examples]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:07, 514.55 examples/s]train dataset size 4278
test dataset size 554
4278
Applying chat template to train_sft (num_proc=10):  15%|█▌        | 662/4278 [00:01<00:06, 573.61 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:01<00:05, 599.00 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:01<00:02, 1076.23 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:18:48,  1.11s/ examples]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:14:00,  1.04s/ examples]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:07, 483.30 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1712/4278 [00:02<00:02, 1054.54 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:08, 467.14 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 856/4278 [00:01<00:04, 686.13 examples/s]Applying chat template to train_sft (num_proc=10):  15%|█▌        | 649/4278 [00:01<00:06, 552.53 examples/s]Applying chat template to train_sft (num_proc=10):  46%|████▌     | 1958/4278 [00:02<00:02, 934.34 examples/s] Applying chat template to train_sft (num_proc=10):  20%|██        | 856/4278 [00:01<00:04, 749.47 examples/s]Applying chat template to train_sft (num_proc=10):  25%|██▌       | 1087/4278 [00:02<00:04, 679.54 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2141/4278 [00:03<00:02, 770.13 examples/s]Applying chat template to train_sft (num_proc=10):  25%|██▍       | 1065/4278 [00:01<00:04, 721.78 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2568/4278 [00:03<00:01, 1157.99 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:23:41,  1.17s/ examples]Applying chat template to train_sft (num_proc=10):  30%|███       | 1285/4278 [00:02<00:04, 617.11 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:09, 425.82 examples/s]Applying chat template to train_sft (num_proc=10):  66%|██████▌   | 2813/4278 [00:03<00:01, 1005.93 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1285/4278 [00:02<00:04, 624.54 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1712/4278 [00:02<00:02, 1053.22 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 759.38 examples/s]Applying chat template to train_sft (num_proc=10):  15%|█▌        | 650/4278 [00:01<00:07, 485.92 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:02<00:01, 1118.51 examples/s]Applying chat template to train_sft (num_proc=10):  45%|████▍     | 1921/4278 [00:02<00:02, 926.33 examples/s] Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:03<00:01, 780.90 examples/s] Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:02<00:01, 1089.31 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3424/4278 [00:04<00:00, 1139.25 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:02<00:06, 534.45 examples/s]Applying chat template to train_sft (num_proc=10):  55%|█████▌    | 2360/4278 [00:03<00:01, 979.51 examples/s] Applying chat template to train_sft (num_proc=10):  85%|████████▌ | 3639/4278 [00:04<00:00, 1075.06 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:02<00:03, 893.58 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2568/4278 [00:03<00:01, 1080.27 examples/s]Applying chat template to train_sft (num_proc=10):  56%|█████▌    | 2405/4278 [00:03<00:01, 975.35 examples/s] Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:04<00:00, 1198.94 examples/s]Applying chat template to train_sft (num_proc=10):  35%|███▌      | 1505/4278 [00:02<00:03, 813.03 examples/s]Applying chat template to train_sft (num_proc=10):  65%|██████▌   | 2794/4278 [00:03<00:01, 832.13 examples/s] Applying chat template to train_sft (num_proc=10):  60%|██████    | 2569/4278 [00:03<00:02, 738.08 examples/s]Applying chat template to train_sft (num_proc=10):  95%|█████████▌| 4079/4278 [00:04<00:00, 940.19 examples/s] Applying chat template to train_sft (num_proc=10):  70%|███████   | 2996/4278 [00:03<00:01, 1140.95 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 693.29 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:04<00:01, 764.32 examples/s]Applying chat template to train_sft (num_proc=10):  74%|███████▍  | 3179/4278 [00:04<00:01, 1029.48 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 823.25 examples/s]
Applying chat template to train_sft (num_proc=10):  79%|███████▉  | 3391/4278 [00:04<00:00, 1186.48 examples/s]Concatenating 10 shards
2024-05-25 03:37:18 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Applying chat template to train_sft (num_proc=10):  50%|█████     | 2141/4278 [00:03<00:02, 791.73 examples/s]Applying chat template to train_sft (num_proc=10):  86%|████████▌ | 3669/4278 [00:04<00:00, 913.66 examples/s] Applying chat template to train_sft (num_proc=10):  80%|████████  | 3425/4278 [00:04<00:01, 706.05 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:04<00:00, 1026.40 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2569/4278 [00:03<00:01, 888.65 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3852/4278 [00:04<00:00, 815.89 examples/s]Spawning 10 processes
2024-05-25 03:37:19 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2996/4278 [00:03<00:01, 1198.52 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:04<00:00, 1237.20 examples/s]Applying chat template to train_sft (num_proc=10):  95%|█████████▌| 4080/4278 [00:05<00:00, 935.78 examples/s] Applying chat template to train_sft (num_proc=10):  75%|███████▍  | 3190/4278 [00:04<00:01, 991.75 examples/s] Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 813.78 examples/s] 
Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 762.49 examples/s]
Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<05:08,  1.79 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3425/4278 [00:04<00:00, 916.02 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 112/554 [00:00<00:02, 214.56 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:04<00:00, 1292.61 examples/s]Applying chat template to test_sft (num_proc=10):  31%|███       | 169/554 [00:00<00:01, 260.37 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 280/554 [00:00<00:00, 406.24 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 389/554 [00:01<00:00, 552.46 examples/s]Applying chat template to train_sft (num_proc=10):  96%|█████████▌| 4108/4278 [00:05<00:00, 1058.55 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 499/554 [00:01<00:00, 617.41 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 791.35 examples/s] 
Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 348.39 examples/s]
Concatenating 10 shards
2024-05-25 03:37:21 - INFO - datasets.arrow_dataset - Concatenating 10 shards
tokenizer padding side left
Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<05:10,  1.78 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 189.71 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<05:49,  1.58 examples/s]Applying chat template to test_sft (num_proc=10):  31%|███       | 169/554 [00:00<00:01, 253.12 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:00<00:01, 312.48 examples/s]Using custom data configuration default-37faee928f8259ce
2024-05-25 03:37:21 - INFO - datasets.builder - Using custom data configuration default-37faee928f8259ce
Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
2024-05-25 03:37:21 - INFO - datasets.info - Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
Overwrite dataset info from restored data version if exists.
2024-05-25 03:37:21 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0
2024-05-25 03:37:21 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0
Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0)
2024-05-25 03:37:21 - INFO - datasets.builder - Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0)
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0
2024-05-25 03:37:21 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0
Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 174.36 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 280/554 [00:01<00:00, 338.30 examples/s]Applying chat template to test_sft (num_proc=10):  31%|███       | 169/554 [00:00<00:01, 229.84 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 390/554 [00:01<00:00, 494.17 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 280/554 [00:01<00:00, 396.62 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 499/554 [00:01<00:00, 538.87 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 389/554 [00:01<00:00, 466.83 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 499/554 [00:01<00:00, 495.56 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<04:09,  2.22 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 320.80 examples/s]
Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 216.15 examples/s]tokenizer padding side left
Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 320.53 examples/s]
Applying chat template to test_sft (num_proc=10):  40%|████      | 224/554 [00:00<00:00, 402.34 examples/s]tokenizer padding side left
Applying chat template to test_sft (num_proc=10):  60%|██████    | 334/554 [00:00<00:00, 458.57 examples/s]Applying chat template to test_sft (num_proc=10):  80%|████████  | 444/554 [00:01<00:00, 517.36 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 570.39 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 374.05 examples/s]
tokenizer padding side left
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
2024-05-25 03:37:23 - WARNING - accelerate.utils.other - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:607] 2024-05-25 03:37:25,015 >> Using auto half precision backend
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
[INFO|trainer.py:1969] 2024-05-25 03:37:25,723 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-25 03:37:25,723 >>   Num examples = 2,462
[INFO|trainer.py:1971] 2024-05-25 03:37:25,723 >>   Num Epochs = 5
[INFO|trainer.py:1972] 2024-05-25 03:37:25,723 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-25 03:37:25,723 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-05-25 03:37:25,723 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-05-25 03:37:25,723 >>   Total optimization steps = 1,540
[INFO|trainer.py:1978] 2024-05-25 03:37:25,725 >>   Number of trainable parameters = 4,505,600
[INFO|integration_utils.py:723] 2024-05-25 03:37:25,789 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSum/experiments/wandb/run-20240525_033728-9u7o4mjh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-snowflake-99
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/huggingface
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/huggingface/runs/9u7o4mjh
  0%|          | 0/1540 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/1540 [00:02<1:03:22,  2.47s/it]  0%|          | 2/1540 [00:04<1:02:20,  2.43s/it]  0%|          | 3/1540 [00:07<1:01:59,  2.42s/it]  0%|          | 4/1540 [00:09<1:01:48,  2.41s/it]  0%|          | 5/1540 [00:12<1:01:40,  2.41s/it]  0%|          | 6/1540 [00:14<1:01:35,  2.41s/it]  0%|          | 7/1540 [00:16<1:01:32,  2.41s/it]  1%|          | 8/1540 [00:19<1:01:30,  2.41s/it]  1%|          | 9/1540 [00:21<1:01:27,  2.41s/it]  1%|          | 10/1540 [00:24<1:01:25,  2.41s/it]  1%|          | 11/1540 [00:26<1:01:24,  2.41s/it]  1%|          | 12/1540 [00:29<1:02:05,  2.44s/it]  1%|          | 13/1540 [00:31<1:02:09,  2.44s/it]  1%|          | 14/1540 [00:33<1:01:56,  2.44s/it]  1%|          | 15/1540 [00:36<1:01:43,  2.43s/it]  1%|          | 16/1540 [00:38<1:01:33,  2.42s/it]  1%|          | 17/1540 [00:41<1:01:26,  2.42s/it]  1%|          | 18/1540 [00:43<1:01:21,  2.42s/it]  1%|          | 19/1540 [00:45<1:01:15,  2.42s/it]  1%|▏         | 20/1540 [00:48<1:01:12,  2.42s/it]                                                   {'loss': 0.7015, 'grad_norm': 0.49373891949653625, 'learning_rate': 3.246753246753247e-05, 'epoch': 0.06}
  1%|▏         | 20/1540 [00:48<1:01:12,  2.42s/it]  1%|▏         | 21/1540 [00:50<1:01:12,  2.42s/it]  1%|▏         | 22/1540 [00:53<1:01:10,  2.42s/it]  1%|▏         | 23/1540 [00:55<1:01:08,  2.42s/it]  2%|▏         | 24/1540 [00:58<1:01:05,  2.42s/it]  2%|▏         | 25/1540 [01:00<1:01:04,  2.42s/it]  2%|▏         | 26/1540 [01:02<1:01:11,  2.43s/it]  2%|▏         | 27/1540 [01:05<1:01:04,  2.42s/it]  2%|▏         | 28/1540 [01:07<1:00:59,  2.42s/it]  2%|▏         | 29/1540 [01:10<1:01:10,  2.43s/it]  2%|▏         | 30/1540 [01:12<1:01:03,  2.43s/it]  2%|▏         | 31/1540 [01:15<1:00:57,  2.42s/it]  2%|▏         | 32/1540 [01:17<1:00:52,  2.42s/it]  2%|▏         | 33/1540 [01:19<1:00:49,  2.42s/it]  2%|▏         | 34/1540 [01:22<1:00:46,  2.42s/it]  2%|▏         | 35/1540 [01:24<1:00:42,  2.42s/it]  2%|▏         | 36/1540 [01:27<1:00:39,  2.42s/it]  2%|▏         | 37/1540 [01:29<1:00:36,  2.42s/it]  2%|▏         | 38/1540 [01:31<1:00:34,  2.42s/it]  3%|▎         | 39/1540 [01:34<1:00:32,  2.42s/it]  3%|▎         | 40/1540 [01:36<1:00:29,  2.42s/it]                                                   {'loss': 0.6736, 'grad_norm': 0.5405275225639343, 'learning_rate': 6.33116883116883e-05, 'epoch': 0.13}
  3%|▎         | 40/1540 [01:36<1:00:29,  2.42s/it]  3%|▎         | 41/1540 [01:39<1:00:28,  2.42s/it]  3%|▎         | 42/1540 [01:41<1:00:40,  2.43s/it]  3%|▎         | 43/1540 [01:44<1:00:34,  2.43s/it]  3%|▎         | 44/1540 [01:46<1:00:28,  2.43s/it]  3%|▎         | 45/1540 [01:48<1:00:23,  2.42s/it]  3%|▎         | 46/1540 [01:51<1:00:20,  2.42s/it]  3%|▎         | 47/1540 [01:53<1:00:17,  2.42s/it]  3%|▎         | 48/1540 [01:56<1:00:13,  2.42s/it]  3%|▎         | 49/1540 [01:58<1:00:11,  2.42s/it]  3%|▎         | 50/1540 [02:01<1:00:08,  2.42s/it]  3%|▎         | 51/1540 [02:03<1:00:06,  2.42s/it]  3%|▎         | 52/1540 [02:05<1:00:03,  2.42s/it]  3%|▎         | 53/1540 [02:08<1:00:00,  2.42s/it]  4%|▎         | 54/1540 [02:10<1:00:07,  2.43s/it]  4%|▎         | 55/1540 [02:13<1:00:02,  2.43s/it]  4%|▎         | 56/1540 [02:15<1:00:33,  2.45s/it]  4%|▎         | 57/1540 [02:18<1:00:18,  2.44s/it]  4%|▍         | 58/1540 [02:20<1:00:08,  2.43s/it]  4%|▍         | 59/1540 [02:22<59:59,  2.43s/it]    4%|▍         | 60/1540 [02:25<59:52,  2.43s/it]                                                 {'loss': 0.6446, 'grad_norm': 0.49160313606262207, 'learning_rate': 9.577922077922078e-05, 'epoch': 0.19}
  4%|▍         | 60/1540 [02:25<59:52,  2.43s/it]  4%|▍         | 61/1540 [02:27<59:48,  2.43s/it]  4%|▍         | 62/1540 [02:30<59:43,  2.42s/it]  4%|▍         | 63/1540 [02:32<59:39,  2.42s/it]  4%|▍         | 64/1540 [02:35<59:35,  2.42s/it]  4%|▍         | 65/1540 [02:37<59:32,  2.42s/it]  4%|▍         | 66/1540 [02:39<59:30,  2.42s/it]  4%|▍         | 67/1540 [02:42<59:27,  2.42s/it]  4%|▍         | 68/1540 [02:44<59:34,  2.43s/it]  4%|▍         | 69/1540 [02:47<59:28,  2.43s/it]  5%|▍         | 70/1540 [02:49<59:36,  2.43s/it]  5%|▍         | 71/1540 [02:52<59:28,  2.43s/it]  5%|▍         | 72/1540 [02:54<59:22,  2.43s/it]  5%|▍         | 73/1540 [02:56<59:17,  2.43s/it]  5%|▍         | 74/1540 [02:59<59:14,  2.42s/it]  5%|▍         | 75/1540 [03:01<59:10,  2.42s/it]  5%|▍         | 76/1540 [03:04<59:08,  2.42s/it]  5%|▌         | 77/1540 [03:06<59:04,  2.42s/it]  5%|▌         | 78/1540 [03:09<59:00,  2.42s/it]  5%|▌         | 79/1540 [03:11<58:58,  2.42s/it]  5%|▌         | 80/1540 [03:13<58:55,  2.42s/it]                                                 {'loss': 0.6608, 'grad_norm': 0.4785288870334625, 'learning_rate': 0.00012824675324675324, 'epoch': 0.26}
  5%|▌         | 80/1540 [03:13<58:55,  2.42s/it]  5%|▌         | 81/1540 [03:16<58:53,  2.42s/it]  5%|▌         | 82/1540 [03:18<58:50,  2.42s/it]  5%|▌         | 83/1540 [03:21<59:02,  2.43s/it]  5%|▌         | 84/1540 [03:23<58:55,  2.43s/it]  6%|▌         | 85/1540 [03:26<58:50,  2.43s/it]  6%|▌         | 86/1540 [03:28<58:46,  2.43s/it]  6%|▌         | 87/1540 [03:30<58:42,  2.42s/it]  6%|▌         | 88/1540 [03:33<58:38,  2.42s/it]  6%|▌         | 89/1540 [03:35<58:35,  2.42s/it]  6%|▌         | 90/1540 [03:38<58:32,  2.42s/it]  6%|▌         | 91/1540 [03:40<58:30,  2.42s/it]  6%|▌         | 92/1540 [03:42<58:27,  2.42s/it]  6%|▌         | 93/1540 [03:45<58:24,  2.42s/it]  6%|▌         | 94/1540 [03:47<58:22,  2.42s/it]  6%|▌         | 95/1540 [03:50<58:19,  2.42s/it]  6%|▌         | 96/1540 [03:52<58:17,  2.42s/it]  6%|▋         | 97/1540 [03:55<58:14,  2.42s/it]  6%|▋         | 98/1540 [03:57<58:22,  2.43s/it]  6%|▋         | 99/1540 [03:59<58:17,  2.43s/it]  6%|▋         | 100/1540 [04:02<58:12,  2.43s/it]                                                  {'loss': 0.6762, 'grad_norm': 0.3613786995410919, 'learning_rate': 0.00016071428571428573, 'epoch': 0.32}
  6%|▋         | 100/1540 [04:02<58:12,  2.43s/it]  7%|▋         | 101/1540 [04:04<58:08,  2.42s/it]  7%|▋         | 102/1540 [04:07<58:04,  2.42s/it]  7%|▋         | 103/1540 [04:09<58:00,  2.42s/it]  7%|▋         | 104/1540 [04:12<57:57,  2.42s/it]  7%|▋         | 105/1540 [04:14<57:54,  2.42s/it]  7%|▋         | 106/1540 [04:16<57:51,  2.42s/it]  7%|▋         | 107/1540 [04:19<57:48,  2.42s/it]  7%|▋         | 108/1540 [04:21<57:46,  2.42s/it]  7%|▋         | 109/1540 [04:24<57:43,  2.42s/it]  7%|▋         | 110/1540 [04:26<57:40,  2.42s/it]  7%|▋         | 111/1540 [04:29<57:50,  2.43s/it]  7%|▋         | 112/1540 [04:31<57:54,  2.43s/it]  7%|▋         | 113/1540 [04:33<57:45,  2.43s/it]  7%|▋         | 114/1540 [04:36<57:39,  2.43s/it]  7%|▋         | 115/1540 [04:38<57:34,  2.42s/it]  8%|▊         | 116/1540 [04:41<57:30,  2.42s/it]  8%|▊         | 117/1540 [04:43<57:26,  2.42s/it]  8%|▊         | 118/1540 [04:45<57:22,  2.42s/it]  8%|▊         | 119/1540 [04:48<57:20,  2.42s/it]  8%|▊         | 120/1540 [04:50<57:17,  2.42s/it]                                                  {'loss': 0.6468, 'grad_norm': 0.366914838552475, 'learning_rate': 0.00019318181818181817, 'epoch': 0.39}
  8%|▊         | 120/1540 [04:50<57:17,  2.42s/it]  8%|▊         | 121/1540 [04:53<57:15,  2.42s/it]  8%|▊         | 122/1540 [04:55<57:11,  2.42s/it]  8%|▊         | 123/1540 [04:58<57:08,  2.42s/it]  8%|▊         | 124/1540 [05:00<57:06,  2.42s/it]  8%|▊         | 125/1540 [05:02<57:03,  2.42s/it]  8%|▊         | 126/1540 [05:05<57:31,  2.44s/it]  8%|▊         | 127/1540 [05:07<57:20,  2.43s/it]  8%|▊         | 128/1540 [05:10<57:11,  2.43s/it]  8%|▊         | 129/1540 [05:12<57:04,  2.43s/it]  8%|▊         | 130/1540 [05:15<56:58,  2.42s/it]  9%|▊         | 131/1540 [05:17<56:54,  2.42s/it]  9%|▊         | 132/1540 [05:19<56:49,  2.42s/it]  9%|▊         | 133/1540 [05:22<56:46,  2.42s/it]  9%|▊         | 134/1540 [05:24<56:43,  2.42s/it]  9%|▉         | 135/1540 [05:27<56:40,  2.42s/it]  9%|▉         | 136/1540 [05:29<56:37,  2.42s/it]  9%|▉         | 137/1540 [05:32<56:35,  2.42s/it]  9%|▉         | 138/1540 [05:34<56:34,  2.42s/it]  9%|▉         | 139/1540 [05:36<56:44,  2.43s/it]  9%|▉         | 140/1540 [05:39<56:37,  2.43s/it]                                                  {'loss': 0.6224, 'grad_norm': 0.40950146317481995, 'learning_rate': 0.00022564935064935067, 'epoch': 0.45}
  9%|▉         | 140/1540 [05:39<56:37,  2.43s/it]  9%|▉         | 141/1540 [05:41<56:32,  2.43s/it]  9%|▉         | 142/1540 [05:44<56:27,  2.42s/it]  9%|▉         | 143/1540 [05:46<56:23,  2.42s/it]  9%|▉         | 144/1540 [05:48<56:20,  2.42s/it]  9%|▉         | 145/1540 [05:51<56:18,  2.42s/it]  9%|▉         | 146/1540 [05:53<56:14,  2.42s/it] 10%|▉         | 147/1540 [05:56<56:11,  2.42s/it] 10%|▉         | 148/1540 [05:58<56:08,  2.42s/it] 10%|▉         | 149/1540 [06:01<56:07,  2.42s/it] 10%|▉         | 150/1540 [06:03<56:05,  2.42s/it] 10%|▉         | 151/1540 [06:05<56:02,  2.42s/it] 10%|▉         | 152/1540 [06:08<56:13,  2.43s/it] 10%|▉         | 153/1540 [06:10<56:06,  2.43s/it] 10%|█         | 154/1540 [06:13<56:00,  2.42s/it] 10%|█         | 155/1540 [06:15<56:11,  2.43s/it] 10%|█         | 156/1540 [06:18<56:03,  2.43s/it] 10%|█         | 157/1540 [06:20<55:56,  2.43s/it] 10%|█         | 158/1540 [06:22<55:50,  2.42s/it] 10%|█         | 159/1540 [06:25<55:45,  2.42s/it] 10%|█         | 160/1540 [06:27<55:42,  2.42s/it]                                                  {'loss': 0.6269, 'grad_norm': 0.36692097783088684, 'learning_rate': 0.00025811688311688314, 'epoch': 0.52}
 10%|█         | 160/1540 [06:27<55:42,  2.42s/it] 10%|█         | 161/1540 [06:30<55:39,  2.42s/it] 11%|█         | 162/1540 [06:32<55:36,  2.42s/it] 11%|█         | 163/1540 [06:35<55:33,  2.42s/it] 11%|█         | 164/1540 [06:37<55:30,  2.42s/it] 11%|█         | 165/1540 [06:39<55:27,  2.42s/it] 11%|█         | 166/1540 [06:42<55:25,  2.42s/it] 11%|█         | 167/1540 [06:44<55:22,  2.42s/it] 11%|█         | 168/1540 [06:47<55:37,  2.43s/it] 11%|█         | 169/1540 [06:49<55:29,  2.43s/it] 11%|█         | 170/1540 [06:52<55:24,  2.43s/it] 11%|█         | 171/1540 [06:54<55:18,  2.42s/it] 11%|█         | 172/1540 [06:56<55:14,  2.42s/it] 11%|█         | 173/1540 [06:59<55:10,  2.42s/it] 11%|█▏        | 174/1540 [07:01<55:07,  2.42s/it] 11%|█▏        | 175/1540 [07:04<55:04,  2.42s/it] 11%|█▏        | 176/1540 [07:06<55:01,  2.42s/it] 11%|█▏        | 177/1540 [07:08<54:59,  2.42s/it] 12%|█▏        | 178/1540 [07:11<54:57,  2.42s/it] 12%|█▏        | 179/1540 [07:13<54:54,  2.42s/it] 12%|█▏        | 180/1540 [07:16<54:52,  2.42s/it]                                                  {'loss': 0.5816, 'grad_norm': 0.31835511326789856, 'learning_rate': 0.0002905844155844156, 'epoch': 0.58}
 12%|█▏        | 180/1540 [07:16<54:52,  2.42s/it] 12%|█▏        | 181/1540 [07:18<54:59,  2.43s/it] 12%|█▏        | 182/1540 [07:21<54:54,  2.43s/it] 12%|█▏        | 183/1540 [07:23<54:49,  2.42s/it] 12%|█▏        | 184/1540 [07:25<55:02,  2.44s/it] 12%|█▏        | 185/1540 [07:28<54:53,  2.43s/it] 12%|█▏        | 186/1540 [07:30<54:46,  2.43s/it] 12%|█▏        | 187/1540 [07:33<54:41,  2.43s/it] 12%|█▏        | 188/1540 [07:35<54:36,  2.42s/it] 12%|█▏        | 189/1540 [07:38<54:33,  2.42s/it] 12%|█▏        | 190/1540 [07:40<54:29,  2.42s/it] 12%|█▏        | 191/1540 [07:42<54:26,  2.42s/it] 12%|█▏        | 192/1540 [07:45<54:23,  2.42s/it] 13%|█▎        | 193/1540 [07:47<54:20,  2.42s/it] 13%|█▎        | 194/1540 [07:50<54:17,  2.42s/it] 13%|█▎        | 195/1540 [07:52<54:46,  2.44s/it] 13%|█▎        | 196/1540 [07:55<54:34,  2.44s/it] 13%|█▎        | 197/1540 [07:57<54:25,  2.43s/it] 13%|█▎        | 198/1540 [07:59<54:18,  2.43s/it] 13%|█▎        | 199/1540 [08:02<54:26,  2.44s/it] 13%|█▎        | 200/1540 [08:04<54:16,  2.43s/it]                                                  {'loss': 0.59, 'grad_norm': 0.3797699213027954, 'learning_rate': 0.000323051948051948, 'epoch': 0.65}
 13%|█▎        | 200/1540 [08:04<54:16,  2.43s/it] 13%|█▎        | 201/1540 [08:07<54:12,  2.43s/it] 13%|█▎        | 202/1540 [08:09<54:07,  2.43s/it] 13%|█▎        | 203/1540 [08:12<54:02,  2.43s/it] 13%|█▎        | 204/1540 [08:14<53:58,  2.42s/it] 13%|█▎        | 205/1540 [08:16<53:54,  2.42s/it] 13%|█▎        | 206/1540 [08:19<53:50,  2.42s/it] 13%|█▎        | 207/1540 [08:21<53:47,  2.42s/it] 14%|█▎        | 208/1540 [08:24<53:44,  2.42s/it] 14%|█▎        | 209/1540 [08:26<53:56,  2.43s/it] 14%|█▎        | 210/1540 [08:29<53:49,  2.43s/it] 14%|█▎        | 211/1540 [08:31<53:43,  2.43s/it] 14%|█▍        | 212/1540 [08:33<53:38,  2.42s/it] 14%|█▍        | 213/1540 [08:36<53:42,  2.43s/it] 14%|█▍        | 214/1540 [08:38<53:37,  2.43s/it] 14%|█▍        | 215/1540 [08:41<53:32,  2.42s/it] 14%|█▍        | 216/1540 [08:43<53:28,  2.42s/it] 14%|█▍        | 217/1540 [08:46<53:24,  2.42s/it] 14%|█▍        | 218/1540 [08:48<53:21,  2.42s/it] 14%|█▍        | 219/1540 [08:50<53:18,  2.42s/it] 14%|█▍        | 220/1540 [08:53<53:18,  2.42s/it]                                                  {'loss': 0.6099, 'grad_norm': 0.3769649863243103, 'learning_rate': 0.00035551948051948054, 'epoch': 0.71}
 14%|█▍        | 220/1540 [08:53<53:18,  2.42s/it] 14%|█▍        | 221/1540 [08:55<53:15,  2.42s/it] 14%|█▍        | 222/1540 [08:58<53:29,  2.44s/it] 14%|█▍        | 223/1540 [09:00<53:21,  2.43s/it] 15%|█▍        | 224/1540 [09:03<53:14,  2.43s/it] 15%|█▍        | 225/1540 [09:05<53:09,  2.43s/it] 15%|█▍        | 226/1540 [09:07<53:04,  2.42s/it] 15%|█▍        | 227/1540 [09:10<53:00,  2.42s/it] 15%|█▍        | 228/1540 [09:12<53:06,  2.43s/it] 15%|█▍        | 229/1540 [09:15<53:00,  2.43s/it] 15%|█▍        | 230/1540 [09:17<52:55,  2.42s/it] 15%|█▌        | 231/1540 [09:19<52:50,  2.42s/it] 15%|█▌        | 232/1540 [09:22<52:47,  2.42s/it] 15%|█▌        | 233/1540 [09:24<52:44,  2.42s/it] 15%|█▌        | 234/1540 [09:27<52:41,  2.42s/it] 15%|█▌        | 235/1540 [09:29<52:38,  2.42s/it] 15%|█▌        | 236/1540 [09:32<52:35,  2.42s/it] 15%|█▌        | 237/1540 [09:34<52:47,  2.43s/it] 15%|█▌        | 238/1540 [09:36<52:41,  2.43s/it] 16%|█▌        | 239/1540 [09:39<52:35,  2.43s/it] 16%|█▌        | 240/1540 [09:41<52:30,  2.42s/it]                                                  {'loss': 0.5795, 'grad_norm': 0.35368606448173523, 'learning_rate': 0.000387987012987013, 'epoch': 0.78}
 16%|█▌        | 240/1540 [09:41<52:30,  2.42s/it] 16%|█▌        | 241/1540 [09:44<52:27,  2.42s/it] 16%|█▌        | 242/1540 [09:46<52:23,  2.42s/it] 16%|█▌        | 243/1540 [09:49<52:19,  2.42s/it] 16%|█▌        | 244/1540 [09:51<52:17,  2.42s/it] 16%|█▌        | 245/1540 [09:53<52:14,  2.42s/it] 16%|█▌        | 246/1540 [09:56<52:11,  2.42s/it] 16%|█▌        | 247/1540 [09:58<52:09,  2.42s/it] 16%|█▌        | 248/1540 [10:01<52:07,  2.42s/it] 16%|█▌        | 249/1540 [10:03<52:04,  2.42s/it] 16%|█▌        | 250/1540 [10:06<52:15,  2.43s/it] 16%|█▋        | 251/1540 [10:08<52:08,  2.43s/it] 16%|█▋        | 252/1540 [10:10<52:02,  2.42s/it] 16%|█▋        | 253/1540 [10:13<51:58,  2.42s/it] 16%|█▋        | 254/1540 [10:15<51:54,  2.42s/it] 17%|█▋        | 255/1540 [10:18<51:51,  2.42s/it] 17%|█▋        | 256/1540 [10:20<51:48,  2.42s/it] 17%|█▋        | 257/1540 [10:22<51:49,  2.42s/it] 17%|█▋        | 258/1540 [10:25<51:45,  2.42s/it] 17%|█▋        | 259/1540 [10:27<51:42,  2.42s/it] 17%|█▋        | 260/1540 [10:30<51:39,  2.42s/it]                                                  {'loss': 0.6057, 'grad_norm': 0.3642495274543762, 'learning_rate': 0.0004204545454545455, 'epoch': 0.84}
 17%|█▋        | 260/1540 [10:30<51:39,  2.42s/it] 17%|█▋        | 261/1540 [10:32<51:37,  2.42s/it] 17%|█▋        | 262/1540 [10:35<51:34,  2.42s/it] 17%|█▋        | 263/1540 [10:37<51:31,  2.42s/it] 17%|█▋        | 264/1540 [10:39<51:29,  2.42s/it] 17%|█▋        | 265/1540 [10:42<51:54,  2.44s/it] 17%|█▋        | 266/1540 [10:44<51:44,  2.44s/it] 17%|█▋        | 267/1540 [10:47<51:34,  2.43s/it] 17%|█▋        | 268/1540 [10:49<51:29,  2.43s/it] 17%|█▋        | 269/1540 [10:52<51:23,  2.43s/it] 18%|█▊        | 270/1540 [10:54<51:18,  2.42s/it] 18%|█▊        | 271/1540 [10:56<51:17,  2.42s/it] 18%|█▊        | 272/1540 [10:59<51:12,  2.42s/it] 18%|█▊        | 273/1540 [11:01<51:08,  2.42s/it] 18%|█▊        | 274/1540 [11:04<51:05,  2.42s/it] 18%|█▊        | 275/1540 [11:06<51:01,  2.42s/it] 18%|█▊        | 276/1540 [11:09<50:59,  2.42s/it] 18%|█▊        | 277/1540 [11:11<50:56,  2.42s/it] 18%|█▊        | 278/1540 [11:13<51:04,  2.43s/it] 18%|█▊        | 279/1540 [11:16<50:58,  2.43s/it] 18%|█▊        | 280/1540 [11:18<50:54,  2.42s/it]                                                  {'loss': 0.5801, 'grad_norm': 0.2892102599143982, 'learning_rate': 0.00045292207792207794, 'epoch': 0.91}
 18%|█▊        | 280/1540 [11:18<50:54,  2.42s/it] 18%|█▊        | 281/1540 [11:21<50:50,  2.42s/it] 18%|█▊        | 282/1540 [11:23<50:46,  2.42s/it] 18%|█▊        | 283/1540 [11:25<50:43,  2.42s/it] 18%|█▊        | 284/1540 [11:28<50:40,  2.42s/it] 19%|█▊        | 285/1540 [11:30<50:37,  2.42s/it] 19%|█▊        | 286/1540 [11:33<50:35,  2.42s/it] 19%|█▊        | 287/1540 [11:35<50:32,  2.42s/it] 19%|█▊        | 288/1540 [11:38<50:29,  2.42s/it] 19%|█▉        | 289/1540 [11:40<50:27,  2.42s/it] 19%|█▉        | 290/1540 [11:42<50:24,  2.42s/it] 19%|█▉        | 291/1540 [11:45<50:29,  2.43s/it] 19%|█▉        | 292/1540 [11:47<50:24,  2.42s/it] 19%|█▉        | 293/1540 [11:50<50:20,  2.42s/it] 19%|█▉        | 294/1540 [11:52<50:17,  2.42s/it] 19%|█▉        | 295/1540 [11:55<50:14,  2.42s/it] 19%|█▉        | 296/1540 [11:57<50:11,  2.42s/it] 19%|█▉        | 297/1540 [11:59<50:08,  2.42s/it] 19%|█▉        | 298/1540 [12:02<50:06,  2.42s/it] 19%|█▉        | 299/1540 [12:04<50:03,  2.42s/it] 19%|█▉        | 300/1540 [12:07<50:01,  2.42s/it]                                                  {'loss': 0.612, 'grad_norm': 0.3086460530757904, 'learning_rate': 0.00048538961038961035, 'epoch': 0.97}
 19%|█▉        | 300/1540 [12:07<50:01,  2.42s/it] 20%|█▉        | 301/1540 [12:09<49:59,  2.42s/it] 20%|█▉        | 302/1540 [12:11<49:57,  2.42s/it] 20%|█▉        | 303/1540 [12:14<49:54,  2.42s/it] 20%|█▉        | 304/1540 [12:16<49:51,  2.42s/it] 20%|█▉        | 305/1540 [12:19<49:48,  2.42s/it] 20%|█▉        | 306/1540 [12:21<49:46,  2.42s/it] 20%|█▉        | 307/1540 [12:24<49:56,  2.43s/it] 20%|██        | 308/1540 [12:26<49:48,  2.43s/it] 20%|██        | 309/1540 [12:28<49:45,  2.42s/it] 20%|██        | 310/1540 [12:31<49:41,  2.42s/it] 20%|██        | 311/1540 [12:33<49:37,  2.42s/it] 20%|██        | 312/1540 [12:36<49:33,  2.42s/it] 20%|██        | 313/1540 [12:38<49:30,  2.42s/it] 20%|██        | 314/1540 [12:41<49:28,  2.42s/it] 20%|██        | 315/1540 [12:43<49:38,  2.43s/it] 21%|██        | 316/1540 [12:45<49:32,  2.43s/it] 21%|██        | 317/1540 [12:48<49:26,  2.43s/it] 21%|██        | 318/1540 [12:50<49:21,  2.42s/it] 21%|██        | 319/1540 [12:53<49:17,  2.42s/it] 21%|██        | 320/1540 [12:55<49:21,  2.43s/it]                                                  {'loss': 0.5679, 'grad_norm': 0.4061155319213867, 'learning_rate': 0.0004999016565957633, 'epoch': 1.04}
 21%|██        | 320/1540 [12:55<49:21,  2.43s/it] 21%|██        | 321/1540 [12:58<49:17,  2.43s/it] 21%|██        | 322/1540 [13:00<49:12,  2.42s/it] 21%|██        | 323/1540 [13:02<49:08,  2.42s/it] 21%|██        | 324/1540 [13:05<49:05,  2.42s/it] 21%|██        | 325/1540 [13:07<49:01,  2.42s/it] 21%|██        | 326/1540 [13:10<48:58,  2.42s/it] 21%|██        | 327/1540 [13:12<48:56,  2.42s/it] 21%|██▏       | 328/1540 [13:14<48:53,  2.42s/it] 21%|██▏       | 329/1540 [13:17<48:54,  2.42s/it] 21%|██▏       | 330/1540 [13:19<48:50,  2.42s/it] 21%|██▏       | 331/1540 [13:22<48:47,  2.42s/it] 22%|██▏       | 332/1540 [13:24<48:44,  2.42s/it] 22%|██▏       | 333/1540 [13:27<48:42,  2.42s/it] 22%|██▏       | 334/1540 [13:29<48:55,  2.43s/it] 22%|██▏       | 335/1540 [13:31<48:48,  2.43s/it] 22%|██▏       | 336/1540 [13:34<48:42,  2.43s/it] 22%|██▏       | 337/1540 [13:36<48:37,  2.42s/it] 22%|██▏       | 338/1540 [13:39<48:33,  2.42s/it] 22%|██▏       | 339/1540 [13:41<48:29,  2.42s/it] 22%|██▏       | 340/1540 [13:44<48:25,  2.42s/it]                                                  {'loss': 0.5416, 'grad_norm': 0.342615008354187, 'learning_rate': 0.0004992192975102804, 'epoch': 1.1}
 22%|██▏       | 340/1540 [13:44<48:25,  2.42s/it] 22%|██▏       | 341/1540 [13:46<48:25,  2.42s/it] 22%|██▏       | 342/1540 [13:48<48:21,  2.42s/it] 22%|██▏       | 343/1540 [13:51<48:17,  2.42s/it] 22%|██▏       | 344/1540 [13:53<48:15,  2.42s/it] 22%|██▏       | 345/1540 [13:56<48:12,  2.42s/it] 22%|██▏       | 346/1540 [13:58<48:09,  2.42s/it] 23%|██▎       | 347/1540 [14:01<48:06,  2.42s/it] 23%|██▎       | 348/1540 [14:03<48:15,  2.43s/it] 23%|██▎       | 349/1540 [14:05<48:10,  2.43s/it] 23%|██▎       | 350/1540 [14:08<48:06,  2.43s/it] 23%|██▎       | 351/1540 [14:10<48:01,  2.42s/it] 23%|██▎       | 352/1540 [14:13<47:58,  2.42s/it] 23%|██▎       | 353/1540 [14:15<47:54,  2.42s/it] 23%|██▎       | 354/1540 [14:18<47:52,  2.42s/it] 23%|██▎       | 355/1540 [14:20<47:48,  2.42s/it] 23%|██▎       | 356/1540 [14:22<47:45,  2.42s/it] 23%|██▎       | 357/1540 [14:25<47:43,  2.42s/it] 23%|██▎       | 358/1540 [14:27<47:50,  2.43s/it] 23%|██▎       | 359/1540 [14:30<47:45,  2.43s/it] 23%|██▎       | 360/1540 [14:32<47:41,  2.42s/it]                                                  {'loss': 0.4921, 'grad_norm': 0.2507372200489044, 'learning_rate': 0.0004978888625516589, 'epoch': 1.17}
 23%|██▎       | 360/1540 [14:32<47:41,  2.42s/it] 23%|██▎       | 361/1540 [14:35<47:46,  2.43s/it] 24%|██▎       | 362/1540 [14:37<47:40,  2.43s/it] 24%|██▎       | 363/1540 [14:39<47:34,  2.43s/it] 24%|██▎       | 364/1540 [14:42<47:30,  2.42s/it] 24%|██▎       | 365/1540 [14:44<47:26,  2.42s/it] 24%|██▍       | 366/1540 [14:47<47:23,  2.42s/it] 24%|██▍       | 367/1540 [14:49<47:20,  2.42s/it] 24%|██▍       | 368/1540 [14:51<47:17,  2.42s/it] 24%|██▍       | 369/1540 [14:54<47:15,  2.42s/it] 24%|██▍       | 370/1540 [14:56<47:12,  2.42s/it] 24%|██▍       | 371/1540 [14:59<47:09,  2.42s/it] 24%|██▍       | 372/1540 [15:01<47:07,  2.42s/it] 24%|██▍       | 373/1540 [15:04<47:18,  2.43s/it] 24%|██▍       | 374/1540 [15:06<47:11,  2.43s/it] 24%|██▍       | 375/1540 [15:08<47:06,  2.43s/it] 24%|██▍       | 376/1540 [15:11<47:16,  2.44s/it] 24%|██▍       | 377/1540 [15:13<47:08,  2.43s/it] 25%|██▍       | 378/1540 [15:16<47:02,  2.43s/it] 25%|██▍       | 379/1540 [15:18<46:57,  2.43s/it] 25%|██▍       | 380/1540 [15:21<46:53,  2.43s/it]                                                  {'loss': 0.5402, 'grad_norm': 0.357341468334198, 'learning_rate': 0.0004959138114150592, 'epoch': 1.23}
 25%|██▍       | 380/1540 [15:21<46:53,  2.43s/it] 25%|██▍       | 381/1540 [15:23<46:49,  2.42s/it] 25%|██▍       | 382/1540 [15:25<46:45,  2.42s/it] 25%|██▍       | 383/1540 [15:28<46:42,  2.42s/it] 25%|██▍       | 384/1540 [15:30<46:38,  2.42s/it] 25%|██▌       | 385/1540 [15:33<46:35,  2.42s/it] 25%|██▌       | 386/1540 [15:35<46:33,  2.42s/it] 25%|██▌       | 387/1540 [15:38<46:43,  2.43s/it] 25%|██▌       | 388/1540 [15:40<46:38,  2.43s/it] 25%|██▌       | 389/1540 [15:42<46:33,  2.43s/it] 25%|██▌       | 390/1540 [15:45<46:28,  2.42s/it] 25%|██▌       | 391/1540 [15:47<46:24,  2.42s/it] 25%|██▌       | 392/1540 [15:50<46:20,  2.42s/it] 26%|██▌       | 393/1540 [15:52<46:17,  2.42s/it] 26%|██▌       | 394/1540 [15:54<46:14,  2.42s/it] 26%|██▌       | 395/1540 [15:57<46:11,  2.42s/it] 26%|██▌       | 396/1540 [15:59<46:09,  2.42s/it] 26%|██▌       | 397/1540 [16:02<46:06,  2.42s/it] 26%|██▌       | 398/1540 [16:04<46:03,  2.42s/it] 26%|██▌       | 399/1540 [16:07<46:01,  2.42s/it] 26%|██▌       | 400/1540 [16:09<45:58,  2.42s/it]                                                  {'loss': 0.5102, 'grad_norm': 0.415126770734787, 'learning_rate': 0.0004932992800711009, 'epoch': 1.3}
 26%|██▌       | 400/1540 [16:09<45:58,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 03:53:45,198 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-400
[INFO|configuration_utils.py:726] 2024-05-25 03:53:46,217 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 03:53:46,219 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 03:53:46,303 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 03:53:46,303 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-400/special_tokens_map.json
 26%|██▌       | 401/1540 [16:13<53:05,  2.80s/it] 26%|██▌       | 402/1540 [16:15<50:53,  2.68s/it] 26%|██▌       | 403/1540 [16:18<49:20,  2.60s/it] 26%|██▌       | 404/1540 [16:20<48:32,  2.56s/it] 26%|██▋       | 405/1540 [16:22<47:42,  2.52s/it] 26%|██▋       | 406/1540 [16:25<47:04,  2.49s/it] 26%|██▋       | 407/1540 [16:27<46:37,  2.47s/it] 26%|██▋       | 408/1540 [16:30<46:18,  2.45s/it] 27%|██▋       | 409/1540 [16:32<46:04,  2.44s/it] 27%|██▋       | 410/1540 [16:35<45:53,  2.44s/it] 27%|██▋       | 411/1540 [16:37<45:44,  2.43s/it] 27%|██▋       | 412/1540 [16:39<45:38,  2.43s/it] 27%|██▋       | 413/1540 [16:42<45:33,  2.43s/it] 27%|██▋       | 414/1540 [16:44<45:28,  2.42s/it] 27%|██▋       | 415/1540 [16:47<46:09,  2.46s/it] 27%|██▋       | 416/1540 [16:49<45:51,  2.45s/it] 27%|██▋       | 417/1540 [16:52<45:48,  2.45s/it] 27%|██▋       | 418/1540 [16:54<45:36,  2.44s/it] 27%|██▋       | 419/1540 [16:56<45:27,  2.43s/it] 27%|██▋       | 420/1540 [16:59<45:20,  2.43s/it]                                                  {'loss': 0.5473, 'grad_norm': 0.31555017828941345, 'learning_rate': 0.0004900520674101607, 'epoch': 1.36}
 27%|██▋       | 420/1540 [16:59<45:20,  2.43s/it] 27%|██▋       | 421/1540 [17:01<45:15,  2.43s/it] 27%|██▋       | 422/1540 [17:04<45:10,  2.42s/it] 27%|██▋       | 423/1540 [17:06<45:06,  2.42s/it] 28%|██▊       | 424/1540 [17:09<45:02,  2.42s/it] 28%|██▊       | 425/1540 [17:11<44:59,  2.42s/it] 28%|██▊       | 426/1540 [17:13<44:57,  2.42s/it] 28%|██▊       | 427/1540 [17:16<44:54,  2.42s/it] 28%|██▊       | 428/1540 [17:18<44:51,  2.42s/it] 28%|██▊       | 429/1540 [17:21<44:48,  2.42s/it] 28%|██▊       | 430/1540 [17:23<44:57,  2.43s/it] 28%|██▊       | 431/1540 [17:26<44:51,  2.43s/it] 28%|██▊       | 432/1540 [17:28<44:46,  2.42s/it] 28%|██▊       | 433/1540 [17:30<44:42,  2.42s/it] 28%|██▊       | 434/1540 [17:33<44:38,  2.42s/it] 28%|██▊       | 435/1540 [17:35<44:35,  2.42s/it] 28%|██▊       | 436/1540 [17:38<44:32,  2.42s/it] 28%|██▊       | 437/1540 [17:40<44:29,  2.42s/it] 28%|██▊       | 438/1540 [17:42<44:26,  2.42s/it] 29%|██▊       | 439/1540 [17:45<44:24,  2.42s/it] 29%|██▊       | 440/1540 [17:47<44:21,  2.42s/it]                                                  {'loss': 0.5006, 'grad_norm': 0.36640074849128723, 'learning_rate': 0.0004861806175623745, 'epoch': 1.43}
 29%|██▊       | 440/1540 [17:47<44:21,  2.42s/it] 29%|██▊       | 441/1540 [17:50<44:19,  2.42s/it] 29%|██▊       | 442/1540 [17:52<44:16,  2.42s/it] 29%|██▉       | 443/1540 [17:55<44:14,  2.42s/it] 29%|██▉       | 444/1540 [17:57<44:11,  2.42s/it] 29%|██▉       | 445/1540 [17:59<44:09,  2.42s/it] 29%|██▉       | 446/1540 [18:02<44:12,  2.42s/it] 29%|██▉       | 447/1540 [18:04<44:08,  2.42s/it] 29%|██▉       | 448/1540 [18:07<44:04,  2.42s/it] 29%|██▉       | 449/1540 [18:09<44:01,  2.42s/it] 29%|██▉       | 450/1540 [18:12<43:58,  2.42s/it] 29%|██▉       | 451/1540 [18:14<43:55,  2.42s/it] 29%|██▉       | 452/1540 [18:16<43:53,  2.42s/it] 29%|██▉       | 453/1540 [18:19<43:50,  2.42s/it] 29%|██▉       | 454/1540 [18:21<43:47,  2.42s/it] 30%|██▉       | 455/1540 [18:24<43:44,  2.42s/it] 30%|██▉       | 456/1540 [18:26<43:42,  2.42s/it] 30%|██▉       | 457/1540 [18:28<43:40,  2.42s/it] 30%|██▉       | 458/1540 [18:31<43:39,  2.42s/it] 30%|██▉       | 459/1540 [18:33<43:41,  2.43s/it] 30%|██▉       | 460/1540 [18:36<43:37,  2.42s/it]                                                  {'loss': 0.539, 'grad_norm': 0.3739998936653137, 'learning_rate': 0.0004816949979393171, 'epoch': 1.49}
 30%|██▉       | 460/1540 [18:36<43:37,  2.42s/it] 30%|██▉       | 461/1540 [18:38<43:34,  2.42s/it] 30%|███       | 462/1540 [18:41<43:31,  2.42s/it] 30%|███       | 463/1540 [18:43<43:27,  2.42s/it] 30%|███       | 464/1540 [18:45<43:25,  2.42s/it] 30%|███       | 465/1540 [18:48<43:21,  2.42s/it] 30%|███       | 466/1540 [18:50<43:19,  2.42s/it] 30%|███       | 467/1540 [18:53<43:16,  2.42s/it] 30%|███       | 468/1540 [18:55<43:13,  2.42s/it] 30%|███       | 469/1540 [18:58<43:11,  2.42s/it] 31%|███       | 470/1540 [19:00<43:08,  2.42s/it] 31%|███       | 471/1540 [19:02<43:06,  2.42s/it] 31%|███       | 472/1540 [19:05<43:12,  2.43s/it] 31%|███       | 473/1540 [19:07<43:19,  2.44s/it] 31%|███       | 474/1540 [19:10<43:11,  2.43s/it] 31%|███       | 475/1540 [19:12<43:05,  2.43s/it] 31%|███       | 476/1540 [19:14<43:00,  2.42s/it] 31%|███       | 477/1540 [19:17<42:56,  2.42s/it] 31%|███       | 478/1540 [19:19<42:52,  2.42s/it] 31%|███       | 479/1540 [19:22<42:48,  2.42s/it] 31%|███       | 480/1540 [19:24<42:45,  2.42s/it]                                                  {'loss': 0.5485, 'grad_norm': 0.37106403708457947, 'learning_rate': 0.00047660687305446235, 'epoch': 1.56}
 31%|███       | 480/1540 [19:24<42:45,  2.42s/it] 31%|███       | 481/1540 [19:27<42:43,  2.42s/it] 31%|███▏      | 482/1540 [19:29<42:40,  2.42s/it] 31%|███▏      | 483/1540 [19:31<42:37,  2.42s/it] 31%|███▏      | 484/1540 [19:34<42:34,  2.42s/it] 31%|███▏      | 485/1540 [19:36<42:32,  2.42s/it] 32%|███▏      | 486/1540 [19:39<42:29,  2.42s/it] 32%|███▏      | 487/1540 [19:41<42:36,  2.43s/it] 32%|███▏      | 488/1540 [19:44<42:31,  2.42s/it] 32%|███▏      | 489/1540 [19:46<42:27,  2.42s/it] 32%|███▏      | 490/1540 [19:48<42:23,  2.42s/it] 32%|███▏      | 491/1540 [19:51<42:19,  2.42s/it] 32%|███▏      | 492/1540 [19:53<42:17,  2.42s/it] 32%|███▏      | 493/1540 [19:56<42:13,  2.42s/it] 32%|███▏      | 494/1540 [19:58<42:11,  2.42s/it] 32%|███▏      | 495/1540 [20:00<42:08,  2.42s/it] 32%|███▏      | 496/1540 [20:03<42:06,  2.42s/it] 32%|███▏      | 497/1540 [20:05<42:04,  2.42s/it] 32%|███▏      | 498/1540 [20:08<42:01,  2.42s/it] 32%|███▏      | 499/1540 [20:10<41:59,  2.42s/it] 32%|███▏      | 500/1540 [20:13<41:56,  2.42s/it]                                                  {'loss': 0.5425, 'grad_norm': nan, 'learning_rate': 0.0004712271175980134, 'epoch': 1.62}
 32%|███▏      | 500/1540 [20:13<41:56,  2.42s/it] 33%|███▎      | 501/1540 [20:15<42:02,  2.43s/it] 33%|███▎      | 502/1540 [20:17<41:56,  2.42s/it] 33%|███▎      | 503/1540 [20:20<41:52,  2.42s/it] 33%|███▎      | 504/1540 [20:22<41:49,  2.42s/it] 33%|███▎      | 505/1540 [20:25<41:45,  2.42s/it] 33%|███▎      | 506/1540 [20:27<41:42,  2.42s/it] 33%|███▎      | 507/1540 [20:30<41:39,  2.42s/it] 33%|███▎      | 508/1540 [20:32<41:37,  2.42s/it] 33%|███▎      | 509/1540 [20:34<41:34,  2.42s/it] 33%|███▎      | 510/1540 [20:37<41:31,  2.42s/it] 33%|███▎      | 511/1540 [20:39<41:29,  2.42s/it] 33%|███▎      | 512/1540 [20:42<41:26,  2.42s/it] 33%|███▎      | 513/1540 [20:44<41:24,  2.42s/it] 33%|███▎      | 514/1540 [20:46<41:22,  2.42s/it] 33%|███▎      | 515/1540 [20:49<41:25,  2.43s/it] 34%|███▎      | 516/1540 [20:51<41:21,  2.42s/it] 34%|███▎      | 517/1540 [20:54<41:17,  2.42s/it] 34%|███▎      | 518/1540 [20:56<41:14,  2.42s/it] 34%|███▎      | 519/1540 [20:59<41:11,  2.42s/it] 34%|███▍      | 520/1540 [21:01<41:09,  2.42s/it]                                                  {'loss': 0.4715, 'grad_norm': 0.349041223526001, 'learning_rate': 0.0004650035600519251, 'epoch': 1.69}
 34%|███▍      | 520/1540 [21:01<41:09,  2.42s/it] 34%|███▍      | 521/1540 [21:03<41:06,  2.42s/it] 34%|███▍      | 522/1540 [21:06<41:03,  2.42s/it] 34%|███▍      | 523/1540 [21:08<41:00,  2.42s/it] 34%|███▍      | 524/1540 [21:11<40:57,  2.42s/it] 34%|███▍      | 525/1540 [21:13<40:55,  2.42s/it] 34%|███▍      | 526/1540 [21:16<40:53,  2.42s/it] 34%|███▍      | 527/1540 [21:18<40:51,  2.42s/it] 34%|███▍      | 528/1540 [21:20<40:51,  2.42s/it] 34%|███▍      | 529/1540 [21:23<40:48,  2.42s/it] 34%|███▍      | 530/1540 [21:25<40:44,  2.42s/it] 34%|███▍      | 531/1540 [21:28<40:41,  2.42s/it] 35%|███▍      | 532/1540 [21:30<40:39,  2.42s/it] 35%|███▍      | 533/1540 [21:32<40:36,  2.42s/it] 35%|███▍      | 534/1540 [21:35<40:33,  2.42s/it] 35%|███▍      | 535/1540 [21:37<40:31,  2.42s/it] 35%|███▍      | 536/1540 [21:40<40:28,  2.42s/it] 35%|███▍      | 537/1540 [21:42<40:26,  2.42s/it] 35%|███▍      | 538/1540 [21:45<40:23,  2.42s/it] 35%|███▌      | 539/1540 [21:47<40:23,  2.42s/it] 35%|███▌      | 540/1540 [21:49<40:21,  2.42s/it]                                                  {'loss': 0.5137, 'grad_norm': 0.378047913312912, 'learning_rate': 0.0004582209020617679, 'epoch': 1.75}
 35%|███▌      | 540/1540 [21:49<40:21,  2.42s/it] 35%|███▌      | 541/1540 [21:52<40:19,  2.42s/it] 35%|███▌      | 542/1540 [21:54<40:16,  2.42s/it] 35%|███▌      | 543/1540 [21:57<40:22,  2.43s/it] 35%|███▌      | 544/1540 [21:59<40:17,  2.43s/it] 35%|███▌      | 545/1540 [22:02<40:12,  2.42s/it] 35%|███▌      | 546/1540 [22:04<40:08,  2.42s/it] 36%|███▌      | 547/1540 [22:06<40:04,  2.42s/it] 36%|███▌      | 548/1540 [22:09<40:01,  2.42s/it] 36%|███▌      | 549/1540 [22:11<39:58,  2.42s/it] 36%|███▌      | 550/1540 [22:14<39:55,  2.42s/it] 36%|███▌      | 551/1540 [22:16<39:53,  2.42s/it] 36%|███▌      | 552/1540 [22:18<39:51,  2.42s/it] 36%|███▌      | 553/1540 [22:21<39:48,  2.42s/it] 36%|███▌      | 554/1540 [22:23<39:45,  2.42s/it] 36%|███▌      | 555/1540 [22:26<39:43,  2.42s/it] 36%|███▌      | 556/1540 [22:28<39:48,  2.43s/it] 36%|███▌      | 557/1540 [22:31<39:43,  2.42s/it] 36%|███▌      | 558/1540 [22:33<39:39,  2.42s/it] 36%|███▋      | 559/1540 [22:35<39:46,  2.43s/it] 36%|███▋      | 560/1540 [22:38<39:39,  2.43s/it]                                                  {'loss': 0.465, 'grad_norm': 0.3873286843299866, 'learning_rate': 0.0004508967814149967, 'epoch': 1.82}
 36%|███▋      | 560/1540 [22:38<39:39,  2.43s/it] 36%|███▋      | 561/1540 [22:40<39:36,  2.43s/it] 36%|███▋      | 562/1540 [22:43<39:31,  2.42s/it] 37%|███▋      | 563/1540 [22:45<39:27,  2.42s/it] 37%|███▋      | 564/1540 [22:48<39:24,  2.42s/it] 37%|███▋      | 565/1540 [22:50<39:21,  2.42s/it] 37%|███▋      | 566/1540 [22:52<39:18,  2.42s/it] 37%|███▋      | 567/1540 [22:55<39:14,  2.42s/it] 37%|███▋      | 568/1540 [22:57<39:12,  2.42s/it] 37%|███▋      | 569/1540 [23:00<39:13,  2.42s/it] 37%|███▋      | 570/1540 [23:02<39:09,  2.42s/it] 37%|███▋      | 571/1540 [23:05<39:06,  2.42s/it] 37%|███▋      | 572/1540 [23:07<39:03,  2.42s/it] 37%|███▋      | 573/1540 [23:09<38:59,  2.42s/it] 37%|███▋      | 574/1540 [23:12<38:57,  2.42s/it] 37%|███▋      | 575/1540 [23:14<38:54,  2.42s/it] 37%|███▋      | 576/1540 [23:17<38:51,  2.42s/it] 37%|███▋      | 577/1540 [23:19<38:49,  2.42s/it] 38%|███▊      | 578/1540 [23:21<38:46,  2.42s/it] 38%|███▊      | 579/1540 [23:24<38:44,  2.42s/it] 38%|███▊      | 580/1540 [23:26<38:41,  2.42s/it]                                                  {'loss': 0.4989, 'grad_norm': 0.36340969800949097, 'learning_rate': 0.0004430502439316204, 'epoch': 1.88}
 38%|███▊      | 580/1540 [23:26<38:41,  2.42s/it] 38%|███▊      | 581/1540 [23:29<38:40,  2.42s/it] 38%|███▊      | 582/1540 [23:31<38:37,  2.42s/it] 38%|███▊      | 583/1540 [23:34<38:35,  2.42s/it] 38%|███▊      | 584/1540 [23:36<38:33,  2.42s/it] 38%|███▊      | 585/1540 [23:38<38:32,  2.42s/it] 38%|███▊      | 586/1540 [23:41<38:29,  2.42s/it] 38%|███▊      | 587/1540 [23:43<38:26,  2.42s/it] 38%|███▊      | 588/1540 [23:46<38:24,  2.42s/it] 38%|███▊      | 589/1540 [23:48<38:21,  2.42s/it] 38%|███▊      | 590/1540 [23:51<38:18,  2.42s/it] 38%|███▊      | 591/1540 [23:53<38:16,  2.42s/it] 38%|███▊      | 592/1540 [23:55<38:13,  2.42s/it] 39%|███▊      | 593/1540 [23:58<38:10,  2.42s/it] 39%|███▊      | 594/1540 [24:00<38:08,  2.42s/it] 39%|███▊      | 595/1540 [24:03<38:06,  2.42s/it] 39%|███▊      | 596/1540 [24:05<38:03,  2.42s/it] 39%|███▉      | 597/1540 [24:07<38:01,  2.42s/it] 39%|███▉      | 598/1540 [24:10<38:03,  2.42s/it] 39%|███▉      | 599/1540 [24:12<37:59,  2.42s/it] 39%|███▉      | 600/1540 [24:15<37:56,  2.42s/it]                                                  {'loss': 0.5333, 'grad_norm': 0.35570234060287476, 'learning_rate': 0.000434701693936992, 'epoch': 1.95}
 39%|███▉      | 600/1540 [24:15<37:56,  2.42s/it] 39%|███▉      | 601/1540 [24:17<37:53,  2.42s/it] 39%|███▉      | 602/1540 [24:20<37:50,  2.42s/it] 39%|███▉      | 603/1540 [24:22<37:47,  2.42s/it] 39%|███▉      | 604/1540 [24:24<37:44,  2.42s/it] 39%|███▉      | 605/1540 [24:27<37:42,  2.42s/it] 39%|███▉      | 606/1540 [24:29<37:39,  2.42s/it] 39%|███▉      | 607/1540 [24:32<37:36,  2.42s/it] 39%|███▉      | 608/1540 [24:34<37:34,  2.42s/it] 40%|███▉      | 609/1540 [24:36<37:32,  2.42s/it] 40%|███▉      | 610/1540 [24:39<37:29,  2.42s/it] 40%|███▉      | 611/1540 [24:41<37:27,  2.42s/it] 40%|███▉      | 612/1540 [24:44<37:35,  2.43s/it] 40%|███▉      | 613/1540 [24:46<37:29,  2.43s/it] 40%|███▉      | 614/1540 [24:49<37:25,  2.42s/it] 40%|███▉      | 615/1540 [24:51<37:21,  2.42s/it] 40%|████      | 616/1540 [24:53<37:18,  2.42s/it] 40%|████      | 617/1540 [24:56<37:16,  2.42s/it] 40%|████      | 618/1540 [24:58<37:12,  2.42s/it] 40%|████      | 619/1540 [25:01<37:09,  2.42s/it] 40%|████      | 620/1540 [25:03<37:07,  2.42s/it]                                                  {'loss': 0.4546, 'grad_norm': 0.3362458050251007, 'learning_rate': 0.00042587284120190896, 'epoch': 2.01}
 40%|████      | 620/1540 [25:03<37:07,  2.42s/it] 40%|████      | 621/1540 [25:06<37:04,  2.42s/it] 40%|████      | 622/1540 [25:08<37:01,  2.42s/it] 40%|████      | 623/1540 [25:10<36:58,  2.42s/it] 41%|████      | 624/1540 [25:13<36:56,  2.42s/it] 41%|████      | 625/1540 [25:15<36:53,  2.42s/it] 41%|████      | 626/1540 [25:18<36:56,  2.42s/it] 41%|████      | 627/1540 [25:20<36:52,  2.42s/it] 41%|████      | 628/1540 [25:23<36:48,  2.42s/it] 41%|████      | 629/1540 [25:25<36:45,  2.42s/it] 41%|████      | 630/1540 [25:27<36:42,  2.42s/it] 41%|████      | 631/1540 [25:30<36:39,  2.42s/it] 41%|████      | 632/1540 [25:32<36:37,  2.42s/it] 41%|████      | 633/1540 [25:35<36:35,  2.42s/it] 41%|████      | 634/1540 [25:37<36:32,  2.42s/it] 41%|████      | 635/1540 [25:39<36:29,  2.42s/it] 41%|████▏     | 636/1540 [25:42<36:27,  2.42s/it] 41%|████▏     | 637/1540 [25:44<36:24,  2.42s/it] 41%|████▏     | 638/1540 [25:47<36:22,  2.42s/it] 41%|████▏     | 639/1540 [25:49<36:20,  2.42s/it] 42%|████▏     | 640/1540 [25:52<36:17,  2.42s/it]                                                  {'loss': 0.4309, 'grad_norm': 0.3581311106681824, 'learning_rate': 0.000416586644488001, 'epoch': 2.08}
 42%|████▏     | 640/1540 [25:52<36:17,  2.42s/it] 42%|████▏     | 641/1540 [25:54<36:15,  2.42s/it] 42%|████▏     | 642/1540 [25:56<36:12,  2.42s/it] 42%|████▏     | 643/1540 [25:59<36:10,  2.42s/it] 42%|████▏     | 644/1540 [26:01<36:07,  2.42s/it] 42%|████▏     | 645/1540 [26:04<36:05,  2.42s/it] 42%|████▏     | 646/1540 [26:06<36:06,  2.42s/it] 42%|████▏     | 647/1540 [26:08<36:02,  2.42s/it] 42%|████▏     | 648/1540 [26:11<35:59,  2.42s/it] 42%|████▏     | 649/1540 [26:13<35:57,  2.42s/it] 42%|████▏     | 650/1540 [26:16<35:54,  2.42s/it] 42%|████▏     | 651/1540 [26:18<35:51,  2.42s/it] 42%|████▏     | 652/1540 [26:21<35:48,  2.42s/it] 42%|████▏     | 653/1540 [26:23<35:45,  2.42s/it] 42%|████▏     | 654/1540 [26:25<35:45,  2.42s/it] 43%|████▎     | 655/1540 [26:28<35:42,  2.42s/it] 43%|████▎     | 656/1540 [26:30<35:39,  2.42s/it] 43%|████▎     | 657/1540 [26:33<35:37,  2.42s/it] 43%|████▎     | 658/1540 [26:35<35:34,  2.42s/it] 43%|████▎     | 659/1540 [26:38<35:31,  2.42s/it] 43%|████▎     | 660/1540 [26:40<35:29,  2.42s/it]                                                  {'loss': 0.4508, 'grad_norm': 0.34593650698661804, 'learning_rate': 0.000406867251845213, 'epoch': 2.14}
 43%|████▎     | 660/1540 [26:40<35:29,  2.42s/it] 43%|████▎     | 661/1540 [26:42<35:27,  2.42s/it] 43%|████▎     | 662/1540 [26:45<35:24,  2.42s/it] 43%|████▎     | 663/1540 [26:47<35:22,  2.42s/it] 43%|████▎     | 664/1540 [26:50<35:19,  2.42s/it] 43%|████▎     | 665/1540 [26:52<35:16,  2.42s/it] 43%|████▎     | 666/1540 [26:54<35:14,  2.42s/it] 43%|████▎     | 667/1540 [26:57<35:18,  2.43s/it] 43%|████▎     | 668/1540 [26:59<35:14,  2.42s/it] 43%|████▎     | 669/1540 [27:02<35:10,  2.42s/it] 44%|████▎     | 670/1540 [27:04<35:06,  2.42s/it] 44%|████▎     | 671/1540 [27:07<35:03,  2.42s/it] 44%|████▎     | 672/1540 [27:09<35:00,  2.42s/it] 44%|████▎     | 673/1540 [27:11<34:58,  2.42s/it] 44%|████▍     | 674/1540 [27:14<34:55,  2.42s/it] 44%|████▍     | 675/1540 [27:16<34:53,  2.42s/it] 44%|████▍     | 676/1540 [27:19<34:50,  2.42s/it] 44%|████▍     | 677/1540 [27:21<34:47,  2.42s/it] 44%|████▍     | 678/1540 [27:24<34:45,  2.42s/it] 44%|████▍     | 679/1540 [27:26<34:42,  2.42s/it] 44%|████▍     | 680/1540 [27:28<34:40,  2.42s/it]                                                  {'loss': 0.4242, 'grad_norm': 0.33592841029167175, 'learning_rate': 0.0003967399378166333, 'epoch': 2.21}
 44%|████▍     | 680/1540 [27:28<34:40,  2.42s/it] 44%|████▍     | 681/1540 [27:31<34:38,  2.42s/it] 44%|████▍     | 682/1540 [27:33<34:49,  2.44s/it] 44%|████▍     | 683/1540 [27:36<34:43,  2.43s/it] 44%|████▍     | 684/1540 [27:38<34:37,  2.43s/it] 44%|████▍     | 685/1540 [27:41<34:32,  2.42s/it] 45%|████▍     | 686/1540 [27:43<34:28,  2.42s/it] 45%|████▍     | 687/1540 [27:45<34:25,  2.42s/it] 45%|████▍     | 688/1540 [27:48<34:23,  2.42s/it] 45%|████▍     | 689/1540 [27:50<34:20,  2.42s/it] 45%|████▍     | 690/1540 [27:53<34:17,  2.42s/it] 45%|████▍     | 691/1540 [27:55<34:14,  2.42s/it] 45%|████▍     | 692/1540 [27:57<34:12,  2.42s/it] 45%|████▌     | 693/1540 [28:00<34:10,  2.42s/it] 45%|████▌     | 694/1540 [28:02<34:07,  2.42s/it] 45%|████▌     | 695/1540 [28:05<34:06,  2.42s/it] 45%|████▌     | 696/1540 [28:07<34:03,  2.42s/it] 45%|████▌     | 697/1540 [28:10<34:00,  2.42s/it] 45%|████▌     | 698/1540 [28:12<33:57,  2.42s/it] 45%|████▌     | 699/1540 [28:14<33:54,  2.42s/it] 45%|████▌     | 700/1540 [28:17<33:52,  2.42s/it]                                                  {'loss': 0.3885, 'grad_norm': 0.3707355260848999, 'learning_rate': 0.00038623103771396195, 'epoch': 2.27}
 45%|████▌     | 700/1540 [28:17<33:52,  2.42s/it] 46%|████▌     | 701/1540 [28:19<33:50,  2.42s/it] 46%|████▌     | 702/1540 [28:22<33:47,  2.42s/it] 46%|████▌     | 703/1540 [28:24<33:45,  2.42s/it] 46%|████▌     | 704/1540 [28:26<33:42,  2.42s/it] 46%|████▌     | 705/1540 [28:29<33:40,  2.42s/it] 46%|████▌     | 706/1540 [28:31<33:37,  2.42s/it] 46%|████▌     | 707/1540 [28:34<33:35,  2.42s/it] 46%|████▌     | 708/1540 [28:36<33:35,  2.42s/it] 46%|████▌     | 709/1540 [28:39<33:32,  2.42s/it] 46%|████▌     | 710/1540 [28:41<33:28,  2.42s/it] 46%|████▌     | 711/1540 [28:43<33:26,  2.42s/it] 46%|████▌     | 712/1540 [28:46<33:23,  2.42s/it] 46%|████▋     | 713/1540 [28:48<33:20,  2.42s/it] 46%|████▋     | 714/1540 [28:51<33:18,  2.42s/it] 46%|████▋     | 715/1540 [28:53<33:16,  2.42s/it] 46%|████▋     | 716/1540 [28:56<33:13,  2.42s/it] 47%|████▋     | 717/1540 [28:58<33:11,  2.42s/it] 47%|████▋     | 718/1540 [29:00<33:08,  2.42s/it] 47%|████▋     | 719/1540 [29:03<33:06,  2.42s/it] 47%|████▋     | 720/1540 [29:05<33:08,  2.43s/it]                                                  {'loss': 0.3994, 'grad_norm': 0.35098645091056824, 'learning_rate': 0.00037536787913453106, 'epoch': 2.34}
 47%|████▋     | 720/1540 [29:05<33:08,  2.43s/it] 47%|████▋     | 721/1540 [29:08<33:05,  2.42s/it] 47%|████▋     | 722/1540 [29:10<33:01,  2.42s/it] 47%|████▋     | 723/1540 [29:12<32:58,  2.42s/it] 47%|████▋     | 724/1540 [29:15<32:57,  2.42s/it] 47%|████▋     | 725/1540 [29:17<32:55,  2.42s/it] 47%|████▋     | 726/1540 [29:20<32:51,  2.42s/it] 47%|████▋     | 727/1540 [29:22<32:48,  2.42s/it] 47%|████▋     | 728/1540 [29:25<32:45,  2.42s/it] 47%|████▋     | 729/1540 [29:27<32:42,  2.42s/it] 47%|████▋     | 730/1540 [29:29<32:40,  2.42s/it] 47%|████▋     | 731/1540 [29:32<32:37,  2.42s/it] 48%|████▊     | 732/1540 [29:34<32:34,  2.42s/it] 48%|████▊     | 733/1540 [29:37<32:33,  2.42s/it] 48%|████▊     | 734/1540 [29:39<32:30,  2.42s/it] 48%|████▊     | 735/1540 [29:42<32:27,  2.42s/it] 48%|████▊     | 736/1540 [29:44<32:25,  2.42s/it] 48%|████▊     | 737/1540 [29:46<32:28,  2.43s/it] 48%|████▊     | 738/1540 [29:49<32:24,  2.42s/it] 48%|████▊     | 739/1540 [29:51<32:20,  2.42s/it] 48%|████▊     | 740/1540 [29:54<32:17,  2.42s/it]                                                  {'loss': 0.4125, 'grad_norm': 0.33518269658088684, 'learning_rate': 0.0003641787108979617, 'epoch': 2.4}
 48%|████▊     | 740/1540 [29:54<32:17,  2.42s/it] 48%|████▊     | 741/1540 [29:56<32:14,  2.42s/it] 48%|████▊     | 742/1540 [29:58<32:12,  2.42s/it] 48%|████▊     | 743/1540 [30:01<32:09,  2.42s/it] 48%|████▊     | 744/1540 [30:03<32:06,  2.42s/it] 48%|████▊     | 745/1540 [30:06<32:03,  2.42s/it] 48%|████▊     | 746/1540 [30:08<32:00,  2.42s/it] 49%|████▊     | 747/1540 [30:11<31:58,  2.42s/it] 49%|████▊     | 748/1540 [30:13<31:55,  2.42s/it] 49%|████▊     | 749/1540 [30:15<31:53,  2.42s/it] 49%|████▊     | 750/1540 [30:18<31:51,  2.42s/it] 49%|████▉     | 751/1540 [30:20<32:05,  2.44s/it] 49%|████▉     | 752/1540 [30:23<31:58,  2.43s/it] 49%|████▉     | 753/1540 [30:25<31:52,  2.43s/it] 49%|████▉     | 754/1540 [30:28<31:47,  2.43s/it] 49%|████▉     | 755/1540 [30:30<31:43,  2.42s/it] 49%|████▉     | 756/1540 [30:32<31:39,  2.42s/it] 49%|████▉     | 757/1540 [30:35<31:36,  2.42s/it] 49%|████▉     | 758/1540 [30:37<31:33,  2.42s/it] 49%|████▉     | 759/1540 [30:40<31:30,  2.42s/it] 49%|████▉     | 760/1540 [30:42<31:27,  2.42s/it]                                                  {'loss': 0.4232, 'grad_norm': 0.34932568669319153, 'learning_rate': 0.00035269262958725125, 'epoch': 2.47}
 49%|████▉     | 760/1540 [30:42<31:27,  2.42s/it] 49%|████▉     | 761/1540 [30:45<31:25,  2.42s/it] 49%|████▉     | 762/1540 [30:47<31:23,  2.42s/it] 50%|████▉     | 763/1540 [30:49<31:20,  2.42s/it] 50%|████▉     | 764/1540 [30:52<31:17,  2.42s/it] 50%|████▉     | 765/1540 [30:54<31:20,  2.43s/it] 50%|████▉     | 766/1540 [30:57<31:16,  2.42s/it] 50%|████▉     | 767/1540 [30:59<31:12,  2.42s/it] 50%|████▉     | 768/1540 [31:01<31:09,  2.42s/it] 50%|████▉     | 769/1540 [31:04<31:07,  2.42s/it] 50%|█████     | 770/1540 [31:06<31:03,  2.42s/it] 50%|█████     | 771/1540 [31:09<31:00,  2.42s/it] 50%|█████     | 772/1540 [31:11<30:58,  2.42s/it] 50%|█████     | 773/1540 [31:14<30:55,  2.42s/it] 50%|█████     | 774/1540 [31:16<30:53,  2.42s/it] 50%|█████     | 775/1540 [31:18<30:51,  2.42s/it] 50%|█████     | 776/1540 [31:21<30:48,  2.42s/it] 50%|█████     | 777/1540 [31:23<30:46,  2.42s/it] 51%|█████     | 778/1540 [31:26<30:47,  2.42s/it] 51%|█████     | 779/1540 [31:28<30:43,  2.42s/it] 51%|█████     | 780/1540 [31:31<30:40,  2.42s/it]                                                  {'loss': 0.4198, 'grad_norm': 0.34612157940864563, 'learning_rate': 0.00034093950388531787, 'epoch': 2.53}
 51%|█████     | 780/1540 [31:31<30:40,  2.42s/it] 51%|█████     | 781/1540 [31:33<30:37,  2.42s/it] 51%|█████     | 782/1540 [31:35<30:34,  2.42s/it] 51%|█████     | 783/1540 [31:38<30:31,  2.42s/it] 51%|█████     | 784/1540 [31:40<30:29,  2.42s/it] 51%|█████     | 785/1540 [31:43<30:26,  2.42s/it] 51%|█████     | 786/1540 [31:45<30:23,  2.42s/it] 51%|█████     | 787/1540 [31:47<30:21,  2.42s/it] 51%|█████     | 788/1540 [31:50<30:18,  2.42s/it] 51%|█████     | 789/1540 [31:52<30:16,  2.42s/it] 51%|█████▏    | 790/1540 [31:55<30:14,  2.42s/it] 51%|█████▏    | 791/1540 [31:57<30:12,  2.42s/it] 51%|█████▏    | 792/1540 [32:00<30:09,  2.42s/it] 51%|█████▏    | 793/1540 [32:02<30:13,  2.43s/it] 52%|█████▏    | 794/1540 [32:04<30:08,  2.42s/it] 52%|█████▏    | 795/1540 [32:07<30:05,  2.42s/it] 52%|█████▏    | 796/1540 [32:09<30:02,  2.42s/it] 52%|█████▏    | 797/1540 [32:12<29:59,  2.42s/it] 52%|█████▏    | 798/1540 [32:14<29:56,  2.42s/it] 52%|█████▏    | 799/1540 [32:17<29:54,  2.42s/it] 52%|█████▏    | 800/1540 [32:19<29:51,  2.42s/it]                                                  {'loss': 0.412, 'grad_norm': 0.35475534200668335, 'learning_rate': 0.00032894989690375627, 'epoch': 2.6}
 52%|█████▏    | 800/1540 [32:19<29:51,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 04:09:55,131 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-800
[INFO|configuration_utils.py:726] 2024-05-25 04:09:56,347 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 04:09:56,349 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 04:09:56,866 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 04:09:56,868 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 04:09:56,921 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 04:09:56,921 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-800/special_tokens_map.json
 52%|█████▏    | 801/1540 [32:23<37:00,  3.01s/it] 52%|█████▏    | 802/1540 [32:26<34:47,  2.83s/it] 52%|█████▏    | 803/1540 [32:28<33:14,  2.71s/it] 52%|█████▏    | 804/1540 [32:31<32:08,  2.62s/it] 52%|█████▏    | 805/1540 [32:33<31:21,  2.56s/it] 52%|█████▏    | 806/1540 [32:35<30:53,  2.53s/it] 52%|█████▏    | 807/1540 [32:38<30:33,  2.50s/it] 52%|█████▏    | 808/1540 [32:40<30:12,  2.48s/it] 53%|█████▎    | 809/1540 [32:43<29:57,  2.46s/it] 53%|█████▎    | 810/1540 [32:45<29:45,  2.45s/it] 53%|█████▎    | 811/1540 [32:48<29:37,  2.44s/it] 53%|█████▎    | 812/1540 [32:50<29:30,  2.43s/it] 53%|█████▎    | 813/1540 [32:52<29:25,  2.43s/it] 53%|█████▎    | 814/1540 [32:55<29:20,  2.43s/it] 53%|█████▎    | 815/1540 [32:57<29:17,  2.42s/it] 53%|█████▎    | 816/1540 [33:00<29:13,  2.42s/it] 53%|█████▎    | 817/1540 [33:02<29:10,  2.42s/it] 53%|█████▎    | 818/1540 [33:04<29:07,  2.42s/it] 53%|█████▎    | 819/1540 [33:07<29:04,  2.42s/it] 53%|█████▎    | 820/1540 [33:09<29:01,  2.42s/it]                                                  {'loss': 0.3962, 'grad_norm': 0.3359602391719818, 'learning_rate': 0.0003167549867057854, 'epoch': 2.66}
 53%|█████▎    | 820/1540 [33:09<29:01,  2.42s/it] 53%|█████▎    | 821/1540 [33:12<29:30,  2.46s/it] 53%|█████▎    | 822/1540 [33:14<29:18,  2.45s/it] 53%|█████▎    | 823/1540 [33:17<29:09,  2.44s/it] 54%|█████▎    | 824/1540 [33:19<29:01,  2.43s/it] 54%|█████▎    | 825/1540 [33:22<28:56,  2.43s/it] 54%|█████▎    | 826/1540 [33:24<28:51,  2.43s/it] 54%|█████▎    | 827/1540 [33:26<28:47,  2.42s/it] 54%|█████▍    | 828/1540 [33:29<28:44,  2.42s/it] 54%|█████▍    | 829/1540 [33:31<28:41,  2.42s/it] 54%|█████▍    | 830/1540 [33:34<28:38,  2.42s/it] 54%|█████▍    | 831/1540 [33:36<28:35,  2.42s/it] 54%|█████▍    | 832/1540 [33:38<28:32,  2.42s/it] 54%|█████▍    | 833/1540 [33:41<28:30,  2.42s/it] 54%|█████▍    | 834/1540 [33:43<28:30,  2.42s/it] 54%|█████▍    | 835/1540 [33:46<28:27,  2.42s/it] 54%|█████▍    | 836/1540 [33:48<28:24,  2.42s/it] 54%|█████▍    | 837/1540 [33:51<28:21,  2.42s/it] 54%|█████▍    | 838/1540 [33:53<28:18,  2.42s/it] 54%|█████▍    | 839/1540 [33:55<28:16,  2.42s/it] 55%|█████▍    | 840/1540 [33:58<28:13,  2.42s/it]                                                  {'loss': 0.3928, 'grad_norm': 0.3621668517589569, 'learning_rate': 0.00030438648523006085, 'epoch': 2.73}
 55%|█████▍    | 840/1540 [33:58<28:13,  2.42s/it] 55%|█████▍    | 841/1540 [34:00<28:12,  2.42s/it] 55%|█████▍    | 842/1540 [34:03<28:09,  2.42s/it] 55%|█████▍    | 843/1540 [34:05<28:07,  2.42s/it] 55%|█████▍    | 844/1540 [34:08<28:04,  2.42s/it] 55%|█████▍    | 845/1540 [34:10<28:01,  2.42s/it] 55%|█████▍    | 846/1540 [34:12<27:58,  2.42s/it] 55%|█████▌    | 847/1540 [34:15<28:02,  2.43s/it] 55%|█████▌    | 848/1540 [34:17<27:58,  2.42s/it] 55%|█████▌    | 849/1540 [34:20<27:56,  2.43s/it] 55%|█████▌    | 850/1540 [34:22<27:52,  2.42s/it] 55%|█████▌    | 851/1540 [34:25<27:49,  2.42s/it] 55%|█████▌    | 852/1540 [34:27<27:45,  2.42s/it] 55%|█████▌    | 853/1540 [34:29<27:42,  2.42s/it] 55%|█████▌    | 854/1540 [34:32<27:40,  2.42s/it] 56%|█████▌    | 855/1540 [34:34<27:37,  2.42s/it] 56%|█████▌    | 856/1540 [34:37<27:34,  2.42s/it] 56%|█████▌    | 857/1540 [34:39<27:32,  2.42s/it] 56%|█████▌    | 858/1540 [34:41<27:29,  2.42s/it] 56%|█████▌    | 859/1540 [34:44<27:27,  2.42s/it] 56%|█████▌    | 860/1540 [34:46<27:24,  2.42s/it]                                                  {'loss': 0.3899, 'grad_norm': 0.36381298303604126, 'learning_rate': 0.0002918765558261841, 'epoch': 2.79}
 56%|█████▌    | 860/1540 [34:46<27:24,  2.42s/it] 56%|█████▌    | 861/1540 [34:49<27:22,  2.42s/it] 56%|█████▌    | 862/1540 [34:51<27:20,  2.42s/it] 56%|█████▌    | 863/1540 [34:54<27:23,  2.43s/it] 56%|█████▌    | 864/1540 [34:56<27:18,  2.42s/it] 56%|█████▌    | 865/1540 [34:58<27:15,  2.42s/it] 56%|█████▌    | 866/1540 [35:01<27:11,  2.42s/it] 56%|█████▋    | 867/1540 [35:03<27:09,  2.42s/it] 56%|█████▋    | 868/1540 [35:06<27:06,  2.42s/it] 56%|█████▋    | 869/1540 [35:08<27:03,  2.42s/it] 56%|█████▋    | 870/1540 [35:10<27:00,  2.42s/it] 57%|█████▋    | 871/1540 [35:13<26:58,  2.42s/it] 57%|█████▋    | 872/1540 [35:15<26:55,  2.42s/it] 57%|█████▋    | 873/1540 [35:18<26:53,  2.42s/it] 57%|█████▋    | 874/1540 [35:20<26:50,  2.42s/it] 57%|█████▋    | 875/1540 [35:23<26:48,  2.42s/it] 57%|█████▋    | 876/1540 [35:25<26:50,  2.43s/it] 57%|█████▋    | 877/1540 [35:27<26:46,  2.42s/it] 57%|█████▋    | 878/1540 [35:30<26:43,  2.42s/it] 57%|█████▋    | 879/1540 [35:32<26:40,  2.42s/it] 57%|█████▋    | 880/1540 [35:35<26:37,  2.42s/it]                                                  {'loss': 0.4197, 'grad_norm': 0.3672630786895752, 'learning_rate': 0.00027925772961635294, 'epoch': 2.86}
 57%|█████▋    | 880/1540 [35:35<26:37,  2.42s/it] 57%|█████▋    | 881/1540 [35:37<26:35,  2.42s/it] 57%|█████▋    | 882/1540 [35:40<26:32,  2.42s/it] 57%|█████▋    | 883/1540 [35:42<26:29,  2.42s/it] 57%|█████▋    | 884/1540 [35:44<26:27,  2.42s/it] 57%|█████▋    | 885/1540 [35:47<26:23,  2.42s/it] 58%|█████▊    | 886/1540 [35:49<26:21,  2.42s/it] 58%|█████▊    | 887/1540 [35:52<26:19,  2.42s/it] 58%|█████▊    | 888/1540 [35:54<26:16,  2.42s/it] 58%|█████▊    | 889/1540 [35:56<26:14,  2.42s/it] 58%|█████▊    | 890/1540 [35:59<26:24,  2.44s/it] 58%|█████▊    | 891/1540 [36:01<26:18,  2.43s/it] 58%|█████▊    | 892/1540 [36:04<26:13,  2.43s/it] 58%|█████▊    | 893/1540 [36:06<26:11,  2.43s/it] 58%|█████▊    | 894/1540 [36:09<26:07,  2.43s/it] 58%|█████▊    | 895/1540 [36:11<26:03,  2.42s/it] 58%|█████▊    | 896/1540 [36:13<25:59,  2.42s/it] 58%|█████▊    | 897/1540 [36:16<25:57,  2.42s/it] 58%|█████▊    | 898/1540 [36:18<25:54,  2.42s/it] 58%|█████▊    | 899/1540 [36:21<25:51,  2.42s/it] 58%|█████▊    | 900/1540 [36:23<25:48,  2.42s/it]                                                  {'loss': 0.3892, 'grad_norm': 0.28361567854881287, 'learning_rate': 0.0002671988642677426, 'epoch': 2.92}
 58%|█████▊    | 900/1540 [36:23<25:48,  2.42s/it] 59%|█████▊    | 901/1540 [36:26<25:46,  2.42s/it] 59%|█████▊    | 902/1540 [36:28<25:43,  2.42s/it] 59%|█████▊    | 903/1540 [36:30<25:41,  2.42s/it] 59%|█████▊    | 904/1540 [36:33<25:40,  2.42s/it] 59%|█████▉    | 905/1540 [36:35<25:37,  2.42s/it] 59%|█████▉    | 906/1540 [36:38<25:34,  2.42s/it] 59%|█████▉    | 907/1540 [36:40<25:32,  2.42s/it] 59%|█████▉    | 908/1540 [36:43<25:29,  2.42s/it] 59%|█████▉    | 909/1540 [36:45<25:26,  2.42s/it] 59%|█████▉    | 910/1540 [36:47<25:24,  2.42s/it] 59%|█████▉    | 911/1540 [36:50<25:21,  2.42s/it] 59%|█████▉    | 912/1540 [36:52<25:19,  2.42s/it] 59%|█████▉    | 913/1540 [36:55<25:16,  2.42s/it] 59%|█████▉    | 914/1540 [36:57<25:14,  2.42s/it] 59%|█████▉    | 915/1540 [36:59<25:11,  2.42s/it] 59%|█████▉    | 916/1540 [37:02<25:09,  2.42s/it] 60%|█████▉    | 917/1540 [37:04<25:16,  2.43s/it] 60%|█████▉    | 918/1540 [37:07<25:11,  2.43s/it] 60%|█████▉    | 919/1540 [37:09<25:07,  2.43s/it] 60%|█████▉    | 920/1540 [37:12<25:02,  2.42s/it]                                                  {'loss': 0.3835, 'grad_norm': 0.33310309052467346, 'learning_rate': 0.0002544622525947115, 'epoch': 2.99}
 60%|█████▉    | 920/1540 [37:12<25:02,  2.42s/it] 60%|█████▉    | 921/1540 [37:14<24:59,  2.42s/it] 60%|█████▉    | 922/1540 [37:16<24:56,  2.42s/it] 60%|█████▉    | 923/1540 [37:19<24:53,  2.42s/it] 60%|██████    | 924/1540 [37:21<24:50,  2.42s/it] 60%|██████    | 925/1540 [37:24<24:48,  2.42s/it] 60%|██████    | 926/1540 [37:26<24:45,  2.42s/it] 60%|██████    | 927/1540 [37:29<24:42,  2.42s/it] 60%|██████    | 928/1540 [37:31<24:40,  2.42s/it] 60%|██████    | 929/1540 [37:33<24:37,  2.42s/it] 60%|██████    | 930/1540 [37:36<24:35,  2.42s/it] 60%|██████    | 931/1540 [37:38<24:33,  2.42s/it] 61%|██████    | 932/1540 [37:41<24:36,  2.43s/it] 61%|██████    | 933/1540 [37:43<24:32,  2.43s/it] 61%|██████    | 934/1540 [37:45<24:28,  2.42s/it] 61%|██████    | 935/1540 [37:48<24:25,  2.42s/it] 61%|██████    | 936/1540 [37:50<24:22,  2.42s/it] 61%|██████    | 937/1540 [37:53<24:19,  2.42s/it] 61%|██████    | 938/1540 [37:55<24:16,  2.42s/it] 61%|██████    | 939/1540 [37:58<24:14,  2.42s/it] 61%|██████    | 940/1540 [38:00<24:11,  2.42s/it]                                                  {'loss': 0.3393, 'grad_norm': 0.298085480928421, 'learning_rate': 0.00024171403717239068, 'epoch': 3.05}
 61%|██████    | 940/1540 [38:00<24:11,  2.42s/it] 61%|██████    | 941/1540 [38:02<24:09,  2.42s/it] 61%|██████    | 942/1540 [38:05<24:06,  2.42s/it] 61%|██████    | 943/1540 [38:07<24:04,  2.42s/it] 61%|██████▏   | 944/1540 [38:10<24:01,  2.42s/it] 61%|██████▏   | 945/1540 [38:12<24:01,  2.42s/it] 61%|██████▏   | 946/1540 [38:15<23:58,  2.42s/it] 61%|██████▏   | 947/1540 [38:17<23:55,  2.42s/it] 62%|██████▏   | 948/1540 [38:19<23:52,  2.42s/it] 62%|██████▏   | 949/1540 [38:22<23:50,  2.42s/it] 62%|██████▏   | 950/1540 [38:24<23:47,  2.42s/it] 62%|██████▏   | 951/1540 [38:27<23:45,  2.42s/it] 62%|██████▏   | 952/1540 [38:29<23:42,  2.42s/it] 62%|██████▏   | 953/1540 [38:31<23:39,  2.42s/it] 62%|██████▏   | 954/1540 [38:34<23:37,  2.42s/it] 62%|██████▏   | 955/1540 [38:36<23:35,  2.42s/it] 62%|██████▏   | 956/1540 [38:39<23:32,  2.42s/it] 62%|██████▏   | 957/1540 [38:41<23:30,  2.42s/it] 62%|██████▏   | 958/1540 [38:44<23:27,  2.42s/it] 62%|██████▏   | 959/1540 [38:46<23:25,  2.42s/it] 62%|██████▏   | 960/1540 [38:48<23:34,  2.44s/it]                                                  {'loss': 0.3463, 'grad_norm': 0.42598769068717957, 'learning_rate': 0.00022898736876768815, 'epoch': 3.12}
 62%|██████▏   | 960/1540 [38:48<23:34,  2.44s/it] 62%|██████▏   | 961/1540 [38:51<23:29,  2.43s/it] 62%|██████▏   | 962/1540 [38:53<23:24,  2.43s/it] 63%|██████▎   | 963/1540 [38:56<23:20,  2.43s/it] 63%|██████▎   | 964/1540 [38:58<23:16,  2.42s/it] 63%|██████▎   | 965/1540 [39:01<23:16,  2.43s/it] 63%|██████▎   | 966/1540 [39:03<23:12,  2.43s/it] 63%|██████▎   | 967/1540 [39:05<23:08,  2.42s/it] 63%|██████▎   | 968/1540 [39:08<23:05,  2.42s/it] 63%|██████▎   | 969/1540 [39:10<23:02,  2.42s/it] 63%|██████▎   | 970/1540 [39:13<22:59,  2.42s/it] 63%|██████▎   | 971/1540 [39:15<22:57,  2.42s/it] 63%|██████▎   | 972/1540 [39:18<22:54,  2.42s/it] 63%|██████▎   | 973/1540 [39:20<22:54,  2.42s/it] 63%|██████▎   | 974/1540 [39:22<22:50,  2.42s/it] 63%|██████▎   | 975/1540 [39:25<22:47,  2.42s/it] 63%|██████▎   | 976/1540 [39:27<22:45,  2.42s/it] 63%|██████▎   | 977/1540 [39:30<22:42,  2.42s/it] 64%|██████▎   | 978/1540 [39:32<22:40,  2.42s/it] 64%|██████▎   | 979/1540 [39:34<22:37,  2.42s/it] 64%|██████▎   | 980/1540 [39:37<22:35,  2.42s/it]                                                  {'loss': 0.3535, 'grad_norm': 0.3332461416721344, 'learning_rate': 0.00021631534211612774, 'epoch': 3.18}
 64%|██████▎   | 980/1540 [39:37<22:35,  2.42s/it] 64%|██████▎   | 981/1540 [39:39<22:33,  2.42s/it] 64%|██████▍   | 982/1540 [39:42<22:30,  2.42s/it] 64%|██████▍   | 983/1540 [39:44<22:27,  2.42s/it] 64%|██████▍   | 984/1540 [39:47<22:25,  2.42s/it] 64%|██████▍   | 985/1540 [39:49<22:23,  2.42s/it] 64%|██████▍   | 986/1540 [39:51<22:27,  2.43s/it] 64%|██████▍   | 987/1540 [39:54<22:23,  2.43s/it] 64%|██████▍   | 988/1540 [39:56<22:19,  2.43s/it] 64%|██████▍   | 989/1540 [39:59<22:15,  2.42s/it] 64%|██████▍   | 990/1540 [40:01<22:12,  2.42s/it] 64%|██████▍   | 991/1540 [40:04<22:09,  2.42s/it] 64%|██████▍   | 992/1540 [40:06<22:06,  2.42s/it] 64%|██████▍   | 993/1540 [40:08<22:04,  2.42s/it] 65%|██████▍   | 994/1540 [40:11<22:07,  2.43s/it] 65%|██████▍   | 995/1540 [40:13<22:03,  2.43s/it] 65%|██████▍   | 996/1540 [40:16<21:59,  2.42s/it] 65%|██████▍   | 997/1540 [40:18<21:55,  2.42s/it] 65%|██████▍   | 998/1540 [40:21<21:52,  2.42s/it] 65%|██████▍   | 999/1540 [40:23<21:49,  2.42s/it] 65%|██████▍   | 1000/1540 [40:25<21:46,  2.42s/it]                                                   {'loss': 0.3426, 'grad_norm': 0.3523094058036804, 'learning_rate': 0.0002037309098615004, 'epoch': 3.25}
 65%|██████▍   | 1000/1540 [40:25<21:46,  2.42s/it] 65%|██████▌   | 1001/1540 [40:28<21:44,  2.42s/it] 65%|██████▌   | 1002/1540 [40:30<21:42,  2.42s/it] 65%|██████▌   | 1003/1540 [40:33<21:39,  2.42s/it] 65%|██████▌   | 1004/1540 [40:35<21:36,  2.42s/it] 65%|██████▌   | 1005/1540 [40:37<21:34,  2.42s/it] 65%|██████▌   | 1006/1540 [40:40<21:31,  2.42s/it] 65%|██████▌   | 1007/1540 [40:42<21:29,  2.42s/it] 65%|██████▌   | 1008/1540 [40:45<21:26,  2.42s/it] 66%|██████▌   | 1009/1540 [40:47<21:30,  2.43s/it] 66%|██████▌   | 1010/1540 [40:50<21:26,  2.43s/it] 66%|██████▌   | 1011/1540 [40:52<21:22,  2.42s/it] 66%|██████▌   | 1012/1540 [40:54<21:19,  2.42s/it] 66%|██████▌   | 1013/1540 [40:57<21:15,  2.42s/it] 66%|██████▌   | 1014/1540 [40:59<21:13,  2.42s/it] 66%|██████▌   | 1015/1540 [41:02<21:11,  2.42s/it] 66%|██████▌   | 1016/1540 [41:04<21:08,  2.42s/it] 66%|██████▌   | 1017/1540 [41:07<21:05,  2.42s/it] 66%|██████▌   | 1018/1540 [41:09<21:03,  2.42s/it] 66%|██████▌   | 1019/1540 [41:11<21:00,  2.42s/it] 66%|██████▌   | 1020/1540 [41:14<20:58,  2.42s/it]                                                   {'loss': 0.3505, 'grad_norm': 0.3131245970726013, 'learning_rate': 0.0001912667968650139, 'epoch': 3.31}
 66%|██████▌   | 1020/1540 [41:14<20:58,  2.42s/it] 66%|██████▋   | 1021/1540 [41:16<20:55,  2.42s/it] 66%|██████▋   | 1022/1540 [41:19<20:53,  2.42s/it] 66%|██████▋   | 1023/1540 [41:21<20:51,  2.42s/it] 66%|██████▋   | 1024/1540 [41:23<20:48,  2.42s/it] 67%|██████▋   | 1025/1540 [41:26<20:45,  2.42s/it] 67%|██████▋   | 1026/1540 [41:28<20:43,  2.42s/it] 67%|██████▋   | 1027/1540 [41:31<20:40,  2.42s/it] 67%|██████▋   | 1028/1540 [41:33<20:38,  2.42s/it] 67%|██████▋   | 1029/1540 [41:36<20:44,  2.44s/it] 67%|██████▋   | 1030/1540 [41:38<20:39,  2.43s/it] 67%|██████▋   | 1031/1540 [41:40<20:35,  2.43s/it] 67%|██████▋   | 1032/1540 [41:43<20:31,  2.42s/it] 67%|██████▋   | 1033/1540 [41:45<20:28,  2.42s/it] 67%|██████▋   | 1034/1540 [41:48<20:24,  2.42s/it] 67%|██████▋   | 1035/1540 [41:50<20:22,  2.42s/it] 67%|██████▋   | 1036/1540 [41:53<20:19,  2.42s/it] 67%|██████▋   | 1037/1540 [41:55<20:16,  2.42s/it] 67%|██████▋   | 1038/1540 [41:57<20:17,  2.43s/it] 67%|██████▋   | 1039/1540 [42:00<20:13,  2.42s/it] 68%|██████▊   | 1040/1540 [42:02<20:11,  2.42s/it]                                                   {'loss': 0.3681, 'grad_norm': 0.3269442915916443, 'learning_rate': 0.00017895541510677497, 'epoch': 3.38}
 68%|██████▊   | 1040/1540 [42:02<20:11,  2.42s/it] 68%|██████▊   | 1041/1540 [42:05<20:08,  2.42s/it] 68%|██████▊   | 1042/1540 [42:07<20:05,  2.42s/it] 68%|██████▊   | 1043/1540 [42:09<20:05,  2.42s/it] 68%|██████▊   | 1044/1540 [42:12<20:01,  2.42s/it] 68%|██████▊   | 1045/1540 [42:14<19:58,  2.42s/it] 68%|██████▊   | 1046/1540 [42:17<19:55,  2.42s/it] 68%|██████▊   | 1047/1540 [42:19<19:52,  2.42s/it] 68%|██████▊   | 1048/1540 [42:22<19:50,  2.42s/it] 68%|██████▊   | 1049/1540 [42:24<19:47,  2.42s/it] 68%|██████▊   | 1050/1540 [42:26<19:45,  2.42s/it] 68%|██████▊   | 1051/1540 [42:29<19:42,  2.42s/it] 68%|██████▊   | 1052/1540 [42:31<19:40,  2.42s/it] 68%|██████▊   | 1053/1540 [42:34<19:37,  2.42s/it] 68%|██████▊   | 1054/1540 [42:36<19:35,  2.42s/it] 69%|██████▊   | 1055/1540 [42:39<19:32,  2.42s/it] 69%|██████▊   | 1056/1540 [42:41<19:35,  2.43s/it] 69%|██████▊   | 1057/1540 [42:43<19:31,  2.43s/it] 69%|██████▊   | 1058/1540 [42:46<19:28,  2.42s/it] 69%|██████▉   | 1059/1540 [42:48<19:25,  2.42s/it] 69%|██████▉   | 1060/1540 [42:51<19:22,  2.42s/it]                                                   {'loss': 0.3377, 'grad_norm': 0.4083103835582733, 'learning_rate': 0.00016682877940089405, 'epoch': 3.44}
 69%|██████▉   | 1060/1540 [42:51<19:22,  2.42s/it] 69%|██████▉   | 1061/1540 [42:53<19:19,  2.42s/it] 69%|██████▉   | 1062/1540 [42:55<19:16,  2.42s/it] 69%|██████▉   | 1063/1540 [42:58<19:13,  2.42s/it] 69%|██████▉   | 1064/1540 [43:00<19:11,  2.42s/it] 69%|██████▉   | 1065/1540 [43:03<19:08,  2.42s/it] 69%|██████▉   | 1066/1540 [43:05<19:06,  2.42s/it] 69%|██████▉   | 1067/1540 [43:08<19:03,  2.42s/it] 69%|██████▉   | 1068/1540 [43:10<19:01,  2.42s/it] 69%|██████▉   | 1069/1540 [43:12<18:59,  2.42s/it] 69%|██████▉   | 1070/1540 [43:15<18:57,  2.42s/it] 70%|██████▉   | 1071/1540 [43:17<18:58,  2.43s/it] 70%|██████▉   | 1072/1540 [43:20<18:54,  2.42s/it] 70%|██████▉   | 1073/1540 [43:22<18:51,  2.42s/it] 70%|██████▉   | 1074/1540 [43:25<18:48,  2.42s/it] 70%|██████▉   | 1075/1540 [43:27<18:45,  2.42s/it] 70%|██████▉   | 1076/1540 [43:29<18:43,  2.42s/it] 70%|██████▉   | 1077/1540 [43:32<18:40,  2.42s/it] 70%|███████   | 1078/1540 [43:34<18:37,  2.42s/it] 70%|███████   | 1079/1540 [43:37<18:35,  2.42s/it] 70%|███████   | 1080/1540 [43:39<18:32,  2.42s/it]                                                   {'loss': 0.3378, 'grad_norm': 0.3484027087688446, 'learning_rate': 0.00015491842414339023, 'epoch': 3.51}
 70%|███████   | 1080/1540 [43:39<18:32,  2.42s/it] 70%|███████   | 1081/1540 [43:41<18:34,  2.43s/it] 70%|███████   | 1082/1540 [43:44<18:30,  2.42s/it] 70%|███████   | 1083/1540 [43:46<18:27,  2.42s/it] 70%|███████   | 1084/1540 [43:49<18:24,  2.42s/it] 70%|███████   | 1085/1540 [43:51<18:21,  2.42s/it] 71%|███████   | 1086/1540 [43:54<18:19,  2.42s/it] 71%|███████   | 1087/1540 [43:56<18:16,  2.42s/it] 71%|███████   | 1088/1540 [43:58<18:13,  2.42s/it] 71%|███████   | 1089/1540 [44:01<18:11,  2.42s/it] 71%|███████   | 1090/1540 [44:03<18:08,  2.42s/it] 71%|███████   | 1091/1540 [44:06<18:06,  2.42s/it] 71%|███████   | 1092/1540 [44:08<18:03,  2.42s/it] 71%|███████   | 1093/1540 [44:11<18:01,  2.42s/it] 71%|███████   | 1094/1540 [44:13<17:58,  2.42s/it] 71%|███████   | 1095/1540 [44:15<17:56,  2.42s/it] 71%|███████   | 1096/1540 [44:18<17:53,  2.42s/it] 71%|███████   | 1097/1540 [44:20<17:51,  2.42s/it] 71%|███████▏  | 1098/1540 [44:23<17:48,  2.42s/it] 71%|███████▏  | 1099/1540 [44:25<17:55,  2.44s/it] 71%|███████▏  | 1100/1540 [44:28<17:50,  2.43s/it]                                                   {'loss': 0.3155, 'grad_norm': 0.3296494483947754, 'learning_rate': 0.0001432553213093876, 'epoch': 3.57}
 71%|███████▏  | 1100/1540 [44:28<17:50,  2.43s/it] 71%|███████▏  | 1101/1540 [44:30<17:46,  2.43s/it] 72%|███████▏  | 1102/1540 [44:32<17:42,  2.43s/it] 72%|███████▏  | 1103/1540 [44:35<17:39,  2.42s/it] 72%|███████▏  | 1104/1540 [44:37<17:35,  2.42s/it] 72%|███████▏  | 1105/1540 [44:40<17:33,  2.42s/it] 72%|███████▏  | 1106/1540 [44:42<17:30,  2.42s/it] 72%|███████▏  | 1107/1540 [44:44<17:27,  2.42s/it] 72%|███████▏  | 1108/1540 [44:47<17:25,  2.42s/it] 72%|███████▏  | 1109/1540 [44:49<17:22,  2.42s/it] 72%|███████▏  | 1110/1540 [44:52<17:20,  2.42s/it] 72%|███████▏  | 1111/1540 [44:54<17:17,  2.42s/it] 72%|███████▏  | 1112/1540 [44:57<17:15,  2.42s/it] 72%|███████▏  | 1113/1540 [44:59<17:12,  2.42s/it] 72%|███████▏  | 1114/1540 [45:01<17:10,  2.42s/it] 72%|███████▏  | 1115/1540 [45:04<17:07,  2.42s/it] 72%|███████▏  | 1116/1540 [45:06<17:05,  2.42s/it] 73%|███████▎  | 1117/1540 [45:09<17:03,  2.42s/it] 73%|███████▎  | 1118/1540 [45:11<17:00,  2.42s/it] 73%|███████▎  | 1119/1540 [45:13<16:58,  2.42s/it] 73%|███████▎  | 1120/1540 [45:16<16:55,  2.42s/it]                                                   {'loss': 0.3198, 'grad_norm': 0.31224653124809265, 'learning_rate': 0.0001318697999128436, 'epoch': 3.64}
 73%|███████▎  | 1120/1540 [45:16<16:55,  2.42s/it] 73%|███████▎  | 1121/1540 [45:18<16:53,  2.42s/it] 73%|███████▎  | 1122/1540 [45:21<16:51,  2.42s/it] 73%|███████▎  | 1123/1540 [45:23<16:48,  2.42s/it] 73%|███████▎  | 1124/1540 [45:26<16:46,  2.42s/it] 73%|███████▎  | 1125/1540 [45:28<16:45,  2.42s/it] 73%|███████▎  | 1126/1540 [45:30<16:42,  2.42s/it] 73%|███████▎  | 1127/1540 [45:33<16:39,  2.42s/it] 73%|███████▎  | 1128/1540 [45:35<16:36,  2.42s/it] 73%|███████▎  | 1129/1540 [45:38<16:34,  2.42s/it] 73%|███████▎  | 1130/1540 [45:40<16:31,  2.42s/it] 73%|███████▎  | 1131/1540 [45:43<16:29,  2.42s/it] 74%|███████▎  | 1132/1540 [45:45<16:27,  2.42s/it] 74%|███████▎  | 1133/1540 [45:47<16:24,  2.42s/it] 74%|███████▎  | 1134/1540 [45:50<16:21,  2.42s/it] 74%|███████▎  | 1135/1540 [45:52<16:19,  2.42s/it] 74%|███████▍  | 1136/1540 [45:55<16:17,  2.42s/it] 74%|███████▍  | 1137/1540 [45:57<16:14,  2.42s/it] 74%|███████▍  | 1138/1540 [45:59<16:12,  2.42s/it] 74%|███████▍  | 1139/1540 [46:02<16:09,  2.42s/it] 74%|███████▍  | 1140/1540 [46:04<16:07,  2.42s/it]                                                   {'loss': 0.3256, 'grad_norm': 0.38824501633644104, 'learning_rate': 0.00012079146713824946, 'epoch': 3.7}
 74%|███████▍  | 1140/1540 [46:04<16:07,  2.42s/it] 74%|███████▍  | 1141/1540 [46:07<16:06,  2.42s/it] 74%|███████▍  | 1142/1540 [46:09<16:03,  2.42s/it] 74%|███████▍  | 1143/1540 [46:12<16:00,  2.42s/it] 74%|███████▍  | 1144/1540 [46:14<15:58,  2.42s/it] 74%|███████▍  | 1145/1540 [46:16<15:56,  2.42s/it] 74%|███████▍  | 1146/1540 [46:19<15:53,  2.42s/it] 74%|███████▍  | 1147/1540 [46:21<15:50,  2.42s/it] 75%|███████▍  | 1148/1540 [46:24<15:48,  2.42s/it] 75%|███████▍  | 1149/1540 [46:26<15:45,  2.42s/it] 75%|███████▍  | 1150/1540 [46:28<15:43,  2.42s/it] 75%|███████▍  | 1151/1540 [46:31<15:40,  2.42s/it] 75%|███████▍  | 1152/1540 [46:33<15:38,  2.42s/it] 75%|███████▍  | 1153/1540 [46:36<15:36,  2.42s/it] 75%|███████▍  | 1154/1540 [46:38<15:36,  2.43s/it] 75%|███████▌  | 1155/1540 [46:41<15:33,  2.42s/it] 75%|███████▌  | 1156/1540 [46:43<15:30,  2.42s/it] 75%|███████▌  | 1157/1540 [46:45<15:27,  2.42s/it] 75%|███████▌  | 1158/1540 [46:48<15:24,  2.42s/it] 75%|███████▌  | 1159/1540 [46:50<15:22,  2.42s/it] 75%|███████▌  | 1160/1540 [46:53<15:19,  2.42s/it]                                                   {'loss': 0.3559, 'grad_norm': 0.32487767934799194, 'learning_rate': 0.00011004913134939388, 'epoch': 3.77}
 75%|███████▌  | 1160/1540 [46:53<15:19,  2.42s/it] 75%|███████▌  | 1161/1540 [46:55<15:17,  2.42s/it] 75%|███████▌  | 1162/1540 [46:58<15:14,  2.42s/it] 76%|███████▌  | 1163/1540 [47:00<15:12,  2.42s/it] 76%|███████▌  | 1164/1540 [47:02<15:09,  2.42s/it] 76%|███████▌  | 1165/1540 [47:05<15:07,  2.42s/it] 76%|███████▌  | 1166/1540 [47:07<15:04,  2.42s/it] 76%|███████▌  | 1167/1540 [47:10<15:02,  2.42s/it] 76%|███████▌  | 1168/1540 [47:12<15:07,  2.44s/it] 76%|███████▌  | 1169/1540 [47:15<15:02,  2.43s/it] 76%|███████▌  | 1170/1540 [47:17<14:58,  2.43s/it] 76%|███████▌  | 1171/1540 [47:19<14:54,  2.43s/it] 76%|███████▌  | 1172/1540 [47:22<14:51,  2.42s/it] 76%|███████▌  | 1173/1540 [47:24<14:48,  2.42s/it] 76%|███████▌  | 1174/1540 [47:27<14:45,  2.42s/it] 76%|███████▋  | 1175/1540 [47:29<14:43,  2.42s/it] 76%|███████▋  | 1176/1540 [47:31<14:40,  2.42s/it] 76%|███████▋  | 1177/1540 [47:34<14:38,  2.42s/it] 76%|███████▋  | 1178/1540 [47:36<14:35,  2.42s/it] 77%|███████▋  | 1179/1540 [47:39<14:33,  2.42s/it] 77%|███████▋  | 1180/1540 [47:41<14:30,  2.42s/it]                                                   {'loss': 0.3356, 'grad_norm': 0.3054658770561218, 'learning_rate': 0.00010018058337216326, 'epoch': 3.83}
 77%|███████▋  | 1180/1540 [47:41<14:30,  2.42s/it] 77%|███████▋  | 1181/1540 [47:44<14:28,  2.42s/it] 77%|███████▋  | 1182/1540 [47:46<14:26,  2.42s/it] 77%|███████▋  | 1183/1540 [47:48<14:23,  2.42s/it] 77%|███████▋  | 1184/1540 [47:51<14:21,  2.42s/it] 77%|███████▋  | 1185/1540 [47:53<14:18,  2.42s/it] 77%|███████▋  | 1186/1540 [47:56<14:16,  2.42s/it] 77%|███████▋  | 1187/1540 [47:58<14:13,  2.42s/it] 77%|███████▋  | 1188/1540 [48:00<14:11,  2.42s/it] 77%|███████▋  | 1189/1540 [48:03<14:09,  2.42s/it] 77%|███████▋  | 1190/1540 [48:05<14:06,  2.42s/it] 77%|███████▋  | 1191/1540 [48:08<14:03,  2.42s/it] 77%|███████▋  | 1192/1540 [48:10<14:01,  2.42s/it] 77%|███████▋  | 1193/1540 [48:13<13:59,  2.42s/it] 78%|███████▊  | 1194/1540 [48:15<13:56,  2.42s/it] 78%|███████▊  | 1195/1540 [48:17<13:55,  2.42s/it] 78%|███████▊  | 1196/1540 [48:20<13:52,  2.42s/it] 78%|███████▊  | 1197/1540 [48:22<13:50,  2.42s/it] 78%|███████▊  | 1198/1540 [48:25<13:47,  2.42s/it] 78%|███████▊  | 1199/1540 [48:27<13:44,  2.42s/it] 78%|███████▊  | 1200/1540 [48:30<13:42,  2.42s/it]                                                   {'loss': 0.3139, 'grad_norm': 0.3283434510231018, 'learning_rate': 9.017292751611219e-05, 'epoch': 3.9}
 78%|███████▊  | 1200/1540 [48:30<13:42,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 04:26:05,697 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-1200
[INFO|configuration_utils.py:726] 2024-05-25 04:26:06,469 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 04:26:06,471 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 04:26:07,021 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 04:26:07,023 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 04:26:07,075 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 04:26:07,076 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/checkpoint-1200/special_tokens_map.json
 78%|███████▊  | 1201/1540 [48:33<16:17,  2.88s/it] 78%|███████▊  | 1202/1540 [48:36<15:27,  2.74s/it] 78%|███████▊  | 1203/1540 [48:38<14:52,  2.65s/it] 78%|███████▊  | 1204/1540 [48:41<14:26,  2.58s/it] 78%|███████▊  | 1205/1540 [48:43<14:08,  2.53s/it] 78%|███████▊  | 1206/1540 [48:46<13:54,  2.50s/it] 78%|███████▊  | 1207/1540 [48:48<13:43,  2.47s/it] 78%|███████▊  | 1208/1540 [48:50<13:35,  2.46s/it] 79%|███████▊  | 1209/1540 [48:53<13:29,  2.45s/it] 79%|███████▊  | 1210/1540 [48:55<13:26,  2.44s/it] 79%|███████▊  | 1211/1540 [48:58<13:21,  2.44s/it] 79%|███████▊  | 1212/1540 [49:00<13:19,  2.44s/it] 79%|███████▉  | 1213/1540 [49:03<13:15,  2.43s/it] 79%|███████▉  | 1214/1540 [49:05<13:11,  2.43s/it] 79%|███████▉  | 1215/1540 [49:07<13:08,  2.43s/it] 79%|███████▉  | 1216/1540 [49:10<13:05,  2.42s/it] 79%|███████▉  | 1217/1540 [49:12<13:02,  2.42s/it] 79%|███████▉  | 1218/1540 [49:15<12:59,  2.42s/it] 79%|███████▉  | 1219/1540 [49:17<12:56,  2.42s/it] 79%|███████▉  | 1220/1540 [49:19<12:54,  2.42s/it]                                                   {'loss': 0.3239, 'grad_norm': 0.3021090626716614, 'learning_rate': 8.058088983484104e-05, 'epoch': 3.96}
 79%|███████▉  | 1220/1540 [49:19<12:54,  2.42s/it] 79%|███████▉  | 1221/1540 [49:22<12:52,  2.42s/it] 79%|███████▉  | 1222/1540 [49:24<12:49,  2.42s/it] 79%|███████▉  | 1223/1540 [49:27<12:48,  2.42s/it] 79%|███████▉  | 1224/1540 [49:29<12:45,  2.42s/it] 80%|███████▉  | 1225/1540 [49:32<12:42,  2.42s/it] 80%|███████▉  | 1226/1540 [49:34<12:42,  2.43s/it] 80%|███████▉  | 1227/1540 [49:36<12:38,  2.42s/it] 80%|███████▉  | 1228/1540 [49:39<12:35,  2.42s/it] 80%|███████▉  | 1229/1540 [49:41<12:33,  2.42s/it] 80%|███████▉  | 1230/1540 [49:44<12:30,  2.42s/it] 80%|███████▉  | 1231/1540 [49:46<12:27,  2.42s/it] 80%|████████  | 1232/1540 [49:49<12:25,  2.42s/it] 80%|████████  | 1233/1540 [49:51<12:22,  2.42s/it] 80%|████████  | 1234/1540 [49:53<12:20,  2.42s/it] 80%|████████  | 1235/1540 [49:56<12:17,  2.42s/it] 80%|████████  | 1236/1540 [49:58<12:15,  2.42s/it] 80%|████████  | 1237/1540 [50:01<12:12,  2.42s/it] 80%|████████  | 1238/1540 [50:03<12:16,  2.44s/it] 80%|████████  | 1239/1540 [50:06<12:12,  2.43s/it] 81%|████████  | 1240/1540 [50:08<12:08,  2.43s/it]                                                   {'loss': 0.308, 'grad_norm': 0.2687188386917114, 'learning_rate': 7.142941369452411e-05, 'epoch': 4.03}
 81%|████████  | 1240/1540 [50:08<12:08,  2.43s/it] 81%|████████  | 1241/1540 [50:10<12:06,  2.43s/it] 81%|████████  | 1242/1540 [50:13<12:02,  2.43s/it] 81%|████████  | 1243/1540 [50:15<11:59,  2.42s/it] 81%|████████  | 1244/1540 [50:18<11:56,  2.42s/it] 81%|████████  | 1245/1540 [50:20<11:54,  2.42s/it] 81%|████████  | 1246/1540 [50:22<11:51,  2.42s/it] 81%|████████  | 1247/1540 [50:25<11:48,  2.42s/it] 81%|████████  | 1248/1540 [50:27<11:46,  2.42s/it] 81%|████████  | 1249/1540 [50:30<11:43,  2.42s/it] 81%|████████  | 1250/1540 [50:32<11:41,  2.42s/it] 81%|████████  | 1251/1540 [50:35<11:39,  2.42s/it] 81%|████████▏ | 1252/1540 [50:37<11:36,  2.42s/it] 81%|████████▏ | 1253/1540 [50:39<11:34,  2.42s/it] 81%|████████▏ | 1254/1540 [50:42<11:31,  2.42s/it] 81%|████████▏ | 1255/1540 [50:44<11:29,  2.42s/it] 82%|████████▏ | 1256/1540 [50:47<11:27,  2.42s/it] 82%|████████▏ | 1257/1540 [50:49<11:24,  2.42s/it] 82%|████████▏ | 1258/1540 [50:52<11:22,  2.42s/it] 82%|████████▏ | 1259/1540 [50:54<11:19,  2.42s/it] 82%|████████▏ | 1260/1540 [50:56<11:17,  2.42s/it]                                                   {'loss': 0.3139, 'grad_norm': 0.3097650110721588, 'learning_rate': 6.274229681447688e-05, 'epoch': 4.09}
 82%|████████▏ | 1260/1540 [50:56<11:17,  2.42s/it] 82%|████████▏ | 1261/1540 [50:59<11:15,  2.42s/it] 82%|████████▏ | 1262/1540 [51:01<11:12,  2.42s/it] 82%|████████▏ | 1263/1540 [51:04<11:10,  2.42s/it] 82%|████████▏ | 1264/1540 [51:06<11:08,  2.42s/it] 82%|████████▏ | 1265/1540 [51:08<11:05,  2.42s/it] 82%|████████▏ | 1266/1540 [51:11<11:03,  2.42s/it] 82%|████████▏ | 1267/1540 [51:13<11:00,  2.42s/it] 82%|████████▏ | 1268/1540 [51:16<10:57,  2.42s/it] 82%|████████▏ | 1269/1540 [51:18<10:55,  2.42s/it] 82%|████████▏ | 1270/1540 [51:21<10:53,  2.42s/it] 83%|████████▎ | 1271/1540 [51:23<10:51,  2.42s/it] 83%|████████▎ | 1272/1540 [51:25<10:49,  2.42s/it] 83%|████████▎ | 1273/1540 [51:28<10:46,  2.42s/it] 83%|████████▎ | 1274/1540 [51:30<10:44,  2.42s/it] 83%|████████▎ | 1275/1540 [51:33<10:41,  2.42s/it] 83%|████████▎ | 1276/1540 [51:35<10:38,  2.42s/it] 83%|████████▎ | 1277/1540 [51:38<10:36,  2.42s/it] 83%|████████▎ | 1278/1540 [51:40<10:33,  2.42s/it] 83%|████████▎ | 1279/1540 [51:42<10:31,  2.42s/it] 83%|████████▎ | 1280/1540 [51:45<10:30,  2.43s/it]                                                   {'loss': 0.3223, 'grad_norm': 0.2660805284976959, 'learning_rate': 5.454212938299255e-05, 'epoch': 4.16}
 83%|████████▎ | 1280/1540 [51:45<10:30,  2.43s/it] 83%|████████▎ | 1281/1540 [51:47<10:27,  2.42s/it] 83%|████████▎ | 1282/1540 [51:50<10:25,  2.42s/it] 83%|████████▎ | 1283/1540 [51:52<10:22,  2.42s/it] 83%|████████▎ | 1284/1540 [51:54<10:21,  2.43s/it] 83%|████████▎ | 1285/1540 [51:57<10:18,  2.42s/it] 84%|████████▎ | 1286/1540 [51:59<10:15,  2.42s/it] 84%|████████▎ | 1287/1540 [52:02<10:12,  2.42s/it] 84%|████████▎ | 1288/1540 [52:04<10:09,  2.42s/it] 84%|████████▎ | 1289/1540 [52:07<10:07,  2.42s/it] 84%|████████▍ | 1290/1540 [52:09<10:05,  2.42s/it] 84%|████████▍ | 1291/1540 [52:11<10:02,  2.42s/it] 84%|████████▍ | 1292/1540 [52:14<10:00,  2.42s/it] 84%|████████▍ | 1293/1540 [52:16<09:57,  2.42s/it] 84%|████████▍ | 1294/1540 [52:19<09:55,  2.42s/it] 84%|████████▍ | 1295/1540 [52:21<09:52,  2.42s/it] 84%|████████▍ | 1296/1540 [52:24<09:50,  2.42s/it] 84%|████████▍ | 1297/1540 [52:26<09:48,  2.42s/it] 84%|████████▍ | 1298/1540 [52:28<09:45,  2.42s/it] 84%|████████▍ | 1299/1540 [52:31<09:44,  2.43s/it] 84%|████████▍ | 1300/1540 [52:33<09:41,  2.42s/it]                                                   {'loss': 0.2963, 'grad_norm': 0.25700512528419495, 'learning_rate': 4.685023531327193e-05, 'epoch': 4.22}
 84%|████████▍ | 1300/1540 [52:33<09:41,  2.42s/it] 84%|████████▍ | 1301/1540 [52:36<09:39,  2.42s/it] 85%|████████▍ | 1302/1540 [52:38<09:36,  2.42s/it] 85%|████████▍ | 1303/1540 [52:40<09:33,  2.42s/it] 85%|████████▍ | 1304/1540 [52:43<09:31,  2.42s/it] 85%|████████▍ | 1305/1540 [52:45<09:28,  2.42s/it] 85%|████████▍ | 1306/1540 [52:48<09:25,  2.42s/it] 85%|████████▍ | 1307/1540 [52:50<09:28,  2.44s/it] 85%|████████▍ | 1308/1540 [52:53<09:24,  2.43s/it] 85%|████████▌ | 1309/1540 [52:55<09:21,  2.43s/it] 85%|████████▌ | 1310/1540 [52:57<09:17,  2.43s/it] 85%|████████▌ | 1311/1540 [53:00<09:15,  2.42s/it] 85%|████████▌ | 1312/1540 [53:02<09:12,  2.42s/it] 85%|████████▌ | 1313/1540 [53:05<09:11,  2.43s/it] 85%|████████▌ | 1314/1540 [53:07<09:08,  2.43s/it] 85%|████████▌ | 1315/1540 [53:10<09:05,  2.42s/it] 85%|████████▌ | 1316/1540 [53:12<09:02,  2.42s/it] 86%|████████▌ | 1317/1540 [53:14<08:59,  2.42s/it] 86%|████████▌ | 1318/1540 [53:17<08:57,  2.42s/it] 86%|████████▌ | 1319/1540 [53:19<08:54,  2.42s/it] 86%|████████▌ | 1320/1540 [53:22<08:52,  2.42s/it]                                                   {'loss': 0.3127, 'grad_norm': 0.2819778025150299, 'learning_rate': 3.968661679220467e-05, 'epoch': 4.29}
 86%|████████▌ | 1320/1540 [53:22<08:52,  2.42s/it] 86%|████████▌ | 1321/1540 [53:24<08:49,  2.42s/it] 86%|████████▌ | 1322/1540 [53:27<08:47,  2.42s/it] 86%|████████▌ | 1323/1540 [53:29<08:44,  2.42s/it] 86%|████████▌ | 1324/1540 [53:31<08:42,  2.42s/it] 86%|████████▌ | 1325/1540 [53:34<08:40,  2.42s/it] 86%|████████▌ | 1326/1540 [53:36<08:37,  2.42s/it] 86%|████████▌ | 1327/1540 [53:39<08:35,  2.42s/it] 86%|████████▌ | 1328/1540 [53:41<08:32,  2.42s/it] 86%|████████▋ | 1329/1540 [53:43<08:30,  2.42s/it] 86%|████████▋ | 1330/1540 [53:46<08:28,  2.42s/it] 86%|████████▋ | 1331/1540 [53:48<08:25,  2.42s/it] 86%|████████▋ | 1332/1540 [53:51<08:23,  2.42s/it] 87%|████████▋ | 1333/1540 [53:53<08:20,  2.42s/it] 87%|████████▋ | 1334/1540 [53:56<08:19,  2.42s/it] 87%|████████▋ | 1335/1540 [53:58<08:16,  2.42s/it] 87%|████████▋ | 1336/1540 [54:00<08:13,  2.42s/it] 87%|████████▋ | 1337/1540 [54:03<08:11,  2.42s/it] 87%|████████▋ | 1338/1540 [54:05<08:08,  2.42s/it] 87%|████████▋ | 1339/1540 [54:08<08:06,  2.42s/it] 87%|████████▋ | 1340/1540 [54:10<08:03,  2.42s/it]                                                   {'loss': 0.2997, 'grad_norm': 0.2815568447113037, 'learning_rate': 3.306990226620032e-05, 'epoch': 4.35}
 87%|████████▋ | 1340/1540 [54:10<08:03,  2.42s/it] 87%|████████▋ | 1341/1540 [54:12<08:01,  2.42s/it] 87%|████████▋ | 1342/1540 [54:15<08:00,  2.43s/it] 87%|████████▋ | 1343/1540 [54:17<07:57,  2.42s/it] 87%|████████▋ | 1344/1540 [54:20<07:54,  2.42s/it] 87%|████████▋ | 1345/1540 [54:22<07:52,  2.42s/it] 87%|████████▋ | 1346/1540 [54:25<07:49,  2.42s/it] 87%|████████▋ | 1347/1540 [54:27<07:47,  2.42s/it] 88%|████████▊ | 1348/1540 [54:29<07:44,  2.42s/it] 88%|████████▊ | 1349/1540 [54:32<07:43,  2.43s/it] 88%|████████▊ | 1350/1540 [54:34<07:40,  2.42s/it] 88%|████████▊ | 1351/1540 [54:37<07:37,  2.42s/it] 88%|████████▊ | 1352/1540 [54:39<07:35,  2.42s/it] 88%|████████▊ | 1353/1540 [54:42<07:32,  2.42s/it] 88%|████████▊ | 1354/1540 [54:44<07:30,  2.42s/it] 88%|████████▊ | 1355/1540 [54:46<07:27,  2.42s/it] 88%|████████▊ | 1356/1540 [54:49<07:25,  2.42s/it] 88%|████████▊ | 1357/1540 [54:51<07:22,  2.42s/it] 88%|████████▊ | 1358/1540 [54:54<07:20,  2.42s/it] 88%|████████▊ | 1359/1540 [54:56<07:17,  2.42s/it] 88%|████████▊ | 1360/1540 [54:58<07:15,  2.42s/it]                                                   {'loss': 0.3024, 'grad_norm': 0.26327088475227356, 'learning_rate': 2.7017297999326535e-05, 'epoch': 4.42}
 88%|████████▊ | 1360/1540 [54:59<07:15,  2.42s/it] 88%|████████▊ | 1361/1540 [55:01<07:13,  2.42s/it] 88%|████████▊ | 1362/1540 [55:03<07:10,  2.42s/it] 89%|████████▊ | 1363/1540 [55:06<07:08,  2.42s/it] 89%|████████▊ | 1364/1540 [55:08<07:05,  2.42s/it] 89%|████████▊ | 1365/1540 [55:11<07:03,  2.42s/it] 89%|████████▊ | 1366/1540 [55:13<07:00,  2.42s/it] 89%|████████▉ | 1367/1540 [55:15<06:58,  2.42s/it] 89%|████████▉ | 1368/1540 [55:18<06:56,  2.42s/it] 89%|████████▉ | 1369/1540 [55:20<06:53,  2.42s/it] 89%|████████▉ | 1370/1540 [55:23<06:51,  2.42s/it] 89%|████████▉ | 1371/1540 [55:25<06:48,  2.42s/it] 89%|████████▉ | 1372/1540 [55:28<06:46,  2.42s/it] 89%|████████▉ | 1373/1540 [55:30<06:43,  2.42s/it] 89%|████████▉ | 1374/1540 [55:32<06:41,  2.42s/it] 89%|████████▉ | 1375/1540 [55:35<06:39,  2.42s/it] 89%|████████▉ | 1376/1540 [55:37<06:36,  2.42s/it] 89%|████████▉ | 1377/1540 [55:40<06:37,  2.44s/it] 89%|████████▉ | 1378/1540 [55:42<06:33,  2.43s/it] 90%|████████▉ | 1379/1540 [55:45<06:30,  2.43s/it] 90%|████████▉ | 1380/1540 [55:47<06:28,  2.43s/it]                                                   {'loss': 0.2747, 'grad_norm': 0.3146466016769409, 'learning_rate': 2.1544543329725387e-05, 'epoch': 4.48}
 90%|████████▉ | 1380/1540 [55:47<06:28,  2.43s/it] 90%|████████▉ | 1381/1540 [55:49<06:25,  2.42s/it] 90%|████████▉ | 1382/1540 [55:52<06:22,  2.42s/it] 90%|████████▉ | 1383/1540 [55:54<06:20,  2.42s/it] 90%|████████▉ | 1384/1540 [55:57<06:17,  2.42s/it] 90%|████████▉ | 1385/1540 [55:59<06:15,  2.42s/it] 90%|█████████ | 1386/1540 [56:01<06:14,  2.43s/it] 90%|█████████ | 1387/1540 [56:04<06:11,  2.43s/it] 90%|█████████ | 1388/1540 [56:06<06:08,  2.43s/it] 90%|█████████ | 1389/1540 [56:09<06:05,  2.42s/it] 90%|█████████ | 1390/1540 [56:11<06:03,  2.42s/it] 90%|█████████ | 1391/1540 [56:14<06:00,  2.42s/it] 90%|█████████ | 1392/1540 [56:16<05:58,  2.42s/it] 90%|█████████ | 1393/1540 [56:18<05:55,  2.42s/it] 91%|█████████ | 1394/1540 [56:21<05:53,  2.42s/it] 91%|█████████ | 1395/1540 [56:23<05:50,  2.42s/it] 91%|█████████ | 1396/1540 [56:26<05:48,  2.42s/it] 91%|█████████ | 1397/1540 [56:28<05:45,  2.42s/it] 91%|█████████ | 1398/1540 [56:31<05:43,  2.42s/it] 91%|█████████ | 1399/1540 [56:33<05:41,  2.42s/it] 91%|█████████ | 1400/1540 [56:35<05:38,  2.42s/it]                                                   {'loss': 0.2974, 'grad_norm': 0.2417582869529724, 'learning_rate': 1.6665869740658312e-05, 'epoch': 4.55}
 91%|█████████ | 1400/1540 [56:35<05:38,  2.42s/it] 91%|█████████ | 1401/1540 [56:38<05:36,  2.42s/it] 91%|█████████ | 1402/1540 [56:40<05:33,  2.42s/it] 91%|█████████ | 1403/1540 [56:43<05:31,  2.42s/it] 91%|█████████ | 1404/1540 [56:45<05:29,  2.42s/it] 91%|█████████ | 1405/1540 [56:47<05:26,  2.42s/it] 91%|█████████▏| 1406/1540 [56:50<05:24,  2.42s/it] 91%|█████████▏| 1407/1540 [56:52<05:21,  2.42s/it] 91%|█████████▏| 1408/1540 [56:55<05:19,  2.42s/it] 91%|█████████▏| 1409/1540 [56:57<05:16,  2.42s/it] 92%|█████████▏| 1410/1540 [57:00<05:14,  2.42s/it] 92%|█████████▏| 1411/1540 [57:02<05:12,  2.42s/it] 92%|█████████▏| 1412/1540 [57:04<05:09,  2.42s/it] 92%|█████████▏| 1413/1540 [57:07<05:07,  2.42s/it] 92%|█████████▏| 1414/1540 [57:09<05:04,  2.42s/it] 92%|█████████▏| 1415/1540 [57:12<05:02,  2.42s/it] 92%|█████████▏| 1416/1540 [57:14<05:00,  2.42s/it] 92%|█████████▏| 1417/1540 [57:16<04:57,  2.42s/it] 92%|█████████▏| 1418/1540 [57:19<04:55,  2.42s/it] 92%|█████████▏| 1419/1540 [57:21<04:53,  2.43s/it] 92%|█████████▏| 1420/1540 [57:24<04:50,  2.42s/it]                                                   {'loss': 0.315, 'grad_norm': 0.22305724024772644, 'learning_rate': 1.2393963852614209e-05, 'epoch': 4.61}
 92%|█████████▏| 1420/1540 [57:24<04:50,  2.42s/it] 92%|█████████▏| 1421/1540 [57:26<04:48,  2.42s/it] 92%|█████████▏| 1422/1540 [57:29<04:45,  2.42s/it] 92%|█████████▏| 1423/1540 [57:31<04:43,  2.42s/it] 92%|█████████▏| 1424/1540 [57:33<04:40,  2.42s/it] 93%|█████████▎| 1425/1540 [57:36<04:38,  2.42s/it] 93%|█████████▎| 1426/1540 [57:38<04:35,  2.42s/it] 93%|█████████▎| 1427/1540 [57:41<04:33,  2.42s/it] 93%|█████████▎| 1428/1540 [57:43<04:30,  2.42s/it] 93%|█████████▎| 1429/1540 [57:46<04:28,  2.42s/it] 93%|█████████▎| 1430/1540 [57:48<04:26,  2.42s/it] 93%|█████████▎| 1431/1540 [57:50<04:23,  2.42s/it] 93%|█████████▎| 1432/1540 [57:53<04:21,  2.42s/it] 93%|█████████▎| 1433/1540 [57:55<04:19,  2.42s/it] 93%|█████████▎| 1434/1540 [57:58<04:16,  2.42s/it] 93%|█████████▎| 1435/1540 [58:00<04:14,  2.42s/it] 93%|█████████▎| 1436/1540 [58:02<04:11,  2.42s/it] 93%|█████████▎| 1437/1540 [58:05<04:09,  2.42s/it] 93%|█████████▎| 1438/1540 [58:07<04:06,  2.42s/it] 93%|█████████▎| 1439/1540 [58:10<04:04,  2.42s/it] 94%|█████████▎| 1440/1540 [58:12<04:01,  2.42s/it]                                                   {'loss': 0.3119, 'grad_norm': 0.21234291791915894, 'learning_rate': 8.739934432715035e-06, 'epoch': 4.68}
 94%|█████████▎| 1440/1540 [58:12<04:01,  2.42s/it] 94%|█████████▎| 1441/1540 [58:15<03:59,  2.42s/it] 94%|█████████▎| 1442/1540 [58:17<03:57,  2.42s/it] 94%|█████████▎| 1443/1540 [58:19<03:54,  2.42s/it] 94%|█████████▍| 1444/1540 [58:22<03:52,  2.42s/it] 94%|█████████▍| 1445/1540 [58:24<03:49,  2.42s/it] 94%|█████████▍| 1446/1540 [58:27<03:48,  2.44s/it] 94%|█████████▍| 1447/1540 [58:29<03:46,  2.43s/it] 94%|█████████▍| 1448/1540 [58:32<03:43,  2.43s/it] 94%|█████████▍| 1449/1540 [58:34<03:40,  2.43s/it] 94%|█████████▍| 1450/1540 [58:36<03:38,  2.42s/it] 94%|█████████▍| 1451/1540 [58:39<03:35,  2.42s/it] 94%|█████████▍| 1452/1540 [58:41<03:33,  2.42s/it] 94%|█████████▍| 1453/1540 [58:44<03:30,  2.42s/it] 94%|█████████▍| 1454/1540 [58:46<03:28,  2.42s/it] 94%|█████████▍| 1455/1540 [58:49<03:25,  2.42s/it] 95%|█████████▍| 1456/1540 [58:51<03:23,  2.42s/it] 95%|█████████▍| 1457/1540 [58:53<03:20,  2.42s/it] 95%|█████████▍| 1458/1540 [58:56<03:18,  2.42s/it] 95%|█████████▍| 1459/1540 [58:58<03:15,  2.42s/it] 95%|█████████▍| 1460/1540 [59:01<03:13,  2.42s/it]                                                   {'loss': 0.3092, 'grad_norm': 0.27669259905815125, 'learning_rate': 5.7132835072101484e-06, 'epoch': 4.74}
 95%|█████████▍| 1460/1540 [59:01<03:13,  2.42s/it] 95%|█████████▍| 1461/1540 [59:03<03:11,  2.42s/it] 95%|█████████▍| 1462/1540 [59:05<03:08,  2.42s/it] 95%|█████████▌| 1463/1540 [59:08<03:06,  2.42s/it] 95%|█████████▌| 1464/1540 [59:10<03:03,  2.42s/it] 95%|█████████▌| 1465/1540 [59:13<03:01,  2.42s/it] 95%|█████████▌| 1466/1540 [59:15<02:59,  2.42s/it] 95%|█████████▌| 1467/1540 [59:18<02:56,  2.42s/it] 95%|█████████▌| 1468/1540 [59:20<02:54,  2.42s/it] 95%|█████████▌| 1469/1540 [59:22<02:51,  2.42s/it] 95%|█████████▌| 1470/1540 [59:25<02:49,  2.42s/it] 96%|█████████▌| 1471/1540 [59:27<02:46,  2.42s/it] 96%|█████████▌| 1472/1540 [59:30<02:44,  2.42s/it] 96%|█████████▌| 1473/1540 [59:32<02:42,  2.43s/it] 96%|█████████▌| 1474/1540 [59:35<02:40,  2.42s/it] 96%|█████████▌| 1475/1540 [59:37<02:37,  2.42s/it] 96%|█████████▌| 1476/1540 [59:39<02:34,  2.42s/it] 96%|█████████▌| 1477/1540 [59:42<02:32,  2.42s/it] 96%|█████████▌| 1478/1540 [59:44<02:30,  2.42s/it] 96%|█████████▌| 1479/1540 [59:47<02:27,  2.42s/it] 96%|█████████▌| 1480/1540 [59:49<02:25,  2.42s/it]                                                   {'loss': 0.309, 'grad_norm': 0.2838241755962372, 'learning_rate': 3.3218816521777827e-06, 'epoch': 4.81}
 96%|█████████▌| 1480/1540 [59:49<02:25,  2.42s/it] 96%|█████████▌| 1481/1540 [59:51<02:22,  2.42s/it] 96%|█████████▌| 1482/1540 [59:54<02:20,  2.42s/it] 96%|█████████▋| 1483/1540 [59:56<02:17,  2.42s/it] 96%|█████████▋| 1484/1540 [59:59<02:15,  2.42s/it] 96%|█████████▋| 1485/1540 [1:00:01<02:13,  2.42s/it] 96%|█████████▋| 1486/1540 [1:00:04<02:10,  2.42s/it] 97%|█████████▋| 1487/1540 [1:00:06<02:08,  2.42s/it] 97%|█████████▋| 1488/1540 [1:00:08<02:06,  2.43s/it] 97%|█████████▋| 1489/1540 [1:00:11<02:03,  2.43s/it] 97%|█████████▋| 1490/1540 [1:00:13<02:01,  2.42s/it] 97%|█████████▋| 1491/1540 [1:00:16<01:58,  2.42s/it] 97%|█████████▋| 1492/1540 [1:00:18<01:56,  2.42s/it] 97%|█████████▋| 1493/1540 [1:00:21<01:53,  2.42s/it] 97%|█████████▋| 1494/1540 [1:00:23<01:51,  2.42s/it] 97%|█████████▋| 1495/1540 [1:00:25<01:48,  2.42s/it] 97%|█████████▋| 1496/1540 [1:00:28<01:46,  2.42s/it] 97%|█████████▋| 1497/1540 [1:00:30<01:44,  2.42s/it] 97%|█████████▋| 1498/1540 [1:00:33<01:41,  2.42s/it] 97%|█████████▋| 1499/1540 [1:00:35<01:39,  2.42s/it] 97%|█████████▋| 1500/1540 [1:00:37<01:36,  2.42s/it]                                                     {'loss': 0.2927, 'grad_norm': 0.28549936413764954, 'learning_rate': 1.571947526689349e-06, 'epoch': 4.87}
 97%|█████████▋| 1500/1540 [1:00:37<01:36,  2.42s/it] 97%|█████████▋| 1501/1540 [1:00:40<01:34,  2.42s/it] 98%|█████████▊| 1502/1540 [1:00:42<01:32,  2.42s/it] 98%|█████████▊| 1503/1540 [1:00:45<01:29,  2.42s/it] 98%|█████████▊| 1504/1540 [1:00:47<01:27,  2.42s/it] 98%|█████████▊| 1505/1540 [1:00:50<01:24,  2.42s/it] 98%|█████████▊| 1506/1540 [1:00:52<01:22,  2.42s/it] 98%|█████████▊| 1507/1540 [1:00:54<01:19,  2.42s/it] 98%|█████████▊| 1508/1540 [1:00:57<01:17,  2.42s/it] 98%|█████████▊| 1509/1540 [1:00:59<01:14,  2.42s/it] 98%|█████████▊| 1510/1540 [1:01:02<01:12,  2.42s/it] 98%|█████████▊| 1511/1540 [1:01:04<01:10,  2.42s/it] 98%|█████████▊| 1512/1540 [1:01:06<01:07,  2.42s/it] 98%|█████████▊| 1513/1540 [1:01:09<01:05,  2.42s/it] 98%|█████████▊| 1514/1540 [1:01:11<01:02,  2.42s/it] 98%|█████████▊| 1515/1540 [1:01:14<01:00,  2.42s/it] 98%|█████████▊| 1516/1540 [1:01:16<00:58,  2.44s/it] 99%|█████████▊| 1517/1540 [1:01:19<00:55,  2.43s/it] 99%|█████████▊| 1518/1540 [1:01:21<00:53,  2.43s/it] 99%|█████████▊| 1519/1540 [1:01:23<00:50,  2.42s/it] 99%|█████████▊| 1520/1540 [1:01:26<00:48,  2.42s/it]                                                     {'loss': 0.3064, 'grad_norm': 0.291903018951416, 'learning_rate': 4.680317016582669e-07, 'epoch': 4.94}
 99%|█████████▊| 1520/1540 [1:01:26<00:48,  2.42s/it] 99%|█████████▉| 1521/1540 [1:01:28<00:46,  2.42s/it] 99%|█████████▉| 1522/1540 [1:01:31<00:43,  2.42s/it] 99%|█████████▉| 1523/1540 [1:01:33<00:41,  2.42s/it] 99%|█████████▉| 1524/1540 [1:01:36<00:38,  2.42s/it] 99%|█████████▉| 1525/1540 [1:01:38<00:36,  2.42s/it] 99%|█████████▉| 1526/1540 [1:01:40<00:33,  2.42s/it] 99%|█████████▉| 1527/1540 [1:01:43<00:31,  2.42s/it] 99%|█████████▉| 1528/1540 [1:01:45<00:29,  2.42s/it] 99%|█████████▉| 1529/1540 [1:01:48<00:26,  2.42s/it] 99%|█████████▉| 1530/1540 [1:01:50<00:24,  2.42s/it] 99%|█████████▉| 1531/1540 [1:01:52<00:21,  2.42s/it] 99%|█████████▉| 1532/1540 [1:01:55<00:19,  2.42s/it]100%|█████████▉| 1533/1540 [1:01:57<00:16,  2.42s/it]100%|█████████▉| 1534/1540 [1:02:00<00:14,  2.42s/it]100%|█████████▉| 1535/1540 [1:02:02<00:12,  2.42s/it]100%|█████████▉| 1536/1540 [1:02:05<00:09,  2.42s/it]100%|█████████▉| 1537/1540 [1:02:07<00:07,  2.42s/it]100%|█████████▉| 1538/1540 [1:02:09<00:04,  2.42s/it]100%|█████████▉| 1539/1540 [1:02:12<00:02,  2.42s/it]100%|██████████| 1540/1540 [1:02:14<00:00,  2.42s/it]                                                     {'loss': 0.2957, 'grad_norm': 0.2915961742401123, 'learning_rate': 1.300482642560552e-08, 'epoch': 5.0}
100%|██████████| 1540/1540 [1:02:14<00:00,  2.42s/it][INFO|trainer.py:2231] 2024-05-25 04:39:50,441 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 3744.7167, 'train_samples_per_second': 3.287, 'train_steps_per_second': 0.411, 'train_loss': 0.4382349971052888, 'epoch': 5.0}
100%|██████████| 1540/1540 [1:02:14<00:00,  2.42s/it]100%|██████████| 1540/1540 [1:02:14<00:00,  2.43s/it]
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4382
  train_runtime            = 1:02:24.71
  train_samples_per_second =      3.287
  train_steps_per_second   =      0.411
[INFO|trainer.py:3203] 2024-05-25 04:39:50,448 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/length_then_extractiveness
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-05-25 04:39:51,624 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 04:39:51,626 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 04:39:52,202 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 04:39:52,204 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 04:39:52,256 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 04:39:52,257 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/special_tokens_map.json
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:471] 2024-05-25 04:39:53,197 >> Configuration saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/final_merged_model/config.json
[INFO|configuration_utils.py:697] 2024-05-25 04:39:53,199 >> Configuration saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/final_merged_model/generation_config.json
[INFO|modeling_utils.py:2474] 2024-05-25 04:39:59,148 >> Model weights saved in /scratch/tathagato/adapter_experiments/length_then_extractiveness/final_merged_model/model.safetensors
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.006 MB uploadedwandb: | 0.006 MB of 0.006 MB uploadedwandb: / 0.006 MB of 0.034 MB uploadedwandb: - 0.034 MB of 0.038 MB uploadedwandb: \ 0.034 MB of 0.038 MB uploadedwandb: | 0.034 MB of 0.038 MB uploadedwandb: / 0.034 MB of 0.038 MB uploadedwandb: - 0.034 MB of 0.038 MB uploadedwandb: \ 0.034 MB of 0.038 MB uploadedwandb: | 0.034 MB of 0.038 MB uploadedwandb: / 0.034 MB of 0.038 MB uploadedwandb: - 0.034 MB of 0.038 MB uploadedwandb: \ 0.034 MB of 0.038 MB uploadedwandb: | 0.034 MB of 0.038 MB uploadedwandb: / 0.034 MB of 0.038 MB uploadedwandb: - 0.034 MB of 0.038 MB uploadedwandb: \ 0.034 MB of 0.038 MB uploadedwandb: | 0.034 MB of 0.038 MB uploadedwandb: / 0.034 MB of 0.038 MB uploadedwandb: - 0.034 MB of 0.038 MB uploadedwandb: \ 0.034 MB of 0.038 MB uploadedwandb: | 0.034 MB of 0.038 MB uploadedwandb: / 0.034 MB of 0.038 MB uploadedwandb: - 0.034 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:     train/grad_norm ▇█▇▄▄▄▄▂▅▂▅▄▄▄▅▄▄▃▄▄▄▃▄▂▃▃▃▅▃▅▃▃▃▂▂▃▁▂▂▃
wandb: train/learning_rate ▁▂▃▄▅▆▆▇██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:          train/loss ██▇▇▇▆▆▆▆▅▅▅▅▄▄▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 1.5729502080466944e+17
wandb:              train/epoch 5.0
wandb:        train/global_step 1540
wandb:          train/grad_norm 0.2916
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.2957
wandb:               train_loss 0.43823
wandb:            train_runtime 3744.7167
wandb: train_samples_per_second 3.287
wandb:   train_steps_per_second 0.411
wandb: 
wandb: 🚀 View run sleek-snowflake-99 at: https://wandb.ai/ihub-drug-discovery/huggingface/runs/9u7o4mjh
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240525_033728-9u7o4mjh/logs
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-05-25 04:40:52 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 04:40:52 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 04:40:52 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch/tathagato/adapter_experiments/extractiveness_then_topic/runs/May25_04-40-52_gnode081,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=20,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/scratch/tathagato/adapter_experiments/extractiveness_then_topic,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch/tathagato/adapter_experiments/extractiveness_then_topic,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=400,
save_strategy=steps,
save_total_limit=400,
seed=0,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
2024-05-25 04:40:52 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'o_proj', 'k_proj', 'v_proj', 'q_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
2024-05-25 04:40:52 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 04:40:52 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1 distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:726] 2024-05-25 04:40:52,556 >> loading configuration file config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 04:40:52,560 >> Model config LlamaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": false,
  "vocab_size": 32000
}

[WARNING|modeling_utils.py:3058] 2024-05-25 04:40:52,714 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|quantizer_bnb_4bit.py:247] 2024-05-25 04:40:52,716 >> The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' 
[WARNING|modeling_utils.py:3058] 2024-05-25 04:40:52,716 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:3283] 2024-05-25 04:40:52,717 >> loading weights file model.safetensors from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors
[INFO|modeling_utils.py:1417] 2024-05-25 04:40:52,736 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:928] 2024-05-25 04:40:52,739 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

[WARNING|modeling_utils.py:3058] 2024-05-25 04:40:52,977 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 04:40:53,024 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:4024] 2024-05-25 04:40:55,840 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-25 04:40:55,841 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-05-25 04:40:56,085 >> loading configuration file generation_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-25 04:40:56,085 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 2048,
  "pad_token_id": 0
}

[INFO|tokenization_utils_base.py:2084] 2024-05-25 04:40:56,494 >> loading file tokenizer.model from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-05-25 04:40:56,494 >> loading file tokenizer.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 04:40:56,494 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-05-25 04:40:56,494 >> loading file special_tokens_map.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 04:40:56,494 >> loading file tokenizer_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json
loading model from : /scratch/tathagato/adapter_experiments/extractiveness/extractiveness
loading model from : /scratch/tathagato/adapter_experiments/extractiveness/extractiveness
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
loading model from : /scratch/tathagato/adapter_experiments/extractiveness/extractiveness
loading model from : /scratch/tathagato/adapter_experiments/extractiveness/extractiveness
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
train dataset size 2013
test dataset size 272
2013
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
Spawning 10 processes
2024-05-25 04:40:58 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to train_sft (num_proc=10):   0%|          | 0/2013 [00:00<?, ? examples/s]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
Applying chat template to train_sft (num_proc=10):   0%|          | 1/2013 [00:00<24:12,  1.38 examples/s]total model parameters : 4505600
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Applying chat template to train_sft (num_proc=10):  10%|█         | 203/2013 [00:00<00:06, 272.55 examples/s]train dataset size 2013
test dataset size 272
2013
train dataset size 2013
test dataset size 272
2013
Applying chat template to train_sft (num_proc=10):  20%|██        | 405/2013 [00:01<00:03, 446.96 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 607/2013 [00:01<00:02, 637.36 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 808/2013 [00:01<00:01, 639.06 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/2013 [00:00<?, ? examples/s]trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Applying chat template to train_sft (num_proc=10):   0%|          | 0/2013 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 1009/2013 [00:01<00:01, 682.83 examples/s]train dataset size 2013
test dataset size 272
2013
Applying chat template to train_sft (num_proc=10):  60%|██████    | 1210/2013 [00:02<00:01, 738.40 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 1411/2013 [00:02<00:00, 821.45 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/2013 [00:00<23:29,  1.43 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/2013 [00:00<24:31,  1.37 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 1612/2013 [00:02<00:00, 786.04 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 203/2013 [00:00<00:06, 276.51 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 203/2013 [00:01<00:07, 246.67 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 1813/2013 [00:02<00:00, 759.81 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 405/2013 [00:01<00:03, 479.59 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/2013 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 405/2013 [00:01<00:03, 413.85 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 607/2013 [00:01<00:02, 593.65 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 2013/2013 [00:03<00:00, 621.04 examples/s]
Applying chat template to train_sft (num_proc=10):  30%|███       | 607/2013 [00:01<00:02, 570.02 examples/s]Concatenating 10 shards
2024-05-25 04:41:02 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Applying chat template to train_sft (num_proc=10):  40%|████      | 808/2013 [00:01<00:01, 638.50 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 808/2013 [00:01<00:01, 623.79 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 1009/2013 [00:01<00:01, 694.64 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/2013 [00:00<25:23,  1.32 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 1009/2013 [00:02<00:01, 649.81 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 1210/2013 [00:02<00:01, 756.58 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 203/2013 [00:01<00:07, 240.77 examples/s]Spawning 10 processes
2024-05-25 04:41:02 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to test_sft (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 1210/2013 [00:02<00:01, 678.77 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 405/2013 [00:01<00:03, 463.96 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 1411/2013 [00:02<00:00, 761.96 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 1411/2013 [00:02<00:00, 675.80 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/272 [00:00<01:51,  2.43 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 1612/2013 [00:02<00:00, 730.70 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 607/2013 [00:01<00:02, 524.10 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 1612/2013 [00:02<00:00, 770.06 examples/s]Applying chat template to test_sft (num_proc=10):  21%|██        | 57/272 [00:00<00:01, 119.80 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 1813/2013 [00:02<00:00, 800.01 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 808/2013 [00:01<00:02, 577.05 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 111/272 [00:00<00:00, 183.89 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 2013/2013 [00:03<00:00, 632.93 examples/s]
Applying chat template to train_sft (num_proc=10):  50%|█████     | 1009/2013 [00:02<00:01, 684.33 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 1813/2013 [00:03<00:00, 648.31 examples/s]Applying chat template to test_sft (num_proc=10):  61%|██████    | 165/272 [00:00<00:00, 206.26 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 245/272 [00:01<00:00, 322.66 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 1210/2013 [00:02<00:01, 681.11 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 2013/2013 [00:03<00:00, 572.62 examples/s]
Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 188.24 examples/s]
Applying chat template to train_sft (num_proc=10):  70%|███████   | 1411/2013 [00:02<00:00, 714.72 examples/s]Concatenating 10 shards
2024-05-25 04:41:04 - INFO - datasets.arrow_dataset - Concatenating 10 shards
tokenizer padding side left
Applying chat template to train_sft (num_proc=10):  80%|████████  | 1612/2013 [00:02<00:00, 771.28 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 1813/2013 [00:03<00:00, 796.41 examples/s]Using custom data configuration default-6ab037a909b14552
2024-05-25 04:41:04 - INFO - datasets.builder - Using custom data configuration default-6ab037a909b14552
Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
2024-05-25 04:41:04 - INFO - datasets.info - Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
Overwrite dataset info from restored data version if exists.
2024-05-25 04:41:05 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0
2024-05-25 04:41:05 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0
Applying chat template to test_sft (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0)
2024-05-25 04:41:05 - INFO - datasets.builder - Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0)
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0
2024-05-25 04:41:05 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-6ab037a909b14552/0.0.0
Applying chat template to test_sft (num_proc=10):   0%|          | 1/272 [00:00<01:37,  2.79 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 2013/2013 [00:03<00:00, 603.79 examples/s]
Applying chat template to test_sft (num_proc=10):  21%|██        | 57/272 [00:00<00:01, 135.89 examples/s]Applying chat template to test_sft (num_proc=10):  40%|████      | 110/272 [00:00<00:00, 226.98 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/272 [00:00<01:50,  2.46 examples/s]Applying chat template to test_sft (num_proc=10):  60%|██████    | 164/272 [00:00<00:00, 272.31 examples/s]Applying chat template to test_sft (num_proc=10):  11%|█         | 29/272 [00:00<00:03, 67.55 examples/s]Applying chat template to test_sft (num_proc=10):  81%|████████  | 219/272 [00:00<00:00, 293.09 examples/s]Applying chat template to test_sft (num_proc=10):  31%|███       | 84/272 [00:00<00:00, 188.69 examples/s]Applying chat template to test_sft (num_proc=10):  50%|█████     | 137/272 [00:00<00:00, 244.37 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 226.19 examples/s]
Applying chat template to test_sft (num_proc=10):  70%|███████   | 191/272 [00:00<00:00, 316.26 examples/s]tokenizer padding side left
Applying chat template to test_sft (num_proc=10):  90%|█████████ | 245/272 [00:01<00:00, 350.68 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/272 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 216.50 examples/s]
tokenizer padding side left
Applying chat template to test_sft (num_proc=10):   0%|          | 1/272 [00:00<01:59,  2.27 examples/s]Applying chat template to test_sft (num_proc=10):  21%|██        | 57/272 [00:00<00:01, 123.49 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 111/272 [00:00<00:00, 199.61 examples/s]Applying chat template to test_sft (num_proc=10):  71%|███████   | 192/272 [00:00<00:00, 311.35 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 246/272 [00:01<00:00, 320.51 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 272/272 [00:01<00:00, 217.58 examples/s]
tokenizer padding side left
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
2024-05-25 04:41:07 - WARNING - accelerate.utils.other - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:607] 2024-05-25 04:41:09,882 >> Using auto half precision backend
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
[INFO|trainer.py:1969] 2024-05-25 04:41:10,163 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-25 04:41:10,163 >>   Num examples = 1,279
[INFO|trainer.py:1971] 2024-05-25 04:41:10,163 >>   Num Epochs = 5
[INFO|trainer.py:1972] 2024-05-25 04:41:10,163 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-25 04:41:10,163 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-05-25 04:41:10,163 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-05-25 04:41:10,163 >>   Total optimization steps = 800
[INFO|trainer.py:1978] 2024-05-25 04:41:10,165 >>   Number of trainable parameters = 4,505,600
[INFO|integration_utils.py:723] 2024-05-25 04:41:10,226 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSum/experiments/wandb/run-20240525_044113-uczqljuo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-grass-100
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/huggingface
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/huggingface/runs/uczqljuo
  0%|          | 0/800 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/800 [00:02<32:45,  2.46s/it]  0%|          | 2/800 [00:04<32:16,  2.43s/it]  0%|          | 3/800 [00:07<32:03,  2.41s/it]  0%|          | 4/800 [00:09<31:59,  2.41s/it]  1%|          | 5/800 [00:12<31:54,  2.41s/it]  1%|          | 6/800 [00:14<31:49,  2.41s/it]  1%|          | 7/800 [00:16<31:47,  2.40s/it]  1%|          | 8/800 [00:19<31:44,  2.40s/it]  1%|          | 9/800 [00:21<31:41,  2.40s/it]  1%|▏         | 10/800 [00:24<31:40,  2.41s/it]  1%|▏         | 11/800 [00:26<31:38,  2.41s/it]  2%|▏         | 12/800 [00:28<31:56,  2.43s/it]  2%|▏         | 13/800 [00:31<31:56,  2.44s/it]  2%|▏         | 14/800 [00:33<31:47,  2.43s/it]  2%|▏         | 15/800 [00:36<31:40,  2.42s/it]  2%|▏         | 16/800 [00:38<31:35,  2.42s/it]  2%|▏         | 17/800 [00:41<31:31,  2.42s/it]  2%|▏         | 18/800 [00:43<31:28,  2.41s/it]  2%|▏         | 19/800 [00:45<31:25,  2.41s/it]  2%|▎         | 20/800 [00:48<31:23,  2.41s/it]                                                {'loss': 0.7247, 'grad_norm': 0.5829604864120483, 'learning_rate': 6.25e-05, 'epoch': 0.12}
  2%|▎         | 20/800 [00:48<31:23,  2.41s/it]  3%|▎         | 21/800 [00:50<31:20,  2.41s/it]  3%|▎         | 22/800 [00:53<31:17,  2.41s/it]  3%|▎         | 23/800 [00:55<31:14,  2.41s/it]  3%|▎         | 24/800 [00:57<31:12,  2.41s/it]  3%|▎         | 25/800 [01:00<31:11,  2.42s/it]  3%|▎         | 26/800 [01:02<31:16,  2.43s/it]  3%|▎         | 27/800 [01:05<31:12,  2.42s/it]  4%|▎         | 28/800 [01:07<31:07,  2.42s/it]  4%|▎         | 29/800 [01:10<31:09,  2.43s/it]  4%|▍         | 30/800 [01:12<31:06,  2.42s/it]  4%|▍         | 31/800 [01:14<31:03,  2.42s/it]  4%|▍         | 32/800 [01:17<30:58,  2.42s/it]  4%|▍         | 33/800 [01:19<30:55,  2.42s/it]  4%|▍         | 34/800 [01:22<30:52,  2.42s/it]  4%|▍         | 35/800 [01:24<30:51,  2.42s/it]  4%|▍         | 36/800 [01:27<30:48,  2.42s/it]  5%|▍         | 37/800 [01:29<30:45,  2.42s/it]  5%|▍         | 38/800 [01:31<30:43,  2.42s/it]  5%|▍         | 39/800 [01:34<30:46,  2.43s/it]  5%|▌         | 40/800 [01:36<30:42,  2.42s/it]                                                {'loss': 0.6589, 'grad_norm': 0.5035312175750732, 'learning_rate': 0.000125, 'epoch': 0.25}
  5%|▌         | 40/800 [01:36<30:42,  2.42s/it]  5%|▌         | 41/800 [01:39<30:38,  2.42s/it]  5%|▌         | 42/800 [01:41<30:39,  2.43s/it]  5%|▌         | 43/800 [01:44<30:36,  2.43s/it]  6%|▌         | 44/800 [01:46<30:32,  2.42s/it]  6%|▌         | 45/800 [01:48<30:29,  2.42s/it]  6%|▌         | 46/800 [01:51<30:26,  2.42s/it]  6%|▌         | 47/800 [01:53<30:22,  2.42s/it]  6%|▌         | 48/800 [01:56<30:19,  2.42s/it]  6%|▌         | 49/800 [01:58<30:17,  2.42s/it]  6%|▋         | 50/800 [02:00<30:14,  2.42s/it]  6%|▋         | 51/800 [02:03<30:11,  2.42s/it]  6%|▋         | 52/800 [02:05<30:09,  2.42s/it]  7%|▋         | 53/800 [02:08<30:07,  2.42s/it]  7%|▋         | 54/800 [02:10<30:10,  2.43s/it]  7%|▋         | 55/800 [02:13<30:06,  2.42s/it]  7%|▋         | 56/800 [02:15<30:17,  2.44s/it]  7%|▋         | 57/800 [02:17<30:10,  2.44s/it]  7%|▋         | 58/800 [02:20<30:03,  2.43s/it]  7%|▋         | 59/800 [02:22<29:58,  2.43s/it]  8%|▊         | 60/800 [02:25<29:54,  2.43s/it]                                                {'loss': 0.6546, 'grad_norm': 0.46619346737861633, 'learning_rate': 0.0001875, 'epoch': 0.38}
  8%|▊         | 60/800 [02:25<29:54,  2.43s/it]  8%|▊         | 61/800 [02:27<29:51,  2.42s/it]  8%|▊         | 62/800 [02:30<29:47,  2.42s/it]  8%|▊         | 63/800 [02:32<29:45,  2.42s/it]  8%|▊         | 64/800 [02:34<29:42,  2.42s/it]  8%|▊         | 65/800 [02:37<29:39,  2.42s/it]  8%|▊         | 66/800 [02:39<29:36,  2.42s/it]  8%|▊         | 67/800 [02:42<29:34,  2.42s/it]  8%|▊         | 68/800 [02:44<29:36,  2.43s/it]  9%|▊         | 69/800 [02:47<29:31,  2.42s/it]  9%|▉         | 70/800 [02:49<29:32,  2.43s/it]  9%|▉         | 71/800 [02:51<29:28,  2.43s/it]  9%|▉         | 72/800 [02:54<29:24,  2.42s/it]  9%|▉         | 73/800 [02:56<29:21,  2.42s/it]  9%|▉         | 74/800 [02:59<29:18,  2.42s/it]  9%|▉         | 75/800 [03:01<29:15,  2.42s/it] 10%|▉         | 76/800 [03:03<29:13,  2.42s/it] 10%|▉         | 77/800 [03:06<29:10,  2.42s/it] 10%|▉         | 78/800 [03:08<29:07,  2.42s/it] 10%|▉         | 79/800 [03:11<29:04,  2.42s/it] 10%|█         | 80/800 [03:13<29:02,  2.42s/it]                                                {'loss': 0.6327, 'grad_norm': 0.3986881375312805, 'learning_rate': 0.00025, 'epoch': 0.5}
 10%|█         | 80/800 [03:13<29:02,  2.42s/it] 10%|█         | 81/800 [03:16<29:00,  2.42s/it] 10%|█         | 82/800 [03:18<28:58,  2.42s/it] 10%|█         | 83/800 [03:20<29:00,  2.43s/it] 10%|█         | 84/800 [03:23<28:56,  2.43s/it] 11%|█         | 85/800 [03:25<28:52,  2.42s/it] 11%|█         | 86/800 [03:28<28:49,  2.42s/it] 11%|█         | 87/800 [03:30<28:46,  2.42s/it] 11%|█         | 88/800 [03:33<28:44,  2.42s/it] 11%|█         | 89/800 [03:35<28:40,  2.42s/it] 11%|█▏        | 90/800 [03:37<28:38,  2.42s/it] 11%|█▏        | 91/800 [03:40<28:36,  2.42s/it] 12%|█▏        | 92/800 [03:42<28:33,  2.42s/it] 12%|█▏        | 93/800 [03:45<28:30,  2.42s/it] 12%|█▏        | 94/800 [03:47<28:28,  2.42s/it] 12%|█▏        | 95/800 [03:49<28:25,  2.42s/it] 12%|█▏        | 96/800 [03:52<28:23,  2.42s/it] 12%|█▏        | 97/800 [03:54<28:26,  2.43s/it] 12%|█▏        | 98/800 [03:57<28:30,  2.44s/it] 12%|█▏        | 99/800 [03:59<28:24,  2.43s/it] 12%|█▎        | 100/800 [04:02<28:19,  2.43s/it]                                                 {'loss': 0.6265, 'grad_norm': 0.42130404710769653, 'learning_rate': 0.0003125, 'epoch': 0.62}
 12%|█▎        | 100/800 [04:02<28:19,  2.43s/it] 13%|█▎        | 101/800 [04:04<28:16,  2.43s/it] 13%|█▎        | 102/800 [04:06<28:12,  2.42s/it] 13%|█▎        | 103/800 [04:09<28:08,  2.42s/it] 13%|█▎        | 104/800 [04:11<28:05,  2.42s/it] 13%|█▎        | 105/800 [04:14<28:03,  2.42s/it] 13%|█▎        | 106/800 [04:16<28:00,  2.42s/it] 13%|█▎        | 107/800 [04:19<27:57,  2.42s/it] 14%|█▎        | 108/800 [04:21<27:54,  2.42s/it] 14%|█▎        | 109/800 [04:23<27:52,  2.42s/it] 14%|█▍        | 110/800 [04:26<27:49,  2.42s/it] 14%|█▍        | 111/800 [04:28<27:53,  2.43s/it] 14%|█▍        | 112/800 [04:31<27:56,  2.44s/it] 14%|█▍        | 113/800 [04:33<27:50,  2.43s/it] 14%|█▍        | 114/800 [04:36<27:45,  2.43s/it] 14%|█▍        | 115/800 [04:38<27:41,  2.43s/it] 14%|█▍        | 116/800 [04:40<27:37,  2.42s/it] 15%|█▍        | 117/800 [04:43<27:34,  2.42s/it] 15%|█▍        | 118/800 [04:45<27:31,  2.42s/it] 15%|█▍        | 119/800 [04:48<27:28,  2.42s/it] 15%|█▌        | 120/800 [04:50<27:25,  2.42s/it]                                                 {'loss': 0.6533, 'grad_norm': 0.34104010462760925, 'learning_rate': 0.000375, 'epoch': 0.75}
 15%|█▌        | 120/800 [04:50<27:25,  2.42s/it] 15%|█▌        | 121/800 [04:53<27:23,  2.42s/it] 15%|█▌        | 122/800 [04:55<27:21,  2.42s/it] 15%|█▌        | 123/800 [04:57<27:18,  2.42s/it] 16%|█▌        | 124/800 [05:00<27:16,  2.42s/it] 16%|█▌        | 125/800 [05:02<27:14,  2.42s/it] 16%|█▌        | 126/800 [05:05<27:24,  2.44s/it] 16%|█▌        | 127/800 [05:07<27:17,  2.43s/it] 16%|█▌        | 128/800 [05:10<27:13,  2.43s/it] 16%|█▌        | 129/800 [05:12<27:08,  2.43s/it] 16%|█▋        | 130/800 [05:14<27:04,  2.43s/it] 16%|█▋        | 131/800 [05:17<27:01,  2.42s/it] 16%|█▋        | 132/800 [05:19<26:57,  2.42s/it] 17%|█▋        | 133/800 [05:22<26:55,  2.42s/it] 17%|█▋        | 134/800 [05:24<26:52,  2.42s/it] 17%|█▋        | 135/800 [05:26<26:50,  2.42s/it] 17%|█▋        | 136/800 [05:29<26:47,  2.42s/it] 17%|█▋        | 137/800 [05:31<26:44,  2.42s/it] 17%|█▋        | 138/800 [05:34<26:42,  2.42s/it] 17%|█▋        | 139/800 [05:36<26:45,  2.43s/it] 18%|█▊        | 140/800 [05:39<26:41,  2.43s/it]                                                 {'loss': 0.6284, 'grad_norm': 0.32044461369514465, 'learning_rate': 0.0004375, 'epoch': 0.88}
 18%|█▊        | 140/800 [05:39<26:41,  2.43s/it] 18%|█▊        | 141/800 [05:41<26:40,  2.43s/it] 18%|█▊        | 142/800 [05:43<26:35,  2.43s/it] 18%|█▊        | 143/800 [05:46<26:32,  2.42s/it] 18%|█▊        | 144/800 [05:48<26:29,  2.42s/it] 18%|█▊        | 145/800 [05:51<26:26,  2.42s/it] 18%|█▊        | 146/800 [05:53<26:23,  2.42s/it] 18%|█▊        | 147/800 [05:56<26:20,  2.42s/it] 18%|█▊        | 148/800 [05:58<26:17,  2.42s/it] 19%|█▊        | 149/800 [06:00<26:15,  2.42s/it] 19%|█▉        | 150/800 [06:03<26:12,  2.42s/it] 19%|█▉        | 151/800 [06:05<26:10,  2.42s/it] 19%|█▉        | 152/800 [06:08<26:14,  2.43s/it] 19%|█▉        | 153/800 [06:10<26:10,  2.43s/it] 19%|█▉        | 154/800 [06:13<26:06,  2.42s/it] 19%|█▉        | 155/800 [06:15<26:06,  2.43s/it] 20%|█▉        | 156/800 [06:17<26:02,  2.43s/it] 20%|█▉        | 157/800 [06:20<25:58,  2.42s/it] 20%|█▉        | 158/800 [06:22<25:55,  2.42s/it] 20%|█▉        | 159/800 [06:25<25:52,  2.42s/it] 20%|██        | 160/800 [06:27<25:48,  2.42s/it]                                                 {'loss': 0.5444, 'grad_norm': 0.44764962792396545, 'learning_rate': 0.0005, 'epoch': 1.0}
 20%|██        | 160/800 [06:27<25:48,  2.42s/it] 20%|██        | 161/800 [06:29<25:47,  2.42s/it] 20%|██        | 162/800 [06:32<25:45,  2.42s/it] 20%|██        | 163/800 [06:34<25:41,  2.42s/it] 20%|██        | 164/800 [06:37<25:39,  2.42s/it] 21%|██        | 165/800 [06:39<25:36,  2.42s/it] 21%|██        | 166/800 [06:42<25:33,  2.42s/it] 21%|██        | 167/800 [06:44<25:31,  2.42s/it] 21%|██        | 168/800 [06:46<25:33,  2.43s/it] 21%|██        | 169/800 [06:49<25:29,  2.42s/it] 21%|██▏       | 170/800 [06:51<25:29,  2.43s/it] 21%|██▏       | 171/800 [06:54<25:25,  2.43s/it] 22%|██▏       | 172/800 [06:56<25:21,  2.42s/it] 22%|██▏       | 173/800 [06:59<25:18,  2.42s/it] 22%|██▏       | 174/800 [07:01<25:15,  2.42s/it] 22%|██▏       | 175/800 [07:03<25:12,  2.42s/it] 22%|██▏       | 176/800 [07:06<25:09,  2.42s/it] 22%|██▏       | 177/800 [07:08<25:07,  2.42s/it] 22%|██▏       | 178/800 [07:11<25:05,  2.42s/it] 22%|██▏       | 179/800 [07:13<25:02,  2.42s/it] 22%|██▎       | 180/800 [07:15<24:59,  2.42s/it]                                                 {'loss': 0.5367, 'grad_norm': 0.291184663772583, 'learning_rate': 0.0004989134688583259, 'epoch': 1.12}
 22%|██▎       | 180/800 [07:15<24:59,  2.42s/it] 23%|██▎       | 181/800 [07:18<25:01,  2.43s/it] 23%|██▎       | 182/800 [07:20<24:57,  2.42s/it] 23%|██▎       | 183/800 [07:23<24:54,  2.42s/it] 23%|██▎       | 184/800 [07:25<24:55,  2.43s/it] 23%|██▎       | 185/800 [07:28<24:51,  2.42s/it] 23%|██▎       | 186/800 [07:30<24:47,  2.42s/it] 23%|██▎       | 187/800 [07:32<24:44,  2.42s/it] 24%|██▎       | 188/800 [07:35<24:41,  2.42s/it] 24%|██▎       | 189/800 [07:37<24:38,  2.42s/it] 24%|██▍       | 190/800 [07:40<24:36,  2.42s/it] 24%|██▍       | 191/800 [07:42<24:33,  2.42s/it] 24%|██▍       | 192/800 [07:45<24:30,  2.42s/it] 24%|██▍       | 193/800 [07:47<24:28,  2.42s/it] 24%|██▍       | 194/800 [07:49<24:26,  2.42s/it] 24%|██▍       | 195/800 [07:52<24:33,  2.44s/it] 24%|██▍       | 196/800 [07:54<24:28,  2.43s/it] 25%|██▍       | 197/800 [07:57<24:23,  2.43s/it] 25%|██▍       | 198/800 [07:59<24:19,  2.42s/it] 25%|██▍       | 199/800 [08:02<24:17,  2.42s/it] 25%|██▌       | 200/800 [08:04<24:13,  2.42s/it]                                                 {'loss': 0.5678, 'grad_norm': 0.2916456162929535, 'learning_rate': 0.0004954327768997885, 'epoch': 1.25}
 25%|██▌       | 200/800 [08:04<24:13,  2.42s/it] 25%|██▌       | 201/800 [08:06<24:11,  2.42s/it] 25%|██▌       | 202/800 [08:09<24:08,  2.42s/it] 25%|██▌       | 203/800 [08:11<24:05,  2.42s/it] 26%|██▌       | 204/800 [08:14<24:02,  2.42s/it] 26%|██▌       | 205/800 [08:16<24:00,  2.42s/it] 26%|██▌       | 206/800 [08:19<23:58,  2.42s/it] 26%|██▌       | 207/800 [08:21<23:55,  2.42s/it] 26%|██▌       | 208/800 [08:23<23:52,  2.42s/it] 26%|██▌       | 209/800 [08:26<23:54,  2.43s/it] 26%|██▋       | 210/800 [08:28<23:50,  2.43s/it] 26%|██▋       | 211/800 [08:31<23:47,  2.42s/it] 26%|██▋       | 212/800 [08:33<23:44,  2.42s/it] 27%|██▋       | 213/800 [08:35<23:42,  2.42s/it] 27%|██▋       | 214/800 [08:38<23:38,  2.42s/it] 27%|██▋       | 215/800 [08:40<23:36,  2.42s/it] 27%|██▋       | 216/800 [08:43<23:33,  2.42s/it] 27%|██▋       | 217/800 [08:45<23:30,  2.42s/it] 27%|██▋       | 218/800 [08:48<23:28,  2.42s/it] 27%|██▋       | 219/800 [08:50<23:25,  2.42s/it] 28%|██▊       | 220/800 [08:52<23:23,  2.42s/it]                                                 {'loss': 0.5694, 'grad_norm': 0.37783586978912354, 'learning_rate': 0.0004895884331325028, 'epoch': 1.38}
 28%|██▊       | 220/800 [08:52<23:23,  2.42s/it] 28%|██▊       | 221/800 [08:55<23:21,  2.42s/it] 28%|██▊       | 222/800 [08:57<23:26,  2.43s/it] 28%|██▊       | 223/800 [09:00<23:21,  2.43s/it] 28%|██▊       | 224/800 [09:02<23:17,  2.43s/it] 28%|██▊       | 225/800 [09:05<23:13,  2.42s/it] 28%|██▊       | 226/800 [09:07<23:10,  2.42s/it] 28%|██▊       | 227/800 [09:09<23:07,  2.42s/it] 28%|██▊       | 228/800 [09:12<23:11,  2.43s/it] 29%|██▊       | 229/800 [09:14<23:06,  2.43s/it] 29%|██▉       | 230/800 [09:17<23:02,  2.43s/it] 29%|██▉       | 231/800 [09:19<22:58,  2.42s/it] 29%|██▉       | 232/800 [09:22<22:55,  2.42s/it] 29%|██▉       | 233/800 [09:24<22:52,  2.42s/it] 29%|██▉       | 234/800 [09:26<22:49,  2.42s/it] 29%|██▉       | 235/800 [09:29<22:47,  2.42s/it] 30%|██▉       | 236/800 [09:31<22:44,  2.42s/it] 30%|██▉       | 237/800 [09:34<22:44,  2.42s/it] 30%|██▉       | 238/800 [09:36<22:41,  2.42s/it] 30%|██▉       | 239/800 [09:38<22:38,  2.42s/it] 30%|███       | 240/800 [09:41<22:35,  2.42s/it]                                                 {'loss': 0.5635, 'grad_norm': 0.36406001448631287, 'learning_rate': 0.000481436721781791, 'epoch': 1.5}
 30%|███       | 240/800 [09:41<22:35,  2.42s/it] 30%|███       | 241/800 [09:43<22:33,  2.42s/it] 30%|███       | 242/800 [09:46<22:30,  2.42s/it] 30%|███       | 243/800 [09:48<22:28,  2.42s/it] 30%|███       | 244/800 [09:51<22:25,  2.42s/it] 31%|███       | 245/800 [09:53<22:23,  2.42s/it] 31%|███       | 246/800 [09:55<22:20,  2.42s/it] 31%|███       | 247/800 [09:58<22:18,  2.42s/it] 31%|███       | 248/800 [10:00<22:16,  2.42s/it] 31%|███       | 249/800 [10:03<22:13,  2.42s/it] 31%|███▏      | 250/800 [10:05<22:17,  2.43s/it] 31%|███▏      | 251/800 [10:08<22:12,  2.43s/it] 32%|███▏      | 252/800 [10:10<22:08,  2.42s/it] 32%|███▏      | 253/800 [10:12<22:05,  2.42s/it] 32%|███▏      | 254/800 [10:15<22:02,  2.42s/it] 32%|███▏      | 255/800 [10:17<21:59,  2.42s/it] 32%|███▏      | 256/800 [10:20<21:56,  2.42s/it] 32%|███▏      | 257/800 [10:22<21:59,  2.43s/it] 32%|███▏      | 258/800 [10:25<21:55,  2.43s/it] 32%|███▏      | 259/800 [10:27<21:51,  2.42s/it] 32%|███▎      | 260/800 [10:29<21:48,  2.42s/it]                                                 {'loss': 0.5666, 'grad_norm': 0.37840816378593445, 'learning_rate': 0.00047105614828413906, 'epoch': 1.62}
 32%|███▎      | 260/800 [10:29<21:48,  2.42s/it] 33%|███▎      | 261/800 [10:32<21:45,  2.42s/it] 33%|███▎      | 262/800 [10:34<21:42,  2.42s/it] 33%|███▎      | 263/800 [10:37<21:39,  2.42s/it] 33%|███▎      | 264/800 [10:39<21:37,  2.42s/it] 33%|███▎      | 265/800 [10:41<21:42,  2.43s/it] 33%|███▎      | 266/800 [10:44<21:37,  2.43s/it] 33%|███▎      | 267/800 [10:46<21:33,  2.43s/it] 34%|███▎      | 268/800 [10:49<21:29,  2.42s/it] 34%|███▎      | 269/800 [10:51<21:26,  2.42s/it] 34%|███▍      | 270/800 [10:54<21:23,  2.42s/it] 34%|███▍      | 271/800 [10:56<21:23,  2.43s/it] 34%|███▍      | 272/800 [10:58<21:20,  2.42s/it] 34%|███▍      | 273/800 [11:01<21:16,  2.42s/it] 34%|███▍      | 274/800 [11:03<21:13,  2.42s/it] 34%|███▍      | 275/800 [11:06<21:10,  2.42s/it] 34%|███▍      | 276/800 [11:08<21:07,  2.42s/it] 35%|███▍      | 277/800 [11:11<21:05,  2.42s/it] 35%|███▍      | 278/800 [11:13<21:09,  2.43s/it] 35%|███▍      | 279/800 [11:15<21:04,  2.43s/it] 35%|███▌      | 280/800 [11:18<21:01,  2.43s/it]                                                 {'loss': 0.5295, 'grad_norm': 0.3589911162853241, 'learning_rate': 0.00045854668323692813, 'epoch': 1.75}
 35%|███▌      | 280/800 [11:18<21:01,  2.43s/it] 35%|███▌      | 281/800 [11:20<20:57,  2.42s/it] 35%|███▌      | 282/800 [11:23<20:54,  2.42s/it] 35%|███▌      | 283/800 [11:25<20:51,  2.42s/it] 36%|███▌      | 284/800 [11:28<20:48,  2.42s/it] 36%|███▌      | 285/800 [11:30<20:46,  2.42s/it] 36%|███▌      | 286/800 [11:32<20:43,  2.42s/it] 36%|███▌      | 287/800 [11:35<20:41,  2.42s/it] 36%|███▌      | 288/800 [11:37<20:38,  2.42s/it] 36%|███▌      | 289/800 [11:40<20:35,  2.42s/it] 36%|███▋      | 290/800 [11:42<20:33,  2.42s/it] 36%|███▋      | 291/800 [11:44<20:31,  2.42s/it] 36%|███▋      | 292/800 [11:47<20:28,  2.42s/it] 37%|███▋      | 293/800 [11:49<20:25,  2.42s/it] 37%|███▋      | 294/800 [11:52<20:23,  2.42s/it] 37%|███▋      | 295/800 [11:54<20:21,  2.42s/it] 37%|███▋      | 296/800 [11:57<20:18,  2.42s/it] 37%|███▋      | 297/800 [11:59<20:16,  2.42s/it] 37%|███▋      | 298/800 [12:01<20:14,  2.42s/it] 37%|███▋      | 299/800 [12:04<20:11,  2.42s/it] 38%|███▊      | 300/800 [12:06<20:13,  2.43s/it]                                                 {'loss': 0.5064, 'grad_norm': 0.3543568551540375, 'learning_rate': 0.000444028799626932, 'epoch': 1.88}
 38%|███▊      | 300/800 [12:06<20:13,  2.43s/it] 38%|███▊      | 301/800 [12:09<20:10,  2.43s/it] 38%|███▊      | 302/800 [12:11<20:07,  2.42s/it] 38%|███▊      | 303/800 [12:13<20:04,  2.42s/it] 38%|███▊      | 304/800 [12:16<20:00,  2.42s/it] 38%|███▊      | 305/800 [12:18<19:58,  2.42s/it] 38%|███▊      | 306/800 [12:21<19:55,  2.42s/it] 38%|███▊      | 307/800 [12:23<19:56,  2.43s/it] 38%|███▊      | 308/800 [12:26<19:52,  2.42s/it] 39%|███▊      | 309/800 [12:28<19:49,  2.42s/it] 39%|███▉      | 310/800 [12:30<19:46,  2.42s/it] 39%|███▉      | 311/800 [12:33<19:43,  2.42s/it] 39%|███▉      | 312/800 [12:35<19:41,  2.42s/it] 39%|███▉      | 313/800 [12:38<19:38,  2.42s/it] 39%|███▉      | 314/800 [12:40<19:35,  2.42s/it] 39%|███▉      | 315/800 [12:43<19:35,  2.42s/it] 40%|███▉      | 316/800 [12:45<19:32,  2.42s/it] 40%|███▉      | 317/800 [12:47<19:28,  2.42s/it] 40%|███▉      | 318/800 [12:50<19:26,  2.42s/it] 40%|███▉      | 319/800 [12:52<19:23,  2.42s/it] 40%|████      | 320/800 [12:55<19:24,  2.43s/it]                                                 {'loss': 0.5477, 'grad_norm': 0.3580779731273651, 'learning_rate': 0.0004276423126095974, 'epoch': 2.0}
 40%|████      | 320/800 [12:55<19:24,  2.43s/it] 40%|████      | 321/800 [12:57<19:21,  2.43s/it] 40%|████      | 322/800 [13:00<19:18,  2.42s/it] 40%|████      | 323/800 [13:02<19:15,  2.42s/it] 40%|████      | 324/800 [13:04<19:12,  2.42s/it] 41%|████      | 325/800 [13:07<19:09,  2.42s/it] 41%|████      | 326/800 [13:09<19:07,  2.42s/it] 41%|████      | 327/800 [13:12<19:05,  2.42s/it] 41%|████      | 328/800 [13:14<19:02,  2.42s/it] 41%|████      | 329/800 [13:16<19:00,  2.42s/it] 41%|████▏     | 330/800 [13:19<18:57,  2.42s/it] 41%|████▏     | 331/800 [13:21<18:55,  2.42s/it] 42%|████▏     | 332/800 [13:24<18:52,  2.42s/it] 42%|████▏     | 333/800 [13:26<18:49,  2.42s/it] 42%|████▏     | 334/800 [13:29<18:56,  2.44s/it] 42%|████▏     | 335/800 [13:31<18:50,  2.43s/it] 42%|████▏     | 336/800 [13:33<18:46,  2.43s/it] 42%|████▏     | 337/800 [13:36<18:42,  2.43s/it] 42%|████▏     | 338/800 [13:38<18:39,  2.42s/it] 42%|████▏     | 339/800 [13:41<18:36,  2.42s/it] 42%|████▎     | 340/800 [13:43<18:33,  2.42s/it]                                                 {'loss': 0.4251, 'grad_norm': 0.3414101302623749, 'learning_rate': 0.0004095450330126663, 'epoch': 2.12}
 42%|████▎     | 340/800 [13:43<18:33,  2.42s/it] 43%|████▎     | 341/800 [13:46<18:31,  2.42s/it] 43%|████▎     | 342/800 [13:48<18:28,  2.42s/it] 43%|████▎     | 343/800 [13:50<18:25,  2.42s/it] 43%|████▎     | 344/800 [13:53<18:24,  2.42s/it] 43%|████▎     | 345/800 [13:55<18:21,  2.42s/it] 43%|████▎     | 346/800 [13:58<18:18,  2.42s/it] 43%|████▎     | 347/800 [14:00<18:16,  2.42s/it] 44%|████▎     | 348/800 [14:03<18:15,  2.42s/it] 44%|████▎     | 349/800 [14:05<18:12,  2.42s/it] 44%|████▍     | 350/800 [14:07<18:09,  2.42s/it] 44%|████▍     | 351/800 [14:10<18:06,  2.42s/it] 44%|████▍     | 352/800 [14:12<18:04,  2.42s/it] 44%|████▍     | 353/800 [14:15<18:02,  2.42s/it] 44%|████▍     | 354/800 [14:17<17:59,  2.42s/it] 44%|████▍     | 355/800 [14:19<17:56,  2.42s/it] 44%|████▍     | 356/800 [14:22<17:54,  2.42s/it] 45%|████▍     | 357/800 [14:24<17:51,  2.42s/it] 45%|████▍     | 358/800 [14:27<17:49,  2.42s/it] 45%|████▍     | 359/800 [14:29<17:46,  2.42s/it] 45%|████▌     | 360/800 [14:32<17:44,  2.42s/it]                                                 {'loss': 0.4626, 'grad_norm': 0.3966505527496338, 'learning_rate': 0.0003899112475316365, 'epoch': 2.25}
 45%|████▌     | 360/800 [14:32<17:44,  2.42s/it] 45%|████▌     | 361/800 [14:34<17:42,  2.42s/it] 45%|████▌     | 362/800 [14:36<17:39,  2.42s/it] 45%|████▌     | 363/800 [14:39<17:37,  2.42s/it] 46%|████▌     | 364/800 [14:41<17:35,  2.42s/it] 46%|████▌     | 365/800 [14:44<17:32,  2.42s/it] 46%|████▌     | 366/800 [14:46<17:30,  2.42s/it] 46%|████▌     | 367/800 [14:48<17:27,  2.42s/it] 46%|████▌     | 368/800 [14:51<17:25,  2.42s/it] 46%|████▌     | 369/800 [14:53<17:22,  2.42s/it] 46%|████▋     | 370/800 [14:56<17:20,  2.42s/it] 46%|████▋     | 371/800 [14:58<17:17,  2.42s/it] 46%|████▋     | 372/800 [15:01<17:16,  2.42s/it] 47%|████▋     | 373/800 [15:03<17:16,  2.43s/it] 47%|████▋     | 374/800 [15:05<17:12,  2.42s/it] 47%|████▋     | 375/800 [15:08<17:09,  2.42s/it] 47%|████▋     | 376/800 [15:10<17:07,  2.42s/it] 47%|████▋     | 377/800 [15:13<17:04,  2.42s/it] 47%|████▋     | 378/800 [15:15<17:02,  2.42s/it] 47%|████▋     | 379/800 [15:18<16:59,  2.42s/it] 48%|████▊     | 380/800 [15:20<16:56,  2.42s/it]                                                 {'loss': 0.4672, 'grad_norm': 0.3372878432273865, 'learning_rate': 0.00036893004025360926, 'epoch': 2.38}
 48%|████▊     | 380/800 [15:20<16:56,  2.42s/it] 48%|████▊     | 381/800 [15:22<16:54,  2.42s/it] 48%|████▊     | 382/800 [15:25<16:51,  2.42s/it] 48%|████▊     | 383/800 [15:27<16:48,  2.42s/it] 48%|████▊     | 384/800 [15:30<16:46,  2.42s/it] 48%|████▊     | 385/800 [15:32<16:43,  2.42s/it] 48%|████▊     | 386/800 [15:34<16:41,  2.42s/it] 48%|████▊     | 387/800 [15:37<16:39,  2.42s/it] 48%|████▊     | 388/800 [15:39<16:38,  2.42s/it] 49%|████▊     | 389/800 [15:42<16:38,  2.43s/it] 49%|████▉     | 390/800 [15:44<16:34,  2.43s/it] 49%|████▉     | 391/800 [15:47<16:31,  2.42s/it] 49%|████▉     | 392/800 [15:49<16:28,  2.42s/it] 49%|████▉     | 393/800 [15:51<16:25,  2.42s/it] 49%|████▉     | 394/800 [15:54<16:22,  2.42s/it] 49%|████▉     | 395/800 [15:56<16:20,  2.42s/it] 50%|████▉     | 396/800 [15:59<16:17,  2.42s/it] 50%|████▉     | 397/800 [16:01<16:15,  2.42s/it] 50%|████▉     | 398/800 [16:04<16:12,  2.42s/it] 50%|████▉     | 399/800 [16:06<16:10,  2.42s/it] 50%|█████     | 400/800 [16:08<16:07,  2.42s/it]                                                 {'loss': 0.4532, 'grad_norm': 0.340384304523468, 'learning_rate': 0.00034680347167416643, 'epoch': 2.5}
 50%|█████     | 400/800 [16:08<16:07,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 04:57:30,617 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/extractiveness_then_topic/checkpoint-400
[INFO|configuration_utils.py:726] 2024-05-25 04:57:32,159 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 04:57:32,162 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 04:57:32,255 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 04:57:32,255 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/checkpoint-400/special_tokens_map.json
 50%|█████     | 401/800 [16:13<19:41,  2.96s/it] 50%|█████     | 402/800 [16:15<18:33,  2.80s/it] 50%|█████     | 403/800 [16:17<17:45,  2.68s/it] 50%|█████     | 404/800 [16:20<17:21,  2.63s/it] 51%|█████     | 405/800 [16:22<16:53,  2.57s/it] 51%|█████     | 406/800 [16:25<16:33,  2.52s/it] 51%|█████     | 407/800 [16:27<16:19,  2.49s/it] 51%|█████     | 408/800 [16:30<16:08,  2.47s/it] 51%|█████     | 409/800 [16:32<15:59,  2.45s/it] 51%|█████▏    | 410/800 [16:34<15:53,  2.44s/it] 51%|█████▏    | 411/800 [16:37<15:47,  2.44s/it] 52%|█████▏    | 412/800 [16:39<15:43,  2.43s/it] 52%|█████▏    | 413/800 [16:42<15:40,  2.43s/it] 52%|█████▏    | 414/800 [16:44<15:36,  2.43s/it] 52%|█████▏    | 415/800 [16:47<15:48,  2.46s/it] 52%|█████▏    | 416/800 [16:49<15:40,  2.45s/it] 52%|█████▏    | 417/800 [16:52<15:35,  2.44s/it] 52%|█████▏    | 418/800 [16:54<15:29,  2.43s/it] 52%|█████▏    | 419/800 [16:56<15:25,  2.43s/it] 52%|█████▎    | 420/800 [16:59<15:22,  2.43s/it]                                                 {'loss': 0.4631, 'grad_norm': 0.43318063020706177, 'learning_rate': 0.00032374463274434097, 'epoch': 2.62}
 52%|█████▎    | 420/800 [16:59<15:22,  2.43s/it] 53%|█████▎    | 421/800 [17:01<15:18,  2.42s/it] 53%|█████▎    | 422/800 [17:04<15:16,  2.42s/it] 53%|█████▎    | 423/800 [17:06<15:13,  2.42s/it] 53%|█████▎    | 424/800 [17:08<15:10,  2.42s/it] 53%|█████▎    | 425/800 [17:11<15:08,  2.42s/it] 53%|█████▎    | 426/800 [17:13<15:05,  2.42s/it] 53%|█████▎    | 427/800 [17:16<15:03,  2.42s/it] 54%|█████▎    | 428/800 [17:18<15:01,  2.42s/it] 54%|█████▎    | 429/800 [17:21<14:58,  2.42s/it] 54%|█████▍    | 430/800 [17:23<14:57,  2.42s/it] 54%|█████▍    | 431/800 [17:25<14:54,  2.42s/it] 54%|█████▍    | 432/800 [17:28<14:51,  2.42s/it] 54%|█████▍    | 433/800 [17:30<14:48,  2.42s/it] 54%|█████▍    | 434/800 [17:33<14:45,  2.42s/it] 54%|█████▍    | 435/800 [17:35<14:43,  2.42s/it] 55%|█████▍    | 436/800 [17:38<14:40,  2.42s/it] 55%|█████▍    | 437/800 [17:40<14:38,  2.42s/it] 55%|█████▍    | 438/800 [17:42<14:36,  2.42s/it] 55%|█████▍    | 439/800 [17:45<14:33,  2.42s/it] 55%|█████▌    | 440/800 [17:47<14:31,  2.42s/it]                                                 {'loss': 0.4716, 'grad_norm': 0.38735485076904297, 'learning_rate': 0.00029997559268827044, 'epoch': 2.75}
 55%|█████▌    | 440/800 [17:47<14:31,  2.42s/it] 55%|█████▌    | 441/800 [17:50<14:28,  2.42s/it] 55%|█████▌    | 442/800 [17:52<14:25,  2.42s/it] 55%|█████▌    | 443/800 [17:54<14:23,  2.42s/it] 56%|█████▌    | 444/800 [17:57<14:20,  2.42s/it] 56%|█████▌    | 445/800 [17:59<14:18,  2.42s/it] 56%|█████▌    | 446/800 [18:02<14:18,  2.43s/it] 56%|█████▌    | 447/800 [18:04<14:15,  2.42s/it] 56%|█████▌    | 448/800 [18:07<14:12,  2.42s/it] 56%|█████▌    | 449/800 [18:09<14:09,  2.42s/it] 56%|█████▋    | 450/800 [18:11<14:07,  2.42s/it] 56%|█████▋    | 451/800 [18:14<14:04,  2.42s/it] 56%|█████▋    | 452/800 [18:16<14:02,  2.42s/it] 57%|█████▋    | 453/800 [18:19<13:59,  2.42s/it] 57%|█████▋    | 454/800 [18:21<13:57,  2.42s/it] 57%|█████▋    | 455/800 [18:24<13:54,  2.42s/it] 57%|█████▋    | 456/800 [18:26<13:52,  2.42s/it] 57%|█████▋    | 457/800 [18:28<13:49,  2.42s/it] 57%|█████▋    | 458/800 [18:31<13:47,  2.42s/it] 57%|█████▋    | 459/800 [18:33<13:50,  2.44s/it] 57%|█████▊    | 460/800 [18:36<13:46,  2.43s/it]                                                 {'loss': 0.4158, 'grad_norm': 0.3683862090110779, 'learning_rate': 0.00027694561576068985, 'epoch': 2.88}
 57%|█████▊    | 460/800 [18:36<13:46,  2.43s/it] 58%|█████▊    | 461/800 [18:38<13:42,  2.43s/it] 58%|█████▊    | 462/800 [18:41<13:39,  2.42s/it] 58%|█████▊    | 463/800 [18:43<13:36,  2.42s/it] 58%|█████▊    | 464/800 [18:45<13:33,  2.42s/it] 58%|█████▊    | 465/800 [18:48<13:30,  2.42s/it] 58%|█████▊    | 466/800 [18:50<13:28,  2.42s/it] 58%|█████▊    | 467/800 [18:53<13:25,  2.42s/it] 58%|█████▊    | 468/800 [18:55<13:23,  2.42s/it] 59%|█████▊    | 469/800 [18:57<13:20,  2.42s/it] 59%|█████▉    | 470/800 [19:00<13:18,  2.42s/it] 59%|█████▉    | 471/800 [19:02<13:15,  2.42s/it] 59%|█████▉    | 472/800 [19:05<13:13,  2.42s/it] 59%|█████▉    | 473/800 [19:07<13:16,  2.43s/it] 59%|█████▉    | 474/800 [19:10<13:12,  2.43s/it] 59%|█████▉    | 475/800 [19:12<13:08,  2.43s/it] 60%|█████▉    | 476/800 [19:14<13:05,  2.42s/it] 60%|█████▉    | 477/800 [19:17<13:02,  2.42s/it] 60%|█████▉    | 478/800 [19:19<12:59,  2.42s/it] 60%|█████▉    | 479/800 [19:22<12:56,  2.42s/it] 60%|██████    | 480/800 [19:24<12:54,  2.42s/it]                                                 {'loss': 0.4199, 'grad_norm': 0.34820064902305603, 'learning_rate': 0.0002524543298342875, 'epoch': 3.0}
 60%|██████    | 480/800 [19:24<12:54,  2.42s/it] 60%|██████    | 481/800 [19:27<12:51,  2.42s/it] 60%|██████    | 482/800 [19:29<12:49,  2.42s/it] 60%|██████    | 483/800 [19:31<12:47,  2.42s/it] 60%|██████    | 484/800 [19:34<12:44,  2.42s/it] 61%|██████    | 485/800 [19:36<12:42,  2.42s/it] 61%|██████    | 486/800 [19:39<12:39,  2.42s/it] 61%|██████    | 487/800 [19:41<12:37,  2.42s/it] 61%|██████    | 488/800 [19:43<12:36,  2.42s/it] 61%|██████    | 489/800 [19:46<12:33,  2.42s/it] 61%|██████▏   | 490/800 [19:48<12:30,  2.42s/it] 61%|██████▏   | 491/800 [19:51<12:27,  2.42s/it] 62%|██████▏   | 492/800 [19:53<12:25,  2.42s/it] 62%|██████▏   | 493/800 [19:56<12:22,  2.42s/it] 62%|██████▏   | 494/800 [19:58<12:20,  2.42s/it] 62%|██████▏   | 495/800 [20:00<12:17,  2.42s/it] 62%|██████▏   | 496/800 [20:03<12:15,  2.42s/it] 62%|██████▏   | 497/800 [20:05<12:12,  2.42s/it] 62%|██████▏   | 498/800 [20:08<12:10,  2.42s/it] 62%|██████▏   | 499/800 [20:10<12:07,  2.42s/it] 62%|██████▎   | 500/800 [20:13<12:07,  2.43s/it]                                                 {'loss': 0.3593, 'grad_norm': 0.2820706367492676, 'learning_rate': 0.00022793940736990766, 'epoch': 3.12}
 62%|██████▎   | 500/800 [20:13<12:07,  2.43s/it] 63%|██████▎   | 501/800 [20:15<12:05,  2.43s/it] 63%|██████▎   | 502/800 [20:17<12:02,  2.42s/it] 63%|██████▎   | 503/800 [20:20<11:59,  2.42s/it] 63%|██████▎   | 504/800 [20:22<11:56,  2.42s/it] 63%|██████▎   | 505/800 [20:25<11:54,  2.42s/it] 63%|██████▎   | 506/800 [20:27<11:51,  2.42s/it] 63%|██████▎   | 507/800 [20:29<11:48,  2.42s/it] 64%|██████▎   | 508/800 [20:32<11:46,  2.42s/it] 64%|██████▎   | 509/800 [20:34<11:44,  2.42s/it] 64%|██████▍   | 510/800 [20:37<11:41,  2.42s/it] 64%|██████▍   | 511/800 [20:39<11:39,  2.42s/it] 64%|██████▍   | 512/800 [20:42<11:36,  2.42s/it] 64%|██████▍   | 513/800 [20:44<11:34,  2.42s/it] 64%|██████▍   | 514/800 [20:46<11:31,  2.42s/it] 64%|██████▍   | 515/800 [20:49<11:31,  2.43s/it] 64%|██████▍   | 516/800 [20:51<11:28,  2.42s/it] 65%|██████▍   | 517/800 [20:54<11:25,  2.42s/it] 65%|██████▍   | 518/800 [20:56<11:22,  2.42s/it] 65%|██████▍   | 519/800 [20:58<11:20,  2.42s/it] 65%|██████▌   | 520/800 [21:01<11:17,  2.42s/it]                                                 {'loss': 0.3526, 'grad_norm': 0.32307159900665283, 'learning_rate': 0.00020363694047210228, 'epoch': 3.25}
 65%|██████▌   | 520/800 [21:01<11:17,  2.42s/it] 65%|██████▌   | 521/800 [21:03<11:15,  2.42s/it] 65%|██████▌   | 522/800 [21:06<11:12,  2.42s/it] 65%|██████▌   | 523/800 [21:08<11:09,  2.42s/it] 66%|██████▌   | 524/800 [21:11<11:07,  2.42s/it] 66%|██████▌   | 525/800 [21:13<11:05,  2.42s/it] 66%|██████▌   | 526/800 [21:15<11:02,  2.42s/it] 66%|██████▌   | 527/800 [21:18<11:00,  2.42s/it] 66%|██████▌   | 528/800 [21:20<10:58,  2.42s/it] 66%|██████▌   | 529/800 [21:23<10:55,  2.42s/it] 66%|██████▋   | 530/800 [21:25<10:53,  2.42s/it] 66%|██████▋   | 531/800 [21:28<10:51,  2.42s/it] 66%|██████▋   | 532/800 [21:30<10:48,  2.42s/it] 67%|██████▋   | 533/800 [21:32<10:45,  2.42s/it] 67%|██████▋   | 534/800 [21:35<10:43,  2.42s/it] 67%|██████▋   | 535/800 [21:37<10:40,  2.42s/it] 67%|██████▋   | 536/800 [21:40<10:38,  2.42s/it] 67%|██████▋   | 537/800 [21:42<10:36,  2.42s/it] 67%|██████▋   | 538/800 [21:44<10:33,  2.42s/it] 67%|██████▋   | 539/800 [21:47<10:31,  2.42s/it] 68%|██████▊   | 540/800 [21:49<10:28,  2.42s/it]                                                 {'loss': 0.3659, 'grad_norm': 0.3034501373767853, 'learning_rate': 0.00017978097518217702, 'epoch': 3.38}
 68%|██████▊   | 540/800 [21:49<10:28,  2.42s/it] 68%|██████▊   | 541/800 [21:52<10:26,  2.42s/it] 68%|██████▊   | 542/800 [21:54<10:24,  2.42s/it] 68%|██████▊   | 543/800 [21:57<10:25,  2.43s/it] 68%|██████▊   | 544/800 [21:59<10:21,  2.43s/it] 68%|██████▊   | 545/800 [22:01<10:18,  2.43s/it] 68%|██████▊   | 546/800 [22:04<10:15,  2.42s/it] 68%|██████▊   | 547/800 [22:06<10:12,  2.42s/it] 68%|██████▊   | 548/800 [22:09<10:10,  2.42s/it] 69%|██████▊   | 549/800 [22:11<10:07,  2.42s/it] 69%|██████▉   | 550/800 [22:14<10:04,  2.42s/it] 69%|██████▉   | 551/800 [22:16<10:02,  2.42s/it] 69%|██████▉   | 552/800 [22:18<09:59,  2.42s/it] 69%|██████▉   | 553/800 [22:21<09:57,  2.42s/it] 69%|██████▉   | 554/800 [22:23<09:54,  2.42s/it] 69%|██████▉   | 555/800 [22:26<09:52,  2.42s/it] 70%|██████▉   | 556/800 [22:28<09:52,  2.43s/it] 70%|██████▉   | 557/800 [22:30<09:49,  2.42s/it] 70%|██████▉   | 558/800 [22:33<09:46,  2.42s/it] 70%|██████▉   | 559/800 [22:35<09:43,  2.42s/it] 70%|███████   | 560/800 [22:38<09:41,  2.42s/it]                                                 {'loss': 0.365, 'grad_norm': 0.31348732113838196, 'learning_rate': 0.00015660125748687094, 'epoch': 3.5}
 70%|███████   | 560/800 [22:38<09:41,  2.42s/it] 70%|███████   | 561/800 [22:40<09:38,  2.42s/it] 70%|███████   | 562/800 [22:43<09:36,  2.42s/it] 70%|███████   | 563/800 [22:45<09:33,  2.42s/it] 70%|███████   | 564/800 [22:47<09:31,  2.42s/it] 71%|███████   | 565/800 [22:50<09:28,  2.42s/it] 71%|███████   | 566/800 [22:52<09:25,  2.42s/it] 71%|███████   | 567/800 [22:55<09:23,  2.42s/it] 71%|███████   | 568/800 [22:57<09:21,  2.42s/it] 71%|███████   | 569/800 [23:00<09:20,  2.43s/it] 71%|███████▏  | 570/800 [23:02<09:17,  2.42s/it] 71%|███████▏  | 571/800 [23:04<09:15,  2.42s/it] 72%|███████▏  | 572/800 [23:07<09:12,  2.42s/it] 72%|███████▏  | 573/800 [23:09<09:09,  2.42s/it] 72%|███████▏  | 574/800 [23:12<09:06,  2.42s/it] 72%|███████▏  | 575/800 [23:14<09:06,  2.43s/it] 72%|███████▏  | 576/800 [23:17<09:03,  2.43s/it] 72%|███████▏  | 577/800 [23:19<09:00,  2.43s/it] 72%|███████▏  | 578/800 [23:21<08:57,  2.42s/it] 72%|███████▏  | 579/800 [23:24<08:55,  2.42s/it] 72%|███████▎  | 580/800 [23:26<08:52,  2.42s/it]                                                 {'loss': 0.3905, 'grad_norm': 0.3257782757282257, 'learning_rate': 0.00013541031734468211, 'epoch': 3.62}
 72%|███████▎  | 580/800 [23:26<08:52,  2.42s/it] 73%|███████▎  | 581/800 [23:29<08:50,  2.42s/it] 73%|███████▎  | 582/800 [23:31<08:47,  2.42s/it] 73%|███████▎  | 583/800 [23:33<08:45,  2.42s/it] 73%|███████▎  | 584/800 [23:36<08:42,  2.42s/it] 73%|███████▎  | 585/800 [23:38<08:41,  2.43s/it] 73%|███████▎  | 586/800 [23:41<08:38,  2.42s/it] 73%|███████▎  | 587/800 [23:43<08:35,  2.42s/it] 74%|███████▎  | 588/800 [23:46<08:33,  2.42s/it] 74%|███████▎  | 589/800 [23:48<08:30,  2.42s/it] 74%|███████▍  | 590/800 [23:50<08:28,  2.42s/it] 74%|███████▍  | 591/800 [23:53<08:25,  2.42s/it] 74%|███████▍  | 592/800 [23:55<08:23,  2.42s/it] 74%|███████▍  | 593/800 [23:58<08:20,  2.42s/it] 74%|███████▍  | 594/800 [24:00<08:18,  2.42s/it] 74%|███████▍  | 595/800 [24:02<08:15,  2.42s/it] 74%|███████▍  | 596/800 [24:05<08:13,  2.42s/it] 75%|███████▍  | 597/800 [24:07<08:10,  2.42s/it] 75%|███████▍  | 598/800 [24:10<08:08,  2.42s/it] 75%|███████▍  | 599/800 [24:12<08:06,  2.42s/it] 75%|███████▌  | 600/800 [24:15<08:03,  2.42s/it]                                                 {'loss': 0.3719, 'grad_norm': 0.35585516691207886, 'learning_rate': 0.00011418349124044405, 'epoch': 3.75}
 75%|███████▌  | 600/800 [24:15<08:03,  2.42s/it] 75%|███████▌  | 601/800 [24:17<08:01,  2.42s/it] 75%|███████▌  | 602/800 [24:19<07:58,  2.42s/it] 75%|███████▌  | 603/800 [24:22<07:56,  2.42s/it] 76%|███████▌  | 604/800 [24:24<07:54,  2.42s/it] 76%|███████▌  | 605/800 [24:27<07:51,  2.42s/it] 76%|███████▌  | 606/800 [24:29<07:49,  2.42s/it] 76%|███████▌  | 607/800 [24:32<07:46,  2.42s/it] 76%|███████▌  | 608/800 [24:34<07:44,  2.42s/it] 76%|███████▌  | 609/800 [24:36<07:41,  2.42s/it] 76%|███████▋  | 610/800 [24:39<07:39,  2.42s/it] 76%|███████▋  | 611/800 [24:41<07:37,  2.42s/it] 76%|███████▋  | 612/800 [24:44<07:39,  2.44s/it] 77%|███████▋  | 613/800 [24:46<07:35,  2.44s/it] 77%|███████▋  | 614/800 [24:49<07:32,  2.43s/it] 77%|███████▋  | 615/800 [24:51<07:28,  2.43s/it] 77%|███████▋  | 616/800 [24:53<07:26,  2.42s/it] 77%|███████▋  | 617/800 [24:56<07:23,  2.42s/it] 77%|███████▋  | 618/800 [24:58<07:20,  2.42s/it] 77%|███████▋  | 619/800 [25:01<07:18,  2.42s/it] 78%|███████▊  | 620/800 [25:03<07:15,  2.42s/it]                                                 {'loss': 0.3982, 'grad_norm': 0.36036524176597595, 'learning_rate': 9.42646523604165e-05, 'epoch': 3.88}
 78%|███████▊  | 620/800 [25:03<07:15,  2.42s/it] 78%|███████▊  | 621/800 [25:05<07:13,  2.42s/it] 78%|███████▊  | 622/800 [25:08<07:10,  2.42s/it] 78%|███████▊  | 623/800 [25:10<07:08,  2.42s/it] 78%|███████▊  | 624/800 [25:13<07:05,  2.42s/it] 78%|███████▊  | 625/800 [25:15<07:03,  2.42s/it] 78%|███████▊  | 626/800 [25:18<07:01,  2.42s/it] 78%|███████▊  | 627/800 [25:20<06:58,  2.42s/it] 78%|███████▊  | 628/800 [25:22<06:56,  2.42s/it] 79%|███████▊  | 629/800 [25:25<06:53,  2.42s/it] 79%|███████▉  | 630/800 [25:27<06:51,  2.42s/it] 79%|███████▉  | 631/800 [25:30<06:48,  2.42s/it] 79%|███████▉  | 632/800 [25:32<06:46,  2.42s/it] 79%|███████▉  | 633/800 [25:34<06:43,  2.42s/it] 79%|███████▉  | 634/800 [25:37<06:41,  2.42s/it] 79%|███████▉  | 635/800 [25:39<06:38,  2.42s/it] 80%|███████▉  | 636/800 [25:42<06:36,  2.42s/it] 80%|███████▉  | 637/800 [25:44<06:34,  2.42s/it] 80%|███████▉  | 638/800 [25:47<06:31,  2.42s/it] 80%|███████▉  | 639/800 [25:49<06:30,  2.42s/it] 80%|████████  | 640/800 [25:51<06:27,  2.42s/it]                                                 {'loss': 0.3823, 'grad_norm': 0.3563460409641266, 'learning_rate': 7.584563001175895e-05, 'epoch': 4.0}
 80%|████████  | 640/800 [25:51<06:27,  2.42s/it] 80%|████████  | 641/800 [25:54<06:25,  2.42s/it] 80%|████████  | 642/800 [25:56<06:22,  2.42s/it] 80%|████████  | 643/800 [25:59<06:19,  2.42s/it] 80%|████████  | 644/800 [26:01<06:17,  2.42s/it] 81%|████████  | 645/800 [26:04<06:14,  2.42s/it] 81%|████████  | 646/800 [26:06<06:12,  2.42s/it] 81%|████████  | 647/800 [26:08<06:10,  2.42s/it] 81%|████████  | 648/800 [26:11<06:07,  2.42s/it] 81%|████████  | 649/800 [26:13<06:05,  2.42s/it] 81%|████████▏ | 650/800 [26:16<06:02,  2.42s/it] 81%|████████▏ | 651/800 [26:18<06:00,  2.42s/it] 82%|████████▏ | 652/800 [26:20<05:58,  2.42s/it] 82%|████████▏ | 653/800 [26:23<05:55,  2.42s/it] 82%|████████▏ | 654/800 [26:25<05:54,  2.43s/it] 82%|████████▏ | 655/800 [26:28<05:51,  2.42s/it] 82%|████████▏ | 656/800 [26:30<05:48,  2.42s/it] 82%|████████▏ | 657/800 [26:33<05:46,  2.42s/it] 82%|████████▏ | 658/800 [26:35<05:43,  2.42s/it] 82%|████████▏ | 659/800 [26:37<05:41,  2.42s/it] 82%|████████▎ | 660/800 [26:40<05:38,  2.42s/it]                                                 {'loss': 0.3306, 'grad_norm': 0.2889557182788849, 'learning_rate': 5.910380944855087e-05, 'epoch': 4.12}
 82%|████████▎ | 660/800 [26:40<05:38,  2.42s/it] 83%|████████▎ | 661/800 [26:42<05:36,  2.42s/it] 83%|████████▎ | 662/800 [26:45<05:34,  2.42s/it] 83%|████████▎ | 663/800 [26:47<05:31,  2.42s/it] 83%|████████▎ | 664/800 [26:50<05:29,  2.42s/it] 83%|████████▎ | 665/800 [26:52<05:26,  2.42s/it] 83%|████████▎ | 666/800 [26:54<05:24,  2.42s/it] 83%|████████▎ | 667/800 [26:57<05:22,  2.43s/it] 84%|████████▎ | 668/800 [26:59<05:20,  2.42s/it] 84%|████████▎ | 669/800 [27:02<05:17,  2.42s/it] 84%|████████▍ | 670/800 [27:04<05:14,  2.42s/it] 84%|████████▍ | 671/800 [27:06<05:12,  2.42s/it] 84%|████████▍ | 672/800 [27:09<05:09,  2.42s/it] 84%|████████▍ | 673/800 [27:11<05:07,  2.42s/it] 84%|████████▍ | 674/800 [27:14<05:04,  2.42s/it] 84%|████████▍ | 675/800 [27:16<05:02,  2.42s/it] 84%|████████▍ | 676/800 [27:19<05:00,  2.42s/it] 85%|████████▍ | 677/800 [27:21<04:57,  2.42s/it] 85%|████████▍ | 678/800 [27:23<04:55,  2.42s/it] 85%|████████▍ | 679/800 [27:26<04:52,  2.42s/it] 85%|████████▌ | 680/800 [27:28<04:50,  2.42s/it]                                                 {'loss': 0.319, 'grad_norm': 0.3253490924835205, 'learning_rate': 4.420042355482601e-05, 'epoch': 4.25}
 85%|████████▌ | 680/800 [27:28<04:50,  2.42s/it] 85%|████████▌ | 681/800 [27:31<04:47,  2.42s/it] 85%|████████▌ | 682/800 [27:33<04:48,  2.44s/it] 85%|████████▌ | 683/800 [27:36<04:44,  2.43s/it] 86%|████████▌ | 684/800 [27:38<04:41,  2.43s/it] 86%|████████▌ | 685/800 [27:40<04:38,  2.43s/it] 86%|████████▌ | 686/800 [27:43<04:36,  2.42s/it] 86%|████████▌ | 687/800 [27:45<04:33,  2.42s/it] 86%|████████▌ | 688/800 [27:48<04:31,  2.42s/it] 86%|████████▌ | 689/800 [27:50<04:28,  2.42s/it] 86%|████████▋ | 690/800 [27:52<04:26,  2.42s/it] 86%|████████▋ | 691/800 [27:55<04:23,  2.42s/it] 86%|████████▋ | 692/800 [27:57<04:21,  2.42s/it] 87%|████████▋ | 693/800 [28:00<04:18,  2.42s/it] 87%|████████▋ | 694/800 [28:02<04:16,  2.42s/it] 87%|████████▋ | 695/800 [28:05<04:14,  2.43s/it] 87%|████████▋ | 696/800 [28:07<04:12,  2.42s/it] 87%|████████▋ | 697/800 [28:09<04:09,  2.42s/it] 87%|████████▋ | 698/800 [28:12<04:06,  2.42s/it] 87%|████████▋ | 699/800 [28:14<04:04,  2.42s/it] 88%|████████▊ | 700/800 [28:17<04:01,  2.42s/it]                                                 {'loss': 0.3355, 'grad_norm': 0.28153544664382935, 'learning_rate': 3.127900008376044e-05, 'epoch': 4.38}
 88%|████████▊ | 700/800 [28:17<04:01,  2.42s/it] 88%|████████▊ | 701/800 [28:19<03:59,  2.42s/it] 88%|████████▊ | 702/800 [28:22<03:57,  2.42s/it] 88%|████████▊ | 703/800 [28:24<03:54,  2.42s/it] 88%|████████▊ | 704/800 [28:26<03:52,  2.42s/it] 88%|████████▊ | 705/800 [28:29<03:49,  2.42s/it] 88%|████████▊ | 706/800 [28:31<03:47,  2.42s/it] 88%|████████▊ | 707/800 [28:34<03:44,  2.42s/it] 88%|████████▊ | 708/800 [28:36<03:42,  2.42s/it] 89%|████████▊ | 709/800 [28:38<03:40,  2.42s/it] 89%|████████▉ | 710/800 [28:41<03:37,  2.42s/it] 89%|████████▉ | 711/800 [28:43<03:35,  2.42s/it] 89%|████████▉ | 712/800 [28:46<03:32,  2.42s/it] 89%|████████▉ | 713/800 [28:48<03:30,  2.42s/it] 89%|████████▉ | 714/800 [28:51<03:27,  2.42s/it] 89%|████████▉ | 715/800 [28:53<03:25,  2.42s/it] 90%|████████▉ | 716/800 [28:55<03:23,  2.42s/it] 90%|████████▉ | 717/800 [28:58<03:20,  2.42s/it] 90%|████████▉ | 718/800 [29:00<03:18,  2.42s/it] 90%|████████▉ | 719/800 [29:03<03:15,  2.42s/it] 90%|█████████ | 720/800 [29:05<03:13,  2.42s/it]                                                 {'loss': 0.3498, 'grad_norm': 0.3372880816459656, 'learning_rate': 2.0463979406949023e-05, 'epoch': 4.5}
 90%|█████████ | 720/800 [29:05<03:13,  2.42s/it] 90%|█████████ | 721/800 [29:08<03:11,  2.42s/it] 90%|█████████ | 722/800 [29:10<03:08,  2.42s/it] 90%|█████████ | 723/800 [29:12<03:06,  2.42s/it] 90%|█████████ | 724/800 [29:15<03:04,  2.43s/it] 91%|█████████ | 725/800 [29:17<03:01,  2.42s/it] 91%|█████████ | 726/800 [29:20<02:59,  2.42s/it] 91%|█████████ | 727/800 [29:22<02:56,  2.42s/it] 91%|█████████ | 728/800 [29:24<02:54,  2.42s/it] 91%|█████████ | 729/800 [29:27<02:51,  2.42s/it] 91%|█████████▏| 730/800 [29:29<02:49,  2.42s/it] 91%|█████████▏| 731/800 [29:32<02:46,  2.42s/it] 92%|█████████▏| 732/800 [29:34<02:44,  2.42s/it] 92%|█████████▏| 733/800 [29:37<02:42,  2.42s/it] 92%|█████████▏| 734/800 [29:39<02:39,  2.42s/it] 92%|█████████▏| 735/800 [29:41<02:37,  2.42s/it] 92%|█████████▏| 736/800 [29:44<02:34,  2.42s/it] 92%|█████████▏| 737/800 [29:46<02:32,  2.42s/it] 92%|█████████▏| 738/800 [29:49<02:29,  2.42s/it] 92%|█████████▏| 739/800 [29:51<02:27,  2.42s/it] 92%|█████████▎| 740/800 [29:53<02:25,  2.42s/it]                                                 {'loss': 0.3219, 'grad_norm': 0.2536163032054901, 'learning_rate': 1.185951608560118e-05, 'epoch': 4.62}
 92%|█████████▎| 740/800 [29:53<02:25,  2.42s/it] 93%|█████████▎| 741/800 [29:56<02:22,  2.42s/it] 93%|█████████▎| 742/800 [29:58<02:20,  2.42s/it] 93%|█████████▎| 743/800 [30:01<02:17,  2.42s/it] 93%|█████████▎| 744/800 [30:03<02:15,  2.42s/it] 93%|█████████▎| 745/800 [30:06<02:13,  2.42s/it] 93%|█████████▎| 746/800 [30:08<02:10,  2.42s/it] 93%|█████████▎| 747/800 [30:10<02:08,  2.42s/it] 94%|█████████▎| 748/800 [30:13<02:05,  2.42s/it] 94%|█████████▎| 749/800 [30:15<02:03,  2.42s/it] 94%|█████████▍| 750/800 [30:18<02:00,  2.42s/it] 94%|█████████▍| 751/800 [30:20<01:59,  2.44s/it] 94%|█████████▍| 752/800 [30:23<01:56,  2.44s/it] 94%|█████████▍| 753/800 [30:25<01:54,  2.43s/it] 94%|█████████▍| 754/800 [30:27<01:51,  2.43s/it] 94%|█████████▍| 755/800 [30:30<01:49,  2.42s/it] 94%|█████████▍| 756/800 [30:32<01:46,  2.42s/it] 95%|█████████▍| 757/800 [30:35<01:44,  2.42s/it] 95%|█████████▍| 758/800 [30:37<01:41,  2.42s/it] 95%|█████████▍| 759/800 [30:40<01:39,  2.42s/it] 95%|█████████▌| 760/800 [30:42<01:36,  2.42s/it]                                                 {'loss': 0.351, 'grad_norm': 0.2972838580608368, 'learning_rate': 5.548475805179587e-06, 'epoch': 4.75}
 95%|█████████▌| 760/800 [30:42<01:36,  2.42s/it] 95%|█████████▌| 761/800 [30:44<01:34,  2.42s/it] 95%|█████████▌| 762/800 [30:47<01:31,  2.42s/it] 95%|█████████▌| 763/800 [30:49<01:29,  2.42s/it] 96%|█████████▌| 764/800 [30:52<01:27,  2.42s/it] 96%|█████████▌| 765/800 [30:54<01:24,  2.43s/it] 96%|█████████▌| 766/800 [30:56<01:22,  2.42s/it] 96%|█████████▌| 767/800 [30:59<01:19,  2.42s/it] 96%|█████████▌| 768/800 [31:01<01:17,  2.42s/it] 96%|█████████▌| 769/800 [31:04<01:15,  2.42s/it] 96%|█████████▋| 770/800 [31:06<01:12,  2.42s/it] 96%|█████████▋| 771/800 [31:09<01:10,  2.42s/it] 96%|█████████▋| 772/800 [31:11<01:07,  2.42s/it] 97%|█████████▋| 773/800 [31:13<01:05,  2.42s/it] 97%|█████████▋| 774/800 [31:16<01:02,  2.42s/it] 97%|█████████▋| 775/800 [31:18<01:00,  2.42s/it] 97%|█████████▋| 776/800 [31:21<00:58,  2.42s/it] 97%|█████████▋| 777/800 [31:23<00:55,  2.42s/it] 97%|█████████▋| 778/800 [31:26<00:53,  2.43s/it] 97%|█████████▋| 779/800 [31:28<00:50,  2.43s/it] 98%|█████████▊| 780/800 [31:30<00:48,  2.42s/it]                                                 {'loss': 0.345, 'grad_norm': 0.3085359036922455, 'learning_rate': 1.5916373335503054e-06, 'epoch': 4.88}
 98%|█████████▊| 780/800 [31:30<00:48,  2.42s/it] 98%|█████████▊| 781/800 [31:33<00:46,  2.42s/it] 98%|█████████▊| 782/800 [31:35<00:43,  2.42s/it] 98%|█████████▊| 783/800 [31:38<00:41,  2.42s/it] 98%|█████████▊| 784/800 [31:40<00:38,  2.42s/it] 98%|█████████▊| 785/800 [31:42<00:36,  2.42s/it] 98%|█████████▊| 786/800 [31:45<00:33,  2.42s/it] 98%|█████████▊| 787/800 [31:47<00:31,  2.42s/it] 98%|█████████▊| 788/800 [31:50<00:29,  2.42s/it] 99%|█████████▊| 789/800 [31:52<00:26,  2.42s/it] 99%|█████████▉| 790/800 [31:55<00:24,  2.42s/it] 99%|█████████▉| 791/800 [31:57<00:21,  2.42s/it] 99%|█████████▉| 792/800 [31:59<00:19,  2.42s/it] 99%|█████████▉| 793/800 [32:02<00:16,  2.43s/it] 99%|█████████▉| 794/800 [32:04<00:14,  2.42s/it] 99%|█████████▉| 795/800 [32:07<00:12,  2.42s/it]100%|█████████▉| 796/800 [32:09<00:09,  2.42s/it]100%|█████████▉| 797/800 [32:12<00:07,  2.42s/it]100%|█████████▉| 798/800 [32:14<00:04,  2.42s/it]100%|█████████▉| 799/800 [32:16<00:02,  2.42s/it]100%|██████████| 800/800 [32:19<00:00,  2.42s/it]                                                 {'loss': 0.3376, 'grad_norm': 0.22933241724967957, 'learning_rate': 2.7107188222991187e-08, 'epoch': 5.0}
100%|██████████| 800/800 [32:19<00:00,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 05:13:41,012 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/extractiveness_then_topic/checkpoint-800
[INFO|configuration_utils.py:726] 2024-05-25 05:13:42,084 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 05:13:42,089 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 05:13:42,990 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 05:13:42,996 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 05:13:43,048 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 05:13:43,049 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/checkpoint-800/special_tokens_map.json
[INFO|trainer.py:2231] 2024-05-25 05:13:43,186 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 1953.0212, 'train_samples_per_second': 3.274, 'train_steps_per_second': 0.41, 'train_loss': 0.4691386204957962, 'epoch': 5.0}
100%|██████████| 800/800 [32:21<00:00,  2.42s/it]100%|██████████| 800/800 [32:21<00:00,  2.43s/it]
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.4691
  train_runtime            = 0:32:33.02
  train_samples_per_second =      3.274
  train_steps_per_second   =       0.41
[INFO|trainer.py:3203] 2024-05-25 05:13:43,192 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/extractiveness_then_topic
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-05-25 05:13:45,242 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 05:13:45,244 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 05:13:45,775 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 05:13:45,777 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 05:13:45,839 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 05:13:45,840 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/special_tokens_map.json
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:471] 2024-05-25 05:13:46,650 >> Configuration saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/final_merged_model/config.json
[INFO|configuration_utils.py:697] 2024-05-25 05:13:46,651 >> Configuration saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/final_merged_model/generation_config.json
[INFO|modeling_utils.py:2474] 2024-05-25 05:13:52,589 >> Model weights saved in /scratch/tathagato/adapter_experiments/extractiveness_then_topic/final_merged_model/model.safetensors
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.006 MB uploadedwandb: | 0.006 MB of 0.006 MB uploadedwandb: / 0.006 MB of 0.006 MB uploadedwandb: - 0.006 MB of 0.029 MB uploadedwandb: \ 0.006 MB of 0.032 MB uploadedwandb: | 0.032 MB of 0.032 MB uploadedwandb: / 0.032 MB of 0.032 MB uploadedwandb: - 0.032 MB of 0.032 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:   train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:     train/grad_norm █▆▆▄▅▃▃▅▂▂▄▄▄▄▃▄▃▄▃▃▅▄▄▃▂▃▂▃▃▄▄▄▂▃▂▃▁▂▃▁
wandb: train/learning_rate ▂▃▄▄▅▆▇██████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:          train/loss █▇▇▆▆▇▆▅▅▅▅▅▅▅▄▅▃▃▄▃▃▄▃▃▂▂▂▂▂▂▂▂▁▁▁▂▁▂▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 8.17116991193088e+16
wandb:              train/epoch 5.0
wandb:        train/global_step 800
wandb:          train/grad_norm 0.22933
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.3376
wandb:               train_loss 0.46914
wandb:            train_runtime 1953.0212
wandb: train_samples_per_second 3.274
wandb:   train_steps_per_second 0.41
wandb: 
wandb: 🚀 View run dark-grass-100 at: https://wandb.ai/ihub-drug-discovery/huggingface/runs/uczqljuo
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240525_044113-uczqljuo/logs
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-05-25 05:14:28 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 05:14:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 05:14:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch/tathagato/adapter_experiments/extractiveness_then_length/runs/May25_05-14-28_gnode081,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=20,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/scratch/tathagato/adapter_experiments/extractiveness_then_length,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch/tathagato/adapter_experiments/extractiveness_then_length,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=400,
save_strategy=steps,
save_total_limit=400,
seed=0,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
2024-05-25 05:14:28 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'q_proj', 'k_proj', 'o_proj', 'v_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
2024-05-25 05:14:28 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 05:14:28 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1 distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:726] 2024-05-25 05:14:28,607 >> loading configuration file config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 05:14:28,612 >> Model config LlamaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": false,
  "vocab_size": 32000
}

[INFO|quantizer_bnb_4bit.py:247] 2024-05-25 05:14:28,717 >> The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' 
[WARNING|modeling_utils.py:3058] 2024-05-25 05:14:28,717 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:3283] 2024-05-25 05:14:28,718 >> loading weights file model.safetensors from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors
[INFO|modeling_utils.py:1417] 2024-05-25 05:14:28,735 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:928] 2024-05-25 05:14:28,737 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

[WARNING|modeling_utils.py:3058] 2024-05-25 05:14:29,123 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 05:14:29,168 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 05:14:29,372 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:4024] 2024-05-25 05:14:31,564 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-25 05:14:31,565 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-05-25 05:14:31,808 >> loading configuration file generation_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-25 05:14:31,808 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 2048,
  "pad_token_id": 0
}

[INFO|tokenization_utils_base.py:2084] 2024-05-25 05:14:32,253 >> loading file tokenizer.model from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-05-25 05:14:32,253 >> loading file tokenizer.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 05:14:32,253 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-05-25 05:14:32,253 >> loading file special_tokens_map.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 05:14:32,253 >> loading file tokenizer_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json
loading model from : /scratch/tathagato/adapter_experiments/extractiveness/extractiveness
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
loading model from : /scratch/tathagato/adapter_experiments/extractiveness/extractiveness
loading model from : /scratch/tathagato/adapter_experiments/extractiveness/extractiveness
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
loading model from : /scratch/tathagato/adapter_experiments/extractiveness/extractiveness
train dataset size 4278
test dataset size 554
4278
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
Spawning 10 processes
2024-05-25 05:14:34 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:11:59,  1.01s/ examples]/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
train dataset size 4278
test dataset size 554
4278
Applying chat template to train_sft (num_proc=10):  10%|█         | 429/4278 [00:01<00:10, 383.57 examples/s]trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:01<00:05, 635.10 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1285/4278 [00:02<00:03, 885.99 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]train dataset size 4278
test dataset size 554
4278
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:02, 967.88 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:02<00:01, 1334.19 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  60%|█████▉    | 2556/4278 [00:02<00:01, 1204.23 examples/s]train dataset size 4278
test dataset size 554
4278
Applying chat template to train_sft (num_proc=10):  65%|██████▌   | 2785/4278 [00:03<00:01, 1187.00 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:15:26,  1.06s/ examples]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2996/4278 [00:03<00:01, 1258.98 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:08, 469.85 examples/s]Applying chat template to train_sft (num_proc=10):  76%|███████▌  | 3260/4278 [00:03<00:01, 1012.89 examples/s]Applying chat template to train_sft (num_proc=10):  15%|█▌        | 644/4278 [00:01<00:06, 519.82 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:20:12,  1.13s/ examples]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:01<00:06, 555.59 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3425/4278 [00:04<00:01, 808.65 examples/s] Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:02<00:03, 990.10 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:04<00:00, 1150.53 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 429/4278 [00:01<00:10, 361.68 examples/s]Applying chat template to train_sft (num_proc=10):  95%|█████████▌| 4067/4278 [00:04<00:00, 1094.64 examples/s]Applying chat template to train_sft (num_proc=10):  36%|███▋      | 1554/4278 [00:02<00:03, 868.02 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:01<00:05, 651.48 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:01<00:02, 1052.77 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 795.35 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:04<00:00, 889.98 examples/s] 
Applying chat template to train_sft (num_proc=10):  50%|████▉     | 2123/4278 [00:02<00:01, 1206.70 examples/s]Concatenating 10 shards
2024-05-25 05:14:39 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:26:50,  1.22s/ examples]Applying chat template to train_sft (num_proc=10):  35%|███▌      | 1509/4278 [00:02<00:03, 896.39 examples/s] Applying chat template to train_sft (num_proc=10):  56%|█████▌    | 2384/4278 [00:03<00:01, 1019.06 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 795.93 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 429/4278 [00:01<00:11, 339.35 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:02<00:01, 1216.61 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 856/4278 [00:01<00:04, 722.96 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2569/4278 [00:03<00:02, 756.24 examples/s] Spawning 10 processes
2024-05-25 05:14:40 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  56%|█████▌    | 2397/4278 [00:03<00:01, 1081.70 examples/s]Applying chat template to train_sft (num_proc=10):  26%|██▌       | 1119/4278 [00:02<00:04, 762.35 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:04<00:01, 867.59 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<04:41,  1.97 examples/s]Applying chat template to train_sft (num_proc=10):  65%|██████▌   | 2793/4278 [00:03<00:01, 980.66 examples/s] Applying chat template to train_sft (num_proc=10):  30%|███       | 1285/4278 [00:02<00:04, 607.48 examples/s]Applying chat template to test_sft (num_proc=10):  10%|█         | 57/554 [00:00<00:04, 120.85 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3425/4278 [00:04<00:00, 1018.36 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 210.28 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:04<00:00, 1323.34 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:03<00:01, 905.06 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:00<00:00, 350.68 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3424/4278 [00:03<00:00, 1283.48 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 762.02 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 280/554 [00:01<00:00, 375.08 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:03<00:01, 1116.75 examples/s]Applying chat template to train_sft (num_proc=10):  95%|█████████▌| 4076/4278 [00:04<00:00, 1064.66 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 390/554 [00:01<00:00, 444.29 examples/s]Applying chat template to train_sft (num_proc=10):  85%|████████▌ | 3646/4278 [00:04<00:00, 1107.73 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 500/554 [00:01<00:00, 553.05 examples/s]Applying chat template to train_sft (num_proc=10):  55%|█████▌    | 2365/4278 [00:03<00:01, 979.25 examples/s] Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 823.13 examples/s] 
Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 349.40 examples/s]
Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3852/4278 [00:04<00:00, 932.86 examples/s] Concatenating 10 shards
2024-05-25 05:14:42 - INFO - datasets.arrow_dataset - Concatenating 10 shards
tokenizer padding side left
Applying chat template to train_sft (num_proc=10):  99%|█████████▉| 4225/4278 [00:04<00:00, 1285.77 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2569/4278 [00:03<00:02, 819.95 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:04<00:00, 861.47 examples/s] 
Using custom data configuration default-8a159e0651bd4009
2024-05-25 05:14:42 - INFO - datasets.builder - Using custom data configuration default-8a159e0651bd4009
Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
2024-05-25 05:14:42 - INFO - datasets.info - Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
Overwrite dataset info from restored data version if exists.
2024-05-25 05:14:42 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0
2024-05-25 05:14:42 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0
Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0)
2024-05-25 05:14:42 - INFO - datasets.builder - Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0)
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0
2024-05-25 05:14:42 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0
Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:04<00:01, 838.88 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3425/4278 [00:04<00:00, 1091.49 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:04<00:00, 1427.81 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10): 100%|█████████▉| 4266/4278 [00:04<00:00, 1258.58 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<05:27,  1.69 examples/s]Applying chat template to test_sft (num_proc=10):  10%|█         | 57/554 [00:00<00:04, 102.42 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 194.02 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 806.95 examples/s] 
Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<04:41,  1.96 examples/s]Applying chat template to test_sft (num_proc=10):  10%|█         | 57/554 [00:00<00:04, 121.13 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:01<00:01, 282.24 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:01, 222.59 examples/s]Applying chat template to test_sft (num_proc=10):  60%|██████    | 335/554 [00:01<00:00, 390.71 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:00<00:00, 359.07 examples/s]Applying chat template to test_sft (num_proc=10):  80%|████████  | 445/554 [00:01<00:00, 441.25 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 280/554 [00:01<00:00, 395.47 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 557.29 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 390/554 [00:01<00:00, 497.03 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 311.96 examples/s]
Applying chat template to test_sft (num_proc=10):  90%|█████████ | 499/554 [00:01<00:00, 633.89 examples/s]tokenizer padding side left
Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 340.17 examples/s]
tokenizer padding side left
Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<04:26,  2.07 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 208.70 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:00<00:00, 348.94 examples/s]Applying chat template to test_sft (num_proc=10):  60%|██████    | 334/554 [00:00<00:00, 489.27 examples/s]Applying chat template to test_sft (num_proc=10):  80%|████████  | 444/554 [00:01<00:00, 552.59 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 556.39 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 364.67 examples/s]
tokenizer padding side left
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
2024-05-25 05:14:46 - WARNING - accelerate.utils.other - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:607] 2024-05-25 05:14:48,296 >> Using auto half precision backend
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
[INFO|trainer.py:1969] 2024-05-25 05:14:49,019 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-25 05:14:49,019 >>   Num examples = 2,459
[INFO|trainer.py:1971] 2024-05-25 05:14:49,019 >>   Num Epochs = 5
[INFO|trainer.py:1972] 2024-05-25 05:14:49,019 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-25 05:14:49,019 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-05-25 05:14:49,019 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-05-25 05:14:49,019 >>   Total optimization steps = 1,535
[INFO|trainer.py:1978] 2024-05-25 05:14:49,021 >>   Number of trainable parameters = 4,505,600
[INFO|integration_utils.py:723] 2024-05-25 05:14:49,082 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSum/experiments/wandb/run-20240525_051452-3zrk9vyz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-haze-101
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/huggingface
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/huggingface/runs/3zrk9vyz
  0%|          | 0/1535 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/1535 [00:02<1:02:28,  2.44s/it]  0%|          | 2/1535 [00:04<1:01:47,  2.42s/it]  0%|          | 3/1535 [00:07<1:01:36,  2.41s/it]  0%|          | 4/1535 [00:09<1:01:26,  2.41s/it]  0%|          | 5/1535 [00:12<1:01:21,  2.41s/it]  0%|          | 6/1535 [00:14<1:01:18,  2.41s/it]  0%|          | 7/1535 [00:16<1:01:15,  2.41s/it]  1%|          | 8/1535 [00:19<1:01:13,  2.41s/it]  1%|          | 9/1535 [00:21<1:01:11,  2.41s/it]  1%|          | 10/1535 [00:24<1:01:09,  2.41s/it]  1%|          | 11/1535 [00:26<1:01:07,  2.41s/it]  1%|          | 12/1535 [00:29<1:01:54,  2.44s/it]  1%|          | 13/1535 [00:31<1:01:56,  2.44s/it]  1%|          | 14/1535 [00:33<1:01:39,  2.43s/it]  1%|          | 15/1535 [00:36<1:01:26,  2.43s/it]  1%|          | 16/1535 [00:38<1:01:18,  2.42s/it]  1%|          | 17/1535 [00:41<1:01:10,  2.42s/it]  1%|          | 18/1535 [00:43<1:01:05,  2.42s/it]  1%|          | 19/1535 [00:45<1:01:00,  2.41s/it]  1%|▏         | 20/1535 [00:48<1:00:58,  2.41s/it]                                                   {'loss': 0.7191, 'grad_norm': 0.5477022528648376, 'learning_rate': 3.2573289902280134e-05, 'epoch': 0.07}
  1%|▏         | 20/1535 [00:48<1:00:58,  2.41s/it]  1%|▏         | 21/1535 [00:50<1:00:56,  2.42s/it]  1%|▏         | 22/1535 [00:53<1:00:53,  2.41s/it]  1%|▏         | 23/1535 [00:55<1:00:53,  2.42s/it]  2%|▏         | 24/1535 [00:58<1:00:52,  2.42s/it]  2%|▏         | 25/1535 [01:00<1:00:49,  2.42s/it]  2%|▏         | 26/1535 [01:02<1:01:05,  2.43s/it]  2%|▏         | 27/1535 [01:05<1:00:58,  2.43s/it]  2%|▏         | 28/1535 [01:07<1:00:53,  2.42s/it]  2%|▏         | 29/1535 [01:10<1:01:06,  2.43s/it]  2%|▏         | 30/1535 [01:12<1:00:55,  2.43s/it]  2%|▏         | 31/1535 [01:15<1:00:49,  2.43s/it]  2%|▏         | 32/1535 [01:17<1:00:44,  2.42s/it]  2%|▏         | 33/1535 [01:19<1:00:39,  2.42s/it]  2%|▏         | 34/1535 [01:22<1:00:34,  2.42s/it]  2%|▏         | 35/1535 [01:24<1:00:31,  2.42s/it]  2%|▏         | 36/1535 [01:27<1:00:28,  2.42s/it]  2%|▏         | 37/1535 [01:29<1:00:23,  2.42s/it]  2%|▏         | 38/1535 [01:31<1:00:21,  2.42s/it]  3%|▎         | 39/1535 [01:34<1:00:38,  2.43s/it]  3%|▎         | 40/1535 [01:36<1:00:29,  2.43s/it]                                                   {'loss': 0.6601, 'grad_norm': 0.38682258129119873, 'learning_rate': 6.514657980456027e-05, 'epoch': 0.13}
  3%|▎         | 40/1535 [01:36<1:00:29,  2.43s/it]  3%|▎         | 41/1535 [01:39<1:00:23,  2.43s/it]  3%|▎         | 42/1535 [01:41<1:00:35,  2.44s/it]  3%|▎         | 43/1535 [01:44<1:00:26,  2.43s/it]  3%|▎         | 44/1535 [01:46<1:00:18,  2.43s/it]  3%|▎         | 45/1535 [01:48<1:00:12,  2.42s/it]  3%|▎         | 46/1535 [01:51<1:00:08,  2.42s/it]  3%|▎         | 47/1535 [01:53<1:00:04,  2.42s/it]  3%|▎         | 48/1535 [01:56<1:00:00,  2.42s/it]  3%|▎         | 49/1535 [01:58<59:56,  2.42s/it]    3%|▎         | 50/1535 [02:01<59:54,  2.42s/it]  3%|▎         | 51/1535 [02:03<59:51,  2.42s/it]  3%|▎         | 52/1535 [02:05<59:47,  2.42s/it]  3%|▎         | 53/1535 [02:08<59:46,  2.42s/it]  4%|▎         | 54/1535 [02:10<1:00:02,  2.43s/it]  4%|▎         | 55/1535 [02:13<59:53,  2.43s/it]    4%|▎         | 56/1535 [02:15<1:00:23,  2.45s/it]  4%|▎         | 57/1535 [02:18<1:00:07,  2.44s/it]  4%|▍         | 58/1535 [02:20<59:55,  2.43s/it]    4%|▍         | 59/1535 [02:22<59:46,  2.43s/it]  4%|▍         | 60/1535 [02:25<59:39,  2.43s/it]                                                 {'loss': 0.6541, 'grad_norm': 0.4414440989494324, 'learning_rate': 9.771986970684039e-05, 'epoch': 0.2}
  4%|▍         | 60/1535 [02:25<59:39,  2.43s/it]  4%|▍         | 61/1535 [02:27<59:34,  2.43s/it]  4%|▍         | 62/1535 [02:30<59:29,  2.42s/it]  4%|▍         | 63/1535 [02:32<59:25,  2.42s/it]  4%|▍         | 64/1535 [02:35<59:21,  2.42s/it]  4%|▍         | 65/1535 [02:37<59:18,  2.42s/it]  4%|▍         | 66/1535 [02:39<59:15,  2.42s/it]  4%|▍         | 67/1535 [02:42<59:12,  2.42s/it]  4%|▍         | 68/1535 [02:44<59:20,  2.43s/it]  4%|▍         | 69/1535 [02:47<59:15,  2.43s/it]  5%|▍         | 70/1535 [02:49<59:25,  2.43s/it]  5%|▍         | 71/1535 [02:52<59:16,  2.43s/it]  5%|▍         | 72/1535 [02:54<59:10,  2.43s/it]  5%|▍         | 73/1535 [02:56<59:05,  2.42s/it]  5%|▍         | 74/1535 [02:59<59:00,  2.42s/it]  5%|▍         | 75/1535 [03:01<58:56,  2.42s/it]  5%|▍         | 76/1535 [03:04<58:53,  2.42s/it]  5%|▌         | 77/1535 [03:06<58:49,  2.42s/it]  5%|▌         | 78/1535 [03:08<58:46,  2.42s/it]  5%|▌         | 79/1535 [03:11<58:43,  2.42s/it]  5%|▌         | 80/1535 [03:13<58:40,  2.42s/it]                                                 {'loss': 0.6705, 'grad_norm': 0.4689149856567383, 'learning_rate': 0.00013029315960912054, 'epoch': 0.26}
  5%|▌         | 80/1535 [03:13<58:40,  2.42s/it]  5%|▌         | 81/1535 [03:16<58:38,  2.42s/it]  5%|▌         | 82/1535 [03:18<58:36,  2.42s/it]  5%|▌         | 83/1535 [03:21<58:50,  2.43s/it]  5%|▌         | 84/1535 [03:23<58:43,  2.43s/it]  6%|▌         | 85/1535 [03:25<58:36,  2.43s/it]  6%|▌         | 86/1535 [03:28<58:31,  2.42s/it]  6%|▌         | 87/1535 [03:30<58:27,  2.42s/it]  6%|▌         | 88/1535 [03:33<58:24,  2.42s/it]  6%|▌         | 89/1535 [03:35<58:20,  2.42s/it]  6%|▌         | 90/1535 [03:38<58:17,  2.42s/it]  6%|▌         | 91/1535 [03:40<58:14,  2.42s/it]  6%|▌         | 92/1535 [03:42<58:11,  2.42s/it]  6%|▌         | 93/1535 [03:45<58:09,  2.42s/it]  6%|▌         | 94/1535 [03:47<58:07,  2.42s/it]  6%|▌         | 95/1535 [03:50<58:04,  2.42s/it]  6%|▋         | 96/1535 [03:52<58:01,  2.42s/it]  6%|▋         | 97/1535 [03:55<58:16,  2.43s/it]  6%|▋         | 98/1535 [03:57<58:27,  2.44s/it]  6%|▋         | 99/1535 [03:59<58:16,  2.43s/it]  7%|▋         | 100/1535 [04:02<58:07,  2.43s/it]                                                  {'loss': 0.6716, 'grad_norm': 0.43029412627220154, 'learning_rate': 0.00016286644951140063, 'epoch': 0.33}
  7%|▋         | 100/1535 [04:02<58:07,  2.43s/it]  7%|▋         | 101/1535 [04:04<58:02,  2.43s/it]  7%|▋         | 102/1535 [04:07<57:55,  2.43s/it]  7%|▋         | 103/1535 [04:09<57:50,  2.42s/it]  7%|▋         | 104/1535 [04:12<57:46,  2.42s/it]  7%|▋         | 105/1535 [04:14<57:43,  2.42s/it]  7%|▋         | 106/1535 [04:16<57:42,  2.42s/it]  7%|▋         | 107/1535 [04:19<57:39,  2.42s/it]  7%|▋         | 108/1535 [04:21<57:35,  2.42s/it]  7%|▋         | 109/1535 [04:24<57:32,  2.42s/it]  7%|▋         | 110/1535 [04:26<57:28,  2.42s/it]  7%|▋         | 111/1535 [04:29<57:47,  2.44s/it]  7%|▋         | 112/1535 [04:31<57:53,  2.44s/it]  7%|▋         | 113/1535 [04:33<57:41,  2.43s/it]  7%|▋         | 114/1535 [04:36<57:32,  2.43s/it]  7%|▋         | 115/1535 [04:38<57:25,  2.43s/it]  8%|▊         | 116/1535 [04:41<57:19,  2.42s/it]  8%|▊         | 117/1535 [04:43<57:15,  2.42s/it]  8%|▊         | 118/1535 [04:45<57:11,  2.42s/it]  8%|▊         | 119/1535 [04:48<57:07,  2.42s/it]  8%|▊         | 120/1535 [04:50<57:04,  2.42s/it]                                                  {'loss': 0.6508, 'grad_norm': 0.395972341299057, 'learning_rate': 0.00019543973941368078, 'epoch': 0.39}
  8%|▊         | 120/1535 [04:50<57:04,  2.42s/it]  8%|▊         | 121/1535 [04:53<57:02,  2.42s/it]  8%|▊         | 122/1535 [04:55<56:58,  2.42s/it]  8%|▊         | 123/1535 [04:58<56:56,  2.42s/it]  8%|▊         | 124/1535 [05:00<56:53,  2.42s/it]  8%|▊         | 125/1535 [05:02<56:51,  2.42s/it]  8%|▊         | 126/1535 [05:05<57:24,  2.44s/it]  8%|▊         | 127/1535 [05:07<57:10,  2.44s/it]  8%|▊         | 128/1535 [05:10<57:00,  2.43s/it]  8%|▊         | 129/1535 [05:12<56:53,  2.43s/it]  8%|▊         | 130/1535 [05:15<56:46,  2.42s/it]  9%|▊         | 131/1535 [05:17<56:42,  2.42s/it]  9%|▊         | 132/1535 [05:19<56:38,  2.42s/it]  9%|▊         | 133/1535 [05:22<56:34,  2.42s/it]  9%|▊         | 134/1535 [05:24<56:30,  2.42s/it]  9%|▉         | 135/1535 [05:27<56:27,  2.42s/it]  9%|▉         | 136/1535 [05:29<56:24,  2.42s/it]  9%|▉         | 137/1535 [05:32<56:22,  2.42s/it]  9%|▉         | 138/1535 [05:34<56:19,  2.42s/it]  9%|▉         | 139/1535 [05:36<56:32,  2.43s/it]  9%|▉         | 140/1535 [05:39<56:25,  2.43s/it]                                                  {'loss': 0.6265, 'grad_norm': 0.3994178771972656, 'learning_rate': 0.0002280130293159609, 'epoch': 0.46}
  9%|▉         | 140/1535 [05:39<56:25,  2.43s/it]  9%|▉         | 141/1535 [05:41<56:29,  2.43s/it]  9%|▉         | 142/1535 [05:44<56:21,  2.43s/it]  9%|▉         | 143/1535 [05:46<56:16,  2.43s/it]  9%|▉         | 144/1535 [05:49<56:11,  2.42s/it]  9%|▉         | 145/1535 [05:51<56:07,  2.42s/it] 10%|▉         | 146/1535 [05:53<56:03,  2.42s/it] 10%|▉         | 147/1535 [05:56<55:59,  2.42s/it] 10%|▉         | 148/1535 [05:58<55:56,  2.42s/it] 10%|▉         | 149/1535 [06:01<55:53,  2.42s/it] 10%|▉         | 150/1535 [06:03<55:50,  2.42s/it] 10%|▉         | 151/1535 [06:05<55:47,  2.42s/it] 10%|▉         | 152/1535 [06:08<55:55,  2.43s/it] 10%|▉         | 153/1535 [06:10<55:50,  2.42s/it] 10%|█         | 154/1535 [06:13<55:46,  2.42s/it] 10%|█         | 155/1535 [06:15<55:44,  2.42s/it] 10%|█         | 156/1535 [06:18<55:41,  2.42s/it] 10%|█         | 157/1535 [06:20<55:37,  2.42s/it] 10%|█         | 158/1535 [06:22<55:35,  2.42s/it] 10%|█         | 159/1535 [06:25<55:32,  2.42s/it] 10%|█         | 160/1535 [06:27<55:29,  2.42s/it]                                                  {'loss': 0.611, 'grad_norm': 0.3212507665157318, 'learning_rate': 0.0002605863192182411, 'epoch': 0.52}
 10%|█         | 160/1535 [06:27<55:29,  2.42s/it] 10%|█         | 161/1535 [06:30<55:28,  2.42s/it] 11%|█         | 162/1535 [06:32<55:26,  2.42s/it] 11%|█         | 163/1535 [06:35<55:24,  2.42s/it] 11%|█         | 164/1535 [06:37<55:20,  2.42s/it] 11%|█         | 165/1535 [06:39<55:17,  2.42s/it] 11%|█         | 166/1535 [06:42<55:15,  2.42s/it] 11%|█         | 167/1535 [06:44<55:12,  2.42s/it] 11%|█         | 168/1535 [06:47<55:19,  2.43s/it] 11%|█         | 169/1535 [06:49<55:15,  2.43s/it] 11%|█         | 170/1535 [06:52<55:19,  2.43s/it] 11%|█         | 171/1535 [06:54<55:13,  2.43s/it] 11%|█         | 172/1535 [06:56<55:08,  2.43s/it] 11%|█▏        | 173/1535 [06:59<55:02,  2.42s/it] 11%|█▏        | 174/1535 [07:01<54:59,  2.42s/it] 11%|█▏        | 175/1535 [07:04<54:56,  2.42s/it] 11%|█▏        | 176/1535 [07:06<54:53,  2.42s/it] 12%|█▏        | 177/1535 [07:08<54:50,  2.42s/it] 12%|█▏        | 178/1535 [07:11<54:47,  2.42s/it] 12%|█▏        | 179/1535 [07:13<54:45,  2.42s/it] 12%|█▏        | 180/1535 [07:16<54:42,  2.42s/it]                                                  {'loss': 0.6118, 'grad_norm': 0.29577863216400146, 'learning_rate': 0.0002915309446254072, 'epoch': 0.59}
 12%|█▏        | 180/1535 [07:16<54:42,  2.42s/it] 12%|█▏        | 181/1535 [07:18<54:55,  2.43s/it] 12%|█▏        | 182/1535 [07:21<54:48,  2.43s/it] 12%|█▏        | 183/1535 [07:23<54:42,  2.43s/it] 12%|█▏        | 184/1535 [07:26<54:46,  2.43s/it] 12%|█▏        | 185/1535 [07:28<54:39,  2.43s/it] 12%|█▏        | 186/1535 [07:30<54:34,  2.43s/it] 12%|█▏        | 187/1535 [07:33<54:29,  2.43s/it] 12%|█▏        | 188/1535 [07:35<54:25,  2.42s/it] 12%|█▏        | 189/1535 [07:38<54:22,  2.42s/it] 12%|█▏        | 190/1535 [07:40<54:19,  2.42s/it] 12%|█▏        | 191/1535 [07:42<54:16,  2.42s/it] 13%|█▎        | 192/1535 [07:45<54:13,  2.42s/it] 13%|█▎        | 193/1535 [07:47<54:10,  2.42s/it] 13%|█▎        | 194/1535 [07:50<54:07,  2.42s/it] 13%|█▎        | 195/1535 [07:52<54:22,  2.43s/it] 13%|█▎        | 196/1535 [07:55<54:15,  2.43s/it] 13%|█▎        | 197/1535 [07:57<54:09,  2.43s/it] 13%|█▎        | 198/1535 [07:59<54:04,  2.43s/it] 13%|█▎        | 199/1535 [08:02<54:08,  2.43s/it] 13%|█▎        | 200/1535 [08:04<54:02,  2.43s/it]                                                  {'loss': 0.6168, 'grad_norm': 0.3810480535030365, 'learning_rate': 0.0003241042345276873, 'epoch': 0.65}
 13%|█▎        | 200/1535 [08:04<54:02,  2.43s/it] 13%|█▎        | 201/1535 [08:07<53:58,  2.43s/it] 13%|█▎        | 202/1535 [08:09<53:53,  2.43s/it] 13%|█▎        | 203/1535 [08:12<53:50,  2.43s/it] 13%|█▎        | 204/1535 [08:14<53:46,  2.42s/it] 13%|█▎        | 205/1535 [08:16<53:43,  2.42s/it] 13%|█▎        | 206/1535 [08:19<53:40,  2.42s/it] 13%|█▎        | 207/1535 [08:21<53:37,  2.42s/it] 14%|█▎        | 208/1535 [08:24<53:35,  2.42s/it] 14%|█▎        | 209/1535 [08:26<53:42,  2.43s/it] 14%|█▎        | 210/1535 [08:29<53:36,  2.43s/it] 14%|█▎        | 211/1535 [08:31<53:32,  2.43s/it] 14%|█▍        | 212/1535 [08:33<53:28,  2.42s/it] 14%|█▍        | 213/1535 [08:36<53:32,  2.43s/it] 14%|█▍        | 214/1535 [08:38<53:26,  2.43s/it] 14%|█▍        | 215/1535 [08:41<53:21,  2.43s/it] 14%|█▍        | 216/1535 [08:43<53:17,  2.42s/it] 14%|█▍        | 217/1535 [08:46<53:14,  2.42s/it] 14%|█▍        | 218/1535 [08:48<53:11,  2.42s/it] 14%|█▍        | 219/1535 [08:50<53:08,  2.42s/it] 14%|█▍        | 220/1535 [08:53<53:05,  2.42s/it]                                                  {'loss': 0.5755, 'grad_norm': 0.35255399346351624, 'learning_rate': 0.0003566775244299674, 'epoch': 0.72}
 14%|█▍        | 220/1535 [08:53<53:05,  2.42s/it] 14%|█▍        | 221/1535 [08:55<53:03,  2.42s/it] 14%|█▍        | 222/1535 [08:58<53:09,  2.43s/it] 15%|█▍        | 223/1535 [09:00<53:04,  2.43s/it] 15%|█▍        | 224/1535 [09:03<52:59,  2.43s/it] 15%|█▍        | 225/1535 [09:05<52:55,  2.42s/it] 15%|█▍        | 226/1535 [09:07<52:52,  2.42s/it] 15%|█▍        | 227/1535 [09:10<52:50,  2.42s/it] 15%|█▍        | 228/1535 [09:12<53:02,  2.43s/it] 15%|█▍        | 229/1535 [09:15<52:54,  2.43s/it] 15%|█▍        | 230/1535 [09:17<52:49,  2.43s/it] 15%|█▌        | 231/1535 [09:20<52:43,  2.43s/it] 15%|█▌        | 232/1535 [09:22<52:39,  2.42s/it] 15%|█▌        | 233/1535 [09:24<52:35,  2.42s/it] 15%|█▌        | 234/1535 [09:27<52:32,  2.42s/it] 15%|█▌        | 235/1535 [09:29<52:29,  2.42s/it] 15%|█▌        | 236/1535 [09:32<52:26,  2.42s/it] 15%|█▌        | 237/1535 [09:34<52:29,  2.43s/it] 16%|█▌        | 238/1535 [09:36<52:25,  2.43s/it] 16%|█▌        | 239/1535 [09:39<52:21,  2.42s/it] 16%|█▌        | 240/1535 [09:41<52:18,  2.42s/it]                                                  {'loss': 0.5786, 'grad_norm': 0.32808059453964233, 'learning_rate': 0.0003892508143322476, 'epoch': 0.78}
 16%|█▌        | 240/1535 [09:41<52:18,  2.42s/it] 16%|█▌        | 241/1535 [09:44<52:16,  2.42s/it] 16%|█▌        | 242/1535 [09:46<52:21,  2.43s/it] 16%|█▌        | 243/1535 [09:49<52:15,  2.43s/it] 16%|█▌        | 244/1535 [09:51<52:11,  2.43s/it] 16%|█▌        | 245/1535 [09:53<52:07,  2.42s/it] 16%|█▌        | 246/1535 [09:56<52:04,  2.42s/it] 16%|█▌        | 247/1535 [09:58<52:01,  2.42s/it] 16%|█▌        | 248/1535 [10:01<51:59,  2.42s/it] 16%|█▌        | 249/1535 [10:03<51:56,  2.42s/it] 16%|█▋        | 250/1535 [10:06<52:08,  2.43s/it] 16%|█▋        | 251/1535 [10:08<52:00,  2.43s/it] 16%|█▋        | 252/1535 [10:10<51:54,  2.43s/it] 16%|█▋        | 253/1535 [10:13<51:50,  2.43s/it] 17%|█▋        | 254/1535 [10:15<51:46,  2.43s/it] 17%|█▋        | 255/1535 [10:18<51:42,  2.42s/it] 17%|█▋        | 256/1535 [10:20<51:39,  2.42s/it] 17%|█▋        | 257/1535 [10:23<51:45,  2.43s/it] 17%|█▋        | 258/1535 [10:25<51:39,  2.43s/it] 17%|█▋        | 259/1535 [10:27<51:35,  2.43s/it] 17%|█▋        | 260/1535 [10:30<51:31,  2.42s/it]                                                  {'loss': 0.5911, 'grad_norm': 0.39393800497055054, 'learning_rate': 0.0004218241042345277, 'epoch': 0.85}
 17%|█▋        | 260/1535 [10:30<51:31,  2.42s/it] 17%|█▋        | 261/1535 [10:32<51:28,  2.42s/it] 17%|█▋        | 262/1535 [10:35<51:24,  2.42s/it] 17%|█▋        | 263/1535 [10:37<51:21,  2.42s/it] 17%|█▋        | 264/1535 [10:40<51:18,  2.42s/it] 17%|█▋        | 265/1535 [10:42<51:28,  2.43s/it] 17%|█▋        | 266/1535 [10:44<51:21,  2.43s/it] 17%|█▋        | 267/1535 [10:47<51:16,  2.43s/it] 17%|█▋        | 268/1535 [10:49<51:12,  2.42s/it] 18%|█▊        | 269/1535 [10:52<51:08,  2.42s/it] 18%|█▊        | 270/1535 [10:54<51:05,  2.42s/it] 18%|█▊        | 271/1535 [10:57<51:10,  2.43s/it] 18%|█▊        | 272/1535 [10:59<51:05,  2.43s/it] 18%|█▊        | 273/1535 [11:01<51:00,  2.43s/it] 18%|█▊        | 274/1535 [11:04<50:57,  2.42s/it] 18%|█▊        | 275/1535 [11:06<50:53,  2.42s/it] 18%|█▊        | 276/1535 [11:09<50:50,  2.42s/it] 18%|█▊        | 277/1535 [11:11<50:47,  2.42s/it] 18%|█▊        | 278/1535 [11:14<50:55,  2.43s/it] 18%|█▊        | 279/1535 [11:16<50:49,  2.43s/it] 18%|█▊        | 280/1535 [11:18<50:44,  2.43s/it]                                                  {'loss': 0.5928, 'grad_norm': 0.3561767339706421, 'learning_rate': 0.0004543973941368078, 'epoch': 0.91}
 18%|█▊        | 280/1535 [11:18<50:44,  2.43s/it] 18%|█▊        | 281/1535 [11:21<50:41,  2.43s/it] 18%|█▊        | 282/1535 [11:23<50:37,  2.42s/it] 18%|█▊        | 283/1535 [11:26<50:34,  2.42s/it] 19%|█▊        | 284/1535 [11:28<50:31,  2.42s/it] 19%|█▊        | 285/1535 [11:30<50:28,  2.42s/it] 19%|█▊        | 286/1535 [11:33<50:33,  2.43s/it] 19%|█▊        | 287/1535 [11:35<50:28,  2.43s/it] 19%|█▉        | 288/1535 [11:38<50:24,  2.43s/it] 19%|█▉        | 289/1535 [11:40<50:20,  2.42s/it] 19%|█▉        | 290/1535 [11:43<50:17,  2.42s/it] 19%|█▉        | 291/1535 [11:45<50:14,  2.42s/it] 19%|█▉        | 292/1535 [11:47<50:11,  2.42s/it] 19%|█▉        | 293/1535 [11:50<50:08,  2.42s/it] 19%|█▉        | 294/1535 [11:52<50:05,  2.42s/it] 19%|█▉        | 295/1535 [11:55<50:02,  2.42s/it] 19%|█▉        | 296/1535 [11:57<50:00,  2.42s/it] 19%|█▉        | 297/1535 [12:00<49:57,  2.42s/it] 19%|█▉        | 298/1535 [12:02<49:55,  2.42s/it] 19%|█▉        | 299/1535 [12:04<49:52,  2.42s/it] 20%|█▉        | 300/1535 [12:07<49:51,  2.42s/it]                                                  {'loss': 0.5946, 'grad_norm': 0.35801902413368225, 'learning_rate': 0.00048697068403908794, 'epoch': 0.98}
 20%|█▉        | 300/1535 [12:07<49:51,  2.42s/it] 20%|█▉        | 301/1535 [12:09<49:49,  2.42s/it] 20%|█▉        | 302/1535 [12:12<49:47,  2.42s/it] 20%|█▉        | 303/1535 [12:14<49:44,  2.42s/it] 20%|█▉        | 304/1535 [12:17<49:41,  2.42s/it] 20%|█▉        | 305/1535 [12:19<49:39,  2.42s/it] 20%|█▉        | 306/1535 [12:21<49:36,  2.42s/it] 20%|██        | 307/1535 [12:24<49:38,  2.43s/it] 20%|██        | 308/1535 [12:26<49:34,  2.42s/it] 20%|██        | 309/1535 [12:29<49:30,  2.42s/it] 20%|██        | 310/1535 [12:31<49:27,  2.42s/it] 20%|██        | 311/1535 [12:33<49:24,  2.42s/it] 20%|██        | 312/1535 [12:36<49:21,  2.42s/it] 20%|██        | 313/1535 [12:38<49:19,  2.42s/it] 20%|██        | 314/1535 [12:41<49:16,  2.42s/it] 21%|██        | 315/1535 [12:43<49:21,  2.43s/it] 21%|██        | 316/1535 [12:46<49:16,  2.43s/it] 21%|██        | 317/1535 [12:48<49:12,  2.42s/it] 21%|██        | 318/1535 [12:50<49:09,  2.42s/it] 21%|██        | 319/1535 [12:53<49:06,  2.42s/it] 21%|██        | 320/1535 [12:55<49:12,  2.43s/it]                                                  {'loss': 0.5805, 'grad_norm': 0.32812991738319397, 'learning_rate': 0.0004998822010531848, 'epoch': 1.04}
 21%|██        | 320/1535 [12:55<49:12,  2.43s/it] 21%|██        | 321/1535 [12:58<49:08,  2.43s/it] 21%|██        | 322/1535 [13:00<49:02,  2.43s/it] 21%|██        | 323/1535 [13:03<48:58,  2.42s/it] 21%|██        | 324/1535 [13:05<48:54,  2.42s/it] 21%|██        | 325/1535 [13:07<48:51,  2.42s/it] 21%|██        | 326/1535 [13:10<48:49,  2.42s/it] 21%|██▏       | 327/1535 [13:12<48:46,  2.42s/it] 21%|██▏       | 328/1535 [13:15<48:43,  2.42s/it] 21%|██▏       | 329/1535 [13:17<48:48,  2.43s/it] 21%|██▏       | 330/1535 [13:20<48:43,  2.43s/it] 22%|██▏       | 331/1535 [13:22<48:39,  2.42s/it] 22%|██▏       | 332/1535 [13:24<48:35,  2.42s/it] 22%|██▏       | 333/1535 [13:27<48:32,  2.42s/it] 22%|██▏       | 334/1535 [13:29<48:41,  2.43s/it] 22%|██▏       | 335/1535 [13:32<48:35,  2.43s/it] 22%|██▏       | 336/1535 [13:34<48:30,  2.43s/it] 22%|██▏       | 337/1535 [13:37<48:25,  2.43s/it] 22%|██▏       | 338/1535 [13:39<48:21,  2.42s/it] 22%|██▏       | 339/1535 [13:41<48:18,  2.42s/it] 22%|██▏       | 340/1535 [13:44<48:16,  2.42s/it]                                                  {'loss': 0.5769, 'grad_norm': 0.3721354603767395, 'learning_rate': 0.0004991627205825621, 'epoch': 1.11}
 22%|██▏       | 340/1535 [13:44<48:16,  2.42s/it] 22%|██▏       | 341/1535 [13:46<48:14,  2.42s/it] 22%|██▏       | 342/1535 [13:49<48:10,  2.42s/it] 22%|██▏       | 343/1535 [13:51<48:07,  2.42s/it] 22%|██▏       | 344/1535 [13:54<48:15,  2.43s/it] 22%|██▏       | 345/1535 [13:56<48:09,  2.43s/it] 23%|██▎       | 346/1535 [13:58<48:04,  2.43s/it] 23%|██▎       | 347/1535 [14:01<48:00,  2.42s/it] 23%|██▎       | 348/1535 [14:03<48:04,  2.43s/it] 23%|██▎       | 349/1535 [14:06<47:59,  2.43s/it] 23%|██▎       | 350/1535 [14:08<47:54,  2.43s/it] 23%|██▎       | 351/1535 [14:10<47:50,  2.42s/it] 23%|██▎       | 352/1535 [14:13<47:47,  2.42s/it] 23%|██▎       | 353/1535 [14:15<47:45,  2.42s/it] 23%|██▎       | 354/1535 [14:18<47:41,  2.42s/it] 23%|██▎       | 355/1535 [14:20<47:38,  2.42s/it] 23%|██▎       | 356/1535 [14:23<47:35,  2.42s/it] 23%|██▎       | 357/1535 [14:25<47:32,  2.42s/it] 23%|██▎       | 358/1535 [14:27<47:33,  2.42s/it] 23%|██▎       | 359/1535 [14:30<47:30,  2.42s/it] 23%|██▎       | 360/1535 [14:32<47:26,  2.42s/it]                                                  {'loss': 0.5174, 'grad_norm': 0.48570215702056885, 'learning_rate': 0.0004977910843763777, 'epoch': 1.17}
 23%|██▎       | 360/1535 [14:32<47:26,  2.42s/it] 24%|██▎       | 361/1535 [14:35<47:24,  2.42s/it] 24%|██▎       | 362/1535 [14:37<47:21,  2.42s/it] 24%|██▎       | 363/1535 [14:40<47:19,  2.42s/it] 24%|██▎       | 364/1535 [14:42<47:16,  2.42s/it] 24%|██▍       | 365/1535 [14:44<47:13,  2.42s/it] 24%|██▍       | 366/1535 [14:47<47:10,  2.42s/it] 24%|██▍       | 367/1535 [14:49<47:08,  2.42s/it] 24%|██▍       | 368/1535 [14:52<47:05,  2.42s/it] 24%|██▍       | 369/1535 [14:54<47:02,  2.42s/it] 24%|██▍       | 370/1535 [14:57<47:00,  2.42s/it] 24%|██▍       | 371/1535 [14:59<46:58,  2.42s/it] 24%|██▍       | 372/1535 [15:01<46:55,  2.42s/it] 24%|██▍       | 373/1535 [15:04<47:00,  2.43s/it] 24%|██▍       | 374/1535 [15:06<46:57,  2.43s/it] 24%|██▍       | 375/1535 [15:09<46:52,  2.42s/it] 24%|██▍       | 376/1535 [15:11<46:53,  2.43s/it] 25%|██▍       | 377/1535 [15:13<46:49,  2.43s/it] 25%|██▍       | 378/1535 [15:16<46:45,  2.42s/it] 25%|██▍       | 379/1535 [15:18<46:41,  2.42s/it] 25%|██▍       | 380/1535 [15:21<46:38,  2.42s/it]                                                  {'loss': 0.5128, 'grad_norm': 0.3361925482749939, 'learning_rate': 0.0004957708825399927, 'epoch': 1.24}
 25%|██▍       | 380/1535 [15:21<46:38,  2.42s/it] 25%|██▍       | 381/1535 [15:23<46:36,  2.42s/it] 25%|██▍       | 382/1535 [15:26<46:33,  2.42s/it] 25%|██▍       | 383/1535 [15:28<46:30,  2.42s/it] 25%|██▌       | 384/1535 [15:30<46:27,  2.42s/it] 25%|██▌       | 385/1535 [15:33<46:24,  2.42s/it] 25%|██▌       | 386/1535 [15:35<46:21,  2.42s/it] 25%|██▌       | 387/1535 [15:38<46:20,  2.42s/it] 25%|██▌       | 388/1535 [15:40<46:17,  2.42s/it] 25%|██▌       | 389/1535 [15:43<46:23,  2.43s/it] 25%|██▌       | 390/1535 [15:45<46:18,  2.43s/it] 25%|██▌       | 391/1535 [15:47<46:14,  2.43s/it] 26%|██▌       | 392/1535 [15:50<46:10,  2.42s/it] 26%|██▌       | 393/1535 [15:52<46:07,  2.42s/it] 26%|██▌       | 394/1535 [15:55<46:04,  2.42s/it] 26%|██▌       | 395/1535 [15:57<46:01,  2.42s/it] 26%|██▌       | 396/1535 [16:00<45:58,  2.42s/it] 26%|██▌       | 397/1535 [16:02<45:55,  2.42s/it] 26%|██▌       | 398/1535 [16:04<45:52,  2.42s/it] 26%|██▌       | 399/1535 [16:07<45:50,  2.42s/it] 26%|██▌       | 400/1535 [16:09<45:47,  2.42s/it]                                                  {'loss': 0.528, 'grad_norm': 0.3597891628742218, 'learning_rate': 0.0004931074027272406, 'epoch': 1.3}
 26%|██▌       | 400/1535 [16:09<45:47,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 05:31:09,098 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-400
[INFO|configuration_utils.py:726] 2024-05-25 05:31:10,603 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 05:31:10,606 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 05:31:10,688 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 05:31:10,689 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-400/special_tokens_map.json
 26%|██▌       | 401/1535 [16:13<55:43,  2.95s/it] 26%|██▌       | 402/1535 [16:16<52:40,  2.79s/it] 26%|██▋       | 403/1535 [16:18<50:31,  2.68s/it] 26%|██▋       | 404/1535 [16:21<49:10,  2.61s/it] 26%|██▋       | 405/1535 [16:23<48:03,  2.55s/it] 26%|██▋       | 406/1535 [16:26<47:16,  2.51s/it] 27%|██▋       | 407/1535 [16:28<46:42,  2.48s/it] 27%|██▋       | 408/1535 [16:30<46:17,  2.46s/it] 27%|██▋       | 409/1535 [16:33<46:00,  2.45s/it] 27%|██▋       | 410/1535 [16:35<45:46,  2.44s/it] 27%|██▋       | 411/1535 [16:38<45:35,  2.43s/it] 27%|██▋       | 412/1535 [16:40<45:28,  2.43s/it] 27%|██▋       | 413/1535 [16:42<45:22,  2.43s/it] 27%|██▋       | 414/1535 [16:45<45:17,  2.42s/it] 27%|██▋       | 415/1535 [16:47<45:57,  2.46s/it] 27%|██▋       | 416/1535 [16:50<45:40,  2.45s/it] 27%|██▋       | 417/1535 [16:52<45:37,  2.45s/it] 27%|██▋       | 418/1535 [16:55<45:23,  2.44s/it] 27%|██▋       | 419/1535 [16:57<45:15,  2.43s/it] 27%|██▋       | 420/1535 [17:00<45:07,  2.43s/it]                                                  {'loss': 0.5196, 'grad_norm': 0.40811651945114136, 'learning_rate': 0.0004899875912715296, 'epoch': 1.37}
 27%|██▋       | 420/1535 [17:00<45:07,  2.43s/it] 27%|██▋       | 421/1535 [17:02<45:02,  2.43s/it] 27%|██▋       | 422/1535 [17:04<44:58,  2.42s/it] 28%|██▊       | 423/1535 [17:07<44:54,  2.42s/it] 28%|██▊       | 424/1535 [17:09<44:50,  2.42s/it] 28%|██▊       | 425/1535 [17:12<44:46,  2.42s/it] 28%|██▊       | 426/1535 [17:14<44:43,  2.42s/it] 28%|██▊       | 427/1535 [17:16<44:41,  2.42s/it] 28%|██▊       | 428/1535 [17:19<44:38,  2.42s/it] 28%|██▊       | 429/1535 [17:21<44:35,  2.42s/it] 28%|██▊       | 430/1535 [17:24<44:43,  2.43s/it] 28%|██▊       | 431/1535 [17:26<44:37,  2.43s/it] 28%|██▊       | 432/1535 [17:29<44:32,  2.42s/it] 28%|██▊       | 433/1535 [17:31<44:28,  2.42s/it] 28%|██▊       | 434/1535 [17:33<44:25,  2.42s/it] 28%|██▊       | 435/1535 [17:36<44:22,  2.42s/it] 28%|██▊       | 436/1535 [17:38<44:19,  2.42s/it] 28%|██▊       | 437/1535 [17:41<44:16,  2.42s/it] 29%|██▊       | 438/1535 [17:43<44:14,  2.42s/it] 29%|██▊       | 439/1535 [17:46<44:11,  2.42s/it] 29%|██▊       | 440/1535 [17:48<44:08,  2.42s/it]                                                  {'loss': 0.5617, 'grad_norm': 0.3522901237010956, 'learning_rate': 0.0004860912879566511, 'epoch': 1.43}
 29%|██▊       | 440/1535 [17:48<44:08,  2.42s/it] 29%|██▊       | 441/1535 [17:50<44:08,  2.42s/it] 29%|██▉       | 442/1535 [17:53<44:05,  2.42s/it] 29%|██▉       | 443/1535 [17:55<44:02,  2.42s/it] 29%|██▉       | 444/1535 [17:58<43:59,  2.42s/it] 29%|██▉       | 445/1535 [18:00<43:57,  2.42s/it] 29%|██▉       | 446/1535 [18:02<43:59,  2.42s/it] 29%|██▉       | 447/1535 [18:05<43:56,  2.42s/it] 29%|██▉       | 448/1535 [18:07<43:54,  2.42s/it] 29%|██▉       | 449/1535 [18:10<43:51,  2.42s/it] 29%|██▉       | 450/1535 [18:12<43:48,  2.42s/it] 29%|██▉       | 451/1535 [18:15<43:45,  2.42s/it] 29%|██▉       | 452/1535 [18:17<43:42,  2.42s/it] 30%|██▉       | 453/1535 [18:19<43:39,  2.42s/it] 30%|██▉       | 454/1535 [18:22<43:37,  2.42s/it] 30%|██▉       | 455/1535 [18:24<43:35,  2.42s/it] 30%|██▉       | 456/1535 [18:27<43:32,  2.42s/it] 30%|██▉       | 457/1535 [18:29<43:30,  2.42s/it] 30%|██▉       | 458/1535 [18:32<43:27,  2.42s/it] 30%|██▉       | 459/1535 [18:34<43:35,  2.43s/it] 30%|██▉       | 460/1535 [18:36<43:29,  2.43s/it]                                                  {'loss': 0.5333, 'grad_norm': 0.3467969596385956, 'learning_rate': 0.000481577041928685, 'epoch': 1.5}
 30%|██▉       | 460/1535 [18:36<43:29,  2.43s/it] 30%|███       | 461/1535 [18:39<43:25,  2.43s/it] 30%|███       | 462/1535 [18:41<43:21,  2.42s/it] 30%|███       | 463/1535 [18:44<43:17,  2.42s/it] 30%|███       | 464/1535 [18:46<43:14,  2.42s/it] 30%|███       | 465/1535 [18:49<43:12,  2.42s/it] 30%|███       | 466/1535 [18:51<43:09,  2.42s/it] 30%|███       | 467/1535 [18:53<43:06,  2.42s/it] 30%|███       | 468/1535 [18:56<43:03,  2.42s/it] 31%|███       | 469/1535 [18:58<43:00,  2.42s/it] 31%|███       | 470/1535 [19:01<42:59,  2.42s/it] 31%|███       | 471/1535 [19:03<42:56,  2.42s/it] 31%|███       | 472/1535 [19:05<42:53,  2.42s/it] 31%|███       | 473/1535 [19:08<43:07,  2.44s/it] 31%|███       | 474/1535 [19:10<43:00,  2.43s/it] 31%|███       | 475/1535 [19:13<42:54,  2.43s/it] 31%|███       | 476/1535 [19:15<42:49,  2.43s/it] 31%|███       | 477/1535 [19:18<42:45,  2.42s/it] 31%|███       | 478/1535 [19:20<42:42,  2.42s/it] 31%|███       | 479/1535 [19:22<42:38,  2.42s/it] 31%|███▏      | 480/1535 [19:25<42:36,  2.42s/it]                                                  {'loss': 0.5276, 'grad_norm': 0.33194389939308167, 'learning_rate': 0.000476456668725012, 'epoch': 1.56}
 31%|███▏      | 480/1535 [19:25<42:36,  2.42s/it] 31%|███▏      | 481/1535 [19:27<42:33,  2.42s/it] 31%|███▏      | 482/1535 [19:30<42:30,  2.42s/it] 31%|███▏      | 483/1535 [19:32<42:27,  2.42s/it] 32%|███▏      | 484/1535 [19:35<42:25,  2.42s/it] 32%|███▏      | 485/1535 [19:37<42:22,  2.42s/it] 32%|███▏      | 486/1535 [19:39<42:19,  2.42s/it] 32%|███▏      | 487/1535 [19:42<42:27,  2.43s/it] 32%|███▏      | 488/1535 [19:44<42:22,  2.43s/it] 32%|███▏      | 489/1535 [19:47<42:17,  2.43s/it] 32%|███▏      | 490/1535 [19:49<42:13,  2.42s/it] 32%|███▏      | 491/1535 [19:52<42:09,  2.42s/it] 32%|███▏      | 492/1535 [19:54<42:06,  2.42s/it] 32%|███▏      | 493/1535 [19:56<42:03,  2.42s/it] 32%|███▏      | 494/1535 [19:59<42:00,  2.42s/it] 32%|███▏      | 495/1535 [20:01<41:58,  2.42s/it] 32%|███▏      | 496/1535 [20:04<41:56,  2.42s/it] 32%|███▏      | 497/1535 [20:06<41:53,  2.42s/it] 32%|███▏      | 498/1535 [20:08<41:51,  2.42s/it] 33%|███▎      | 499/1535 [20:11<41:48,  2.42s/it] 33%|███▎      | 500/1535 [20:13<41:46,  2.42s/it]                                                  {'loss': 0.5211, 'grad_norm': 0.29971131682395935, 'learning_rate': 0.0004707435703535453, 'epoch': 1.63}
 33%|███▎      | 500/1535 [20:13<41:46,  2.42s/it] 33%|███▎      | 501/1535 [20:16<41:47,  2.42s/it] 33%|███▎      | 502/1535 [20:18<41:44,  2.42s/it] 33%|███▎      | 503/1535 [20:21<41:40,  2.42s/it] 33%|███▎      | 504/1535 [20:23<41:37,  2.42s/it] 33%|███▎      | 505/1535 [20:25<41:34,  2.42s/it] 33%|███▎      | 506/1535 [20:28<41:31,  2.42s/it] 33%|███▎      | 507/1535 [20:30<41:29,  2.42s/it] 33%|███▎      | 508/1535 [20:33<41:26,  2.42s/it] 33%|███▎      | 509/1535 [20:35<41:24,  2.42s/it] 33%|███▎      | 510/1535 [20:38<41:21,  2.42s/it] 33%|███▎      | 511/1535 [20:40<41:19,  2.42s/it] 33%|███▎      | 512/1535 [20:42<41:16,  2.42s/it] 33%|███▎      | 513/1535 [20:45<41:14,  2.42s/it] 33%|███▎      | 514/1535 [20:47<41:11,  2.42s/it] 34%|███▎      | 515/1535 [20:50<41:10,  2.42s/it] 34%|███▎      | 516/1535 [20:52<41:07,  2.42s/it] 34%|███▎      | 517/1535 [20:55<41:07,  2.42s/it] 34%|███▎      | 518/1535 [20:57<41:05,  2.42s/it] 34%|███▍      | 519/1535 [20:59<41:02,  2.42s/it] 34%|███▍      | 520/1535 [21:02<40:58,  2.42s/it]                                                  {'loss': 0.4895, 'grad_norm': 0.3427269160747528, 'learning_rate': 0.00046445270021446504, 'epoch': 1.69}
 34%|███▍      | 520/1535 [21:02<40:58,  2.42s/it] 34%|███▍      | 521/1535 [21:04<40:56,  2.42s/it] 34%|███▍      | 522/1535 [21:07<40:53,  2.42s/it] 34%|███▍      | 523/1535 [21:09<40:50,  2.42s/it] 34%|███▍      | 524/1535 [21:11<40:47,  2.42s/it] 34%|███▍      | 525/1535 [21:14<40:45,  2.42s/it] 34%|███▍      | 526/1535 [21:16<40:42,  2.42s/it] 34%|███▍      | 527/1535 [21:19<40:40,  2.42s/it] 34%|███▍      | 528/1535 [21:21<40:42,  2.43s/it] 34%|███▍      | 529/1535 [21:24<40:38,  2.42s/it] 35%|███▍      | 530/1535 [21:26<40:35,  2.42s/it] 35%|███▍      | 531/1535 [21:28<40:32,  2.42s/it] 35%|███▍      | 532/1535 [21:31<40:29,  2.42s/it] 35%|███▍      | 533/1535 [21:33<40:26,  2.42s/it] 35%|███▍      | 534/1535 [21:36<40:23,  2.42s/it] 35%|███▍      | 535/1535 [21:38<40:20,  2.42s/it] 35%|███▍      | 536/1535 [21:41<40:18,  2.42s/it] 35%|███▍      | 537/1535 [21:43<40:15,  2.42s/it] 35%|███▌      | 538/1535 [21:45<40:13,  2.42s/it] 35%|███▌      | 539/1535 [21:48<40:11,  2.42s/it] 35%|███▌      | 540/1535 [21:50<40:08,  2.42s/it]                                                  {'loss': 0.4908, 'grad_norm': 0.33904391527175903, 'learning_rate': 0.00045760052396135386, 'epoch': 1.76}
 35%|███▌      | 540/1535 [21:50<40:08,  2.42s/it] 35%|███▌      | 541/1535 [21:53<40:06,  2.42s/it] 35%|███▌      | 542/1535 [21:55<40:04,  2.42s/it] 35%|███▌      | 543/1535 [21:58<40:12,  2.43s/it] 35%|███▌      | 544/1535 [22:00<40:07,  2.43s/it] 36%|███▌      | 545/1535 [22:02<40:01,  2.43s/it] 36%|███▌      | 546/1535 [22:05<39:58,  2.42s/it] 36%|███▌      | 547/1535 [22:07<39:54,  2.42s/it] 36%|███▌      | 548/1535 [22:10<39:53,  2.43s/it] 36%|███▌      | 549/1535 [22:12<39:49,  2.42s/it] 36%|███▌      | 550/1535 [22:14<39:46,  2.42s/it] 36%|███▌      | 551/1535 [22:17<39:43,  2.42s/it] 36%|███▌      | 552/1535 [22:19<39:42,  2.42s/it] 36%|███▌      | 553/1535 [22:22<39:39,  2.42s/it] 36%|███▌      | 554/1535 [22:24<39:36,  2.42s/it] 36%|███▌      | 555/1535 [22:27<39:33,  2.42s/it] 36%|███▌      | 556/1535 [22:29<39:38,  2.43s/it] 36%|███▋      | 557/1535 [22:31<39:33,  2.43s/it] 36%|███▋      | 558/1535 [22:34<39:29,  2.42s/it] 36%|███▋      | 559/1535 [22:36<39:25,  2.42s/it] 36%|███▋      | 560/1535 [22:39<39:22,  2.42s/it]                                                  {'loss': 0.4881, 'grad_norm': 0.45041418075561523, 'learning_rate': 0.00045020497640417914, 'epoch': 1.82}
 36%|███▋      | 560/1535 [22:39<39:22,  2.42s/it] 37%|███▋      | 561/1535 [22:41<39:19,  2.42s/it] 37%|███▋      | 562/1535 [22:44<39:16,  2.42s/it] 37%|███▋      | 563/1535 [22:46<39:13,  2.42s/it] 37%|███▋      | 564/1535 [22:48<39:11,  2.42s/it] 37%|███▋      | 565/1535 [22:51<39:09,  2.42s/it] 37%|███▋      | 566/1535 [22:53<39:06,  2.42s/it] 37%|███▋      | 567/1535 [22:56<39:04,  2.42s/it] 37%|███▋      | 568/1535 [22:58<39:01,  2.42s/it] 37%|███▋      | 569/1535 [23:01<38:58,  2.42s/it] 37%|███▋      | 570/1535 [23:03<38:56,  2.42s/it] 37%|███▋      | 571/1535 [23:05<38:54,  2.42s/it] 37%|███▋      | 572/1535 [23:08<38:51,  2.42s/it] 37%|███▋      | 573/1535 [23:10<38:49,  2.42s/it] 37%|███▋      | 574/1535 [23:13<38:46,  2.42s/it] 37%|███▋      | 575/1535 [23:15<38:44,  2.42s/it] 38%|███▊      | 576/1535 [23:17<38:42,  2.42s/it] 38%|███▊      | 577/1535 [23:20<38:39,  2.42s/it] 38%|███▊      | 578/1535 [23:22<38:37,  2.42s/it] 38%|███▊      | 579/1535 [23:25<38:34,  2.42s/it] 38%|███▊      | 580/1535 [23:27<38:33,  2.42s/it]                                                  {'loss': 0.475, 'grad_norm': 0.3754814863204956, 'learning_rate': 0.0004422854145669198, 'epoch': 1.89}
 38%|███▊      | 580/1535 [23:27<38:33,  2.42s/it] 38%|███▊      | 581/1535 [23:30<38:31,  2.42s/it] 38%|███▊      | 582/1535 [23:32<38:28,  2.42s/it] 38%|███▊      | 583/1535 [23:34<38:25,  2.42s/it] 38%|███▊      | 584/1535 [23:37<38:22,  2.42s/it] 38%|███▊      | 585/1535 [23:39<38:26,  2.43s/it] 38%|███▊      | 586/1535 [23:42<38:21,  2.43s/it] 38%|███▊      | 587/1535 [23:44<38:17,  2.42s/it] 38%|███▊      | 588/1535 [23:47<38:14,  2.42s/it] 38%|███▊      | 589/1535 [23:49<38:11,  2.42s/it] 38%|███▊      | 590/1535 [23:51<38:09,  2.42s/it] 39%|███▊      | 591/1535 [23:54<38:06,  2.42s/it] 39%|███▊      | 592/1535 [23:56<38:03,  2.42s/it] 39%|███▊      | 593/1535 [23:59<38:01,  2.42s/it] 39%|███▊      | 594/1535 [24:01<37:58,  2.42s/it] 39%|███▉      | 595/1535 [24:03<37:55,  2.42s/it] 39%|███▉      | 596/1535 [24:06<37:53,  2.42s/it] 39%|███▉      | 597/1535 [24:08<37:50,  2.42s/it] 39%|███▉      | 598/1535 [24:11<37:49,  2.42s/it] 39%|███▉      | 599/1535 [24:13<37:46,  2.42s/it] 39%|███▉      | 600/1535 [24:16<37:45,  2.42s/it]                                                  {'loss': 0.5044, 'grad_norm': 0.40352994203567505, 'learning_rate': 0.00043386256702270773, 'epoch': 1.95}
 39%|███▉      | 600/1535 [24:16<37:45,  2.42s/it] 39%|███▉      | 601/1535 [24:18<37:42,  2.42s/it] 39%|███▉      | 602/1535 [24:20<37:39,  2.42s/it] 39%|███▉      | 603/1535 [24:23<37:37,  2.42s/it] 39%|███▉      | 604/1535 [24:25<37:40,  2.43s/it] 39%|███▉      | 605/1535 [24:28<37:35,  2.43s/it] 39%|███▉      | 606/1535 [24:30<37:32,  2.42s/it] 40%|███▉      | 607/1535 [24:33<37:28,  2.42s/it] 40%|███▉      | 608/1535 [24:35<37:25,  2.42s/it] 40%|███▉      | 609/1535 [24:37<37:22,  2.42s/it] 40%|███▉      | 610/1535 [24:40<37:20,  2.42s/it] 40%|███▉      | 611/1535 [24:42<37:17,  2.42s/it] 40%|███▉      | 612/1535 [24:45<37:22,  2.43s/it] 40%|███▉      | 613/1535 [24:47<37:17,  2.43s/it] 40%|████      | 614/1535 [24:50<37:13,  2.43s/it] 40%|████      | 615/1535 [24:52<37:09,  2.42s/it] 40%|████      | 616/1535 [24:54<37:07,  2.42s/it] 40%|████      | 617/1535 [24:57<37:04,  2.42s/it] 40%|████      | 618/1535 [24:59<37:01,  2.42s/it] 40%|████      | 619/1535 [25:02<36:58,  2.42s/it] 40%|████      | 620/1535 [25:04<36:55,  2.42s/it]                                                  {'loss': 0.4601, 'grad_norm': 0.3429182171821594, 'learning_rate': 0.0004249584796390903, 'epoch': 2.02}
 40%|████      | 620/1535 [25:04<36:55,  2.42s/it] 40%|████      | 621/1535 [25:06<36:53,  2.42s/it] 41%|████      | 622/1535 [25:09<36:50,  2.42s/it] 41%|████      | 623/1535 [25:11<36:48,  2.42s/it] 41%|████      | 624/1535 [25:14<36:45,  2.42s/it] 41%|████      | 625/1535 [25:16<36:43,  2.42s/it] 41%|████      | 626/1535 [25:19<36:47,  2.43s/it] 41%|████      | 627/1535 [25:21<36:43,  2.43s/it] 41%|████      | 628/1535 [25:23<36:39,  2.43s/it] 41%|████      | 629/1535 [25:26<36:36,  2.42s/it] 41%|████      | 630/1535 [25:28<36:32,  2.42s/it] 41%|████      | 631/1535 [25:31<36:31,  2.42s/it] 41%|████      | 632/1535 [25:33<36:29,  2.42s/it] 41%|████      | 633/1535 [25:36<36:25,  2.42s/it] 41%|████▏     | 634/1535 [25:38<36:22,  2.42s/it] 41%|████▏     | 635/1535 [25:40<36:19,  2.42s/it] 41%|████▏     | 636/1535 [25:43<36:16,  2.42s/it] 41%|████▏     | 637/1535 [25:45<36:15,  2.42s/it] 42%|████▏     | 638/1535 [25:48<36:13,  2.42s/it] 42%|████▏     | 639/1535 [25:50<36:10,  2.42s/it] 42%|████▏     | 640/1535 [25:53<36:07,  2.42s/it]                                                  {'loss': 0.4197, 'grad_norm': 0.32660481333732605, 'learning_rate': 0.000415596457875422, 'epoch': 2.08}
 42%|████▏     | 640/1535 [25:53<36:07,  2.42s/it] 42%|████▏     | 641/1535 [25:55<36:05,  2.42s/it] 42%|████▏     | 642/1535 [25:57<36:02,  2.42s/it] 42%|████▏     | 643/1535 [26:00<35:59,  2.42s/it] 42%|████▏     | 644/1535 [26:02<35:57,  2.42s/it] 42%|████▏     | 645/1535 [26:05<35:54,  2.42s/it] 42%|████▏     | 646/1535 [26:07<35:52,  2.42s/it] 42%|████▏     | 647/1535 [26:09<35:49,  2.42s/it] 42%|████▏     | 648/1535 [26:12<35:47,  2.42s/it] 42%|████▏     | 649/1535 [26:14<35:45,  2.42s/it] 42%|████▏     | 650/1535 [26:17<35:42,  2.42s/it] 42%|████▏     | 651/1535 [26:19<35:40,  2.42s/it] 42%|████▏     | 652/1535 [26:22<35:37,  2.42s/it] 43%|████▎     | 653/1535 [26:24<35:35,  2.42s/it] 43%|████▎     | 654/1535 [26:26<35:38,  2.43s/it] 43%|████▎     | 655/1535 [26:29<35:34,  2.43s/it] 43%|████▎     | 656/1535 [26:31<35:30,  2.42s/it] 43%|████▎     | 657/1535 [26:34<35:27,  2.42s/it] 43%|████▎     | 658/1535 [26:36<35:24,  2.42s/it] 43%|████▎     | 659/1535 [26:39<35:21,  2.42s/it] 43%|████▎     | 660/1535 [26:41<35:18,  2.42s/it]                                                  {'loss': 0.447, 'grad_norm': 0.327488511800766, 'learning_rate': 0.00040580100578341385, 'epoch': 2.15}
 43%|████▎     | 660/1535 [26:41<35:18,  2.42s/it] 43%|████▎     | 661/1535 [26:43<35:16,  2.42s/it] 43%|████▎     | 662/1535 [26:46<35:13,  2.42s/it] 43%|████▎     | 663/1535 [26:48<35:10,  2.42s/it] 43%|████▎     | 664/1535 [26:51<35:08,  2.42s/it] 43%|████▎     | 665/1535 [26:53<35:06,  2.42s/it] 43%|████▎     | 666/1535 [26:55<35:04,  2.42s/it] 43%|████▎     | 667/1535 [26:58<35:05,  2.43s/it] 44%|████▎     | 668/1535 [27:00<35:02,  2.43s/it] 44%|████▎     | 669/1535 [27:03<34:59,  2.42s/it] 44%|████▎     | 670/1535 [27:05<34:56,  2.42s/it] 44%|████▎     | 671/1535 [27:08<34:53,  2.42s/it] 44%|████▍     | 672/1535 [27:10<34:50,  2.42s/it] 44%|████▍     | 673/1535 [27:12<34:47,  2.42s/it] 44%|████▍     | 674/1535 [27:15<34:45,  2.42s/it] 44%|████▍     | 675/1535 [27:17<34:48,  2.43s/it] 44%|████▍     | 676/1535 [27:20<34:44,  2.43s/it] 44%|████▍     | 677/1535 [27:22<34:40,  2.42s/it] 44%|████▍     | 678/1535 [27:25<34:37,  2.42s/it] 44%|████▍     | 679/1535 [27:27<34:34,  2.42s/it] 44%|████▍     | 680/1535 [27:29<34:31,  2.42s/it]                                                  {'loss': 0.4034, 'grad_norm': 0.406791627407074, 'learning_rate': 0.000395597761870501, 'epoch': 2.21}
 44%|████▍     | 680/1535 [27:29<34:31,  2.42s/it] 44%|████▍     | 681/1535 [27:32<34:28,  2.42s/it] 44%|████▍     | 682/1535 [27:34<34:39,  2.44s/it] 44%|████▍     | 683/1535 [27:37<34:32,  2.43s/it] 45%|████▍     | 684/1535 [27:39<34:27,  2.43s/it] 45%|████▍     | 685/1535 [27:42<34:22,  2.43s/it] 45%|████▍     | 686/1535 [27:44<34:18,  2.42s/it] 45%|████▍     | 687/1535 [27:46<34:15,  2.42s/it] 45%|████▍     | 688/1535 [27:49<34:12,  2.42s/it] 45%|████▍     | 689/1535 [27:51<34:09,  2.42s/it] 45%|████▍     | 690/1535 [27:54<34:06,  2.42s/it] 45%|████▌     | 691/1535 [27:56<34:10,  2.43s/it] 45%|████▌     | 692/1535 [27:59<34:05,  2.43s/it] 45%|████▌     | 693/1535 [28:01<34:01,  2.42s/it] 45%|████▌     | 694/1535 [28:03<33:58,  2.42s/it] 45%|████▌     | 695/1535 [28:06<34:00,  2.43s/it] 45%|████▌     | 696/1535 [28:08<33:56,  2.43s/it] 45%|████▌     | 697/1535 [28:11<33:52,  2.43s/it] 45%|████▌     | 698/1535 [28:13<33:48,  2.42s/it] 46%|████▌     | 699/1535 [28:16<33:45,  2.42s/it] 46%|████▌     | 700/1535 [28:18<33:42,  2.42s/it]                                                  {'loss': 0.4077, 'grad_norm': 0.3274359405040741, 'learning_rate': 0.0003850134319938983, 'epoch': 2.28}
 46%|████▌     | 700/1535 [28:18<33:42,  2.42s/it] 46%|████▌     | 701/1535 [28:20<33:40,  2.42s/it] 46%|████▌     | 702/1535 [28:23<33:37,  2.42s/it] 46%|████▌     | 703/1535 [28:25<33:34,  2.42s/it] 46%|████▌     | 704/1535 [28:28<33:37,  2.43s/it] 46%|████▌     | 705/1535 [28:30<33:33,  2.43s/it] 46%|████▌     | 706/1535 [28:32<33:29,  2.42s/it] 46%|████▌     | 707/1535 [28:35<33:26,  2.42s/it] 46%|████▌     | 708/1535 [28:37<33:23,  2.42s/it] 46%|████▌     | 709/1535 [28:40<33:20,  2.42s/it] 46%|████▋     | 710/1535 [28:42<33:17,  2.42s/it] 46%|████▋     | 711/1535 [28:45<33:15,  2.42s/it] 46%|████▋     | 712/1535 [28:47<33:12,  2.42s/it] 46%|████▋     | 713/1535 [28:49<33:09,  2.42s/it] 47%|████▋     | 714/1535 [28:52<33:07,  2.42s/it] 47%|████▋     | 715/1535 [28:54<33:04,  2.42s/it] 47%|████▋     | 716/1535 [28:57<33:02,  2.42s/it] 47%|████▋     | 717/1535 [28:59<33:00,  2.42s/it] 47%|████▋     | 718/1535 [29:02<32:57,  2.42s/it] 47%|████▋     | 719/1535 [29:04<32:55,  2.42s/it] 47%|████▋     | 720/1535 [29:06<32:52,  2.42s/it]                                                  {'loss': 0.414, 'grad_norm': 0.4189653694629669, 'learning_rate': 0.0003740757194609865, 'epoch': 2.34}
 47%|████▋     | 720/1535 [29:06<32:52,  2.42s/it] 47%|████▋     | 721/1535 [29:09<32:51,  2.42s/it] 47%|████▋     | 722/1535 [29:11<32:48,  2.42s/it] 47%|████▋     | 723/1535 [29:14<32:45,  2.42s/it] 47%|████▋     | 724/1535 [29:16<32:49,  2.43s/it] 47%|████▋     | 725/1535 [29:19<32:44,  2.43s/it] 47%|████▋     | 726/1535 [29:21<32:41,  2.42s/it] 47%|████▋     | 727/1535 [29:23<32:37,  2.42s/it] 47%|████▋     | 728/1535 [29:26<32:34,  2.42s/it] 47%|████▋     | 729/1535 [29:28<32:32,  2.42s/it] 48%|████▊     | 730/1535 [29:31<32:29,  2.42s/it] 48%|████▊     | 731/1535 [29:33<32:26,  2.42s/it] 48%|████▊     | 732/1535 [29:35<32:23,  2.42s/it] 48%|████▊     | 733/1535 [29:38<32:27,  2.43s/it] 48%|████▊     | 734/1535 [29:40<32:23,  2.43s/it] 48%|████▊     | 735/1535 [29:43<32:19,  2.42s/it] 48%|████▊     | 736/1535 [29:45<32:16,  2.42s/it] 48%|████▊     | 737/1535 [29:48<32:15,  2.43s/it] 48%|████▊     | 738/1535 [29:50<32:12,  2.42s/it] 48%|████▊     | 739/1535 [29:52<32:08,  2.42s/it] 48%|████▊     | 740/1535 [29:55<32:05,  2.42s/it]                                                  {'loss': 0.4094, 'grad_norm': 0.40143823623657227, 'learning_rate': 0.00036281325251898323, 'epoch': 2.41}
 48%|████▊     | 740/1535 [29:55<32:05,  2.42s/it] 48%|████▊     | 741/1535 [29:57<32:03,  2.42s/it] 48%|████▊     | 742/1535 [30:00<32:00,  2.42s/it] 48%|████▊     | 743/1535 [30:02<31:58,  2.42s/it] 48%|████▊     | 744/1535 [30:05<31:55,  2.42s/it] 49%|████▊     | 745/1535 [30:07<31:52,  2.42s/it] 49%|████▊     | 746/1535 [30:09<31:50,  2.42s/it] 49%|████▊     | 747/1535 [30:12<31:47,  2.42s/it] 49%|████▊     | 748/1535 [30:14<31:45,  2.42s/it] 49%|████▉     | 749/1535 [30:17<31:42,  2.42s/it] 49%|████▉     | 750/1535 [30:19<31:40,  2.42s/it] 49%|████▉     | 751/1535 [30:22<31:52,  2.44s/it] 49%|████▉     | 752/1535 [30:24<31:45,  2.43s/it] 49%|████▉     | 753/1535 [30:26<31:40,  2.43s/it] 49%|████▉     | 754/1535 [30:29<31:35,  2.43s/it] 49%|████▉     | 755/1535 [30:31<31:31,  2.43s/it] 49%|████▉     | 756/1535 [30:34<31:28,  2.42s/it] 49%|████▉     | 757/1535 [30:36<31:25,  2.42s/it] 49%|████▉     | 758/1535 [30:38<31:22,  2.42s/it] 49%|████▉     | 759/1535 [30:41<31:19,  2.42s/it] 50%|████▉     | 760/1535 [30:43<31:16,  2.42s/it]                                                  {'loss': 0.4132, 'grad_norm': 0.41986024379730225, 'learning_rate': 0.00035125550942368696, 'epoch': 2.47}
 50%|████▉     | 760/1535 [30:43<31:16,  2.42s/it] 50%|████▉     | 761/1535 [30:46<31:14,  2.42s/it] 50%|████▉     | 762/1535 [30:48<31:11,  2.42s/it] 50%|████▉     | 763/1535 [30:51<31:08,  2.42s/it] 50%|████▉     | 764/1535 [30:53<31:06,  2.42s/it] 50%|████▉     | 765/1535 [30:55<31:06,  2.42s/it] 50%|████▉     | 766/1535 [30:58<31:02,  2.42s/it] 50%|████▉     | 767/1535 [31:00<30:59,  2.42s/it] 50%|█████     | 768/1535 [31:03<30:57,  2.42s/it] 50%|█████     | 769/1535 [31:05<30:54,  2.42s/it] 50%|█████     | 770/1535 [31:08<30:52,  2.42s/it] 50%|█████     | 771/1535 [31:10<30:49,  2.42s/it] 50%|█████     | 772/1535 [31:12<30:47,  2.42s/it] 50%|█████     | 773/1535 [31:15<30:44,  2.42s/it] 50%|█████     | 774/1535 [31:17<30:41,  2.42s/it] 50%|█████     | 775/1535 [31:20<30:39,  2.42s/it] 51%|█████     | 776/1535 [31:22<30:36,  2.42s/it] 51%|█████     | 777/1535 [31:24<30:34,  2.42s/it] 51%|█████     | 778/1535 [31:27<30:32,  2.42s/it] 51%|█████     | 779/1535 [31:29<30:29,  2.42s/it] 51%|█████     | 780/1535 [31:32<30:27,  2.42s/it]                                                  {'loss': 0.3998, 'grad_norm': 0.3652585744857788, 'learning_rate': 0.0003400296989439203, 'epoch': 2.54}
 51%|█████     | 780/1535 [31:32<30:27,  2.42s/it] 51%|█████     | 781/1535 [31:34<30:25,  2.42s/it] 51%|█████     | 782/1535 [31:37<30:22,  2.42s/it] 51%|█████     | 783/1535 [31:39<30:20,  2.42s/it] 51%|█████     | 784/1535 [31:41<30:17,  2.42s/it] 51%|█████     | 785/1535 [31:44<30:15,  2.42s/it] 51%|█████     | 786/1535 [31:46<30:13,  2.42s/it] 51%|█████▏    | 787/1535 [31:49<30:10,  2.42s/it] 51%|█████▏    | 788/1535 [31:51<30:08,  2.42s/it] 51%|█████▏    | 789/1535 [31:54<30:05,  2.42s/it] 51%|█████▏    | 790/1535 [31:56<30:03,  2.42s/it] 52%|█████▏    | 791/1535 [31:58<30:05,  2.43s/it] 52%|█████▏    | 792/1535 [32:01<30:01,  2.43s/it] 52%|█████▏    | 793/1535 [32:03<30:03,  2.43s/it] 52%|█████▏    | 794/1535 [32:06<29:58,  2.43s/it] 52%|█████▏    | 795/1535 [32:08<29:54,  2.42s/it] 52%|█████▏    | 796/1535 [32:11<29:50,  2.42s/it] 52%|█████▏    | 797/1535 [32:13<29:47,  2.42s/it] 52%|█████▏    | 798/1535 [32:15<29:44,  2.42s/it] 52%|█████▏    | 799/1535 [32:18<29:42,  2.42s/it] 52%|█████▏    | 800/1535 [32:20<29:39,  2.42s/it]                                                  {'loss': 0.423, 'grad_norm': 0.4252341091632843, 'learning_rate': 0.00032798380982472774, 'epoch': 2.6}
 52%|█████▏    | 800/1535 [32:20<29:39,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 05:47:20,097 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-800
[INFO|configuration_utils.py:726] 2024-05-25 05:47:21,142 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 05:47:21,145 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 05:47:22,159 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 05:47:22,162 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 05:47:22,212 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 05:47:22,213 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-800/special_tokens_map.json
 52%|█████▏    | 801/1535 [32:25<37:51,  3.09s/it] 52%|█████▏    | 802/1535 [32:27<35:19,  2.89s/it] 52%|█████▏    | 803/1535 [32:30<33:32,  2.75s/it] 52%|█████▏    | 804/1535 [32:32<32:16,  2.65s/it] 52%|█████▏    | 805/1535 [32:35<31:23,  2.58s/it] 53%|█████▎    | 806/1535 [32:37<30:49,  2.54s/it] 53%|█████▎    | 807/1535 [32:39<30:27,  2.51s/it] 53%|█████▎    | 808/1535 [32:42<30:04,  2.48s/it] 53%|█████▎    | 809/1535 [32:44<29:48,  2.46s/it] 53%|█████▎    | 810/1535 [32:47<29:36,  2.45s/it] 53%|█████▎    | 811/1535 [32:49<29:27,  2.44s/it] 53%|█████▎    | 812/1535 [32:52<29:20,  2.43s/it] 53%|█████▎    | 813/1535 [32:54<29:14,  2.43s/it] 53%|█████▎    | 814/1535 [32:56<29:09,  2.43s/it] 53%|█████▎    | 815/1535 [32:59<29:05,  2.42s/it] 53%|█████▎    | 816/1535 [33:01<29:01,  2.42s/it] 53%|█████▎    | 817/1535 [33:04<28:58,  2.42s/it] 53%|█████▎    | 818/1535 [33:06<28:55,  2.42s/it] 53%|█████▎    | 819/1535 [33:08<28:52,  2.42s/it] 53%|█████▎    | 820/1535 [33:11<28:49,  2.42s/it]                                                  {'loss': 0.3975, 'grad_norm': 0.351152628660202, 'learning_rate': 0.0003157338067484422, 'epoch': 2.67}
 53%|█████▎    | 820/1535 [33:11<28:49,  2.42s/it] 53%|█████▎    | 821/1535 [33:13<29:21,  2.47s/it] 54%|█████▎    | 822/1535 [33:16<29:08,  2.45s/it] 54%|█████▎    | 823/1535 [33:18<28:58,  2.44s/it] 54%|█████▎    | 824/1535 [33:21<28:51,  2.44s/it] 54%|█████▎    | 825/1535 [33:23<28:45,  2.43s/it] 54%|█████▍    | 826/1535 [33:26<28:40,  2.43s/it] 54%|█████▍    | 827/1535 [33:28<28:36,  2.42s/it] 54%|█████▍    | 828/1535 [33:30<28:32,  2.42s/it] 54%|█████▍    | 829/1535 [33:33<28:29,  2.42s/it] 54%|█████▍    | 830/1535 [33:35<28:26,  2.42s/it] 54%|█████▍    | 831/1535 [33:38<28:24,  2.42s/it] 54%|█████▍    | 832/1535 [33:40<28:21,  2.42s/it] 54%|█████▍    | 833/1535 [33:42<28:18,  2.42s/it] 54%|█████▍    | 834/1535 [33:45<28:16,  2.42s/it] 54%|█████▍    | 835/1535 [33:47<28:13,  2.42s/it] 54%|█████▍    | 836/1535 [33:50<28:10,  2.42s/it] 55%|█████▍    | 837/1535 [33:52<28:08,  2.42s/it] 55%|█████▍    | 838/1535 [33:55<28:05,  2.42s/it] 55%|█████▍    | 839/1535 [33:57<28:03,  2.42s/it] 55%|█████▍    | 840/1535 [33:59<28:01,  2.42s/it]                                                  {'loss': 0.4011, 'grad_norm': 0.3768521547317505, 'learning_rate': 0.0003033117527369644, 'epoch': 2.73}
 55%|█████▍    | 840/1535 [33:59<28:01,  2.42s/it] 55%|█████▍    | 841/1535 [34:02<27:58,  2.42s/it] 55%|█████▍    | 842/1535 [34:04<27:56,  2.42s/it] 55%|█████▍    | 843/1535 [34:07<27:54,  2.42s/it] 55%|█████▍    | 844/1535 [34:09<27:51,  2.42s/it] 55%|█████▌    | 845/1535 [34:12<27:48,  2.42s/it] 55%|█████▌    | 846/1535 [34:14<27:46,  2.42s/it] 55%|█████▌    | 847/1535 [34:16<27:46,  2.42s/it] 55%|█████▌    | 848/1535 [34:19<27:43,  2.42s/it] 55%|█████▌    | 849/1535 [34:21<27:44,  2.43s/it] 55%|█████▌    | 850/1535 [34:24<27:40,  2.42s/it] 55%|█████▌    | 851/1535 [34:26<27:36,  2.42s/it] 56%|█████▌    | 852/1535 [34:28<27:33,  2.42s/it] 56%|█████▌    | 853/1535 [34:31<27:31,  2.42s/it] 56%|█████▌    | 854/1535 [34:33<27:28,  2.42s/it] 56%|█████▌    | 855/1535 [34:36<27:25,  2.42s/it] 56%|█████▌    | 856/1535 [34:38<27:23,  2.42s/it] 56%|█████▌    | 857/1535 [34:41<27:20,  2.42s/it] 56%|█████▌    | 858/1535 [34:43<27:17,  2.42s/it] 56%|█████▌    | 859/1535 [34:45<27:15,  2.42s/it] 56%|█████▌    | 860/1535 [34:48<27:12,  2.42s/it]                                                  {'loss': 0.3873, 'grad_norm': 0.3639216721057892, 'learning_rate': 0.00029075016113640033, 'epoch': 2.8}
 56%|█████▌    | 860/1535 [34:48<27:12,  2.42s/it] 56%|█████▌    | 861/1535 [34:50<27:10,  2.42s/it] 56%|█████▌    | 862/1535 [34:53<27:07,  2.42s/it] 56%|█████▌    | 863/1535 [34:55<27:10,  2.43s/it] 56%|█████▋    | 864/1535 [34:58<27:06,  2.42s/it] 56%|█████▋    | 865/1535 [35:00<27:02,  2.42s/it] 56%|█████▋    | 866/1535 [35:02<26:59,  2.42s/it] 56%|█████▋    | 867/1535 [35:05<26:56,  2.42s/it] 57%|█████▋    | 868/1535 [35:07<26:53,  2.42s/it] 57%|█████▋    | 869/1535 [35:10<26:51,  2.42s/it] 57%|█████▋    | 870/1535 [35:12<26:48,  2.42s/it] 57%|█████▋    | 871/1535 [35:14<26:45,  2.42s/it] 57%|█████▋    | 872/1535 [35:17<26:43,  2.42s/it] 57%|█████▋    | 873/1535 [35:19<26:41,  2.42s/it] 57%|█████▋    | 874/1535 [35:22<26:38,  2.42s/it] 57%|█████▋    | 875/1535 [35:24<26:36,  2.42s/it] 57%|█████▋    | 876/1535 [35:27<26:36,  2.42s/it] 57%|█████▋    | 877/1535 [35:29<26:32,  2.42s/it] 57%|█████▋    | 878/1535 [35:31<26:29,  2.42s/it] 57%|█████▋    | 879/1535 [35:34<26:27,  2.42s/it] 57%|█████▋    | 880/1535 [35:36<26:24,  2.42s/it]                                                  {'loss': 0.4273, 'grad_norm': 0.4193864166736603, 'learning_rate': 0.000278081910516991, 'epoch': 2.86}
 57%|█████▋    | 880/1535 [35:36<26:24,  2.42s/it] 57%|█████▋    | 881/1535 [35:39<26:22,  2.42s/it] 57%|█████▋    | 882/1535 [35:41<26:19,  2.42s/it] 58%|█████▊    | 883/1535 [35:43<26:17,  2.42s/it] 58%|█████▊    | 884/1535 [35:46<26:14,  2.42s/it] 58%|█████▊    | 885/1535 [35:48<26:12,  2.42s/it] 58%|█████▊    | 886/1535 [35:51<26:09,  2.42s/it] 58%|█████▊    | 887/1535 [35:53<26:06,  2.42s/it] 58%|█████▊    | 888/1535 [35:56<26:04,  2.42s/it] 58%|█████▊    | 889/1535 [35:58<26:01,  2.42s/it] 58%|█████▊    | 890/1535 [36:00<26:12,  2.44s/it] 58%|█████▊    | 891/1535 [36:03<26:06,  2.43s/it] 58%|█████▊    | 892/1535 [36:05<26:01,  2.43s/it] 58%|█████▊    | 893/1535 [36:08<26:01,  2.43s/it] 58%|█████▊    | 894/1535 [36:10<25:56,  2.43s/it] 58%|█████▊    | 895/1535 [36:13<25:52,  2.43s/it] 58%|█████▊    | 896/1535 [36:15<25:48,  2.42s/it] 58%|█████▊    | 897/1535 [36:17<25:45,  2.42s/it] 59%|█████▊    | 898/1535 [36:20<25:42,  2.42s/it] 59%|█████▊    | 899/1535 [36:22<25:39,  2.42s/it] 59%|█████▊    | 900/1535 [36:25<25:36,  2.42s/it]                                                  {'loss': 0.3892, 'grad_norm': 0.3689459562301636, 'learning_rate': 0.0002653401586171077, 'epoch': 2.93}
 59%|█████▊    | 900/1535 [36:25<25:36,  2.42s/it] 59%|█████▊    | 901/1535 [36:27<25:34,  2.42s/it] 59%|█████▉    | 902/1535 [36:30<25:32,  2.42s/it] 59%|█████▉    | 903/1535 [36:32<25:29,  2.42s/it] 59%|█████▉    | 904/1535 [36:34<25:28,  2.42s/it] 59%|█████▉    | 905/1535 [36:37<25:25,  2.42s/it] 59%|█████▉    | 906/1535 [36:39<25:23,  2.42s/it] 59%|█████▉    | 907/1535 [36:42<25:20,  2.42s/it] 59%|█████▉    | 908/1535 [36:44<25:17,  2.42s/it] 59%|█████▉    | 909/1535 [36:46<25:15,  2.42s/it] 59%|█████▉    | 910/1535 [36:49<25:12,  2.42s/it] 59%|█████▉    | 911/1535 [36:51<25:10,  2.42s/it] 59%|█████▉    | 912/1535 [36:54<25:07,  2.42s/it] 59%|█████▉    | 913/1535 [36:56<25:05,  2.42s/it] 60%|█████▉    | 914/1535 [36:59<25:03,  2.42s/it] 60%|█████▉    | 915/1535 [37:01<25:00,  2.42s/it] 60%|█████▉    | 916/1535 [37:03<24:58,  2.42s/it] 60%|█████▉    | 917/1535 [37:06<24:59,  2.43s/it] 60%|█████▉    | 918/1535 [37:08<24:55,  2.42s/it] 60%|█████▉    | 919/1535 [37:11<24:52,  2.42s/it] 60%|█████▉    | 920/1535 [37:13<24:49,  2.42s/it]                                                  {'loss': 0.3917, 'grad_norm': 0.32466310262680054, 'learning_rate': 0.00025255825555655644, 'epoch': 2.99}
 60%|█████▉    | 920/1535 [37:13<24:49,  2.42s/it] 60%|██████    | 921/1535 [37:16<24:47,  2.42s/it] 60%|██████    | 922/1535 [37:18<24:44,  2.42s/it] 60%|██████    | 923/1535 [37:20<24:41,  2.42s/it] 60%|██████    | 924/1535 [37:23<24:39,  2.42s/it] 60%|██████    | 925/1535 [37:25<24:36,  2.42s/it] 60%|██████    | 926/1535 [37:28<24:34,  2.42s/it] 60%|██████    | 927/1535 [37:30<24:32,  2.42s/it] 60%|██████    | 928/1535 [37:32<24:29,  2.42s/it] 61%|██████    | 929/1535 [37:35<24:27,  2.42s/it] 61%|██████    | 930/1535 [37:37<24:24,  2.42s/it] 61%|██████    | 931/1535 [37:40<24:22,  2.42s/it] 61%|██████    | 932/1535 [37:42<24:23,  2.43s/it] 61%|██████    | 933/1535 [37:45<24:19,  2.42s/it] 61%|██████    | 934/1535 [37:47<24:16,  2.42s/it] 61%|██████    | 935/1535 [37:49<24:13,  2.42s/it] 61%|██████    | 936/1535 [37:52<24:11,  2.42s/it] 61%|██████    | 937/1535 [37:54<24:08,  2.42s/it] 61%|██████    | 938/1535 [37:57<24:05,  2.42s/it] 61%|██████    | 939/1535 [37:59<24:03,  2.42s/it] 61%|██████    | 940/1535 [38:02<24:00,  2.42s/it]                                                  {'loss': 0.3803, 'grad_norm': 0.449515700340271, 'learning_rate': 0.0002397696565463449, 'epoch': 3.06}
 61%|██████    | 940/1535 [38:02<24:00,  2.42s/it] 61%|██████▏   | 941/1535 [38:04<23:58,  2.42s/it] 61%|██████▏   | 942/1535 [38:06<23:55,  2.42s/it] 61%|██████▏   | 943/1535 [38:09<23:52,  2.42s/it] 61%|██████▏   | 944/1535 [38:11<23:50,  2.42s/it] 62%|██████▏   | 945/1535 [38:14<23:48,  2.42s/it] 62%|██████▏   | 946/1535 [38:16<23:46,  2.42s/it] 62%|██████▏   | 947/1535 [38:19<23:43,  2.42s/it] 62%|██████▏   | 948/1535 [38:21<23:40,  2.42s/it] 62%|██████▏   | 949/1535 [38:23<23:38,  2.42s/it] 62%|██████▏   | 950/1535 [38:26<23:35,  2.42s/it] 62%|██████▏   | 951/1535 [38:28<23:33,  2.42s/it] 62%|██████▏   | 952/1535 [38:31<23:31,  2.42s/it] 62%|██████▏   | 953/1535 [38:33<23:28,  2.42s/it] 62%|██████▏   | 954/1535 [38:35<23:26,  2.42s/it] 62%|██████▏   | 955/1535 [38:38<23:23,  2.42s/it] 62%|██████▏   | 956/1535 [38:40<23:21,  2.42s/it] 62%|██████▏   | 957/1535 [38:43<23:19,  2.42s/it] 62%|██████▏   | 958/1535 [38:45<23:16,  2.42s/it] 62%|██████▏   | 959/1535 [38:48<23:14,  2.42s/it] 63%|██████▎   | 960/1535 [38:50<23:23,  2.44s/it]                                                  {'loss': 0.3371, 'grad_norm': 0.3396831154823303, 'learning_rate': 0.00022764477333910645, 'epoch': 3.12}
 63%|██████▎   | 960/1535 [38:50<23:23,  2.44s/it] 63%|██████▎   | 961/1535 [38:52<23:18,  2.44s/it] 63%|██████▎   | 962/1535 [38:55<23:13,  2.43s/it] 63%|██████▎   | 963/1535 [38:57<23:08,  2.43s/it] 63%|██████▎   | 964/1535 [39:00<23:05,  2.43s/it] 63%|██████▎   | 965/1535 [39:02<23:06,  2.43s/it] 63%|██████▎   | 966/1535 [39:05<23:01,  2.43s/it] 63%|██████▎   | 967/1535 [39:07<22:58,  2.43s/it] 63%|██████▎   | 968/1535 [39:09<22:54,  2.42s/it] 63%|██████▎   | 969/1535 [39:12<22:51,  2.42s/it] 63%|██████▎   | 970/1535 [39:14<22:48,  2.42s/it] 63%|██████▎   | 971/1535 [39:17<22:46,  2.42s/it] 63%|██████▎   | 972/1535 [39:19<22:43,  2.42s/it] 63%|██████▎   | 973/1535 [39:22<22:40,  2.42s/it] 63%|██████▎   | 974/1535 [39:24<22:38,  2.42s/it] 64%|██████▎   | 975/1535 [39:26<22:35,  2.42s/it] 64%|██████▎   | 976/1535 [39:29<22:33,  2.42s/it] 64%|██████▎   | 977/1535 [39:31<22:30,  2.42s/it] 64%|██████▎   | 978/1535 [39:34<22:28,  2.42s/it] 64%|██████▍   | 979/1535 [39:36<22:25,  2.42s/it] 64%|██████▍   | 980/1535 [39:38<22:23,  2.42s/it]                                                  {'loss': 0.3225, 'grad_norm': 0.3892490863800049, 'learning_rate': 0.0002149393303482419, 'epoch': 3.19}
 64%|██████▍   | 980/1535 [39:38<22:23,  2.42s/it] 64%|██████▍   | 981/1535 [39:41<22:21,  2.42s/it] 64%|██████▍   | 982/1535 [39:43<22:18,  2.42s/it] 64%|██████▍   | 983/1535 [39:46<22:16,  2.42s/it] 64%|██████▍   | 984/1535 [39:48<22:13,  2.42s/it] 64%|██████▍   | 985/1535 [39:51<22:11,  2.42s/it] 64%|██████▍   | 986/1535 [39:53<22:09,  2.42s/it] 64%|██████▍   | 987/1535 [39:55<22:07,  2.42s/it] 64%|██████▍   | 988/1535 [39:58<22:04,  2.42s/it] 64%|██████▍   | 989/1535 [40:00<22:02,  2.42s/it] 64%|██████▍   | 990/1535 [40:03<21:59,  2.42s/it] 65%|██████▍   | 991/1535 [40:05<21:57,  2.42s/it] 65%|██████▍   | 992/1535 [40:08<21:54,  2.42s/it] 65%|██████▍   | 993/1535 [40:10<21:52,  2.42s/it] 65%|██████▍   | 994/1535 [40:12<21:49,  2.42s/it] 65%|██████▍   | 995/1535 [40:15<21:47,  2.42s/it] 65%|██████▍   | 996/1535 [40:17<21:44,  2.42s/it] 65%|██████▍   | 997/1535 [40:20<21:42,  2.42s/it] 65%|██████▌   | 998/1535 [40:22<21:39,  2.42s/it] 65%|██████▌   | 999/1535 [40:24<21:37,  2.42s/it] 65%|██████▌   | 1000/1535 [40:27<21:34,  2.42s/it]                                                   {'loss': 0.3385, 'grad_norm': 0.37954235076904297, 'learning_rate': 0.00020232565476444598, 'epoch': 3.25}
 65%|██████▌   | 1000/1535 [40:27<21:34,  2.42s/it] 65%|██████▌   | 1001/1535 [40:29<21:32,  2.42s/it] 65%|██████▌   | 1002/1535 [40:32<21:36,  2.43s/it] 65%|██████▌   | 1003/1535 [40:34<21:32,  2.43s/it] 65%|██████▌   | 1004/1535 [40:37<21:28,  2.43s/it] 65%|██████▌   | 1005/1535 [40:39<21:25,  2.42s/it] 66%|██████▌   | 1006/1535 [40:41<21:21,  2.42s/it] 66%|██████▌   | 1007/1535 [40:44<21:19,  2.42s/it] 66%|██████▌   | 1008/1535 [40:46<21:16,  2.42s/it] 66%|██████▌   | 1009/1535 [40:49<21:14,  2.42s/it] 66%|██████▌   | 1010/1535 [40:51<21:11,  2.42s/it] 66%|██████▌   | 1011/1535 [40:54<21:08,  2.42s/it] 66%|██████▌   | 1012/1535 [40:56<21:06,  2.42s/it] 66%|██████▌   | 1013/1535 [40:58<21:03,  2.42s/it] 66%|██████▌   | 1014/1535 [41:01<21:01,  2.42s/it] 66%|██████▌   | 1015/1535 [41:03<20:58,  2.42s/it] 66%|██████▌   | 1016/1535 [41:06<20:56,  2.42s/it] 66%|██████▋   | 1017/1535 [41:08<20:54,  2.42s/it] 66%|██████▋   | 1018/1535 [41:11<20:51,  2.42s/it] 66%|██████▋   | 1019/1535 [41:13<20:49,  2.42s/it] 66%|██████▋   | 1020/1535 [41:15<20:46,  2.42s/it]                                                   {'loss': 0.3733, 'grad_norm': 0.3266822099685669, 'learning_rate': 0.00018983676148199847, 'epoch': 3.32}
 66%|██████▋   | 1020/1535 [41:15<20:46,  2.42s/it] 67%|██████▋   | 1021/1535 [41:18<20:44,  2.42s/it] 67%|██████▋   | 1022/1535 [41:20<20:41,  2.42s/it] 67%|██████▋   | 1023/1535 [41:23<20:40,  2.42s/it] 67%|██████▋   | 1024/1535 [41:25<20:37,  2.42s/it] 67%|██████▋   | 1025/1535 [41:27<20:34,  2.42s/it] 67%|██████▋   | 1026/1535 [41:30<20:32,  2.42s/it] 67%|██████▋   | 1027/1535 [41:32<20:29,  2.42s/it] 67%|██████▋   | 1028/1535 [41:35<20:27,  2.42s/it] 67%|██████▋   | 1029/1535 [41:37<20:37,  2.45s/it] 67%|██████▋   | 1030/1535 [41:40<20:31,  2.44s/it] 67%|██████▋   | 1031/1535 [41:42<20:26,  2.43s/it] 67%|██████▋   | 1032/1535 [41:44<20:22,  2.43s/it] 67%|██████▋   | 1033/1535 [41:47<20:17,  2.43s/it] 67%|██████▋   | 1034/1535 [41:49<20:14,  2.42s/it] 67%|██████▋   | 1035/1535 [41:52<20:11,  2.42s/it] 67%|██████▋   | 1036/1535 [41:54<20:08,  2.42s/it] 68%|██████▊   | 1037/1535 [41:57<20:05,  2.42s/it] 68%|██████▊   | 1038/1535 [41:59<20:03,  2.42s/it] 68%|██████▊   | 1039/1535 [42:01<20:01,  2.42s/it] 68%|██████▊   | 1040/1535 [42:04<19:58,  2.42s/it]                                                   {'loss': 0.345, 'grad_norm': 0.2849625051021576, 'learning_rate': 0.0001781176698634246, 'epoch': 3.38}
 68%|██████▊   | 1040/1535 [42:04<19:58,  2.42s/it] 68%|██████▊   | 1041/1535 [42:06<19:56,  2.42s/it] 68%|██████▊   | 1042/1535 [42:09<19:53,  2.42s/it] 68%|██████▊   | 1043/1535 [42:11<19:51,  2.42s/it] 68%|██████▊   | 1044/1535 [42:14<19:48,  2.42s/it] 68%|██████▊   | 1045/1535 [42:16<19:46,  2.42s/it] 68%|██████▊   | 1046/1535 [42:18<19:43,  2.42s/it] 68%|██████▊   | 1047/1535 [42:21<19:41,  2.42s/it] 68%|██████▊   | 1048/1535 [42:23<19:38,  2.42s/it] 68%|██████▊   | 1049/1535 [42:26<19:36,  2.42s/it] 68%|██████▊   | 1050/1535 [42:28<19:33,  2.42s/it] 68%|██████▊   | 1051/1535 [42:30<19:31,  2.42s/it] 69%|██████▊   | 1052/1535 [42:33<19:28,  2.42s/it] 69%|██████▊   | 1053/1535 [42:35<19:26,  2.42s/it] 69%|██████▊   | 1054/1535 [42:38<19:24,  2.42s/it] 69%|██████▊   | 1055/1535 [42:40<19:21,  2.42s/it] 69%|██████▉   | 1056/1535 [42:43<19:21,  2.42s/it] 69%|██████▉   | 1057/1535 [42:45<19:18,  2.42s/it] 69%|██████▉   | 1058/1535 [42:47<19:15,  2.42s/it] 69%|██████▉   | 1059/1535 [42:50<19:12,  2.42s/it] 69%|██████▉   | 1060/1535 [42:52<19:10,  2.42s/it]                                                   {'loss': 0.346, 'grad_norm': 0.332626074552536, 'learning_rate': 0.00016596574732671287, 'epoch': 3.45}
 69%|██████▉   | 1060/1535 [42:52<19:10,  2.42s/it] 69%|██████▉   | 1061/1535 [42:55<19:07,  2.42s/it] 69%|██████▉   | 1062/1535 [42:57<19:05,  2.42s/it] 69%|██████▉   | 1063/1535 [43:00<19:02,  2.42s/it] 69%|██████▉   | 1064/1535 [43:02<19:00,  2.42s/it] 69%|██████▉   | 1065/1535 [43:04<18:57,  2.42s/it] 69%|██████▉   | 1066/1535 [43:07<18:55,  2.42s/it] 70%|██████▉   | 1067/1535 [43:09<18:52,  2.42s/it] 70%|██████▉   | 1068/1535 [43:12<18:50,  2.42s/it] 70%|██████▉   | 1069/1535 [43:14<18:47,  2.42s/it] 70%|██████▉   | 1070/1535 [43:16<18:45,  2.42s/it] 70%|██████▉   | 1071/1535 [43:19<18:44,  2.42s/it] 70%|██████▉   | 1072/1535 [43:21<18:41,  2.42s/it] 70%|██████▉   | 1073/1535 [43:24<18:38,  2.42s/it] 70%|██████▉   | 1074/1535 [43:26<18:36,  2.42s/it] 70%|███████   | 1075/1535 [43:29<18:33,  2.42s/it] 70%|███████   | 1076/1535 [43:31<18:30,  2.42s/it] 70%|███████   | 1077/1535 [43:33<18:28,  2.42s/it] 70%|███████   | 1078/1535 [43:36<18:26,  2.42s/it] 70%|███████   | 1079/1535 [43:38<18:23,  2.42s/it] 70%|███████   | 1080/1535 [43:41<18:21,  2.42s/it]                                                   {'loss': 0.3323, 'grad_norm': 0.32714009284973145, 'learning_rate': 0.00015403377510895899, 'epoch': 3.51}
 70%|███████   | 1080/1535 [43:41<18:21,  2.42s/it] 70%|███████   | 1081/1535 [43:43<18:20,  2.42s/it] 70%|███████   | 1082/1535 [43:46<18:17,  2.42s/it] 71%|███████   | 1083/1535 [43:48<18:14,  2.42s/it] 71%|███████   | 1084/1535 [43:50<18:12,  2.42s/it] 71%|███████   | 1085/1535 [43:53<18:09,  2.42s/it] 71%|███████   | 1086/1535 [43:55<18:07,  2.42s/it] 71%|███████   | 1087/1535 [43:58<18:04,  2.42s/it] 71%|███████   | 1088/1535 [44:00<18:02,  2.42s/it] 71%|███████   | 1089/1535 [44:02<17:59,  2.42s/it] 71%|███████   | 1090/1535 [44:05<17:57,  2.42s/it] 71%|███████   | 1091/1535 [44:07<17:54,  2.42s/it] 71%|███████   | 1092/1535 [44:10<17:52,  2.42s/it] 71%|███████   | 1093/1535 [44:12<17:49,  2.42s/it] 71%|███████▏  | 1094/1535 [44:15<17:47,  2.42s/it] 71%|███████▏  | 1095/1535 [44:17<17:45,  2.42s/it] 71%|███████▏  | 1096/1535 [44:19<17:42,  2.42s/it] 71%|███████▏  | 1097/1535 [44:22<17:40,  2.42s/it] 72%|███████▏  | 1098/1535 [44:24<17:37,  2.42s/it] 72%|███████▏  | 1099/1535 [44:27<17:44,  2.44s/it] 72%|███████▏  | 1100/1535 [44:29<17:39,  2.44s/it]                                                   {'loss': 0.3324, 'grad_norm': 0.33847305178642273, 'learning_rate': 0.00014235298382162898, 'epoch': 3.58}
 72%|███████▏  | 1100/1535 [44:29<17:39,  2.44s/it] 72%|███████▏  | 1101/1535 [44:32<17:35,  2.43s/it] 72%|███████▏  | 1102/1535 [44:34<17:31,  2.43s/it] 72%|███████▏  | 1103/1535 [44:36<17:28,  2.43s/it] 72%|███████▏  | 1104/1535 [44:39<17:25,  2.42s/it] 72%|███████▏  | 1105/1535 [44:41<17:22,  2.42s/it] 72%|███████▏  | 1106/1535 [44:44<17:19,  2.42s/it] 72%|███████▏  | 1107/1535 [44:46<17:16,  2.42s/it] 72%|███████▏  | 1108/1535 [44:49<17:13,  2.42s/it] 72%|███████▏  | 1109/1535 [44:51<17:11,  2.42s/it] 72%|███████▏  | 1110/1535 [44:53<17:09,  2.42s/it] 72%|███████▏  | 1111/1535 [44:56<17:06,  2.42s/it] 72%|███████▏  | 1112/1535 [44:58<17:06,  2.43s/it] 73%|███████▎  | 1113/1535 [45:01<17:03,  2.42s/it] 73%|███████▎  | 1114/1535 [45:03<17:00,  2.42s/it] 73%|███████▎  | 1115/1535 [45:06<16:57,  2.42s/it] 73%|███████▎  | 1116/1535 [45:08<16:54,  2.42s/it] 73%|███████▎  | 1117/1535 [45:10<16:51,  2.42s/it] 73%|███████▎  | 1118/1535 [45:13<16:49,  2.42s/it] 73%|███████▎  | 1119/1535 [45:15<16:46,  2.42s/it] 73%|███████▎  | 1120/1535 [45:18<16:44,  2.42s/it]                                                   {'loss': 0.3476, 'grad_norm': 0.35946354269981384, 'learning_rate': 0.00013095394663801348, 'epoch': 3.64}
 73%|███████▎  | 1120/1535 [45:18<16:44,  2.42s/it] 73%|███████▎  | 1121/1535 [45:20<16:42,  2.42s/it] 73%|███████▎  | 1122/1535 [45:22<16:39,  2.42s/it] 73%|███████▎  | 1123/1535 [45:25<16:37,  2.42s/it] 73%|███████▎  | 1124/1535 [45:27<16:34,  2.42s/it] 73%|███████▎  | 1125/1535 [45:30<16:36,  2.43s/it] 73%|███████▎  | 1126/1535 [45:32<16:32,  2.43s/it] 73%|███████▎  | 1127/1535 [45:35<16:29,  2.42s/it] 73%|███████▎  | 1128/1535 [45:37<16:26,  2.42s/it] 74%|███████▎  | 1129/1535 [45:39<16:23,  2.42s/it] 74%|███████▎  | 1130/1535 [45:42<16:20,  2.42s/it] 74%|███████▎  | 1131/1535 [45:44<16:18,  2.42s/it] 74%|███████▎  | 1132/1535 [45:47<16:15,  2.42s/it] 74%|███████▍  | 1133/1535 [45:49<16:13,  2.42s/it] 74%|███████▍  | 1134/1535 [45:52<16:10,  2.42s/it] 74%|███████▍  | 1135/1535 [45:54<16:08,  2.42s/it] 74%|███████▍  | 1136/1535 [45:56<16:05,  2.42s/it] 74%|███████▍  | 1137/1535 [45:59<16:03,  2.42s/it] 74%|███████▍  | 1138/1535 [46:01<16:01,  2.42s/it] 74%|███████▍  | 1139/1535 [46:04<16:01,  2.43s/it] 74%|███████▍  | 1140/1535 [46:06<15:58,  2.43s/it]                                                   {'loss': 0.3397, 'grad_norm': 0.2762756049633026, 'learning_rate': 0.00011986649927134372, 'epoch': 3.71}
 74%|███████▍  | 1140/1535 [46:06<15:58,  2.43s/it] 74%|███████▍  | 1141/1535 [46:09<15:56,  2.43s/it] 74%|███████▍  | 1142/1535 [46:11<15:53,  2.43s/it] 74%|███████▍  | 1143/1535 [46:13<15:50,  2.42s/it] 75%|███████▍  | 1144/1535 [46:16<15:47,  2.42s/it] 75%|███████▍  | 1145/1535 [46:18<15:44,  2.42s/it] 75%|███████▍  | 1146/1535 [46:21<15:41,  2.42s/it] 75%|███████▍  | 1147/1535 [46:23<15:39,  2.42s/it] 75%|███████▍  | 1148/1535 [46:25<15:37,  2.42s/it] 75%|███████▍  | 1149/1535 [46:28<15:34,  2.42s/it] 75%|███████▍  | 1150/1535 [46:30<15:32,  2.42s/it] 75%|███████▍  | 1151/1535 [46:33<15:29,  2.42s/it] 75%|███████▌  | 1152/1535 [46:35<15:27,  2.42s/it] 75%|███████▌  | 1153/1535 [46:38<15:24,  2.42s/it] 75%|███████▌  | 1154/1535 [46:40<15:27,  2.43s/it] 75%|███████▌  | 1155/1535 [46:42<15:23,  2.43s/it] 75%|███████▌  | 1156/1535 [46:45<15:19,  2.43s/it] 75%|███████▌  | 1157/1535 [46:47<15:16,  2.43s/it] 75%|███████▌  | 1158/1535 [46:50<15:13,  2.42s/it] 76%|███████▌  | 1159/1535 [46:52<15:11,  2.42s/it] 76%|███████▌  | 1160/1535 [46:55<15:08,  2.42s/it]                                                   {'loss': 0.3076, 'grad_norm': 0.2934170663356781, 'learning_rate': 0.00010911966188312681, 'epoch': 3.77}
 76%|███████▌  | 1160/1535 [46:55<15:08,  2.42s/it] 76%|███████▌  | 1161/1535 [46:57<15:06,  2.42s/it] 76%|███████▌  | 1162/1535 [46:59<15:03,  2.42s/it] 76%|███████▌  | 1163/1535 [47:02<15:00,  2.42s/it] 76%|███████▌  | 1164/1535 [47:04<14:58,  2.42s/it] 76%|███████▌  | 1165/1535 [47:07<14:55,  2.42s/it] 76%|███████▌  | 1166/1535 [47:09<14:53,  2.42s/it] 76%|███████▌  | 1167/1535 [47:11<14:50,  2.42s/it] 76%|███████▌  | 1168/1535 [47:14<14:55,  2.44s/it] 76%|███████▌  | 1169/1535 [47:16<14:51,  2.43s/it] 76%|███████▌  | 1170/1535 [47:19<14:47,  2.43s/it] 76%|███████▋  | 1171/1535 [47:21<14:43,  2.43s/it] 76%|███████▋  | 1172/1535 [47:24<14:40,  2.43s/it] 76%|███████▋  | 1173/1535 [47:26<14:37,  2.42s/it] 76%|███████▋  | 1174/1535 [47:28<14:34,  2.42s/it] 77%|███████▋  | 1175/1535 [47:31<14:31,  2.42s/it] 77%|███████▋  | 1176/1535 [47:33<14:29,  2.42s/it] 77%|███████▋  | 1177/1535 [47:36<14:26,  2.42s/it] 77%|███████▋  | 1178/1535 [47:38<14:24,  2.42s/it] 77%|███████▋  | 1179/1535 [47:41<14:21,  2.42s/it] 77%|███████▋  | 1180/1535 [47:43<14:19,  2.42s/it]                                                   {'loss': 0.342, 'grad_norm': 0.3451685607433319, 'learning_rate': 9.874156312609836e-05, 'epoch': 3.84}
 77%|███████▋  | 1180/1535 [47:43<14:19,  2.42s/it] 77%|███████▋  | 1181/1535 [47:45<14:16,  2.42s/it] 77%|███████▋  | 1182/1535 [47:48<14:14,  2.42s/it] 77%|███████▋  | 1183/1535 [47:50<14:12,  2.42s/it] 77%|███████▋  | 1184/1535 [47:53<14:09,  2.42s/it] 77%|███████▋  | 1185/1535 [47:55<14:07,  2.42s/it] 77%|███████▋  | 1186/1535 [47:58<14:04,  2.42s/it] 77%|███████▋  | 1187/1535 [48:00<14:02,  2.42s/it] 77%|███████▋  | 1188/1535 [48:02<13:59,  2.42s/it] 77%|███████▋  | 1189/1535 [48:05<13:57,  2.42s/it] 78%|███████▊  | 1190/1535 [48:07<13:55,  2.42s/it] 78%|███████▊  | 1191/1535 [48:10<13:52,  2.42s/it] 78%|███████▊  | 1192/1535 [48:12<13:50,  2.42s/it] 78%|███████▊  | 1193/1535 [48:14<13:47,  2.42s/it] 78%|███████▊  | 1194/1535 [48:17<13:45,  2.42s/it] 78%|███████▊  | 1195/1535 [48:19<13:44,  2.42s/it] 78%|███████▊  | 1196/1535 [48:22<13:41,  2.42s/it] 78%|███████▊  | 1197/1535 [48:24<13:41,  2.43s/it] 78%|███████▊  | 1198/1535 [48:27<13:37,  2.43s/it] 78%|███████▊  | 1199/1535 [48:29<13:34,  2.42s/it] 78%|███████▊  | 1200/1535 [48:31<13:31,  2.42s/it]                                                   {'loss': 0.3357, 'grad_norm': 0.3438768684864044, 'learning_rate': 8.875936652059871e-05, 'epoch': 3.9}
 78%|███████▊  | 1200/1535 [48:31<13:31,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 06:03:31,354 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-1200
[INFO|configuration_utils.py:726] 2024-05-25 06:03:32,737 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 06:03:32,739 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 06:03:33,285 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 06:03:33,287 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 06:03:33,338 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 06:03:33,339 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/checkpoint-1200/special_tokens_map.json
 78%|███████▊  | 1201/1535 [48:36<17:03,  3.07s/it] 78%|███████▊  | 1202/1535 [48:38<15:55,  2.87s/it] 78%|███████▊  | 1203/1535 [48:41<15:08,  2.74s/it] 78%|███████▊  | 1204/1535 [48:43<14:33,  2.64s/it] 79%|███████▊  | 1205/1535 [48:46<14:09,  2.57s/it] 79%|███████▊  | 1206/1535 [48:48<13:51,  2.53s/it] 79%|███████▊  | 1207/1535 [48:51<13:38,  2.49s/it] 79%|███████▊  | 1208/1535 [48:53<13:28,  2.47s/it] 79%|███████▉  | 1209/1535 [48:55<13:20,  2.46s/it] 79%|███████▉  | 1210/1535 [48:58<13:15,  2.45s/it] 79%|███████▉  | 1211/1535 [49:00<13:10,  2.44s/it] 79%|███████▉  | 1212/1535 [49:03<13:10,  2.45s/it] 79%|███████▉  | 1213/1535 [49:05<13:05,  2.44s/it] 79%|███████▉  | 1214/1535 [49:08<13:00,  2.43s/it] 79%|███████▉  | 1215/1535 [49:10<12:56,  2.43s/it] 79%|███████▉  | 1216/1535 [49:12<12:53,  2.42s/it] 79%|███████▉  | 1217/1535 [49:15<12:50,  2.42s/it] 79%|███████▉  | 1218/1535 [49:17<12:47,  2.42s/it] 79%|███████▉  | 1219/1535 [49:20<12:44,  2.42s/it] 79%|███████▉  | 1220/1535 [49:22<12:42,  2.42s/it]                                                   {'loss': 0.3325, 'grad_norm': 0.3620160222053528, 'learning_rate': 7.9199199357077e-05, 'epoch': 3.97}
 79%|███████▉  | 1220/1535 [49:22<12:42,  2.42s/it] 80%|███████▉  | 1221/1535 [49:24<12:39,  2.42s/it] 80%|███████▉  | 1222/1535 [49:27<12:37,  2.42s/it] 80%|███████▉  | 1223/1535 [49:29<12:34,  2.42s/it] 80%|███████▉  | 1224/1535 [49:32<12:32,  2.42s/it] 80%|███████▉  | 1225/1535 [49:34<12:29,  2.42s/it] 80%|███████▉  | 1226/1535 [49:37<12:29,  2.43s/it] 80%|███████▉  | 1227/1535 [49:39<12:26,  2.42s/it] 80%|████████  | 1228/1535 [49:41<12:23,  2.42s/it] 80%|████████  | 1229/1535 [49:44<12:20,  2.42s/it] 80%|████████  | 1230/1535 [49:46<12:17,  2.42s/it] 80%|████████  | 1231/1535 [49:49<12:15,  2.42s/it] 80%|████████  | 1232/1535 [49:51<12:13,  2.42s/it] 80%|████████  | 1233/1535 [49:53<12:10,  2.42s/it] 80%|████████  | 1234/1535 [49:56<12:07,  2.42s/it] 80%|████████  | 1235/1535 [49:58<12:05,  2.42s/it] 81%|████████  | 1236/1535 [50:01<12:04,  2.42s/it] 81%|████████  | 1237/1535 [50:03<12:01,  2.42s/it] 81%|████████  | 1238/1535 [50:06<12:04,  2.44s/it] 81%|████████  | 1239/1535 [50:08<12:00,  2.43s/it] 81%|████████  | 1240/1535 [50:11<11:56,  2.43s/it]                                                   {'loss': 0.3303, 'grad_norm': 0.3192557692527771, 'learning_rate': 7.008608431081178e-05, 'epoch': 4.03}
 81%|████████  | 1240/1535 [50:11<11:56,  2.43s/it] 81%|████████  | 1241/1535 [50:13<11:55,  2.43s/it] 81%|████████  | 1242/1535 [50:15<11:51,  2.43s/it] 81%|████████  | 1243/1535 [50:18<11:48,  2.43s/it] 81%|████████  | 1244/1535 [50:20<11:45,  2.42s/it] 81%|████████  | 1245/1535 [50:23<11:42,  2.42s/it] 81%|████████  | 1246/1535 [50:25<11:39,  2.42s/it] 81%|████████  | 1247/1535 [50:27<11:37,  2.42s/it] 81%|████████▏ | 1248/1535 [50:30<11:34,  2.42s/it] 81%|████████▏ | 1249/1535 [50:32<11:31,  2.42s/it] 81%|████████▏ | 1250/1535 [50:35<11:29,  2.42s/it] 81%|████████▏ | 1251/1535 [50:37<11:28,  2.42s/it] 82%|████████▏ | 1252/1535 [50:40<11:25,  2.42s/it] 82%|████████▏ | 1253/1535 [50:42<11:22,  2.42s/it] 82%|████████▏ | 1254/1535 [50:44<11:19,  2.42s/it] 82%|████████▏ | 1255/1535 [50:47<11:18,  2.42s/it] 82%|████████▏ | 1256/1535 [50:49<11:15,  2.42s/it] 82%|████████▏ | 1257/1535 [50:52<11:12,  2.42s/it] 82%|████████▏ | 1258/1535 [50:54<11:10,  2.42s/it] 82%|████████▏ | 1259/1535 [50:57<11:08,  2.42s/it] 82%|████████▏ | 1260/1535 [50:59<11:05,  2.42s/it]                                                   {'loss': 0.2844, 'grad_norm': 0.2730634808540344, 'learning_rate': 6.14438739478383e-05, 'epoch': 4.1}
 82%|████████▏ | 1260/1535 [50:59<11:05,  2.42s/it] 82%|████████▏ | 1261/1535 [51:01<11:03,  2.42s/it] 82%|████████▏ | 1262/1535 [51:04<11:00,  2.42s/it] 82%|████████▏ | 1263/1535 [51:06<10:58,  2.42s/it] 82%|████████▏ | 1264/1535 [51:09<10:57,  2.43s/it] 82%|████████▏ | 1265/1535 [51:11<10:54,  2.43s/it] 82%|████████▏ | 1266/1535 [51:13<10:51,  2.42s/it] 83%|████████▎ | 1267/1535 [51:16<10:49,  2.42s/it] 83%|████████▎ | 1268/1535 [51:18<10:46,  2.42s/it] 83%|████████▎ | 1269/1535 [51:21<10:44,  2.42s/it] 83%|████████▎ | 1270/1535 [51:23<10:42,  2.42s/it] 83%|████████▎ | 1271/1535 [51:26<10:39,  2.42s/it] 83%|████████▎ | 1272/1535 [51:28<10:36,  2.42s/it] 83%|████████▎ | 1273/1535 [51:30<10:34,  2.42s/it] 83%|████████▎ | 1274/1535 [51:33<10:31,  2.42s/it] 83%|████████▎ | 1275/1535 [51:35<10:29,  2.42s/it] 83%|████████▎ | 1276/1535 [51:38<10:26,  2.42s/it] 83%|████████▎ | 1277/1535 [51:40<10:24,  2.42s/it] 83%|████████▎ | 1278/1535 [51:43<10:21,  2.42s/it] 83%|████████▎ | 1279/1535 [51:45<10:19,  2.42s/it] 83%|████████▎ | 1280/1535 [51:47<10:18,  2.42s/it]                                                   {'loss': 0.3215, 'grad_norm': 0.2759706377983093, 'learning_rate': 5.329518829350788e-05, 'epoch': 4.16}
 83%|████████▎ | 1280/1535 [51:47<10:18,  2.42s/it] 83%|████████▎ | 1281/1535 [51:50<10:15,  2.42s/it] 84%|████████▎ | 1282/1535 [51:52<10:12,  2.42s/it] 84%|████████▎ | 1283/1535 [51:55<10:10,  2.42s/it] 84%|████████▎ | 1284/1535 [51:57<10:07,  2.42s/it] 84%|████████▎ | 1285/1535 [51:59<10:05,  2.42s/it] 84%|████████▍ | 1286/1535 [52:02<10:02,  2.42s/it] 84%|████████▍ | 1287/1535 [52:04<10:00,  2.42s/it] 84%|████████▍ | 1288/1535 [52:07<09:57,  2.42s/it] 84%|████████▍ | 1289/1535 [52:09<09:55,  2.42s/it] 84%|████████▍ | 1290/1535 [52:12<09:52,  2.42s/it] 84%|████████▍ | 1291/1535 [52:14<09:50,  2.42s/it] 84%|████████▍ | 1292/1535 [52:16<09:48,  2.42s/it] 84%|████████▍ | 1293/1535 [52:19<09:46,  2.42s/it] 84%|████████▍ | 1294/1535 [52:21<09:43,  2.42s/it] 84%|████████▍ | 1295/1535 [52:24<09:41,  2.42s/it] 84%|████████▍ | 1296/1535 [52:26<09:38,  2.42s/it] 84%|████████▍ | 1297/1535 [52:29<09:36,  2.42s/it] 85%|████████▍ | 1298/1535 [52:31<09:33,  2.42s/it] 85%|████████▍ | 1299/1535 [52:33<09:31,  2.42s/it] 85%|████████▍ | 1300/1535 [52:36<09:28,  2.42s/it]                                                   {'loss': 0.2971, 'grad_norm': 0.2868780195713043, 'learning_rate': 4.566135562708437e-05, 'epoch': 4.23}
 85%|████████▍ | 1300/1535 [52:36<09:28,  2.42s/it] 85%|████████▍ | 1301/1535 [52:38<09:26,  2.42s/it] 85%|████████▍ | 1302/1535 [52:41<09:23,  2.42s/it] 85%|████████▍ | 1303/1535 [52:43<09:21,  2.42s/it] 85%|████████▍ | 1304/1535 [52:45<09:19,  2.42s/it] 85%|████████▌ | 1305/1535 [52:48<09:16,  2.42s/it] 85%|████████▌ | 1306/1535 [52:50<09:14,  2.42s/it] 85%|████████▌ | 1307/1535 [52:53<09:15,  2.43s/it] 85%|████████▌ | 1308/1535 [52:55<09:11,  2.43s/it] 85%|████████▌ | 1309/1535 [52:58<09:08,  2.43s/it] 85%|████████▌ | 1310/1535 [53:00<09:05,  2.43s/it] 85%|████████▌ | 1311/1535 [53:02<09:02,  2.42s/it] 85%|████████▌ | 1312/1535 [53:05<09:00,  2.42s/it] 86%|████████▌ | 1313/1535 [53:07<08:58,  2.42s/it] 86%|████████▌ | 1314/1535 [53:10<08:55,  2.42s/it] 86%|████████▌ | 1315/1535 [53:12<08:52,  2.42s/it] 86%|████████▌ | 1316/1535 [53:15<08:50,  2.42s/it] 86%|████████▌ | 1317/1535 [53:17<08:47,  2.42s/it] 86%|████████▌ | 1318/1535 [53:19<08:45,  2.42s/it] 86%|████████▌ | 1319/1535 [53:22<08:42,  2.42s/it] 86%|████████▌ | 1320/1535 [53:24<08:40,  2.42s/it]                                                   {'loss': 0.3168, 'grad_norm': 0.3072579503059387, 'learning_rate': 3.8562356657343586e-05, 'epoch': 4.29}
 86%|████████▌ | 1320/1535 [53:24<08:40,  2.42s/it] 86%|████████▌ | 1321/1535 [53:27<08:38,  2.43s/it] 86%|████████▌ | 1322/1535 [53:29<08:36,  2.42s/it] 86%|████████▌ | 1323/1535 [53:32<08:33,  2.42s/it] 86%|████████▋ | 1324/1535 [53:34<08:31,  2.42s/it] 86%|████████▋ | 1325/1535 [53:36<08:28,  2.42s/it] 86%|████████▋ | 1326/1535 [53:39<08:26,  2.42s/it] 86%|████████▋ | 1327/1535 [53:41<08:23,  2.42s/it] 87%|████████▋ | 1328/1535 [53:44<08:21,  2.42s/it] 87%|████████▋ | 1329/1535 [53:46<08:18,  2.42s/it] 87%|████████▋ | 1330/1535 [53:48<08:16,  2.42s/it] 87%|████████▋ | 1331/1535 [53:51<08:13,  2.42s/it] 87%|████████▋ | 1332/1535 [53:53<08:11,  2.42s/it] 87%|████████▋ | 1333/1535 [53:56<08:08,  2.42s/it] 87%|████████▋ | 1334/1535 [53:58<08:08,  2.43s/it] 87%|████████▋ | 1335/1535 [54:01<08:05,  2.43s/it] 87%|████████▋ | 1336/1535 [54:03<08:02,  2.42s/it] 87%|████████▋ | 1337/1535 [54:05<07:59,  2.42s/it] 87%|████████▋ | 1338/1535 [54:08<07:57,  2.42s/it] 87%|████████▋ | 1339/1535 [54:10<07:54,  2.42s/it] 87%|████████▋ | 1340/1535 [54:13<07:52,  2.42s/it]                                                   {'loss': 0.3199, 'grad_norm': 0.3304714858531952, 'learning_rate': 3.2016772225287845e-05, 'epoch': 4.36}
 87%|████████▋ | 1340/1535 [54:13<07:52,  2.42s/it] 87%|████████▋ | 1341/1535 [54:15<07:49,  2.42s/it] 87%|████████▋ | 1342/1535 [54:18<07:48,  2.43s/it] 87%|████████▋ | 1343/1535 [54:20<07:45,  2.43s/it] 88%|████████▊ | 1344/1535 [54:22<07:43,  2.42s/it] 88%|████████▊ | 1345/1535 [54:25<07:40,  2.42s/it] 88%|████████▊ | 1346/1535 [54:27<07:37,  2.42s/it] 88%|████████▊ | 1347/1535 [54:30<07:35,  2.42s/it] 88%|████████▊ | 1348/1535 [54:32<07:33,  2.42s/it] 88%|████████▊ | 1349/1535 [54:35<07:30,  2.42s/it] 88%|████████▊ | 1350/1535 [54:37<07:28,  2.42s/it] 88%|████████▊ | 1351/1535 [54:39<07:25,  2.42s/it] 88%|████████▊ | 1352/1535 [54:42<07:23,  2.42s/it] 88%|████████▊ | 1353/1535 [54:44<07:20,  2.42s/it] 88%|████████▊ | 1354/1535 [54:47<07:18,  2.42s/it] 88%|████████▊ | 1355/1535 [54:49<07:15,  2.42s/it] 88%|████████▊ | 1356/1535 [54:51<07:13,  2.42s/it] 88%|████████▊ | 1357/1535 [54:54<07:12,  2.43s/it] 88%|████████▊ | 1358/1535 [54:56<07:09,  2.43s/it] 89%|████████▊ | 1359/1535 [54:59<07:06,  2.42s/it] 89%|████████▊ | 1360/1535 [55:01<07:04,  2.42s/it]                                                   {'loss': 0.3014, 'grad_norm': 0.2683049440383911, 'learning_rate': 2.604173467085949e-05, 'epoch': 4.42}
 89%|████████▊ | 1360/1535 [55:01<07:04,  2.42s/it] 89%|████████▊ | 1361/1535 [55:04<07:01,  2.42s/it] 89%|████████▊ | 1362/1535 [55:06<06:59,  2.42s/it] 89%|████████▉ | 1363/1535 [55:08<06:56,  2.42s/it] 89%|████████▉ | 1364/1535 [55:11<06:54,  2.42s/it] 89%|████████▉ | 1365/1535 [55:13<06:51,  2.42s/it] 89%|████████▉ | 1366/1535 [55:16<06:49,  2.42s/it] 89%|████████▉ | 1367/1535 [55:18<06:46,  2.42s/it] 89%|████████▉ | 1368/1535 [55:21<06:44,  2.42s/it] 89%|████████▉ | 1369/1535 [55:23<06:41,  2.42s/it] 89%|████████▉ | 1370/1535 [55:25<06:39,  2.42s/it] 89%|████████▉ | 1371/1535 [55:28<06:38,  2.43s/it] 89%|████████▉ | 1372/1535 [55:30<06:35,  2.43s/it] 89%|████████▉ | 1373/1535 [55:33<06:32,  2.42s/it] 90%|████████▉ | 1374/1535 [55:35<06:30,  2.42s/it] 90%|████████▉ | 1375/1535 [55:38<06:27,  2.42s/it] 90%|████████▉ | 1376/1535 [55:40<06:24,  2.42s/it] 90%|████████▉ | 1377/1535 [55:42<06:24,  2.44s/it] 90%|████████▉ | 1378/1535 [55:45<06:21,  2.43s/it] 90%|████████▉ | 1379/1535 [55:47<06:18,  2.43s/it] 90%|████████▉ | 1380/1535 [55:50<06:15,  2.43s/it]                                                   {'loss': 0.3082, 'grad_norm': 0.2667210102081299, 'learning_rate': 2.0652882990944534e-05, 'epoch': 4.49}
 90%|████████▉ | 1380/1535 [55:50<06:15,  2.43s/it] 90%|████████▉ | 1381/1535 [55:52<06:13,  2.42s/it] 90%|█████████ | 1382/1535 [55:54<06:10,  2.42s/it] 90%|█████████ | 1383/1535 [55:57<06:08,  2.42s/it] 90%|█████████ | 1384/1535 [55:59<06:05,  2.42s/it] 90%|█████████ | 1385/1535 [56:02<06:03,  2.42s/it] 90%|█████████ | 1386/1535 [56:04<06:00,  2.42s/it] 90%|█████████ | 1387/1535 [56:07<05:58,  2.42s/it] 90%|█████████ | 1388/1535 [56:09<05:55,  2.42s/it] 90%|█████████ | 1389/1535 [56:11<05:53,  2.42s/it] 91%|█████████ | 1390/1535 [56:14<05:51,  2.42s/it] 91%|█████████ | 1391/1535 [56:16<05:48,  2.42s/it] 91%|█████████ | 1392/1535 [56:19<05:46,  2.42s/it] 91%|█████████ | 1393/1535 [56:21<05:43,  2.42s/it] 91%|█████████ | 1394/1535 [56:24<05:41,  2.42s/it] 91%|█████████ | 1395/1535 [56:26<05:38,  2.42s/it] 91%|█████████ | 1396/1535 [56:28<05:36,  2.42s/it] 91%|█████████ | 1397/1535 [56:31<05:34,  2.42s/it] 91%|█████████ | 1398/1535 [56:33<05:31,  2.42s/it] 91%|█████████ | 1399/1535 [56:36<05:29,  2.42s/it] 91%|█████████ | 1400/1535 [56:38<05:26,  2.42s/it]                                                   {'loss': 0.3158, 'grad_norm': 0.28500425815582275, 'learning_rate': 1.586432190603626e-05, 'epoch': 4.55}
 91%|█████████ | 1400/1535 [56:38<05:26,  2.42s/it] 91%|█████████▏| 1401/1535 [56:40<05:24,  2.42s/it] 91%|█████████▏| 1402/1535 [56:43<05:21,  2.42s/it] 91%|█████████▏| 1403/1535 [56:45<05:20,  2.43s/it] 91%|█████████▏| 1404/1535 [56:48<05:17,  2.43s/it] 92%|█████████▏| 1405/1535 [56:50<05:15,  2.42s/it] 92%|█████████▏| 1406/1535 [56:53<05:12,  2.42s/it] 92%|█████████▏| 1407/1535 [56:55<05:10,  2.42s/it] 92%|█████████▏| 1408/1535 [56:57<05:07,  2.42s/it] 92%|█████████▏| 1409/1535 [57:00<05:05,  2.42s/it] 92%|█████████▏| 1410/1535 [57:02<05:02,  2.42s/it] 92%|█████████▏| 1411/1535 [57:05<05:00,  2.42s/it] 92%|█████████▏| 1412/1535 [57:07<04:57,  2.42s/it] 92%|█████████▏| 1413/1535 [57:10<04:55,  2.42s/it] 92%|█████████▏| 1414/1535 [57:12<04:53,  2.42s/it] 92%|█████████▏| 1415/1535 [57:14<04:50,  2.42s/it] 92%|█████████▏| 1416/1535 [57:17<04:48,  2.42s/it] 92%|█████████▏| 1417/1535 [57:19<04:45,  2.42s/it] 92%|█████████▏| 1418/1535 [57:22<04:43,  2.42s/it] 92%|█████████▏| 1419/1535 [57:24<04:41,  2.43s/it] 93%|█████████▎| 1420/1535 [57:27<04:38,  2.43s/it]                                                   {'loss': 0.2959, 'grad_norm': 0.28616198897361755, 'learning_rate': 1.1688584942696367e-05, 'epoch': 4.62}
 93%|█████████▎| 1420/1535 [57:27<04:38,  2.43s/it] 93%|█████████▎| 1421/1535 [57:29<04:36,  2.42s/it] 93%|█████████▎| 1422/1535 [57:31<04:33,  2.42s/it] 93%|█████████▎| 1423/1535 [57:34<04:31,  2.42s/it] 93%|█████████▎| 1424/1535 [57:36<04:28,  2.42s/it] 93%|█████████▎| 1425/1535 [57:39<04:26,  2.42s/it] 93%|█████████▎| 1426/1535 [57:41<04:23,  2.42s/it] 93%|█████████▎| 1427/1535 [57:43<04:21,  2.42s/it] 93%|█████████▎| 1428/1535 [57:46<04:18,  2.42s/it] 93%|█████████▎| 1429/1535 [57:48<04:17,  2.43s/it] 93%|█████████▎| 1430/1535 [57:51<04:14,  2.42s/it] 93%|█████████▎| 1431/1535 [57:53<04:12,  2.42s/it] 93%|█████████▎| 1432/1535 [57:56<04:10,  2.43s/it] 93%|█████████▎| 1433/1535 [57:58<04:07,  2.43s/it] 93%|█████████▎| 1434/1535 [58:00<04:04,  2.42s/it] 93%|█████████▎| 1435/1535 [58:03<04:02,  2.42s/it] 94%|█████████▎| 1436/1535 [58:05<03:59,  2.42s/it] 94%|█████████▎| 1437/1535 [58:08<03:57,  2.42s/it] 94%|█████████▎| 1438/1535 [58:10<03:54,  2.42s/it] 94%|█████████▎| 1439/1535 [58:13<03:52,  2.42s/it] 94%|█████████▍| 1440/1535 [58:15<03:49,  2.42s/it]                                                   {'loss': 0.3064, 'grad_norm': 0.2694741487503052, 'learning_rate': 8.136601628441876e-06, 'epoch': 4.68}
 94%|█████████▍| 1440/1535 [58:15<03:49,  2.42s/it] 94%|█████████▍| 1441/1535 [58:17<03:47,  2.42s/it] 94%|█████████▍| 1442/1535 [58:20<03:45,  2.42s/it] 94%|█████████▍| 1443/1535 [58:22<03:42,  2.42s/it] 94%|█████████▍| 1444/1535 [58:25<03:40,  2.42s/it] 94%|█████████▍| 1445/1535 [58:27<03:37,  2.42s/it] 94%|█████████▍| 1446/1535 [58:30<03:36,  2.43s/it] 94%|█████████▍| 1447/1535 [58:32<03:33,  2.43s/it] 94%|█████████▍| 1448/1535 [58:34<03:31,  2.43s/it] 94%|█████████▍| 1449/1535 [58:37<03:28,  2.42s/it] 94%|█████████▍| 1450/1535 [58:39<03:25,  2.42s/it] 95%|█████████▍| 1451/1535 [58:42<03:23,  2.42s/it] 95%|█████████▍| 1452/1535 [58:44<03:20,  2.42s/it] 95%|█████████▍| 1453/1535 [58:46<03:18,  2.42s/it] 95%|█████████▍| 1454/1535 [58:49<03:16,  2.42s/it] 95%|█████████▍| 1455/1535 [58:51<03:13,  2.42s/it] 95%|█████████▍| 1456/1535 [58:54<03:11,  2.42s/it] 95%|█████████▍| 1457/1535 [58:56<03:08,  2.42s/it] 95%|█████████▍| 1458/1535 [58:59<03:06,  2.42s/it] 95%|█████████▌| 1459/1535 [59:01<03:03,  2.42s/it] 95%|█████████▌| 1460/1535 [59:03<03:01,  2.42s/it]                                                   {'loss': 0.314, 'grad_norm': 0.30857518315315247, 'learning_rate': 5.217668884921506e-06, 'epoch': 4.75}
 95%|█████████▌| 1460/1535 [59:03<03:01,  2.42s/it] 95%|█████████▌| 1461/1535 [59:06<02:59,  2.42s/it] 95%|█████████▌| 1462/1535 [59:08<02:56,  2.42s/it] 95%|█████████▌| 1463/1535 [59:11<02:54,  2.42s/it] 95%|█████████▌| 1464/1535 [59:13<02:51,  2.42s/it] 95%|█████████▌| 1465/1535 [59:16<02:49,  2.42s/it] 96%|█████████▌| 1466/1535 [59:18<02:46,  2.42s/it] 96%|█████████▌| 1467/1535 [59:20<02:44,  2.42s/it] 96%|█████████▌| 1468/1535 [59:23<02:42,  2.42s/it] 96%|█████████▌| 1469/1535 [59:25<02:39,  2.42s/it] 96%|█████████▌| 1470/1535 [59:28<02:37,  2.42s/it] 96%|█████████▌| 1471/1535 [59:30<02:34,  2.42s/it] 96%|█████████▌| 1472/1535 [59:32<02:32,  2.42s/it] 96%|█████████▌| 1473/1535 [59:35<02:30,  2.43s/it] 96%|█████████▌| 1474/1535 [59:37<02:27,  2.43s/it] 96%|█████████▌| 1475/1535 [59:40<02:25,  2.42s/it] 96%|█████████▌| 1476/1535 [59:42<02:22,  2.42s/it] 96%|█████████▌| 1477/1535 [59:45<02:20,  2.42s/it] 96%|█████████▋| 1478/1535 [59:47<02:18,  2.42s/it] 96%|█████████▋| 1479/1535 [59:49<02:15,  2.42s/it] 96%|█████████▋| 1480/1535 [59:52<02:13,  2.42s/it]                                                   {'loss': 0.3023, 'grad_norm': 0.30392852425575256, 'learning_rate': 2.9394266942558978e-06, 'epoch': 4.81}
 96%|█████████▋| 1480/1535 [59:52<02:13,  2.42s/it] 96%|█████████▋| 1481/1535 [59:54<02:10,  2.42s/it] 97%|█████████▋| 1482/1535 [59:57<02:08,  2.42s/it] 97%|█████████▋| 1483/1535 [59:59<02:05,  2.42s/it] 97%|█████████▋| 1484/1535 [1:00:02<02:03,  2.42s/it] 97%|█████████▋| 1485/1535 [1:00:04<02:01,  2.42s/it] 97%|█████████▋| 1486/1535 [1:00:06<01:58,  2.42s/it] 97%|█████████▋| 1487/1535 [1:00:09<01:56,  2.43s/it] 97%|█████████▋| 1488/1535 [1:00:11<01:54,  2.43s/it] 97%|█████████▋| 1489/1535 [1:00:14<01:51,  2.43s/it] 97%|█████████▋| 1490/1535 [1:00:16<01:49,  2.42s/it] 97%|█████████▋| 1491/1535 [1:00:19<01:46,  2.42s/it] 97%|█████████▋| 1492/1535 [1:00:21<01:44,  2.42s/it] 97%|█████████▋| 1493/1535 [1:00:23<01:41,  2.42s/it] 97%|█████████▋| 1494/1535 [1:00:26<01:39,  2.42s/it] 97%|█████████▋| 1495/1535 [1:00:28<01:36,  2.42s/it] 97%|█████████▋| 1496/1535 [1:00:31<01:34,  2.42s/it] 98%|█████████▊| 1497/1535 [1:00:33<01:31,  2.42s/it] 98%|█████████▊| 1498/1535 [1:00:35<01:29,  2.42s/it] 98%|█████████▊| 1499/1535 [1:00:38<01:27,  2.42s/it] 98%|█████████▊| 1500/1535 [1:00:40<01:24,  2.42s/it]                                                     {'loss': 0.3001, 'grad_norm': 0.33405908942222595, 'learning_rate': 1.3078381022336715e-06, 'epoch': 4.88}
 98%|█████████▊| 1500/1535 [1:00:40<01:24,  2.42s/it] 98%|█████████▊| 1501/1535 [1:00:43<01:22,  2.43s/it] 98%|█████████▊| 1502/1535 [1:00:45<01:20,  2.43s/it] 98%|█████████▊| 1503/1535 [1:00:48<01:17,  2.43s/it] 98%|█████████▊| 1504/1535 [1:00:50<01:15,  2.43s/it] 98%|█████████▊| 1505/1535 [1:00:52<01:12,  2.42s/it] 98%|█████████▊| 1506/1535 [1:00:55<01:10,  2.42s/it] 98%|█████████▊| 1507/1535 [1:00:57<01:07,  2.42s/it] 98%|█████████▊| 1508/1535 [1:01:00<01:05,  2.42s/it] 98%|█████████▊| 1509/1535 [1:01:02<01:02,  2.42s/it] 98%|█████████▊| 1510/1535 [1:01:05<01:00,  2.42s/it] 98%|█████████▊| 1511/1535 [1:01:07<00:58,  2.42s/it] 99%|█████████▊| 1512/1535 [1:01:09<00:55,  2.42s/it] 99%|█████████▊| 1513/1535 [1:01:12<00:53,  2.42s/it] 99%|█████████▊| 1514/1535 [1:01:14<00:50,  2.42s/it] 99%|█████████▊| 1515/1535 [1:01:17<00:48,  2.42s/it] 99%|█████████▉| 1516/1535 [1:01:19<00:46,  2.44s/it] 99%|█████████▉| 1517/1535 [1:01:22<00:43,  2.43s/it] 99%|█████████▉| 1518/1535 [1:01:24<00:41,  2.43s/it] 99%|█████████▉| 1519/1535 [1:01:26<00:38,  2.43s/it] 99%|█████████▉| 1520/1535 [1:01:29<00:36,  2.42s/it]                                                     {'loss': 0.2817, 'grad_norm': 0.2866583466529846, 'learning_rate': 3.271736107015033e-07, 'epoch': 4.94}
 99%|█████████▉| 1520/1535 [1:01:29<00:36,  2.42s/it] 99%|█████████▉| 1521/1535 [1:01:31<00:33,  2.42s/it] 99%|█████████▉| 1522/1535 [1:01:34<00:31,  2.42s/it] 99%|█████████▉| 1523/1535 [1:01:36<00:29,  2.42s/it] 99%|█████████▉| 1524/1535 [1:01:38<00:26,  2.42s/it] 99%|█████████▉| 1525/1535 [1:01:41<00:24,  2.42s/it] 99%|█████████▉| 1526/1535 [1:01:43<00:21,  2.42s/it] 99%|█████████▉| 1527/1535 [1:01:46<00:19,  2.42s/it]100%|█████████▉| 1528/1535 [1:01:48<00:16,  2.42s/it]100%|█████████▉| 1529/1535 [1:01:51<00:14,  2.42s/it]100%|█████████▉| 1530/1535 [1:01:53<00:12,  2.42s/it]100%|█████████▉| 1531/1535 [1:01:55<00:09,  2.42s/it]100%|█████████▉| 1532/1535 [1:01:58<00:07,  2.42s/it]100%|█████████▉| 1533/1535 [1:02:00<00:04,  2.42s/it]100%|█████████▉| 1534/1535 [1:02:03<00:02,  2.42s/it]100%|██████████| 1535/1535 [1:02:05<00:00,  2.42s/it][INFO|trainer.py:2231] 2024-05-25 06:17:04,997 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 3735.9759, 'train_samples_per_second': 3.291, 'train_steps_per_second': 0.411, 'train_loss': 0.4406219469996151, 'epoch': 4.99}
100%|██████████| 1535/1535 [1:02:05<00:00,  2.42s/it]100%|██████████| 1535/1535 [1:02:05<00:00,  2.43s/it]
***** train metrics *****
  epoch                    =       4.99
  train_loss               =     0.4406
  train_runtime            = 1:02:15.97
  train_samples_per_second =      3.291
  train_steps_per_second   =      0.411
[INFO|trainer.py:3203] 2024-05-25 06:17:05,002 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/extractiveness_then_length
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-05-25 06:17:05,637 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 06:17:05,639 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 06:17:06,160 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 06:17:06,163 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 06:17:06,211 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 06:17:06,211 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/special_tokens_map.json
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:471] 2024-05-25 06:17:06,544 >> Configuration saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/final_merged_model/config.json
[INFO|configuration_utils.py:697] 2024-05-25 06:17:06,545 >> Configuration saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/final_merged_model/generation_config.json
[INFO|modeling_utils.py:2474] 2024-05-25 06:17:13,267 >> Model weights saved in /scratch/tathagato/adapter_experiments/extractiveness_then_length/final_merged_model/model.safetensors
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.006 MB uploadedwandb: | 0.006 MB of 0.006 MB uploadedwandb: / 0.006 MB of 0.034 MB uploadedwandb: - 0.006 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb: | 0.038 MB of 0.038 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:     train/grad_norm █▄▆▄▂▄▂▃▂▆▃▃▃▃▃▄▃▂▂▄▃▃▃▄▆▄▂▁▂▃▂▃▂▁▂▁▁▁▂▁
wandb: train/learning_rate ▁▂▃▄▅▆▆▇██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁
wandb:          train/loss █▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▂▂▂▂▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 1.567843226180649e+17
wandb:              train/epoch 4.99
wandb:        train/global_step 1535
wandb:          train/grad_norm 0.28666
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.2817
wandb:               train_loss 0.44062
wandb:            train_runtime 3735.9759
wandb: train_samples_per_second 3.291
wandb:   train_steps_per_second 0.411
wandb: 
wandb: 🚀 View run firm-haze-101 at: https://wandb.ai/ihub-drug-discovery/huggingface/runs/3zrk9vyz
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240525_051452-3zrk9vyz/logs
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-05-25 06:17:46 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 06:17:46 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 06:17:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 06:17:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch/tathagato/adapter_experiments/topic_then_length/runs/May25_06-17-46_gnode081,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=20,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/scratch/tathagato/adapter_experiments/topic_then_length,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch/tathagato/adapter_experiments/topic_then_length,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=400,
save_strategy=steps,
save_total_limit=400,
seed=0,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
2024-05-25 06:17:46 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'v_proj', 'k_proj', 'q_proj', 'o_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
2024-05-25 06:17:46 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1 distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:726] 2024-05-25 06:17:46,989 >> loading configuration file config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 06:17:46,994 >> Model config LlamaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": false,
  "vocab_size": 32000
}

[INFO|quantizer_bnb_4bit.py:247] 2024-05-25 06:17:47,103 >> The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' 
[WARNING|modeling_utils.py:3058] 2024-05-25 06:17:47,104 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:3283] 2024-05-25 06:17:47,104 >> loading weights file model.safetensors from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors
[INFO|modeling_utils.py:1417] 2024-05-25 06:17:47,124 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:928] 2024-05-25 06:17:47,128 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

[WARNING|modeling_utils.py:3058] 2024-05-25 06:17:47,433 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 06:17:47,447 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 06:17:47,499 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:4024] 2024-05-25 06:17:50,279 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-25 06:17:50,280 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-05-25 06:17:50,510 >> loading configuration file generation_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-25 06:17:50,511 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 2048,
  "pad_token_id": 0
}

loading model from : /scratch/tathagato/adapter_experiments/topic/topic
[INFO|tokenization_utils_base.py:2084] 2024-05-25 06:17:51,310 >> loading file tokenizer.model from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-05-25 06:17:51,310 >> loading file tokenizer.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 06:17:51,310 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-05-25 06:17:51,310 >> loading file special_tokens_map.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 06:17:51,310 >> loading file tokenizer_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json
loading model from : /scratch/tathagato/adapter_experiments/topic/topic
loading model from : /scratch/tathagato/adapter_experiments/topic/topic
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
loading model from : /scratch/tathagato/adapter_experiments/topic/topic
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
train dataset size 4278
test dataset size 554
4278
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Spawning 10 processes
2024-05-25 06:17:54 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
train dataset size 4278
test dataset size 554
4278
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
train dataset size 4278
test dataset size 554
4278
Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:14:49,  1.05s/ examples]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]train dataset size 4278
test dataset size 554
4278
Applying chat template to train_sft (num_proc=10):  10%|█         | 429/4278 [00:01<00:09, 389.78 examples/s]Applying chat template to train_sft (num_proc=10):  20%|█▉        | 853/4278 [00:01<00:04, 810.26 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  26%|██▌       | 1091/4278 [00:01<00:03, 799.55 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:01<00:03, 940.96 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  36%|███▋      | 1554/4278 [00:02<00:02, 911.44 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:15:08,  1.05s/ examples]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:07, 505.62 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 708.76 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:14:36,  1.05s/ examples]Applying chat template to train_sft (num_proc=10):  50%|████▉     | 2123/4278 [00:02<00:01, 1125.09 examples/s]Applying chat template to train_sft (num_proc=10):  16%|█▌        | 684/4278 [00:01<00:06, 583.63 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:07, 484.55 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:21:57,  1.15s/ examples]Applying chat template to train_sft (num_proc=10):  56%|█████▌    | 2385/4278 [00:03<00:02, 935.36 examples/s] Applying chat template to train_sft (num_proc=10):  15%|█▍        | 629/4278 [00:01<00:07, 504.61 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:01<00:06, 514.13 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:08, 470.10 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2568/4278 [00:03<00:01, 1022.43 examples/s]Applying chat template to train_sft (num_proc=10):  66%|██████▌   | 2824/4278 [00:03<00:01, 987.60 examples/s] Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:01<00:06, 546.24 examples/s]Applying chat template to train_sft (num_proc=10):  16%|█▌        | 686/4278 [00:01<00:06, 548.04 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1285/4278 [00:02<00:04, 680.33 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2996/4278 [00:03<00:01, 1069.36 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:02<00:03, 971.82 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 830.83 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:02<00:06, 492.10 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:02<00:01, 1178.73 examples/s]Applying chat template to train_sft (num_proc=10):  75%|███████▌  | 3216/4278 [00:04<00:01, 762.27 examples/s] Applying chat template to train_sft (num_proc=10):  36%|███▌      | 1528/4278 [00:02<00:03, 754.24 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3424/4278 [00:04<00:01, 844.36 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1285/4278 [00:02<00:04, 673.10 examples/s]Applying chat template to train_sft (num_proc=10):  55%|█████▌    | 2356/4278 [00:03<00:02, 921.27 examples/s] Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:04, 640.97 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2569/4278 [00:03<00:02, 789.01 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 735.55 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2996/4278 [00:03<00:01, 1136.22 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:03<00:02, 1068.68 examples/s]Applying chat template to train_sft (num_proc=10):  45%|████▌     | 1937/4278 [00:03<00:04, 559.43 examples/s]Applying chat template to train_sft (num_proc=10):  85%|████████▌ | 3637/4278 [00:05<00:01, 515.66 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2141/4278 [00:03<00:03, 632.83 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 936.72 examples/s]Applying chat template to train_sft (num_proc=10):  55%|█████▌    | 2371/4278 [00:03<00:02, 929.38 examples/s] Applying chat template to train_sft (num_proc=10):  58%|█████▊    | 2468/4278 [00:03<00:02, 893.19 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 766.25 examples/s]
Concatenating 10 shards
2024-05-25 06:17:59 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Applying chat template to train_sft (num_proc=10):  76%|███████▌  | 3243/4278 [00:04<00:01, 745.98 examples/s] Applying chat template to train_sft (num_proc=10):  60%|██████    | 2569/4278 [00:03<00:02, 833.39 examples/s]Applying chat template to train_sft (num_proc=10):  66%|██████▌   | 2806/4278 [00:04<00:01, 853.49 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2996/4278 [00:04<00:01, 940.66 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3425/4278 [00:04<00:01, 705.00 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:04<00:01, 899.36 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:04<00:00, 1026.50 examples/s]Applying chat template to train_sft (num_proc=10):  75%|███████▌  | 3218/4278 [00:04<00:01, 835.74 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3425/4278 [00:04<00:00, 1004.69 examples/s]Spawning 10 processes
2024-05-25 06:18:00 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  94%|█████████▍| 4034/4278 [00:05<00:00, 828.98 examples/s] Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:04<00:00, 1320.94 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3425/4278 [00:05<00:01, 735.39 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:05<00:00, 1107.38 examples/s]Applying chat template to train_sft (num_proc=10):  96%|█████████▌| 4110/4278 [00:04<00:00, 1184.20 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 746.54 examples/s]
Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<04:22,  2.11 examples/s]Applying chat template to train_sft (num_proc=10):  95%|█████████▌| 4074/4278 [00:05<00:00, 1058.09 examples/s]Applying chat template to test_sft (num_proc=10):  10%|█         | 57/554 [00:00<00:03, 126.01 examples/s]Applying chat template to test_sft (num_proc=10):  31%|███       | 169/554 [00:00<00:01, 303.95 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 802.59 examples/s] 
Applying chat template to test_sft (num_proc=10):  50%|█████     | 279/554 [00:00<00:00, 480.82 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 733.38 examples/s] 
Applying chat template to test_sft (num_proc=10):  70%|███████   | 389/554 [00:01<00:00, 541.34 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 499/554 [00:01<00:00, 503.40 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 348.61 examples/s]
Concatenating 10 shards
2024-05-25 06:18:02 - INFO - datasets.arrow_dataset - Concatenating 10 shards
tokenizer padding side left
Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<05:09,  1.79 examples/s]Applying chat template to test_sft (num_proc=10):  30%|███       | 168/554 [00:00<00:01, 296.48 examples/s]Using custom data configuration default-8a159e0651bd4009
2024-05-25 06:18:02 - INFO - datasets.builder - Using custom data configuration default-8a159e0651bd4009
Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
2024-05-25 06:18:02 - INFO - datasets.info - Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
Overwrite dataset info from restored data version if exists.
2024-05-25 06:18:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0
2024-05-25 06:18:02 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0
Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0)
2024-05-25 06:18:02 - INFO - datasets.builder - Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0)
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0
2024-05-25 06:18:02 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-8a159e0651bd4009/0.0.0
Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:00<00:01, 291.93 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 280/554 [00:01<00:00, 297.25 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<06:18,  1.46 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 390/554 [00:01<00:00, 449.41 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<07:24,  1.24 examples/s]Applying chat template to test_sft (num_proc=10):  10%|█         | 57/554 [00:00<00:05, 86.25 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 500/554 [00:01<00:00, 524.76 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 112/554 [00:00<00:02, 166.86 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:00<00:00, 360.51 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 341.40 examples/s]
Applying chat template to test_sft (num_proc=10):  41%|████      | 226/554 [00:01<00:01, 286.65 examples/s]Applying chat template to test_sft (num_proc=10):  60%|██████    | 334/554 [00:01<00:00, 411.31 examples/s]tokenizer padding side left
Applying chat template to test_sft (num_proc=10):  60%|██████    | 335/554 [00:01<00:00, 369.34 examples/s]Applying chat template to test_sft (num_proc=10):  80%|████████  | 445/554 [00:01<00:00, 430.15 examples/s]Applying chat template to test_sft (num_proc=10):  80%|████████  | 445/554 [00:01<00:00, 476.27 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 334.80 examples/s]
Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 321.04 examples/s]
tokenizer padding side left
tokenizer padding side left
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
2024-05-25 06:18:04 - WARNING - accelerate.utils.other - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:607] 2024-05-25 06:18:07,139 >> Using auto half precision backend
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
[INFO|trainer.py:1969] 2024-05-25 06:18:07,402 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-25 06:18:07,402 >>   Num examples = 2,459
[INFO|trainer.py:1971] 2024-05-25 06:18:07,402 >>   Num Epochs = 5
[INFO|trainer.py:1972] 2024-05-25 06:18:07,402 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-25 06:18:07,402 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-05-25 06:18:07,402 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-05-25 06:18:07,402 >>   Total optimization steps = 1,535
[INFO|trainer.py:1978] 2024-05-25 06:18:07,404 >>   Number of trainable parameters = 4,505,600
[INFO|integration_utils.py:723] 2024-05-25 06:18:07,467 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSum/experiments/wandb/run-20240525_061813-mzcv67nv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-snowflake-102
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/huggingface
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/huggingface/runs/mzcv67nv
  0%|          | 0/1535 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/1535 [00:02<1:03:18,  2.48s/it]  0%|          | 2/1535 [00:04<1:02:08,  2.43s/it]  0%|          | 3/1535 [00:07<1:01:43,  2.42s/it]  0%|          | 4/1535 [00:09<1:01:29,  2.41s/it]  0%|          | 5/1535 [00:12<1:01:23,  2.41s/it]  0%|          | 6/1535 [00:14<1:01:21,  2.41s/it]  0%|          | 7/1535 [00:16<1:01:18,  2.41s/it]  1%|          | 8/1535 [00:19<1:01:15,  2.41s/it]  1%|          | 9/1535 [00:21<1:01:42,  2.43s/it]  1%|          | 10/1535 [00:24<1:01:30,  2.42s/it]  1%|          | 11/1535 [00:26<1:01:21,  2.42s/it]  1%|          | 12/1535 [00:29<1:01:54,  2.44s/it]  1%|          | 13/1535 [00:31<1:01:57,  2.44s/it]  1%|          | 14/1535 [00:33<1:01:39,  2.43s/it]  1%|          | 15/1535 [00:36<1:01:26,  2.43s/it]  1%|          | 16/1535 [00:38<1:01:18,  2.42s/it]  1%|          | 17/1535 [00:41<1:01:11,  2.42s/it]  1%|          | 18/1535 [00:43<1:01:05,  2.42s/it]  1%|          | 19/1535 [00:45<1:01:01,  2.42s/it]  1%|▏         | 20/1535 [00:48<1:00:57,  2.41s/it]                                                   {'loss': 0.995, 'grad_norm': 0.4360886216163635, 'learning_rate': 3.2573289902280134e-05, 'epoch': 0.07}
  1%|▏         | 20/1535 [00:48<1:00:57,  2.41s/it]  1%|▏         | 21/1535 [00:50<1:00:55,  2.41s/it]  1%|▏         | 22/1535 [00:53<1:00:51,  2.41s/it]  1%|▏         | 23/1535 [00:55<1:00:48,  2.41s/it]  2%|▏         | 24/1535 [00:58<1:00:46,  2.41s/it]  2%|▏         | 25/1535 [01:00<1:00:44,  2.41s/it]  2%|▏         | 26/1535 [01:02<1:00:55,  2.42s/it]  2%|▏         | 27/1535 [01:05<1:00:51,  2.42s/it]  2%|▏         | 28/1535 [01:07<1:00:45,  2.42s/it]  2%|▏         | 29/1535 [01:10<1:00:55,  2.43s/it]  2%|▏         | 30/1535 [01:12<1:00:47,  2.42s/it]  2%|▏         | 31/1535 [01:15<1:00:42,  2.42s/it]  2%|▏         | 32/1535 [01:17<1:00:38,  2.42s/it]  2%|▏         | 33/1535 [01:19<1:00:33,  2.42s/it]  2%|▏         | 34/1535 [01:22<1:00:28,  2.42s/it]  2%|▏         | 35/1535 [01:24<1:00:25,  2.42s/it]  2%|▏         | 36/1535 [01:27<1:00:24,  2.42s/it]  2%|▏         | 37/1535 [01:29<1:00:22,  2.42s/it]  2%|▏         | 38/1535 [01:31<1:00:18,  2.42s/it]  3%|▎         | 39/1535 [01:34<1:00:15,  2.42s/it]  3%|▎         | 40/1535 [01:36<1:00:13,  2.42s/it]                                                   {'loss': 0.922, 'grad_norm': 0.3298640847206116, 'learning_rate': 6.514657980456027e-05, 'epoch': 0.13}
  3%|▎         | 40/1535 [01:36<1:00:13,  2.42s/it]  3%|▎         | 41/1535 [01:39<1:00:13,  2.42s/it]  3%|▎         | 42/1535 [01:41<1:00:24,  2.43s/it]  3%|▎         | 43/1535 [01:44<1:00:17,  2.42s/it]  3%|▎         | 44/1535 [01:46<1:00:11,  2.42s/it]  3%|▎         | 45/1535 [01:48<1:00:08,  2.42s/it]  3%|▎         | 46/1535 [01:51<1:00:04,  2.42s/it]  3%|▎         | 47/1535 [01:53<1:00:00,  2.42s/it]  3%|▎         | 48/1535 [01:56<59:59,  2.42s/it]    3%|▎         | 49/1535 [01:58<59:56,  2.42s/it]  3%|▎         | 50/1535 [02:00<59:54,  2.42s/it]  3%|▎         | 51/1535 [02:03<59:50,  2.42s/it]  3%|▎         | 52/1535 [02:05<59:48,  2.42s/it]  3%|▎         | 53/1535 [02:08<59:48,  2.42s/it]  4%|▎         | 54/1535 [02:10<59:51,  2.43s/it]  4%|▎         | 55/1535 [02:13<59:46,  2.42s/it]  4%|▎         | 56/1535 [02:15<1:00:13,  2.44s/it]  4%|▎         | 57/1535 [02:18<1:00:01,  2.44s/it]  4%|▍         | 58/1535 [02:20<59:52,  2.43s/it]    4%|▍         | 59/1535 [02:22<59:44,  2.43s/it]  4%|▍         | 60/1535 [02:25<59:38,  2.43s/it]                                                 {'loss': 0.9145, 'grad_norm': 0.3781942129135132, 'learning_rate': 9.771986970684039e-05, 'epoch': 0.2}
  4%|▍         | 60/1535 [02:25<59:38,  2.43s/it]  4%|▍         | 61/1535 [02:27<59:34,  2.43s/it]  4%|▍         | 62/1535 [02:30<59:30,  2.42s/it]  4%|▍         | 63/1535 [02:32<59:26,  2.42s/it]  4%|▍         | 64/1535 [02:34<59:22,  2.42s/it]  4%|▍         | 65/1535 [02:37<59:19,  2.42s/it]  4%|▍         | 66/1535 [02:39<59:16,  2.42s/it]  4%|▍         | 67/1535 [02:42<59:13,  2.42s/it]  4%|▍         | 68/1535 [02:44<59:28,  2.43s/it]  4%|▍         | 69/1535 [02:47<59:20,  2.43s/it]  5%|▍         | 70/1535 [02:49<59:17,  2.43s/it]  5%|▍         | 71/1535 [02:51<59:11,  2.43s/it]  5%|▍         | 72/1535 [02:54<59:06,  2.42s/it]  5%|▍         | 73/1535 [02:56<59:02,  2.42s/it]  5%|▍         | 74/1535 [02:59<58:58,  2.42s/it]  5%|▍         | 75/1535 [03:01<58:56,  2.42s/it]  5%|▍         | 76/1535 [03:04<58:52,  2.42s/it]  5%|▌         | 77/1535 [03:06<58:49,  2.42s/it]  5%|▌         | 78/1535 [03:08<58:47,  2.42s/it]  5%|▌         | 79/1535 [03:11<58:44,  2.42s/it]  5%|▌         | 80/1535 [03:13<58:43,  2.42s/it]                                                 {'loss': 0.9484, 'grad_norm': 0.3839850425720215, 'learning_rate': 0.00013029315960912054, 'epoch': 0.26}
  5%|▌         | 80/1535 [03:13<58:43,  2.42s/it]  5%|▌         | 81/1535 [03:16<58:41,  2.42s/it]  5%|▌         | 82/1535 [03:18<58:38,  2.42s/it]  5%|▌         | 83/1535 [03:21<58:45,  2.43s/it]  5%|▌         | 84/1535 [03:23<58:39,  2.43s/it]  6%|▌         | 85/1535 [03:25<58:35,  2.42s/it]  6%|▌         | 86/1535 [03:28<58:31,  2.42s/it]  6%|▌         | 87/1535 [03:30<58:26,  2.42s/it]  6%|▌         | 88/1535 [03:33<58:23,  2.42s/it]  6%|▌         | 89/1535 [03:35<58:21,  2.42s/it]  6%|▌         | 90/1535 [03:37<58:18,  2.42s/it]  6%|▌         | 91/1535 [03:40<58:15,  2.42s/it]  6%|▌         | 92/1535 [03:42<58:13,  2.42s/it]  6%|▌         | 93/1535 [03:45<58:10,  2.42s/it]  6%|▌         | 94/1535 [03:47<58:07,  2.42s/it]  6%|▌         | 95/1535 [03:50<58:05,  2.42s/it]  6%|▋         | 96/1535 [03:52<58:03,  2.42s/it]  6%|▋         | 97/1535 [03:54<58:00,  2.42s/it]  6%|▋         | 98/1535 [03:57<58:02,  2.42s/it]  6%|▋         | 99/1535 [03:59<57:59,  2.42s/it]  7%|▋         | 100/1535 [04:02<57:56,  2.42s/it]                                                  {'loss': 0.9398, 'grad_norm': 0.3480112552642822, 'learning_rate': 0.00016286644951140063, 'epoch': 0.33}
  7%|▋         | 100/1535 [04:02<57:56,  2.42s/it]  7%|▋         | 101/1535 [04:04<57:53,  2.42s/it]  7%|▋         | 102/1535 [04:07<57:50,  2.42s/it]  7%|▋         | 103/1535 [04:09<57:47,  2.42s/it]  7%|▋         | 104/1535 [04:11<57:44,  2.42s/it]  7%|▋         | 105/1535 [04:14<57:42,  2.42s/it]  7%|▋         | 106/1535 [04:16<57:39,  2.42s/it]  7%|▋         | 107/1535 [04:19<57:36,  2.42s/it]  7%|▋         | 108/1535 [04:21<57:34,  2.42s/it]  7%|▋         | 109/1535 [04:23<57:32,  2.42s/it]  7%|▋         | 110/1535 [04:26<57:29,  2.42s/it]  7%|▋         | 111/1535 [04:28<57:30,  2.42s/it]  7%|▋         | 112/1535 [04:31<57:43,  2.43s/it]  7%|▋         | 113/1535 [04:33<57:35,  2.43s/it]  7%|▋         | 114/1535 [04:36<57:28,  2.43s/it]  7%|▋         | 115/1535 [04:38<57:23,  2.43s/it]  8%|▊         | 116/1535 [04:40<57:19,  2.42s/it]  8%|▊         | 117/1535 [04:43<57:15,  2.42s/it]  8%|▊         | 118/1535 [04:45<57:13,  2.42s/it]  8%|▊         | 119/1535 [04:48<57:09,  2.42s/it]  8%|▊         | 120/1535 [04:50<57:05,  2.42s/it]                                                  {'loss': 0.9182, 'grad_norm': 0.3665439784526825, 'learning_rate': 0.00019543973941368078, 'epoch': 0.39}
  8%|▊         | 120/1535 [04:50<57:05,  2.42s/it]  8%|▊         | 121/1535 [04:53<57:03,  2.42s/it]  8%|▊         | 122/1535 [04:55<57:00,  2.42s/it]  8%|▊         | 123/1535 [04:57<56:56,  2.42s/it]  8%|▊         | 124/1535 [05:00<56:53,  2.42s/it]  8%|▊         | 125/1535 [05:02<56:50,  2.42s/it]  8%|▊         | 126/1535 [05:05<57:18,  2.44s/it]  8%|▊         | 127/1535 [05:07<57:06,  2.43s/it]  8%|▊         | 128/1535 [05:10<56:57,  2.43s/it]  8%|▊         | 129/1535 [05:12<56:50,  2.43s/it]  8%|▊         | 130/1535 [05:14<56:45,  2.42s/it]  9%|▊         | 131/1535 [05:17<56:40,  2.42s/it]  9%|▊         | 132/1535 [05:19<56:37,  2.42s/it]  9%|▊         | 133/1535 [05:22<56:33,  2.42s/it]  9%|▊         | 134/1535 [05:24<56:30,  2.42s/it]  9%|▉         | 135/1535 [05:27<56:27,  2.42s/it]  9%|▉         | 136/1535 [05:29<56:24,  2.42s/it]  9%|▉         | 137/1535 [05:31<56:21,  2.42s/it]  9%|▉         | 138/1535 [05:34<56:19,  2.42s/it]  9%|▉         | 139/1535 [05:36<56:33,  2.43s/it]  9%|▉         | 140/1535 [05:39<56:25,  2.43s/it]                                                  {'loss': 0.9011, 'grad_norm': 0.32195115089416504, 'learning_rate': 0.0002280130293159609, 'epoch': 0.46}
  9%|▉         | 140/1535 [05:39<56:25,  2.43s/it]  9%|▉         | 141/1535 [05:41<56:29,  2.43s/it]  9%|▉         | 142/1535 [05:43<56:21,  2.43s/it]  9%|▉         | 143/1535 [05:46<56:15,  2.43s/it]  9%|▉         | 144/1535 [05:48<56:10,  2.42s/it]  9%|▉         | 145/1535 [05:51<56:06,  2.42s/it] 10%|▉         | 146/1535 [05:53<56:02,  2.42s/it] 10%|▉         | 147/1535 [05:56<55:59,  2.42s/it] 10%|▉         | 148/1535 [05:58<55:56,  2.42s/it] 10%|▉         | 149/1535 [06:00<55:53,  2.42s/it] 10%|▉         | 150/1535 [06:03<55:50,  2.42s/it] 10%|▉         | 151/1535 [06:05<55:47,  2.42s/it] 10%|▉         | 152/1535 [06:08<55:46,  2.42s/it] 10%|▉         | 153/1535 [06:10<55:43,  2.42s/it] 10%|█         | 154/1535 [06:13<55:41,  2.42s/it] 10%|█         | 155/1535 [06:15<55:38,  2.42s/it] 10%|█         | 156/1535 [06:17<55:35,  2.42s/it] 10%|█         | 157/1535 [06:20<55:34,  2.42s/it] 10%|█         | 158/1535 [06:22<55:31,  2.42s/it] 10%|█         | 159/1535 [06:25<55:33,  2.42s/it] 10%|█         | 160/1535 [06:27<55:30,  2.42s/it]                                                  {'loss': 0.8921, 'grad_norm': 0.27384600043296814, 'learning_rate': 0.0002605863192182411, 'epoch': 0.52}
 10%|█         | 160/1535 [06:27<55:30,  2.42s/it] 10%|█         | 161/1535 [06:29<55:28,  2.42s/it] 11%|█         | 162/1535 [06:32<55:26,  2.42s/it] 11%|█         | 163/1535 [06:34<55:24,  2.42s/it] 11%|█         | 164/1535 [06:37<55:20,  2.42s/it] 11%|█         | 165/1535 [06:39<55:17,  2.42s/it] 11%|█         | 166/1535 [06:42<55:14,  2.42s/it] 11%|█         | 167/1535 [06:44<55:11,  2.42s/it] 11%|█         | 168/1535 [06:46<55:20,  2.43s/it] 11%|█         | 169/1535 [06:49<55:14,  2.43s/it] 11%|█         | 170/1535 [06:51<55:18,  2.43s/it] 11%|█         | 171/1535 [06:54<55:11,  2.43s/it] 11%|█         | 172/1535 [06:56<55:06,  2.43s/it] 11%|█▏        | 173/1535 [06:59<55:01,  2.42s/it] 11%|█▏        | 174/1535 [07:01<54:57,  2.42s/it] 11%|█▏        | 175/1535 [07:03<54:55,  2.42s/it] 11%|█▏        | 176/1535 [07:06<54:51,  2.42s/it] 12%|█▏        | 177/1535 [07:08<54:48,  2.42s/it] 12%|█▏        | 178/1535 [07:11<54:46,  2.42s/it] 12%|█▏        | 179/1535 [07:13<54:42,  2.42s/it] 12%|█▏        | 180/1535 [07:16<54:40,  2.42s/it]                                                  {'loss': 0.8794, 'grad_norm': 0.273595929145813, 'learning_rate': 0.0002931596091205212, 'epoch': 0.59}
 12%|█▏        | 180/1535 [07:16<54:40,  2.42s/it] 12%|█▏        | 181/1535 [07:18<54:47,  2.43s/it] 12%|█▏        | 182/1535 [07:20<54:41,  2.43s/it] 12%|█▏        | 183/1535 [07:23<54:37,  2.42s/it] 12%|█▏        | 184/1535 [07:25<54:33,  2.42s/it] 12%|█▏        | 185/1535 [07:28<54:30,  2.42s/it] 12%|█▏        | 186/1535 [07:30<54:27,  2.42s/it] 12%|█▏        | 187/1535 [07:32<54:24,  2.42s/it] 12%|█▏        | 188/1535 [07:35<54:21,  2.42s/it] 12%|█▏        | 189/1535 [07:37<54:18,  2.42s/it] 12%|█▏        | 190/1535 [07:40<54:16,  2.42s/it] 12%|█▏        | 191/1535 [07:42<54:13,  2.42s/it] 13%|█▎        | 192/1535 [07:45<54:11,  2.42s/it] 13%|█▎        | 193/1535 [07:47<54:09,  2.42s/it] 13%|█▎        | 194/1535 [07:49<54:06,  2.42s/it] 13%|█▎        | 195/1535 [07:52<54:31,  2.44s/it] 13%|█▎        | 196/1535 [07:54<54:20,  2.44s/it] 13%|█▎        | 197/1535 [07:57<54:12,  2.43s/it] 13%|█▎        | 198/1535 [07:59<54:05,  2.43s/it] 13%|█▎        | 199/1535 [08:02<54:00,  2.43s/it] 13%|█▎        | 200/1535 [08:04<53:56,  2.42s/it]                                                  {'loss': 0.869, 'grad_norm': 0.5656566619873047, 'learning_rate': 0.00032573289902280126, 'epoch': 0.65}
 13%|█▎        | 200/1535 [08:04<53:56,  2.42s/it] 13%|█▎        | 201/1535 [08:06<53:53,  2.42s/it] 13%|█▎        | 202/1535 [08:09<53:49,  2.42s/it] 13%|█▎        | 203/1535 [08:11<53:46,  2.42s/it] 13%|█▎        | 204/1535 [08:14<53:43,  2.42s/it] 13%|█▎        | 205/1535 [08:16<53:40,  2.42s/it] 13%|█▎        | 206/1535 [08:19<53:37,  2.42s/it] 13%|█▎        | 207/1535 [08:21<53:35,  2.42s/it] 14%|█▎        | 208/1535 [08:23<53:32,  2.42s/it] 14%|█▎        | 209/1535 [08:26<53:30,  2.42s/it] 14%|█▎        | 210/1535 [08:28<53:28,  2.42s/it] 14%|█▎        | 211/1535 [08:31<53:25,  2.42s/it] 14%|█▍        | 212/1535 [08:33<53:22,  2.42s/it] 14%|█▍        | 213/1535 [08:35<53:20,  2.42s/it] 14%|█▍        | 214/1535 [08:38<53:17,  2.42s/it] 14%|█▍        | 215/1535 [08:40<53:14,  2.42s/it] 14%|█▍        | 216/1535 [08:43<53:12,  2.42s/it] 14%|█▍        | 217/1535 [08:45<53:10,  2.42s/it] 14%|█▍        | 218/1535 [08:48<53:08,  2.42s/it] 14%|█▍        | 219/1535 [08:50<53:05,  2.42s/it] 14%|█▍        | 220/1535 [08:52<53:03,  2.42s/it]                                                  {'loss': 0.8115, 'grad_norm': 0.3522568941116333, 'learning_rate': 0.00035830618892508144, 'epoch': 0.72}
 14%|█▍        | 220/1535 [08:52<53:03,  2.42s/it] 14%|█▍        | 221/1535 [08:55<53:03,  2.42s/it] 14%|█▍        | 222/1535 [08:57<53:11,  2.43s/it] 15%|█▍        | 223/1535 [09:00<53:08,  2.43s/it] 15%|█▍        | 224/1535 [09:02<53:04,  2.43s/it] 15%|█▍        | 225/1535 [09:05<53:02,  2.43s/it] 15%|█▍        | 226/1535 [09:07<52:59,  2.43s/it] 15%|█▍        | 227/1535 [09:09<52:59,  2.43s/it] 15%|█▍        | 228/1535 [09:12<53:10,  2.44s/it] 15%|█▍        | 229/1535 [09:14<53:00,  2.44s/it] 15%|█▍        | 230/1535 [09:17<52:55,  2.43s/it] 15%|█▌        | 231/1535 [09:19<52:51,  2.43s/it] 15%|█▌        | 232/1535 [09:22<52:46,  2.43s/it] 15%|█▌        | 233/1535 [09:24<52:43,  2.43s/it] 15%|█▌        | 234/1535 [09:26<52:40,  2.43s/it] 15%|█▌        | 235/1535 [09:29<52:40,  2.43s/it] 15%|█▌        | 236/1535 [09:31<52:39,  2.43s/it] 15%|█▌        | 237/1535 [09:34<52:49,  2.44s/it] 16%|█▌        | 238/1535 [09:36<52:41,  2.44s/it] 16%|█▌        | 239/1535 [09:39<52:35,  2.43s/it] 16%|█▌        | 240/1535 [09:41<52:29,  2.43s/it]                                                  {'loss': 0.8413, 'grad_norm': 0.28503161668777466, 'learning_rate': 0.0003892508143322476, 'epoch': 0.78}
 16%|█▌        | 240/1535 [09:41<52:29,  2.43s/it] 16%|█▌        | 241/1535 [09:44<52:29,  2.43s/it] 16%|█▌        | 242/1535 [09:46<52:23,  2.43s/it] 16%|█▌        | 243/1535 [09:48<52:19,  2.43s/it] 16%|█▌        | 244/1535 [09:51<52:16,  2.43s/it] 16%|█▌        | 245/1535 [09:53<52:12,  2.43s/it] 16%|█▌        | 246/1535 [09:56<52:10,  2.43s/it] 16%|█▌        | 247/1535 [09:58<52:08,  2.43s/it] 16%|█▌        | 248/1535 [10:01<52:05,  2.43s/it] 16%|█▌        | 249/1535 [10:03<52:01,  2.43s/it] 16%|█▋        | 250/1535 [10:05<52:04,  2.43s/it] 16%|█▋        | 251/1535 [10:08<52:00,  2.43s/it] 16%|█▋        | 252/1535 [10:10<51:57,  2.43s/it] 16%|█▋        | 253/1535 [10:13<51:54,  2.43s/it] 17%|█▋        | 254/1535 [10:15<51:54,  2.43s/it] 17%|█▋        | 255/1535 [10:18<51:51,  2.43s/it] 17%|█▋        | 256/1535 [10:20<51:47,  2.43s/it] 17%|█▋        | 257/1535 [10:22<51:56,  2.44s/it] 17%|█▋        | 258/1535 [10:25<51:49,  2.43s/it] 17%|█▋        | 259/1535 [10:27<51:43,  2.43s/it] 17%|█▋        | 260/1535 [10:30<51:39,  2.43s/it]                                                  {'loss': 0.8306, 'grad_norm': 0.3644712269306183, 'learning_rate': 0.0004218241042345277, 'epoch': 0.85}
 17%|█▋        | 260/1535 [10:30<51:39,  2.43s/it] 17%|█▋        | 261/1535 [10:32<51:35,  2.43s/it] 17%|█▋        | 262/1535 [10:35<51:32,  2.43s/it] 17%|█▋        | 263/1535 [10:37<51:29,  2.43s/it] 17%|█▋        | 264/1535 [10:39<51:25,  2.43s/it] 17%|█▋        | 265/1535 [10:42<51:48,  2.45s/it] 17%|█▋        | 266/1535 [10:44<51:37,  2.44s/it] 17%|█▋        | 267/1535 [10:47<51:30,  2.44s/it] 17%|█▋        | 268/1535 [10:49<51:24,  2.43s/it] 18%|█▊        | 269/1535 [10:52<51:21,  2.43s/it] 18%|█▊        | 270/1535 [10:54<51:17,  2.43s/it] 18%|█▊        | 271/1535 [10:57<51:23,  2.44s/it] 18%|█▊        | 272/1535 [10:59<51:17,  2.44s/it] 18%|█▊        | 273/1535 [11:01<51:13,  2.44s/it] 18%|█▊        | 274/1535 [11:04<51:07,  2.43s/it] 18%|█▊        | 275/1535 [11:06<51:04,  2.43s/it] 18%|█▊        | 276/1535 [11:09<51:00,  2.43s/it] 18%|█▊        | 277/1535 [11:11<50:56,  2.43s/it] 18%|█▊        | 278/1535 [11:14<51:07,  2.44s/it] 18%|█▊        | 279/1535 [11:16<50:59,  2.44s/it] 18%|█▊        | 280/1535 [11:18<50:53,  2.43s/it]                                                  {'loss': 0.8388, 'grad_norm': 0.5672602653503418, 'learning_rate': 0.0004543973941368078, 'epoch': 0.91}
 18%|█▊        | 280/1535 [11:18<50:53,  2.43s/it] 18%|█▊        | 281/1535 [11:21<50:51,  2.43s/it] 18%|█▊        | 282/1535 [11:23<50:47,  2.43s/it] 18%|█▊        | 283/1535 [11:26<50:42,  2.43s/it] 19%|█▊        | 284/1535 [11:28<50:38,  2.43s/it] 19%|█▊        | 285/1535 [11:31<50:36,  2.43s/it] 19%|█▊        | 286/1535 [11:33<50:34,  2.43s/it] 19%|█▊        | 287/1535 [11:35<50:31,  2.43s/it] 19%|█▉        | 288/1535 [11:38<50:29,  2.43s/it] 19%|█▉        | 289/1535 [11:40<50:27,  2.43s/it] 19%|█▉        | 290/1535 [11:43<50:24,  2.43s/it] 19%|█▉        | 291/1535 [11:45<50:21,  2.43s/it] 19%|█▉        | 292/1535 [11:48<50:18,  2.43s/it] 19%|█▉        | 293/1535 [11:50<50:15,  2.43s/it] 19%|█▉        | 294/1535 [11:52<50:13,  2.43s/it] 19%|█▉        | 295/1535 [11:55<50:11,  2.43s/it] 19%|█▉        | 296/1535 [11:57<50:08,  2.43s/it] 19%|█▉        | 297/1535 [12:00<50:05,  2.43s/it] 19%|█▉        | 298/1535 [12:02<50:03,  2.43s/it] 19%|█▉        | 299/1535 [12:05<50:00,  2.43s/it] 20%|█▉        | 300/1535 [12:07<50:14,  2.44s/it]                                                  {'loss': 0.8518, 'grad_norm': 0.4071104824542999, 'learning_rate': 0.00048697068403908794, 'epoch': 0.98}
 20%|█▉        | 300/1535 [12:07<50:14,  2.44s/it] 20%|█▉        | 301/1535 [12:09<50:07,  2.44s/it] 20%|█▉        | 302/1535 [12:12<50:00,  2.43s/it] 20%|█▉        | 303/1535 [12:14<49:55,  2.43s/it] 20%|█▉        | 304/1535 [12:17<49:52,  2.43s/it] 20%|█▉        | 305/1535 [12:19<49:49,  2.43s/it] 20%|█▉        | 306/1535 [12:22<49:45,  2.43s/it] 20%|██        | 307/1535 [12:24<49:47,  2.43s/it] 20%|██        | 308/1535 [12:26<49:46,  2.43s/it] 20%|██        | 309/1535 [12:29<49:41,  2.43s/it] 20%|██        | 310/1535 [12:31<49:39,  2.43s/it] 20%|██        | 311/1535 [12:34<49:34,  2.43s/it] 20%|██        | 312/1535 [12:36<49:29,  2.43s/it] 20%|██        | 313/1535 [12:39<49:25,  2.43s/it] 20%|██        | 314/1535 [12:41<49:25,  2.43s/it] 21%|██        | 315/1535 [12:44<49:34,  2.44s/it] 21%|██        | 316/1535 [12:46<49:26,  2.43s/it] 21%|██        | 317/1535 [12:48<49:22,  2.43s/it] 21%|██        | 318/1535 [12:51<49:17,  2.43s/it] 21%|██        | 319/1535 [12:53<49:14,  2.43s/it] 21%|██        | 320/1535 [12:56<49:22,  2.44s/it]                                                  {'loss': 0.7975, 'grad_norm': 0.421007364988327, 'learning_rate': 0.0004998822010531848, 'epoch': 1.04}
 21%|██        | 320/1535 [12:56<49:22,  2.44s/it] 21%|██        | 321/1535 [12:58<49:17,  2.44s/it] 21%|██        | 322/1535 [13:01<49:10,  2.43s/it] 21%|██        | 323/1535 [13:03<49:05,  2.43s/it] 21%|██        | 324/1535 [13:05<49:00,  2.43s/it] 21%|██        | 325/1535 [13:08<48:59,  2.43s/it] 21%|██        | 326/1535 [13:10<48:56,  2.43s/it] 21%|██▏       | 327/1535 [13:13<48:52,  2.43s/it] 21%|██▏       | 328/1535 [13:15<48:48,  2.43s/it] 21%|██▏       | 329/1535 [13:18<48:57,  2.44s/it] 21%|██▏       | 330/1535 [13:20<48:54,  2.44s/it] 22%|██▏       | 331/1535 [13:22<48:48,  2.43s/it] 22%|██▏       | 332/1535 [13:25<48:44,  2.43s/it] 22%|██▏       | 333/1535 [13:27<48:40,  2.43s/it] 22%|██▏       | 334/1535 [13:30<48:54,  2.44s/it] 22%|██▏       | 335/1535 [13:32<48:46,  2.44s/it] 22%|██▏       | 336/1535 [13:35<48:40,  2.44s/it] 22%|██▏       | 337/1535 [13:37<48:34,  2.43s/it] 22%|██▏       | 338/1535 [13:39<48:28,  2.43s/it] 22%|██▏       | 339/1535 [13:42<48:25,  2.43s/it] 22%|██▏       | 340/1535 [13:44<48:21,  2.43s/it]                                                  {'loss': 0.8239, 'grad_norm': 0.33487609028816223, 'learning_rate': 0.0004991627205825621, 'epoch': 1.11}
 22%|██▏       | 340/1535 [13:44<48:21,  2.43s/it] 22%|██▏       | 341/1535 [13:47<48:19,  2.43s/it] 22%|██▏       | 342/1535 [13:49<48:18,  2.43s/it] 22%|██▏       | 343/1535 [13:52<48:18,  2.43s/it] 22%|██▏       | 344/1535 [13:54<48:24,  2.44s/it] 22%|██▏       | 345/1535 [13:56<48:18,  2.44s/it] 23%|██▎       | 346/1535 [13:59<48:11,  2.43s/it] 23%|██▎       | 347/1535 [14:01<48:08,  2.43s/it] 23%|██▎       | 348/1535 [14:04<48:25,  2.45s/it] 23%|██▎       | 349/1535 [14:06<48:14,  2.44s/it] 23%|██▎       | 350/1535 [14:09<48:04,  2.43s/it] 23%|██▎       | 351/1535 [14:11<48:00,  2.43s/it] 23%|██▎       | 352/1535 [14:14<47:56,  2.43s/it] 23%|██▎       | 353/1535 [14:16<47:51,  2.43s/it] 23%|██▎       | 354/1535 [14:18<47:48,  2.43s/it] 23%|██▎       | 355/1535 [14:21<47:44,  2.43s/it] 23%|██▎       | 356/1535 [14:23<47:40,  2.43s/it] 23%|██▎       | 357/1535 [14:26<47:37,  2.43s/it] 23%|██▎       | 358/1535 [14:28<47:40,  2.43s/it] 23%|██▎       | 359/1535 [14:30<47:36,  2.43s/it] 23%|██▎       | 360/1535 [14:33<47:32,  2.43s/it]                                                  {'loss': 0.7519, 'grad_norm': 0.4019373953342438, 'learning_rate': 0.0004978751063614575, 'epoch': 1.17}
 23%|██▎       | 360/1535 [14:33<47:32,  2.43s/it] 24%|██▎       | 361/1535 [14:35<47:31,  2.43s/it] 24%|██▎       | 362/1535 [14:38<47:28,  2.43s/it] 24%|██▎       | 363/1535 [14:40<47:24,  2.43s/it] 24%|██▎       | 364/1535 [14:43<47:20,  2.43s/it] 24%|██▍       | 365/1535 [14:45<47:19,  2.43s/it] 24%|██▍       | 366/1535 [14:47<47:18,  2.43s/it] 24%|██▍       | 367/1535 [14:50<47:18,  2.43s/it] 24%|██▍       | 368/1535 [14:52<47:15,  2.43s/it] 24%|██▍       | 369/1535 [14:55<47:11,  2.43s/it] 24%|██▍       | 370/1535 [14:57<47:07,  2.43s/it] 24%|██▍       | 371/1535 [15:00<47:04,  2.43s/it] 24%|██▍       | 372/1535 [15:02<47:02,  2.43s/it] 24%|██▍       | 373/1535 [15:05<47:11,  2.44s/it] 24%|██▍       | 374/1535 [15:07<47:05,  2.43s/it] 24%|██▍       | 375/1535 [15:09<47:01,  2.43s/it] 24%|██▍       | 376/1535 [15:12<46:57,  2.43s/it] 25%|██▍       | 377/1535 [15:14<46:52,  2.43s/it] 25%|██▍       | 378/1535 [15:17<46:52,  2.43s/it] 25%|██▍       | 379/1535 [15:19<46:48,  2.43s/it] 25%|██▍       | 380/1535 [15:22<46:42,  2.43s/it]                                                  {'loss': 0.7089, 'grad_norm': 0.3233855366706848, 'learning_rate': 0.0004958872212869485, 'epoch': 1.24}
 25%|██▍       | 380/1535 [15:22<46:42,  2.43s/it] 25%|██▍       | 381/1535 [15:24<46:38,  2.43s/it] 25%|██▍       | 382/1535 [15:26<46:34,  2.42s/it] 25%|██▍       | 383/1535 [15:29<46:29,  2.42s/it] 25%|██▌       | 384/1535 [15:31<46:26,  2.42s/it] 25%|██▌       | 385/1535 [15:34<46:22,  2.42s/it] 25%|██▌       | 386/1535 [15:36<46:20,  2.42s/it] 25%|██▌       | 387/1535 [15:38<46:16,  2.42s/it] 25%|██▌       | 388/1535 [15:41<46:14,  2.42s/it] 25%|██▌       | 389/1535 [15:43<46:13,  2.42s/it] 25%|██▌       | 390/1535 [15:46<46:10,  2.42s/it] 25%|██▌       | 391/1535 [15:48<46:07,  2.42s/it] 26%|██▌       | 392/1535 [15:51<46:04,  2.42s/it] 26%|██▌       | 393/1535 [15:53<46:01,  2.42s/it] 26%|██▌       | 394/1535 [15:55<45:59,  2.42s/it] 26%|██▌       | 395/1535 [15:58<45:56,  2.42s/it] 26%|██▌       | 396/1535 [16:00<45:54,  2.42s/it] 26%|██▌       | 397/1535 [16:03<45:51,  2.42s/it] 26%|██▌       | 398/1535 [16:05<45:49,  2.42s/it] 26%|██▌       | 399/1535 [16:07<45:47,  2.42s/it] 26%|██▌       | 400/1535 [16:10<45:44,  2.42s/it]                                                  {'loss': 0.7683, 'grad_norm': 0.4505038559436798, 'learning_rate': 0.0004932557537323291, 'epoch': 1.3}
 26%|██▌       | 400/1535 [16:10<45:44,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 06:34:39,842 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-400
[INFO|configuration_utils.py:726] 2024-05-25 06:34:41,198 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 06:34:41,203 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 06:34:41,290 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 06:34:41,290 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-400/special_tokens_map.json
 26%|██▌       | 401/1535 [16:14<54:48,  2.90s/it] 26%|██▌       | 402/1535 [16:16<52:01,  2.76s/it] 26%|██▋       | 403/1535 [16:19<50:04,  2.65s/it] 26%|██▋       | 404/1535 [16:21<49:05,  2.60s/it] 26%|██▋       | 405/1535 [16:24<48:00,  2.55s/it] 26%|██▋       | 406/1535 [16:26<47:13,  2.51s/it] 27%|██▋       | 407/1535 [16:28<46:40,  2.48s/it] 27%|██▋       | 408/1535 [16:31<46:16,  2.46s/it] 27%|██▋       | 409/1535 [16:33<45:58,  2.45s/it] 27%|██▋       | 410/1535 [16:36<45:46,  2.44s/it] 27%|██▋       | 411/1535 [16:38<45:35,  2.43s/it] 27%|██▋       | 412/1535 [16:41<45:28,  2.43s/it] 27%|██▋       | 413/1535 [16:43<45:21,  2.43s/it] 27%|██▋       | 414/1535 [16:45<45:16,  2.42s/it] 27%|██▋       | 415/1535 [16:48<45:56,  2.46s/it] 27%|██▋       | 416/1535 [16:50<45:39,  2.45s/it] 27%|██▋       | 417/1535 [16:53<45:26,  2.44s/it] 27%|██▋       | 418/1535 [16:55<45:20,  2.44s/it] 27%|██▋       | 419/1535 [16:58<45:13,  2.43s/it] 27%|██▋       | 420/1535 [17:00<45:06,  2.43s/it]                                                  {'loss': 0.7405, 'grad_norm': 0.5857354998588562, 'learning_rate': 0.0004899875912715296, 'epoch': 1.37}
 27%|██▋       | 420/1535 [17:00<45:06,  2.43s/it] 27%|██▋       | 421/1535 [17:02<45:01,  2.43s/it] 27%|██▋       | 422/1535 [17:05<44:57,  2.42s/it] 28%|██▊       | 423/1535 [17:07<44:53,  2.42s/it] 28%|██▊       | 424/1535 [17:10<44:49,  2.42s/it] 28%|██▊       | 425/1535 [17:12<44:45,  2.42s/it] 28%|██▊       | 426/1535 [17:15<44:42,  2.42s/it] 28%|██▊       | 427/1535 [17:17<44:41,  2.42s/it] 28%|██▊       | 428/1535 [17:19<44:38,  2.42s/it] 28%|██▊       | 429/1535 [17:22<44:35,  2.42s/it] 28%|██▊       | 430/1535 [17:24<44:45,  2.43s/it] 28%|██▊       | 431/1535 [17:27<44:38,  2.43s/it] 28%|██▊       | 432/1535 [17:29<44:33,  2.42s/it] 28%|██▊       | 433/1535 [17:32<44:29,  2.42s/it] 28%|██▊       | 434/1535 [17:34<44:25,  2.42s/it] 28%|██▊       | 435/1535 [17:36<44:21,  2.42s/it] 28%|██▊       | 436/1535 [17:39<44:19,  2.42s/it] 28%|██▊       | 437/1535 [17:41<44:16,  2.42s/it] 29%|██▊       | 438/1535 [17:44<44:13,  2.42s/it] 29%|██▊       | 439/1535 [17:46<44:10,  2.42s/it] 29%|██▊       | 440/1535 [17:48<44:08,  2.42s/it]                                                  {'loss': 0.7827, 'grad_norm': 0.3485219180583954, 'learning_rate': 0.0004860912879566511, 'epoch': 1.43}
 29%|██▊       | 440/1535 [17:48<44:08,  2.42s/it] 29%|██▊       | 441/1535 [17:51<44:06,  2.42s/it] 29%|██▉       | 442/1535 [17:53<44:03,  2.42s/it] 29%|██▉       | 443/1535 [17:56<44:02,  2.42s/it] 29%|██▉       | 444/1535 [17:58<43:59,  2.42s/it] 29%|██▉       | 445/1535 [18:01<43:56,  2.42s/it] 29%|██▉       | 446/1535 [18:03<44:03,  2.43s/it] 29%|██▉       | 447/1535 [18:05<43:57,  2.42s/it] 29%|██▉       | 448/1535 [18:08<43:52,  2.42s/it] 29%|██▉       | 449/1535 [18:10<43:49,  2.42s/it] 29%|██▉       | 450/1535 [18:13<43:45,  2.42s/it] 29%|██▉       | 451/1535 [18:15<43:42,  2.42s/it] 29%|██▉       | 452/1535 [18:18<43:40,  2.42s/it] 30%|██▉       | 453/1535 [18:20<43:38,  2.42s/it] 30%|██▉       | 454/1535 [18:22<43:35,  2.42s/it] 30%|██▉       | 455/1535 [18:25<43:32,  2.42s/it] 30%|██▉       | 456/1535 [18:27<43:29,  2.42s/it] 30%|██▉       | 457/1535 [18:30<43:27,  2.42s/it] 30%|██▉       | 458/1535 [18:32<43:24,  2.42s/it] 30%|██▉       | 459/1535 [18:34<43:32,  2.43s/it] 30%|██▉       | 460/1535 [18:37<43:27,  2.43s/it]                                                  {'loss': 0.7592, 'grad_norm': 0.38668933510780334, 'learning_rate': 0.000481577041928685, 'epoch': 1.5}
 30%|██▉       | 460/1535 [18:37<43:27,  2.43s/it] 30%|███       | 461/1535 [18:39<43:23,  2.42s/it] 30%|███       | 462/1535 [18:42<43:18,  2.42s/it] 30%|███       | 463/1535 [18:44<43:15,  2.42s/it] 30%|███       | 464/1535 [18:47<43:12,  2.42s/it] 30%|███       | 465/1535 [18:49<43:08,  2.42s/it] 30%|███       | 466/1535 [18:51<43:06,  2.42s/it] 30%|███       | 467/1535 [18:54<43:03,  2.42s/it] 30%|███       | 468/1535 [18:56<43:00,  2.42s/it] 31%|███       | 469/1535 [18:59<42:57,  2.42s/it] 31%|███       | 470/1535 [19:01<42:55,  2.42s/it] 31%|███       | 471/1535 [19:03<42:53,  2.42s/it] 31%|███       | 472/1535 [19:06<42:54,  2.42s/it] 31%|███       | 473/1535 [19:08<43:06,  2.44s/it] 31%|███       | 474/1535 [19:11<42:58,  2.43s/it] 31%|███       | 475/1535 [19:13<42:52,  2.43s/it] 31%|███       | 476/1535 [19:16<42:47,  2.42s/it] 31%|███       | 477/1535 [19:18<42:42,  2.42s/it] 31%|███       | 478/1535 [19:20<42:38,  2.42s/it] 31%|███       | 479/1535 [19:23<42:35,  2.42s/it] 31%|███▏      | 480/1535 [19:25<42:32,  2.42s/it]                                                  {'loss': 0.7399, 'grad_norm': 0.4345325827598572, 'learning_rate': 0.000476456668725012, 'epoch': 1.56}
 31%|███▏      | 480/1535 [19:25<42:32,  2.42s/it] 31%|███▏      | 481/1535 [19:28<42:30,  2.42s/it] 31%|███▏      | 482/1535 [19:30<42:27,  2.42s/it] 31%|███▏      | 483/1535 [19:33<42:24,  2.42s/it] 32%|███▏      | 484/1535 [19:35<42:22,  2.42s/it] 32%|███▏      | 485/1535 [19:37<42:19,  2.42s/it] 32%|███▏      | 486/1535 [19:40<42:16,  2.42s/it] 32%|███▏      | 487/1535 [19:42<42:23,  2.43s/it] 32%|███▏      | 488/1535 [19:45<42:20,  2.43s/it] 32%|███▏      | 489/1535 [19:47<42:15,  2.42s/it] 32%|███▏      | 490/1535 [19:50<42:11,  2.42s/it] 32%|███▏      | 491/1535 [19:52<42:07,  2.42s/it] 32%|███▏      | 492/1535 [19:54<42:04,  2.42s/it] 32%|███▏      | 493/1535 [19:57<42:00,  2.42s/it] 32%|███▏      | 494/1535 [19:59<41:58,  2.42s/it] 32%|███▏      | 495/1535 [20:02<41:55,  2.42s/it] 32%|███▏      | 496/1535 [20:04<41:53,  2.42s/it] 32%|███▏      | 497/1535 [20:06<41:50,  2.42s/it] 32%|███▏      | 498/1535 [20:09<41:48,  2.42s/it] 33%|███▎      | 499/1535 [20:11<41:45,  2.42s/it] 33%|███▎      | 500/1535 [20:14<41:44,  2.42s/it]                                                  {'loss': 0.7225, 'grad_norm': 0.3290550410747528, 'learning_rate': 0.0004707435703535453, 'epoch': 1.63}
 33%|███▎      | 500/1535 [20:14<41:44,  2.42s/it] 33%|███▎      | 501/1535 [20:16<41:47,  2.42s/it] 33%|███▎      | 502/1535 [20:19<41:42,  2.42s/it] 33%|███▎      | 503/1535 [20:21<41:39,  2.42s/it] 33%|███▎      | 504/1535 [20:23<41:35,  2.42s/it] 33%|███▎      | 505/1535 [20:26<41:32,  2.42s/it] 33%|███▎      | 506/1535 [20:28<41:29,  2.42s/it] 33%|███▎      | 507/1535 [20:31<41:26,  2.42s/it] 33%|███▎      | 508/1535 [20:33<41:24,  2.42s/it] 33%|███▎      | 509/1535 [20:36<41:21,  2.42s/it] 33%|███▎      | 510/1535 [20:38<41:19,  2.42s/it] 33%|███▎      | 511/1535 [20:40<41:16,  2.42s/it] 33%|███▎      | 512/1535 [20:43<41:13,  2.42s/it] 33%|███▎      | 513/1535 [20:45<41:12,  2.42s/it] 33%|███▎      | 514/1535 [20:48<41:09,  2.42s/it] 34%|███▎      | 515/1535 [20:50<41:14,  2.43s/it] 34%|███▎      | 516/1535 [20:52<41:10,  2.42s/it] 34%|███▎      | 517/1535 [20:55<41:06,  2.42s/it] 34%|███▎      | 518/1535 [20:57<41:02,  2.42s/it] 34%|███▍      | 519/1535 [21:00<40:58,  2.42s/it] 34%|███▍      | 520/1535 [21:02<40:55,  2.42s/it]                                                  {'loss': 0.7036, 'grad_norm': 0.43005672097206116, 'learning_rate': 0.00046445270021446504, 'epoch': 1.69}
 34%|███▍      | 520/1535 [21:02<40:55,  2.42s/it] 34%|███▍      | 521/1535 [21:05<40:53,  2.42s/it] 34%|███▍      | 522/1535 [21:07<40:51,  2.42s/it] 34%|███▍      | 523/1535 [21:09<40:48,  2.42s/it] 34%|███▍      | 524/1535 [21:12<40:45,  2.42s/it] 34%|███▍      | 525/1535 [21:14<40:43,  2.42s/it] 34%|███▍      | 526/1535 [21:17<40:42,  2.42s/it] 34%|███▍      | 527/1535 [21:19<40:39,  2.42s/it] 34%|███▍      | 528/1535 [21:22<40:44,  2.43s/it] 34%|███▍      | 529/1535 [21:24<40:39,  2.42s/it] 35%|███▍      | 530/1535 [21:26<40:36,  2.42s/it] 35%|███▍      | 531/1535 [21:29<40:32,  2.42s/it] 35%|███▍      | 532/1535 [21:31<40:28,  2.42s/it] 35%|███▍      | 533/1535 [21:34<40:24,  2.42s/it] 35%|███▍      | 534/1535 [21:36<40:21,  2.42s/it] 35%|███▍      | 535/1535 [21:38<40:18,  2.42s/it] 35%|███▍      | 536/1535 [21:41<40:16,  2.42s/it] 35%|███▍      | 537/1535 [21:43<40:13,  2.42s/it] 35%|███▌      | 538/1535 [21:46<40:12,  2.42s/it] 35%|███▌      | 539/1535 [21:48<40:09,  2.42s/it] 35%|███▌      | 540/1535 [21:51<40:06,  2.42s/it]                                                  {'loss': 0.6789, 'grad_norm': 0.42975887656211853, 'learning_rate': 0.00045760052396135386, 'epoch': 1.76}
 35%|███▌      | 540/1535 [21:51<40:06,  2.42s/it] 35%|███▌      | 541/1535 [21:53<40:04,  2.42s/it] 35%|███▌      | 542/1535 [21:55<40:01,  2.42s/it] 35%|███▌      | 543/1535 [21:58<40:14,  2.43s/it] 35%|███▌      | 544/1535 [22:00<40:07,  2.43s/it] 36%|███▌      | 545/1535 [22:03<40:01,  2.43s/it] 36%|███▌      | 546/1535 [22:05<40:04,  2.43s/it] 36%|███▌      | 547/1535 [22:08<39:57,  2.43s/it] 36%|███▌      | 548/1535 [22:10<39:53,  2.42s/it] 36%|███▌      | 549/1535 [22:12<39:48,  2.42s/it] 36%|███▌      | 550/1535 [22:15<39:45,  2.42s/it] 36%|███▌      | 551/1535 [22:17<39:41,  2.42s/it] 36%|███▌      | 552/1535 [22:20<39:38,  2.42s/it] 36%|███▌      | 553/1535 [22:22<39:35,  2.42s/it] 36%|███▌      | 554/1535 [22:24<39:32,  2.42s/it] 36%|███▌      | 555/1535 [22:27<39:30,  2.42s/it] 36%|███▌      | 556/1535 [22:29<39:33,  2.42s/it] 36%|███▋      | 557/1535 [22:32<39:28,  2.42s/it] 36%|███▋      | 558/1535 [22:34<39:25,  2.42s/it] 36%|███▋      | 559/1535 [22:37<39:22,  2.42s/it] 36%|███▋      | 560/1535 [22:39<39:19,  2.42s/it]                                                  {'loss': 0.6908, 'grad_norm': 0.5068284869194031, 'learning_rate': 0.00045020497640417914, 'epoch': 1.82}
 36%|███▋      | 560/1535 [22:39<39:19,  2.42s/it] 37%|███▋      | 561/1535 [22:41<39:16,  2.42s/it] 37%|███▋      | 562/1535 [22:44<39:13,  2.42s/it] 37%|███▋      | 563/1535 [22:46<39:11,  2.42s/it] 37%|███▋      | 564/1535 [22:49<39:08,  2.42s/it] 37%|███▋      | 565/1535 [22:51<39:06,  2.42s/it] 37%|███▋      | 566/1535 [22:54<39:03,  2.42s/it] 37%|███▋      | 567/1535 [22:56<39:00,  2.42s/it] 37%|███▋      | 568/1535 [22:58<38:57,  2.42s/it] 37%|███▋      | 569/1535 [23:01<38:59,  2.42s/it] 37%|███▋      | 570/1535 [23:03<38:56,  2.42s/it] 37%|███▋      | 571/1535 [23:06<38:53,  2.42s/it] 37%|███▋      | 572/1535 [23:08<38:49,  2.42s/it] 37%|███▋      | 573/1535 [23:10<38:47,  2.42s/it] 37%|███▋      | 574/1535 [23:13<38:45,  2.42s/it] 37%|███▋      | 575/1535 [23:15<38:43,  2.42s/it] 38%|███▊      | 576/1535 [23:18<38:40,  2.42s/it] 38%|███▊      | 577/1535 [23:20<38:37,  2.42s/it] 38%|███▊      | 578/1535 [23:23<38:34,  2.42s/it] 38%|███▊      | 579/1535 [23:25<38:31,  2.42s/it] 38%|███▊      | 580/1535 [23:27<38:28,  2.42s/it]                                                  {'loss': 0.6758, 'grad_norm': 0.4724443554878235, 'learning_rate': 0.0004422854145669198, 'epoch': 1.89}
 38%|███▊      | 580/1535 [23:27<38:28,  2.42s/it] 38%|███▊      | 581/1535 [23:30<38:26,  2.42s/it] 38%|███▊      | 582/1535 [23:32<38:24,  2.42s/it] 38%|███▊      | 583/1535 [23:35<38:21,  2.42s/it] 38%|███▊      | 584/1535 [23:37<38:19,  2.42s/it] 38%|███▊      | 585/1535 [23:40<38:23,  2.42s/it] 38%|███▊      | 586/1535 [23:42<38:18,  2.42s/it] 38%|███▊      | 587/1535 [23:44<38:14,  2.42s/it] 38%|███▊      | 588/1535 [23:47<38:13,  2.42s/it] 38%|███▊      | 589/1535 [23:49<38:10,  2.42s/it] 38%|███▊      | 590/1535 [23:52<38:06,  2.42s/it] 39%|███▊      | 591/1535 [23:54<38:03,  2.42s/it] 39%|███▊      | 592/1535 [23:56<38:01,  2.42s/it] 39%|███▊      | 593/1535 [23:59<37:58,  2.42s/it] 39%|███▊      | 594/1535 [24:01<37:55,  2.42s/it] 39%|███▉      | 595/1535 [24:04<37:53,  2.42s/it] 39%|███▉      | 596/1535 [24:06<37:50,  2.42s/it] 39%|███▉      | 597/1535 [24:09<37:48,  2.42s/it] 39%|███▉      | 598/1535 [24:11<37:53,  2.43s/it] 39%|███▉      | 599/1535 [24:13<37:48,  2.42s/it] 39%|███▉      | 600/1535 [24:16<37:44,  2.42s/it]                                                  {'loss': 0.7034, 'grad_norm': 0.526114821434021, 'learning_rate': 0.00043386256702270773, 'epoch': 1.95}
 39%|███▉      | 600/1535 [24:16<37:44,  2.42s/it] 39%|███▉      | 601/1535 [24:18<37:40,  2.42s/it] 39%|███▉      | 602/1535 [24:21<37:37,  2.42s/it] 39%|███▉      | 603/1535 [24:23<37:34,  2.42s/it] 39%|███▉      | 604/1535 [24:25<37:32,  2.42s/it] 39%|███▉      | 605/1535 [24:28<37:29,  2.42s/it] 39%|███▉      | 606/1535 [24:30<37:27,  2.42s/it] 40%|███▉      | 607/1535 [24:33<37:24,  2.42s/it] 40%|███▉      | 608/1535 [24:35<37:21,  2.42s/it] 40%|███▉      | 609/1535 [24:38<37:19,  2.42s/it] 40%|███▉      | 610/1535 [24:40<37:16,  2.42s/it] 40%|███▉      | 611/1535 [24:42<37:14,  2.42s/it] 40%|███▉      | 612/1535 [24:45<37:21,  2.43s/it] 40%|███▉      | 613/1535 [24:47<37:16,  2.43s/it] 40%|████      | 614/1535 [24:50<37:11,  2.42s/it] 40%|████      | 615/1535 [24:52<37:06,  2.42s/it] 40%|████      | 616/1535 [24:55<37:04,  2.42s/it] 40%|████      | 617/1535 [24:57<37:00,  2.42s/it] 40%|████      | 618/1535 [24:59<36:57,  2.42s/it] 40%|████      | 619/1535 [25:02<36:55,  2.42s/it] 40%|████      | 620/1535 [25:04<36:52,  2.42s/it]                                                  {'loss': 0.6429, 'grad_norm': 0.39615413546562195, 'learning_rate': 0.0004249584796390903, 'epoch': 2.02}
 40%|████      | 620/1535 [25:04<36:52,  2.42s/it] 40%|████      | 621/1535 [25:07<36:50,  2.42s/it] 41%|████      | 622/1535 [25:09<36:47,  2.42s/it] 41%|████      | 623/1535 [25:11<36:44,  2.42s/it] 41%|████      | 624/1535 [25:14<36:43,  2.42s/it] 41%|████      | 625/1535 [25:16<36:40,  2.42s/it] 41%|████      | 626/1535 [25:19<36:44,  2.43s/it] 41%|████      | 627/1535 [25:21<36:39,  2.42s/it] 41%|████      | 628/1535 [25:24<36:35,  2.42s/it] 41%|████      | 629/1535 [25:26<36:32,  2.42s/it] 41%|████      | 630/1535 [25:28<36:29,  2.42s/it] 41%|████      | 631/1535 [25:31<36:26,  2.42s/it] 41%|████      | 632/1535 [25:33<36:23,  2.42s/it] 41%|████      | 633/1535 [25:36<36:24,  2.42s/it] 41%|████▏     | 634/1535 [25:38<36:21,  2.42s/it] 41%|████▏     | 635/1535 [25:40<36:17,  2.42s/it] 41%|████▏     | 636/1535 [25:43<36:14,  2.42s/it] 41%|████▏     | 637/1535 [25:45<36:11,  2.42s/it] 42%|████▏     | 638/1535 [25:48<36:09,  2.42s/it] 42%|████▏     | 639/1535 [25:50<36:12,  2.42s/it] 42%|████▏     | 640/1535 [25:53<36:08,  2.42s/it]                                                  {'loss': 0.5799, 'grad_norm': 0.45962774753570557, 'learning_rate': 0.000415596457875422, 'epoch': 2.08}
 42%|████▏     | 640/1535 [25:53<36:08,  2.42s/it] 42%|████▏     | 641/1535 [25:55<36:04,  2.42s/it] 42%|████▏     | 642/1535 [25:57<36:01,  2.42s/it] 42%|████▏     | 643/1535 [26:00<35:58,  2.42s/it] 42%|████▏     | 644/1535 [26:02<35:55,  2.42s/it] 42%|████▏     | 645/1535 [26:05<35:52,  2.42s/it] 42%|████▏     | 646/1535 [26:07<35:53,  2.42s/it] 42%|████▏     | 647/1535 [26:10<35:49,  2.42s/it] 42%|████▏     | 648/1535 [26:12<35:45,  2.42s/it] 42%|████▏     | 649/1535 [26:14<35:43,  2.42s/it] 42%|████▏     | 650/1535 [26:17<35:40,  2.42s/it] 42%|████▏     | 651/1535 [26:19<35:37,  2.42s/it] 42%|████▏     | 652/1535 [26:22<35:34,  2.42s/it] 43%|████▎     | 653/1535 [26:24<35:32,  2.42s/it] 43%|████▎     | 654/1535 [26:26<35:34,  2.42s/it] 43%|████▎     | 655/1535 [26:29<35:31,  2.42s/it] 43%|████▎     | 656/1535 [26:31<35:27,  2.42s/it] 43%|████▎     | 657/1535 [26:34<35:24,  2.42s/it] 43%|████▎     | 658/1535 [26:36<35:21,  2.42s/it] 43%|████▎     | 659/1535 [26:39<35:18,  2.42s/it] 43%|████▎     | 660/1535 [26:41<35:15,  2.42s/it]                                                  {'loss': 0.6236, 'grad_norm': 0.4348863959312439, 'learning_rate': 0.00040580100578341385, 'epoch': 2.15}
 43%|████▎     | 660/1535 [26:41<35:15,  2.42s/it] 43%|████▎     | 661/1535 [26:43<35:13,  2.42s/it] 43%|████▎     | 662/1535 [26:46<35:11,  2.42s/it] 43%|████▎     | 663/1535 [26:48<35:08,  2.42s/it] 43%|████▎     | 664/1535 [26:51<35:06,  2.42s/it] 43%|████▎     | 665/1535 [26:53<35:03,  2.42s/it] 43%|████▎     | 666/1535 [26:55<35:01,  2.42s/it] 43%|████▎     | 667/1535 [26:58<35:01,  2.42s/it] 44%|████▎     | 668/1535 [27:00<34:58,  2.42s/it] 44%|████▎     | 669/1535 [27:03<34:55,  2.42s/it] 44%|████▎     | 670/1535 [27:05<34:52,  2.42s/it] 44%|████▎     | 671/1535 [27:08<34:49,  2.42s/it] 44%|████▍     | 672/1535 [27:10<34:47,  2.42s/it] 44%|████▍     | 673/1535 [27:12<34:44,  2.42s/it] 44%|████▍     | 674/1535 [27:15<34:41,  2.42s/it] 44%|████▍     | 675/1535 [27:17<34:40,  2.42s/it] 44%|████▍     | 676/1535 [27:20<34:37,  2.42s/it] 44%|████▍     | 677/1535 [27:22<34:35,  2.42s/it] 44%|████▍     | 678/1535 [27:25<34:32,  2.42s/it] 44%|████▍     | 679/1535 [27:27<34:29,  2.42s/it] 44%|████▍     | 680/1535 [27:29<34:27,  2.42s/it]                                                  {'loss': 0.5749, 'grad_norm': 0.4455295503139496, 'learning_rate': 0.000395597761870501, 'epoch': 2.21}
 44%|████▍     | 680/1535 [27:29<34:27,  2.42s/it] 44%|████▍     | 681/1535 [27:32<34:25,  2.42s/it] 44%|████▍     | 682/1535 [27:34<34:44,  2.44s/it] 44%|████▍     | 683/1535 [27:37<34:35,  2.44s/it] 45%|████▍     | 684/1535 [27:39<34:28,  2.43s/it] 45%|████▍     | 685/1535 [27:42<34:22,  2.43s/it] 45%|████▍     | 686/1535 [27:44<34:17,  2.42s/it] 45%|████▍     | 687/1535 [27:46<34:13,  2.42s/it] 45%|████▍     | 688/1535 [27:49<34:10,  2.42s/it] 45%|████▍     | 689/1535 [27:51<34:07,  2.42s/it] 45%|████▍     | 690/1535 [27:54<34:04,  2.42s/it] 45%|████▌     | 691/1535 [27:56<34:02,  2.42s/it] 45%|████▌     | 692/1535 [27:58<33:59,  2.42s/it] 45%|████▌     | 693/1535 [28:01<33:56,  2.42s/it] 45%|████▌     | 694/1535 [28:03<33:53,  2.42s/it] 45%|████▌     | 695/1535 [28:06<33:51,  2.42s/it] 45%|████▌     | 696/1535 [28:08<33:48,  2.42s/it] 45%|████▌     | 697/1535 [28:11<33:46,  2.42s/it] 45%|████▌     | 698/1535 [28:13<33:43,  2.42s/it] 46%|████▌     | 699/1535 [28:15<33:41,  2.42s/it] 46%|████▌     | 700/1535 [28:18<33:38,  2.42s/it]                                                  {'loss': 0.5641, 'grad_norm': 0.3939966559410095, 'learning_rate': 0.0003850134319938983, 'epoch': 2.28}
 46%|████▌     | 700/1535 [28:18<33:38,  2.42s/it] 46%|████▌     | 701/1535 [28:20<33:36,  2.42s/it] 46%|████▌     | 702/1535 [28:23<33:33,  2.42s/it] 46%|████▌     | 703/1535 [28:25<33:31,  2.42s/it] 46%|████▌     | 704/1535 [28:27<33:29,  2.42s/it] 46%|████▌     | 705/1535 [28:30<33:26,  2.42s/it] 46%|████▌     | 706/1535 [28:32<33:24,  2.42s/it] 46%|████▌     | 707/1535 [28:35<33:22,  2.42s/it] 46%|████▌     | 708/1535 [28:37<33:21,  2.42s/it] 46%|████▌     | 709/1535 [28:40<33:18,  2.42s/it] 46%|████▋     | 710/1535 [28:42<33:15,  2.42s/it] 46%|████▋     | 711/1535 [28:44<33:12,  2.42s/it] 46%|████▋     | 712/1535 [28:47<33:10,  2.42s/it] 46%|████▋     | 713/1535 [28:49<33:08,  2.42s/it] 47%|████▋     | 714/1535 [28:52<33:05,  2.42s/it] 47%|████▋     | 715/1535 [28:54<33:03,  2.42s/it] 47%|████▋     | 716/1535 [28:57<33:00,  2.42s/it] 47%|████▋     | 717/1535 [28:59<32:57,  2.42s/it] 47%|████▋     | 718/1535 [29:01<32:55,  2.42s/it] 47%|████▋     | 719/1535 [29:04<32:53,  2.42s/it] 47%|████▋     | 720/1535 [29:06<32:50,  2.42s/it]                                                  {'loss': 0.5816, 'grad_norm': 0.5401895046234131, 'learning_rate': 0.0003740757194609865, 'epoch': 2.34}
 47%|████▋     | 720/1535 [29:06<32:50,  2.42s/it] 47%|████▋     | 721/1535 [29:09<32:48,  2.42s/it] 47%|████▋     | 722/1535 [29:11<32:45,  2.42s/it] 47%|████▋     | 723/1535 [29:13<32:43,  2.42s/it] 47%|████▋     | 724/1535 [29:16<32:47,  2.43s/it] 47%|████▋     | 725/1535 [29:18<32:43,  2.42s/it] 47%|████▋     | 726/1535 [29:21<32:39,  2.42s/it] 47%|████▋     | 727/1535 [29:23<32:35,  2.42s/it] 47%|████▋     | 728/1535 [29:26<32:32,  2.42s/it] 47%|████▋     | 729/1535 [29:28<32:29,  2.42s/it] 48%|████▊     | 730/1535 [29:30<32:26,  2.42s/it] 48%|████▊     | 731/1535 [29:33<32:24,  2.42s/it] 48%|████▊     | 732/1535 [29:35<32:21,  2.42s/it] 48%|████▊     | 733/1535 [29:38<32:20,  2.42s/it] 48%|████▊     | 734/1535 [29:40<32:17,  2.42s/it] 48%|████▊     | 735/1535 [29:42<32:14,  2.42s/it] 48%|████▊     | 736/1535 [29:45<32:12,  2.42s/it] 48%|████▊     | 737/1535 [29:47<32:12,  2.42s/it] 48%|████▊     | 738/1535 [29:50<32:08,  2.42s/it] 48%|████▊     | 739/1535 [29:52<32:05,  2.42s/it] 48%|████▊     | 740/1535 [29:55<32:02,  2.42s/it]                                                  {'loss': 0.5629, 'grad_norm': 0.48406514525413513, 'learning_rate': 0.00036281325251898323, 'epoch': 2.41}
 48%|████▊     | 740/1535 [29:55<32:02,  2.42s/it] 48%|████▊     | 741/1535 [29:57<32:00,  2.42s/it] 48%|████▊     | 742/1535 [29:59<31:57,  2.42s/it] 48%|████▊     | 743/1535 [30:02<31:55,  2.42s/it] 48%|████▊     | 744/1535 [30:04<31:53,  2.42s/it] 49%|████▊     | 745/1535 [30:07<31:50,  2.42s/it] 49%|████▊     | 746/1535 [30:09<31:47,  2.42s/it] 49%|████▊     | 747/1535 [30:11<31:45,  2.42s/it] 49%|████▊     | 748/1535 [30:14<31:42,  2.42s/it] 49%|████▉     | 749/1535 [30:16<31:40,  2.42s/it] 49%|████▉     | 750/1535 [30:19<31:37,  2.42s/it] 49%|████▉     | 751/1535 [30:21<31:47,  2.43s/it] 49%|████▉     | 752/1535 [30:24<31:41,  2.43s/it] 49%|████▉     | 753/1535 [30:26<31:36,  2.42s/it] 49%|████▉     | 754/1535 [30:28<31:31,  2.42s/it] 49%|████▉     | 755/1535 [30:31<31:29,  2.42s/it] 49%|████▉     | 756/1535 [30:33<31:25,  2.42s/it] 49%|████▉     | 757/1535 [30:36<31:22,  2.42s/it] 49%|████▉     | 758/1535 [30:38<31:19,  2.42s/it] 49%|████▉     | 759/1535 [30:41<31:17,  2.42s/it] 50%|████▉     | 760/1535 [30:43<31:14,  2.42s/it]                                                  {'loss': 0.574, 'grad_norm': 0.591850221157074, 'learning_rate': 0.00035125550942368696, 'epoch': 2.47}
 50%|████▉     | 760/1535 [30:43<31:14,  2.42s/it] 50%|████▉     | 761/1535 [30:45<31:12,  2.42s/it] 50%|████▉     | 762/1535 [30:48<31:09,  2.42s/it] 50%|████▉     | 763/1535 [30:50<31:07,  2.42s/it] 50%|████▉     | 764/1535 [30:53<31:04,  2.42s/it] 50%|████▉     | 765/1535 [30:55<31:08,  2.43s/it] 50%|████▉     | 766/1535 [30:58<31:03,  2.42s/it] 50%|████▉     | 767/1535 [31:00<31:00,  2.42s/it] 50%|█████     | 768/1535 [31:02<30:56,  2.42s/it] 50%|█████     | 769/1535 [31:05<30:54,  2.42s/it] 50%|█████     | 770/1535 [31:07<30:50,  2.42s/it] 50%|█████     | 771/1535 [31:10<30:48,  2.42s/it] 50%|█████     | 772/1535 [31:12<30:45,  2.42s/it] 50%|█████     | 773/1535 [31:14<30:42,  2.42s/it] 50%|█████     | 774/1535 [31:17<30:39,  2.42s/it] 50%|█████     | 775/1535 [31:19<30:37,  2.42s/it] 51%|█████     | 776/1535 [31:22<30:35,  2.42s/it] 51%|█████     | 777/1535 [31:24<30:32,  2.42s/it] 51%|█████     | 778/1535 [31:27<30:31,  2.42s/it] 51%|█████     | 779/1535 [31:29<30:28,  2.42s/it] 51%|█████     | 780/1535 [31:31<30:26,  2.42s/it]                                                  {'loss': 0.5451, 'grad_norm': 0.47976624965667725, 'learning_rate': 0.0003394327412834182, 'epoch': 2.54}
 51%|█████     | 780/1535 [31:31<30:26,  2.42s/it] 51%|█████     | 781/1535 [31:34<30:23,  2.42s/it] 51%|█████     | 782/1535 [31:36<30:21,  2.42s/it] 51%|█████     | 783/1535 [31:39<30:18,  2.42s/it] 51%|█████     | 784/1535 [31:41<30:16,  2.42s/it] 51%|█████     | 785/1535 [31:43<30:13,  2.42s/it] 51%|█████     | 786/1535 [31:46<30:11,  2.42s/it] 51%|█████▏    | 787/1535 [31:48<30:08,  2.42s/it] 51%|█████▏    | 788/1535 [31:51<30:06,  2.42s/it] 51%|█████▏    | 789/1535 [31:53<30:03,  2.42s/it] 51%|█████▏    | 790/1535 [31:56<30:01,  2.42s/it] 52%|█████▏    | 791/1535 [31:58<29:58,  2.42s/it] 52%|█████▏    | 792/1535 [32:00<29:56,  2.42s/it] 52%|█████▏    | 793/1535 [32:03<29:57,  2.42s/it] 52%|█████▏    | 794/1535 [32:05<29:53,  2.42s/it] 52%|█████▏    | 795/1535 [32:08<29:50,  2.42s/it] 52%|█████▏    | 796/1535 [32:10<29:47,  2.42s/it] 52%|█████▏    | 797/1535 [32:12<29:45,  2.42s/it] 52%|█████▏    | 798/1535 [32:15<29:42,  2.42s/it] 52%|█████▏    | 799/1535 [32:17<29:39,  2.42s/it] 52%|█████▏    | 800/1535 [32:20<29:37,  2.42s/it]                                                  {'loss': 0.5824, 'grad_norm': 0.5507333278656006, 'learning_rate': 0.0003273758928801043, 'epoch': 2.6}
 52%|█████▏    | 800/1535 [32:20<29:37,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 06:50:49,707 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-800
[INFO|configuration_utils.py:726] 2024-05-25 06:50:51,189 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 06:50:51,192 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 06:50:52,144 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 06:50:52,146 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 06:50:52,197 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 06:50:52,197 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-800/special_tokens_map.json
 52%|█████▏    | 801/1535 [32:25<39:12,  3.21s/it] 52%|█████▏    | 802/1535 [32:27<36:16,  2.97s/it] 52%|█████▏    | 803/1535 [32:30<34:12,  2.80s/it] 52%|█████▏    | 804/1535 [32:32<32:44,  2.69s/it] 52%|█████▏    | 805/1535 [32:34<31:42,  2.61s/it] 53%|█████▎    | 806/1535 [32:37<31:04,  2.56s/it] 53%|█████▎    | 807/1535 [32:39<30:36,  2.52s/it] 53%|█████▎    | 808/1535 [32:42<30:10,  2.49s/it] 53%|█████▎    | 809/1535 [32:44<29:52,  2.47s/it] 53%|█████▎    | 810/1535 [32:47<29:38,  2.45s/it] 53%|█████▎    | 811/1535 [32:49<29:28,  2.44s/it] 53%|█████▎    | 812/1535 [32:51<29:20,  2.44s/it] 53%|█████▎    | 813/1535 [32:54<29:14,  2.43s/it] 53%|█████▎    | 814/1535 [32:56<29:09,  2.43s/it] 53%|█████▎    | 815/1535 [32:59<29:04,  2.42s/it] 53%|█████▎    | 816/1535 [33:01<29:00,  2.42s/it] 53%|█████▎    | 817/1535 [33:04<28:57,  2.42s/it] 53%|█████▎    | 818/1535 [33:06<28:54,  2.42s/it] 53%|█████▎    | 819/1535 [33:08<28:52,  2.42s/it] 53%|█████▎    | 820/1535 [33:11<28:49,  2.42s/it]                                                  {'loss': 0.5513, 'grad_norm': 0.38561123609542847, 'learning_rate': 0.0003151165216747518, 'epoch': 2.67}
 53%|█████▎    | 820/1535 [33:11<28:49,  2.42s/it] 53%|█████▎    | 821/1535 [33:13<29:17,  2.46s/it] 54%|█████▎    | 822/1535 [33:16<29:05,  2.45s/it] 54%|█████▎    | 823/1535 [33:18<28:56,  2.44s/it] 54%|█████▎    | 824/1535 [33:21<28:49,  2.43s/it] 54%|█████▎    | 825/1535 [33:23<28:44,  2.43s/it] 54%|█████▍    | 826/1535 [33:25<28:39,  2.43s/it] 54%|█████▍    | 827/1535 [33:28<28:35,  2.42s/it] 54%|█████▍    | 828/1535 [33:30<28:32,  2.42s/it] 54%|█████▍    | 829/1535 [33:33<28:29,  2.42s/it] 54%|█████▍    | 830/1535 [33:35<28:26,  2.42s/it] 54%|█████▍    | 831/1535 [33:38<28:23,  2.42s/it] 54%|█████▍    | 832/1535 [33:40<28:20,  2.42s/it] 54%|█████▍    | 833/1535 [33:42<28:17,  2.42s/it] 54%|█████▍    | 834/1535 [33:45<28:15,  2.42s/it] 54%|█████▍    | 835/1535 [33:47<28:15,  2.42s/it] 54%|█████▍    | 836/1535 [33:50<28:12,  2.42s/it] 55%|█████▍    | 837/1535 [33:52<28:09,  2.42s/it] 55%|█████▍    | 838/1535 [33:54<28:06,  2.42s/it] 55%|█████▍    | 839/1535 [33:57<28:03,  2.42s/it] 55%|█████▍    | 840/1535 [33:59<28:00,  2.42s/it]                                                  {'loss': 0.5499, 'grad_norm': 0.5119378566741943, 'learning_rate': 0.00030268671520929807, 'epoch': 2.73}
 55%|█████▍    | 840/1535 [33:59<28:00,  2.42s/it] 55%|█████▍    | 841/1535 [34:02<27:58,  2.42s/it] 55%|█████▍    | 842/1535 [34:04<27:55,  2.42s/it] 55%|█████▍    | 843/1535 [34:07<27:53,  2.42s/it] 55%|█████▍    | 844/1535 [34:09<27:50,  2.42s/it] 55%|█████▌    | 845/1535 [34:11<27:48,  2.42s/it] 55%|█████▌    | 846/1535 [34:14<27:45,  2.42s/it] 55%|█████▌    | 847/1535 [34:16<27:44,  2.42s/it] 55%|█████▌    | 848/1535 [34:19<27:41,  2.42s/it] 55%|█████▌    | 849/1535 [34:21<27:39,  2.42s/it] 55%|█████▌    | 850/1535 [34:23<27:36,  2.42s/it] 55%|█████▌    | 851/1535 [34:26<27:34,  2.42s/it] 56%|█████▌    | 852/1535 [34:28<27:31,  2.42s/it] 56%|█████▌    | 853/1535 [34:31<27:29,  2.42s/it] 56%|█████▌    | 854/1535 [34:33<27:26,  2.42s/it] 56%|█████▌    | 855/1535 [34:36<27:23,  2.42s/it] 56%|█████▌    | 856/1535 [34:38<27:21,  2.42s/it] 56%|█████▌    | 857/1535 [34:40<27:19,  2.42s/it] 56%|█████▌    | 858/1535 [34:43<27:16,  2.42s/it] 56%|█████▌    | 859/1535 [34:45<27:14,  2.42s/it] 56%|█████▌    | 860/1535 [34:48<27:11,  2.42s/it]                                                  {'loss': 0.5382, 'grad_norm': 0.4674646556377411, 'learning_rate': 0.0002901190071210361, 'epoch': 2.8}
 56%|█████▌    | 860/1535 [34:48<27:11,  2.42s/it] 56%|█████▌    | 861/1535 [34:50<27:09,  2.42s/it] 56%|█████▌    | 862/1535 [34:52<27:07,  2.42s/it] 56%|█████▌    | 863/1535 [34:55<27:10,  2.43s/it] 56%|█████▋    | 864/1535 [34:57<27:06,  2.42s/it] 56%|█████▋    | 865/1535 [35:00<27:02,  2.42s/it] 56%|█████▋    | 866/1535 [35:02<26:59,  2.42s/it] 56%|█████▋    | 867/1535 [35:05<26:56,  2.42s/it] 57%|█████▋    | 868/1535 [35:07<26:53,  2.42s/it] 57%|█████▋    | 869/1535 [35:09<26:50,  2.42s/it] 57%|█████▋    | 870/1535 [35:12<26:48,  2.42s/it] 57%|█████▋    | 871/1535 [35:14<26:45,  2.42s/it] 57%|█████▋    | 872/1535 [35:17<26:43,  2.42s/it] 57%|█████▋    | 873/1535 [35:19<26:40,  2.42s/it] 57%|█████▋    | 874/1535 [35:22<26:38,  2.42s/it] 57%|█████▋    | 875/1535 [35:24<26:35,  2.42s/it] 57%|█████▋    | 876/1535 [35:26<26:33,  2.42s/it] 57%|█████▋    | 877/1535 [35:29<26:30,  2.42s/it] 57%|█████▋    | 878/1535 [35:31<26:30,  2.42s/it] 57%|█████▋    | 879/1535 [35:34<26:27,  2.42s/it] 57%|█████▋    | 880/1535 [35:36<26:24,  2.42s/it]                                                  {'loss': 0.5698, 'grad_norm': 0.48014169931411743, 'learning_rate': 0.00027744629198943376, 'epoch': 2.86}
 57%|█████▋    | 880/1535 [35:36<26:24,  2.42s/it] 57%|█████▋    | 881/1535 [35:38<26:22,  2.42s/it] 57%|█████▋    | 882/1535 [35:41<26:19,  2.42s/it] 58%|█████▊    | 883/1535 [35:43<26:16,  2.42s/it] 58%|█████▊    | 884/1535 [35:46<26:14,  2.42s/it] 58%|█████▊    | 885/1535 [35:48<26:11,  2.42s/it] 58%|█████▊    | 886/1535 [35:51<26:09,  2.42s/it] 58%|█████▊    | 887/1535 [35:53<26:06,  2.42s/it] 58%|█████▊    | 888/1535 [35:55<26:04,  2.42s/it] 58%|█████▊    | 889/1535 [35:58<26:02,  2.42s/it] 58%|█████▊    | 890/1535 [36:00<26:06,  2.43s/it] 58%|█████▊    | 891/1535 [36:03<26:01,  2.42s/it] 58%|█████▊    | 892/1535 [36:05<25:57,  2.42s/it] 58%|█████▊    | 893/1535 [36:07<25:54,  2.42s/it] 58%|█████▊    | 894/1535 [36:10<25:51,  2.42s/it] 58%|█████▊    | 895/1535 [36:12<25:48,  2.42s/it] 58%|█████▊    | 896/1535 [36:15<25:45,  2.42s/it] 58%|█████▊    | 897/1535 [36:17<25:43,  2.42s/it] 59%|█████▊    | 898/1535 [36:20<25:40,  2.42s/it] 59%|█████▊    | 899/1535 [36:22<25:37,  2.42s/it] 59%|█████▊    | 900/1535 [36:24<25:35,  2.42s/it]                                                  {'loss': 0.5278, 'grad_norm': 0.4659918546676636, 'learning_rate': 0.0002647017392382271, 'epoch': 2.93}
 59%|█████▊    | 900/1535 [36:24<25:35,  2.42s/it] 59%|█████▊    | 901/1535 [36:27<25:33,  2.42s/it] 59%|█████▉    | 902/1535 [36:29<25:30,  2.42s/it] 59%|█████▉    | 903/1535 [36:32<25:28,  2.42s/it] 59%|█████▉    | 904/1535 [36:34<25:25,  2.42s/it] 59%|█████▉    | 905/1535 [36:37<25:23,  2.42s/it] 59%|█████▉    | 906/1535 [36:39<25:21,  2.42s/it] 59%|█████▉    | 907/1535 [36:41<25:21,  2.42s/it] 59%|█████▉    | 908/1535 [36:44<25:18,  2.42s/it] 59%|█████▉    | 909/1535 [36:46<25:15,  2.42s/it] 59%|█████▉    | 910/1535 [36:49<25:12,  2.42s/it] 59%|█████▉    | 911/1535 [36:51<25:09,  2.42s/it] 59%|█████▉    | 912/1535 [36:53<25:06,  2.42s/it] 59%|█████▉    | 913/1535 [36:56<25:04,  2.42s/it] 60%|█████▉    | 914/1535 [36:58<25:01,  2.42s/it] 60%|█████▉    | 915/1535 [37:01<24:59,  2.42s/it] 60%|█████▉    | 916/1535 [37:03<24:56,  2.42s/it] 60%|█████▉    | 917/1535 [37:06<24:55,  2.42s/it] 60%|█████▉    | 918/1535 [37:08<24:52,  2.42s/it] 60%|█████▉    | 919/1535 [37:10<24:50,  2.42s/it] 60%|█████▉    | 920/1535 [37:13<24:47,  2.42s/it]                                                  {'loss': 0.5232, 'grad_norm': 0.493157297372818, 'learning_rate': 0.000251918706318139, 'epoch': 2.99}
 60%|█████▉    | 920/1535 [37:13<24:47,  2.42s/it] 60%|██████    | 921/1535 [37:15<24:45,  2.42s/it] 60%|██████    | 922/1535 [37:18<24:47,  2.43s/it] 60%|██████    | 923/1535 [37:20<24:43,  2.42s/it] 60%|██████    | 924/1535 [37:22<24:39,  2.42s/it] 60%|██████    | 925/1535 [37:25<24:36,  2.42s/it] 60%|██████    | 926/1535 [37:27<24:33,  2.42s/it] 60%|██████    | 927/1535 [37:30<24:30,  2.42s/it] 60%|██████    | 928/1535 [37:32<24:28,  2.42s/it] 61%|██████    | 929/1535 [37:35<24:25,  2.42s/it] 61%|██████    | 930/1535 [37:37<24:23,  2.42s/it] 61%|██████    | 931/1535 [37:39<24:20,  2.42s/it] 61%|██████    | 932/1535 [37:42<24:22,  2.43s/it] 61%|██████    | 933/1535 [37:44<24:18,  2.42s/it] 61%|██████    | 934/1535 [37:47<24:15,  2.42s/it] 61%|██████    | 935/1535 [37:49<24:12,  2.42s/it] 61%|██████    | 936/1535 [37:52<24:09,  2.42s/it] 61%|██████    | 937/1535 [37:54<24:06,  2.42s/it] 61%|██████    | 938/1535 [37:56<24:04,  2.42s/it] 61%|██████    | 939/1535 [37:59<24:01,  2.42s/it] 61%|██████    | 940/1535 [38:01<23:59,  2.42s/it]                                                  {'loss': 0.5097, 'grad_norm': 0.45879796147346497, 'learning_rate': 0.00023913065139745916, 'epoch': 3.06}
 61%|██████    | 940/1535 [38:01<23:59,  2.42s/it] 61%|██████▏   | 941/1535 [38:04<23:56,  2.42s/it] 61%|██████▏   | 942/1535 [38:06<23:54,  2.42s/it] 61%|██████▏   | 943/1535 [38:08<23:51,  2.42s/it] 61%|██████▏   | 944/1535 [38:11<23:49,  2.42s/it] 62%|██████▏   | 945/1535 [38:13<23:48,  2.42s/it] 62%|██████▏   | 946/1535 [38:16<23:45,  2.42s/it] 62%|██████▏   | 947/1535 [38:18<23:42,  2.42s/it] 62%|██████▏   | 948/1535 [38:21<23:40,  2.42s/it] 62%|██████▏   | 949/1535 [38:23<23:37,  2.42s/it] 62%|██████▏   | 950/1535 [38:25<23:34,  2.42s/it] 62%|██████▏   | 951/1535 [38:28<23:36,  2.43s/it] 62%|██████▏   | 952/1535 [38:30<23:32,  2.42s/it] 62%|██████▏   | 953/1535 [38:33<23:29,  2.42s/it] 62%|██████▏   | 954/1535 [38:35<23:26,  2.42s/it] 62%|██████▏   | 955/1535 [38:38<23:23,  2.42s/it] 62%|██████▏   | 956/1535 [38:40<23:20,  2.42s/it] 62%|██████▏   | 957/1535 [38:42<23:17,  2.42s/it] 62%|██████▏   | 958/1535 [38:45<23:15,  2.42s/it] 62%|██████▏   | 959/1535 [38:47<23:12,  2.42s/it] 63%|██████▎   | 960/1535 [38:50<23:15,  2.43s/it]                                                  {'loss': 0.455, 'grad_norm': 0.45278510451316833, 'learning_rate': 0.00022637104578900538, 'epoch': 3.12}
 63%|██████▎   | 960/1535 [38:50<23:15,  2.43s/it] 63%|██████▎   | 961/1535 [38:52<23:12,  2.43s/it] 63%|██████▎   | 962/1535 [38:54<23:08,  2.42s/it] 63%|██████▎   | 963/1535 [38:57<23:05,  2.42s/it] 63%|██████▎   | 964/1535 [38:59<23:01,  2.42s/it] 63%|██████▎   | 965/1535 [39:02<22:59,  2.42s/it] 63%|██████▎   | 966/1535 [39:04<22:57,  2.42s/it] 63%|██████▎   | 967/1535 [39:07<22:54,  2.42s/it] 63%|██████▎   | 968/1535 [39:09<22:51,  2.42s/it] 63%|██████▎   | 969/1535 [39:11<22:48,  2.42s/it] 63%|██████▎   | 970/1535 [39:14<22:46,  2.42s/it] 63%|██████▎   | 971/1535 [39:16<22:43,  2.42s/it] 63%|██████▎   | 972/1535 [39:19<22:41,  2.42s/it] 63%|██████▎   | 973/1535 [39:21<22:39,  2.42s/it] 63%|██████▎   | 974/1535 [39:23<22:36,  2.42s/it] 64%|██████▎   | 975/1535 [39:26<22:34,  2.42s/it] 64%|██████▎   | 976/1535 [39:28<22:31,  2.42s/it] 64%|██████▎   | 977/1535 [39:31<22:29,  2.42s/it] 64%|██████▎   | 978/1535 [39:33<22:26,  2.42s/it] 64%|██████▍   | 979/1535 [39:36<22:24,  2.42s/it] 64%|██████▍   | 980/1535 [39:38<22:23,  2.42s/it]                                                  {'loss': 0.4357, 'grad_norm': 0.4659072160720825, 'learning_rate': 0.00021367328634268212, 'epoch': 3.19}
 64%|██████▍   | 980/1535 [39:38<22:23,  2.42s/it] 64%|██████▍   | 981/1535 [39:40<22:20,  2.42s/it] 64%|██████▍   | 982/1535 [39:43<22:17,  2.42s/it] 64%|██████▍   | 983/1535 [39:45<22:15,  2.42s/it] 64%|██████▍   | 984/1535 [39:48<22:12,  2.42s/it] 64%|██████▍   | 985/1535 [39:50<22:10,  2.42s/it] 64%|██████▍   | 986/1535 [39:53<22:09,  2.42s/it] 64%|██████▍   | 987/1535 [39:55<22:06,  2.42s/it] 64%|██████▍   | 988/1535 [39:57<22:03,  2.42s/it] 64%|██████▍   | 989/1535 [40:00<22:00,  2.42s/it] 64%|██████▍   | 990/1535 [40:02<21:58,  2.42s/it] 65%|██████▍   | 991/1535 [40:05<21:55,  2.42s/it] 65%|██████▍   | 992/1535 [40:07<21:53,  2.42s/it] 65%|██████▍   | 993/1535 [40:09<21:50,  2.42s/it] 65%|██████▍   | 994/1535 [40:12<21:49,  2.42s/it] 65%|██████▍   | 995/1535 [40:14<21:46,  2.42s/it] 65%|██████▍   | 996/1535 [40:17<21:43,  2.42s/it] 65%|██████▍   | 997/1535 [40:19<21:41,  2.42s/it] 65%|██████▌   | 998/1535 [40:22<21:38,  2.42s/it] 65%|██████▌   | 999/1535 [40:24<21:36,  2.42s/it] 65%|██████▌   | 1000/1535 [40:26<21:33,  2.42s/it]                                                   {'loss': 0.457, 'grad_norm': 0.5247214436531067, 'learning_rate': 0.0002010706080329363, 'epoch': 3.25}
 65%|██████▌   | 1000/1535 [40:26<21:33,  2.42s/it] 65%|██████▌   | 1001/1535 [40:29<21:31,  2.42s/it] 65%|██████▌   | 1002/1535 [40:31<21:36,  2.43s/it] 65%|██████▌   | 1003/1535 [40:34<21:31,  2.43s/it] 65%|██████▌   | 1004/1535 [40:36<21:27,  2.42s/it] 65%|██████▌   | 1005/1535 [40:39<21:23,  2.42s/it] 66%|██████▌   | 1006/1535 [40:41<21:20,  2.42s/it] 66%|██████▌   | 1007/1535 [40:43<21:17,  2.42s/it] 66%|██████▌   | 1008/1535 [40:46<21:14,  2.42s/it] 66%|██████▌   | 1009/1535 [40:48<21:12,  2.42s/it] 66%|██████▌   | 1010/1535 [40:51<21:09,  2.42s/it] 66%|██████▌   | 1011/1535 [40:53<21:06,  2.42s/it] 66%|██████▌   | 1012/1535 [40:55<21:04,  2.42s/it] 66%|██████▌   | 1013/1535 [40:58<21:02,  2.42s/it] 66%|██████▌   | 1014/1535 [41:00<20:59,  2.42s/it] 66%|██████▌   | 1015/1535 [41:03<20:58,  2.42s/it] 66%|██████▌   | 1016/1535 [41:05<20:55,  2.42s/it] 66%|██████▋   | 1017/1535 [41:08<20:52,  2.42s/it] 66%|██████▋   | 1018/1535 [41:10<20:50,  2.42s/it] 66%|██████▋   | 1019/1535 [41:12<20:47,  2.42s/it] 66%|██████▋   | 1020/1535 [41:15<20:45,  2.42s/it]                                                   {'loss': 0.5105, 'grad_norm': 0.3935543894767761, 'learning_rate': 0.00018859599696990486, 'epoch': 3.32}
 66%|██████▋   | 1020/1535 [41:15<20:45,  2.42s/it] 67%|██████▋   | 1021/1535 [41:17<20:43,  2.42s/it] 67%|██████▋   | 1022/1535 [41:20<20:40,  2.42s/it] 67%|██████▋   | 1023/1535 [41:22<20:38,  2.42s/it] 67%|██████▋   | 1024/1535 [41:24<20:35,  2.42s/it] 67%|██████▋   | 1025/1535 [41:27<20:33,  2.42s/it] 67%|██████▋   | 1026/1535 [41:29<20:30,  2.42s/it] 67%|██████▋   | 1027/1535 [41:32<20:28,  2.42s/it] 67%|██████▋   | 1028/1535 [41:34<20:25,  2.42s/it] 67%|██████▋   | 1029/1535 [41:37<20:29,  2.43s/it] 67%|██████▋   | 1030/1535 [41:39<20:25,  2.43s/it] 67%|██████▋   | 1031/1535 [41:41<20:21,  2.42s/it] 67%|██████▋   | 1032/1535 [41:44<20:18,  2.42s/it] 67%|██████▋   | 1033/1535 [41:46<20:15,  2.42s/it] 67%|██████▋   | 1034/1535 [41:49<20:12,  2.42s/it] 67%|██████▋   | 1035/1535 [41:51<20:09,  2.42s/it] 67%|██████▋   | 1036/1535 [41:54<20:07,  2.42s/it] 68%|██████▊   | 1037/1535 [41:56<20:04,  2.42s/it] 68%|██████▊   | 1038/1535 [41:58<20:02,  2.42s/it] 68%|██████▊   | 1039/1535 [42:01<19:59,  2.42s/it] 68%|██████▊   | 1040/1535 [42:03<19:57,  2.42s/it]                                                   {'loss': 0.4544, 'grad_norm': 0.39053478837013245, 'learning_rate': 0.00017628210406193647, 'epoch': 3.38}
 68%|██████▊   | 1040/1535 [42:03<19:57,  2.42s/it] 68%|██████▊   | 1041/1535 [42:06<19:55,  2.42s/it] 68%|██████▊   | 1042/1535 [42:08<19:52,  2.42s/it] 68%|██████▊   | 1043/1535 [42:10<19:49,  2.42s/it] 68%|██████▊   | 1044/1535 [42:13<19:47,  2.42s/it] 68%|██████▊   | 1045/1535 [42:15<19:45,  2.42s/it] 68%|██████▊   | 1046/1535 [42:18<19:42,  2.42s/it] 68%|██████▊   | 1047/1535 [42:20<19:40,  2.42s/it] 68%|██████▊   | 1048/1535 [42:23<19:37,  2.42s/it] 68%|██████▊   | 1049/1535 [42:25<19:35,  2.42s/it] 68%|██████▊   | 1050/1535 [42:27<19:32,  2.42s/it] 68%|██████▊   | 1051/1535 [42:30<19:30,  2.42s/it] 69%|██████▊   | 1052/1535 [42:32<19:29,  2.42s/it] 69%|██████▊   | 1053/1535 [42:35<19:26,  2.42s/it] 69%|██████▊   | 1054/1535 [42:37<19:23,  2.42s/it] 69%|██████▊   | 1055/1535 [42:39<19:21,  2.42s/it] 69%|██████▉   | 1056/1535 [42:42<19:18,  2.42s/it] 69%|██████▉   | 1057/1535 [42:44<19:15,  2.42s/it] 69%|██████▉   | 1058/1535 [42:47<19:13,  2.42s/it] 69%|██████▉   | 1059/1535 [42:49<19:11,  2.42s/it] 69%|██████▉   | 1060/1535 [42:52<19:08,  2.42s/it]                                                   {'loss': 0.4542, 'grad_norm': 0.4207398593425751, 'learning_rate': 0.00016416115955546705, 'epoch': 3.45}
 69%|██████▉   | 1060/1535 [42:52<19:08,  2.42s/it] 69%|██████▉   | 1061/1535 [42:54<19:06,  2.42s/it] 69%|██████▉   | 1062/1535 [42:56<19:05,  2.42s/it] 69%|██████▉   | 1063/1535 [42:59<19:02,  2.42s/it] 69%|██████▉   | 1064/1535 [43:01<18:59,  2.42s/it] 69%|██████▉   | 1065/1535 [43:04<18:57,  2.42s/it] 69%|██████▉   | 1066/1535 [43:06<18:54,  2.42s/it] 70%|██████▉   | 1067/1535 [43:09<18:55,  2.43s/it] 70%|██████▉   | 1068/1535 [43:11<18:51,  2.42s/it] 70%|██████▉   | 1069/1535 [43:13<18:48,  2.42s/it] 70%|██████▉   | 1070/1535 [43:16<18:45,  2.42s/it] 70%|██████▉   | 1071/1535 [43:18<18:46,  2.43s/it] 70%|██████▉   | 1072/1535 [43:21<18:42,  2.43s/it] 70%|██████▉   | 1073/1535 [43:23<18:39,  2.42s/it] 70%|██████▉   | 1074/1535 [43:25<18:36,  2.42s/it] 70%|███████   | 1075/1535 [43:28<18:33,  2.42s/it] 70%|███████   | 1076/1535 [43:30<18:30,  2.42s/it] 70%|███████   | 1077/1535 [43:33<18:28,  2.42s/it] 70%|███████   | 1078/1535 [43:35<18:25,  2.42s/it] 70%|███████   | 1079/1535 [43:38<18:22,  2.42s/it] 70%|███████   | 1080/1535 [43:40<18:20,  2.42s/it]                                                   {'loss': 0.4475, 'grad_norm': 0.4223670959472656, 'learning_rate': 0.00015226488867593104, 'epoch': 3.51}
 70%|███████   | 1080/1535 [43:40<18:20,  2.42s/it] 70%|███████   | 1081/1535 [43:42<18:21,  2.43s/it] 70%|███████   | 1082/1535 [43:45<18:18,  2.42s/it] 71%|███████   | 1083/1535 [43:47<18:14,  2.42s/it] 71%|███████   | 1084/1535 [43:50<18:13,  2.43s/it] 71%|███████   | 1085/1535 [43:52<18:10,  2.42s/it] 71%|███████   | 1086/1535 [43:55<18:07,  2.42s/it] 71%|███████   | 1087/1535 [43:57<18:04,  2.42s/it] 71%|███████   | 1088/1535 [43:59<18:01,  2.42s/it] 71%|███████   | 1089/1535 [44:02<17:58,  2.42s/it] 71%|███████   | 1090/1535 [44:04<17:56,  2.42s/it] 71%|███████   | 1091/1535 [44:07<17:53,  2.42s/it] 71%|███████   | 1092/1535 [44:09<17:51,  2.42s/it] 71%|███████   | 1093/1535 [44:11<17:49,  2.42s/it] 71%|███████▏  | 1094/1535 [44:14<17:46,  2.42s/it] 71%|███████▏  | 1095/1535 [44:16<17:44,  2.42s/it] 71%|███████▏  | 1096/1535 [44:19<17:45,  2.43s/it] 71%|███████▏  | 1097/1535 [44:21<17:41,  2.42s/it] 72%|███████▏  | 1098/1535 [44:24<17:38,  2.42s/it] 72%|███████▏  | 1099/1535 [44:26<17:39,  2.43s/it] 72%|███████▏  | 1100/1535 [44:28<17:35,  2.43s/it]                                                   {'loss': 0.4446, 'grad_norm': 0.43527165055274963, 'learning_rate': 0.00014062442859050868, 'epoch': 3.58}
 72%|███████▏  | 1100/1535 [44:28<17:35,  2.43s/it] 72%|███████▏  | 1101/1535 [44:31<17:32,  2.43s/it] 72%|███████▏  | 1102/1535 [44:33<17:29,  2.42s/it] 72%|███████▏  | 1103/1535 [44:36<17:26,  2.42s/it] 72%|███████▏  | 1104/1535 [44:38<17:23,  2.42s/it] 72%|███████▏  | 1105/1535 [44:41<17:20,  2.42s/it] 72%|███████▏  | 1106/1535 [44:43<17:17,  2.42s/it] 72%|███████▏  | 1107/1535 [44:45<17:15,  2.42s/it] 72%|███████▏  | 1108/1535 [44:48<17:12,  2.42s/it] 72%|███████▏  | 1109/1535 [44:50<17:10,  2.42s/it] 72%|███████▏  | 1110/1535 [44:53<17:07,  2.42s/it] 72%|███████▏  | 1111/1535 [44:55<17:05,  2.42s/it] 72%|███████▏  | 1112/1535 [44:57<17:02,  2.42s/it] 73%|███████▎  | 1113/1535 [45:00<17:00,  2.42s/it] 73%|███████▎  | 1114/1535 [45:02<16:57,  2.42s/it] 73%|███████▎  | 1115/1535 [45:05<16:55,  2.42s/it] 73%|███████▎  | 1116/1535 [45:07<16:53,  2.42s/it] 73%|███████▎  | 1117/1535 [45:10<16:51,  2.42s/it] 73%|███████▎  | 1118/1535 [45:12<16:48,  2.42s/it] 73%|███████▎  | 1119/1535 [45:14<16:45,  2.42s/it] 73%|███████▎  | 1120/1535 [45:17<16:43,  2.42s/it]                                                   {'loss': 0.4744, 'grad_norm': 0.46019798517227173, 'learning_rate': 0.00012927024691005096, 'epoch': 3.64}
 73%|███████▎  | 1120/1535 [45:17<16:43,  2.42s/it] 73%|███████▎  | 1121/1535 [45:19<16:41,  2.42s/it] 73%|███████▎  | 1122/1535 [45:22<16:38,  2.42s/it] 73%|███████▎  | 1123/1535 [45:24<16:36,  2.42s/it] 73%|███████▎  | 1124/1535 [45:27<16:33,  2.42s/it] 73%|███████▎  | 1125/1535 [45:29<16:34,  2.43s/it] 73%|███████▎  | 1126/1535 [45:31<16:31,  2.42s/it] 73%|███████▎  | 1127/1535 [45:34<16:28,  2.42s/it] 73%|███████▎  | 1128/1535 [45:36<16:25,  2.42s/it] 74%|███████▎  | 1129/1535 [45:39<16:22,  2.42s/it] 74%|███████▎  | 1130/1535 [45:41<16:19,  2.42s/it] 74%|███████▎  | 1131/1535 [45:43<16:17,  2.42s/it] 74%|███████▎  | 1132/1535 [45:46<16:14,  2.42s/it] 74%|███████▍  | 1133/1535 [45:48<16:12,  2.42s/it] 74%|███████▍  | 1134/1535 [45:51<16:09,  2.42s/it] 74%|███████▍  | 1135/1535 [45:53<16:07,  2.42s/it] 74%|███████▍  | 1136/1535 [45:56<16:04,  2.42s/it] 74%|███████▍  | 1137/1535 [45:58<16:02,  2.42s/it] 74%|███████▍  | 1138/1535 [46:00<15:59,  2.42s/it] 74%|███████▍  | 1139/1535 [46:03<15:57,  2.42s/it] 74%|███████▍  | 1140/1535 [46:05<15:55,  2.42s/it]                                                   {'loss': 0.4446, 'grad_norm': 0.38022130727767944, 'learning_rate': 0.00011823206194349351, 'epoch': 3.71}
 74%|███████▍  | 1140/1535 [46:05<15:55,  2.42s/it] 74%|███████▍  | 1141/1535 [46:08<15:55,  2.43s/it] 74%|███████▍  | 1142/1535 [46:10<15:52,  2.42s/it] 74%|███████▍  | 1143/1535 [46:13<15:49,  2.42s/it] 75%|███████▍  | 1144/1535 [46:15<15:46,  2.42s/it] 75%|███████▍  | 1145/1535 [46:17<15:43,  2.42s/it] 75%|███████▍  | 1146/1535 [46:20<15:40,  2.42s/it] 75%|███████▍  | 1147/1535 [46:22<15:38,  2.42s/it] 75%|███████▍  | 1148/1535 [46:25<15:35,  2.42s/it] 75%|███████▍  | 1149/1535 [46:27<15:33,  2.42s/it] 75%|███████▍  | 1150/1535 [46:29<15:30,  2.42s/it] 75%|███████▍  | 1151/1535 [46:32<15:28,  2.42s/it] 75%|███████▌  | 1152/1535 [46:34<15:26,  2.42s/it] 75%|███████▌  | 1153/1535 [46:37<15:23,  2.42s/it] 75%|███████▌  | 1154/1535 [46:39<15:21,  2.42s/it] 75%|███████▌  | 1155/1535 [46:42<15:18,  2.42s/it] 75%|███████▌  | 1156/1535 [46:44<15:16,  2.42s/it] 75%|███████▌  | 1157/1535 [46:46<15:13,  2.42s/it] 75%|███████▌  | 1158/1535 [46:49<15:11,  2.42s/it] 76%|███████▌  | 1159/1535 [46:51<15:09,  2.42s/it] 76%|███████▌  | 1160/1535 [46:54<15:06,  2.42s/it]                                                   {'loss': 0.4185, 'grad_norm': 0.3914676010608673, 'learning_rate': 0.00010753876491348524, 'epoch': 3.77}
 76%|███████▌  | 1160/1535 [46:54<15:06,  2.42s/it] 76%|███████▌  | 1161/1535 [46:56<15:04,  2.42s/it] 76%|███████▌  | 1162/1535 [46:58<15:02,  2.42s/it] 76%|███████▌  | 1163/1535 [47:01<14:59,  2.42s/it] 76%|███████▌  | 1164/1535 [47:03<14:57,  2.42s/it] 76%|███████▌  | 1165/1535 [47:06<14:54,  2.42s/it] 76%|███████▌  | 1166/1535 [47:08<14:52,  2.42s/it] 76%|███████▌  | 1167/1535 [47:11<14:49,  2.42s/it] 76%|███████▌  | 1168/1535 [47:13<14:51,  2.43s/it] 76%|███████▌  | 1169/1535 [47:15<14:47,  2.43s/it] 76%|███████▌  | 1170/1535 [47:18<14:44,  2.42s/it] 76%|███████▋  | 1171/1535 [47:20<14:41,  2.42s/it] 76%|███████▋  | 1172/1535 [47:23<14:38,  2.42s/it] 76%|███████▋  | 1173/1535 [47:25<14:35,  2.42s/it] 76%|███████▋  | 1174/1535 [47:27<14:33,  2.42s/it] 77%|███████▋  | 1175/1535 [47:30<14:30,  2.42s/it] 77%|███████▋  | 1176/1535 [47:32<14:28,  2.42s/it] 77%|███████▋  | 1177/1535 [47:35<14:26,  2.42s/it] 77%|███████▋  | 1178/1535 [47:37<14:23,  2.42s/it] 77%|███████▋  | 1179/1535 [47:40<14:21,  2.42s/it] 77%|███████▋  | 1180/1535 [47:42<14:18,  2.42s/it]                                                   {'loss': 0.4574, 'grad_norm': 0.46706423163414, 'learning_rate': 9.721834433682289e-05, 'epoch': 3.84}
 77%|███████▋  | 1180/1535 [47:42<14:18,  2.42s/it] 77%|███████▋  | 1181/1535 [47:44<14:16,  2.42s/it] 77%|███████▋  | 1182/1535 [47:47<14:15,  2.42s/it] 77%|███████▋  | 1183/1535 [47:49<14:14,  2.43s/it] 77%|███████▋  | 1184/1535 [47:52<14:11,  2.42s/it] 77%|███████▋  | 1185/1535 [47:54<14:08,  2.42s/it] 77%|███████▋  | 1186/1535 [47:57<14:05,  2.42s/it] 77%|███████▋  | 1187/1535 [47:59<14:02,  2.42s/it] 77%|███████▋  | 1188/1535 [48:01<13:59,  2.42s/it] 77%|███████▋  | 1189/1535 [48:04<13:57,  2.42s/it] 78%|███████▊  | 1190/1535 [48:06<13:54,  2.42s/it] 78%|███████▊  | 1191/1535 [48:09<13:51,  2.42s/it] 78%|███████▊  | 1192/1535 [48:11<13:49,  2.42s/it] 78%|███████▊  | 1193/1535 [48:13<13:47,  2.42s/it] 78%|███████▊  | 1194/1535 [48:16<13:44,  2.42s/it] 78%|███████▊  | 1195/1535 [48:18<13:44,  2.43s/it] 78%|███████▊  | 1196/1535 [48:21<13:41,  2.42s/it] 78%|███████▊  | 1197/1535 [48:23<13:38,  2.42s/it] 78%|███████▊  | 1198/1535 [48:26<13:35,  2.42s/it] 78%|███████▊  | 1199/1535 [48:28<13:33,  2.42s/it] 78%|███████▊  | 1200/1535 [48:30<13:30,  2.42s/it]                                                   {'loss': 0.4456, 'grad_norm': 1.3085664510726929, 'learning_rate': 8.729781276761883e-05, 'epoch': 3.9}
 78%|███████▊  | 1200/1535 [48:30<13:30,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 07:07:00,400 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-1200
[INFO|configuration_utils.py:726] 2024-05-25 07:07:01,086 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 07:07:01,089 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 07:07:02,005 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 07:07:02,008 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 07:07:02,058 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 07:07:02,059 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/topic_then_length/checkpoint-1200/special_tokens_map.json
 78%|███████▊  | 1201/1535 [48:35<16:26,  2.95s/it] 78%|███████▊  | 1202/1535 [48:37<15:29,  2.79s/it] 78%|███████▊  | 1203/1535 [48:39<14:49,  2.68s/it] 78%|███████▊  | 1204/1535 [48:42<14:21,  2.60s/it] 79%|███████▊  | 1205/1535 [48:44<14:00,  2.55s/it] 79%|███████▊  | 1206/1535 [48:47<13:45,  2.51s/it] 79%|███████▊  | 1207/1535 [48:49<13:34,  2.48s/it] 79%|███████▊  | 1208/1535 [48:52<13:25,  2.46s/it] 79%|███████▉  | 1209/1535 [48:54<13:18,  2.45s/it] 79%|███████▉  | 1210/1535 [48:56<13:13,  2.44s/it] 79%|███████▉  | 1211/1535 [48:59<13:08,  2.44s/it] 79%|███████▉  | 1212/1535 [49:01<13:08,  2.44s/it] 79%|███████▉  | 1213/1535 [49:04<13:03,  2.43s/it] 79%|███████▉  | 1214/1535 [49:06<12:59,  2.43s/it] 79%|███████▉  | 1215/1535 [49:09<12:56,  2.43s/it] 79%|███████▉  | 1216/1535 [49:11<12:53,  2.42s/it] 79%|███████▉  | 1217/1535 [49:13<12:50,  2.42s/it] 79%|███████▉  | 1218/1535 [49:16<12:47,  2.42s/it] 79%|███████▉  | 1219/1535 [49:18<12:44,  2.42s/it] 79%|███████▉  | 1220/1535 [49:21<12:42,  2.42s/it]                                                   {'loss': 0.4504, 'grad_norm': 0.41601622104644775, 'learning_rate': 7.780313609494147e-05, 'epoch': 3.97}
 79%|███████▉  | 1220/1535 [49:21<12:42,  2.42s/it] 80%|███████▉  | 1221/1535 [49:23<12:39,  2.42s/it] 80%|███████▉  | 1222/1535 [49:25<12:37,  2.42s/it] 80%|███████▉  | 1223/1535 [49:28<12:36,  2.42s/it] 80%|███████▉  | 1224/1535 [49:30<12:33,  2.42s/it] 80%|███████▉  | 1225/1535 [49:33<12:30,  2.42s/it] 80%|███████▉  | 1226/1535 [49:35<12:30,  2.43s/it] 80%|███████▉  | 1227/1535 [49:38<12:27,  2.43s/it] 80%|████████  | 1228/1535 [49:40<12:24,  2.42s/it] 80%|████████  | 1229/1535 [49:42<12:21,  2.42s/it] 80%|████████  | 1230/1535 [49:45<12:18,  2.42s/it] 80%|████████  | 1231/1535 [49:47<12:15,  2.42s/it] 80%|████████  | 1232/1535 [49:50<12:13,  2.42s/it] 80%|████████  | 1233/1535 [49:52<12:10,  2.42s/it] 80%|████████  | 1234/1535 [49:55<12:08,  2.42s/it] 80%|████████  | 1235/1535 [49:57<12:05,  2.42s/it] 81%|████████  | 1236/1535 [49:59<12:03,  2.42s/it] 81%|████████  | 1237/1535 [50:02<12:00,  2.42s/it] 81%|████████  | 1238/1535 [50:04<12:02,  2.43s/it] 81%|████████  | 1239/1535 [50:07<11:58,  2.43s/it] 81%|████████  | 1240/1535 [50:09<11:55,  2.43s/it]                                                   {'loss': 0.4322, 'grad_norm': 0.40095609426498413, 'learning_rate': 6.875916557998654e-05, 'epoch': 4.03}
 81%|████████  | 1240/1535 [50:09<11:55,  2.43s/it] 81%|████████  | 1241/1535 [50:12<11:54,  2.43s/it] 81%|████████  | 1242/1535 [50:14<11:51,  2.43s/it] 81%|████████  | 1243/1535 [50:16<11:48,  2.42s/it] 81%|████████  | 1244/1535 [50:19<11:45,  2.42s/it] 81%|████████  | 1245/1535 [50:21<11:42,  2.42s/it] 81%|████████  | 1246/1535 [50:24<11:39,  2.42s/it] 81%|████████  | 1247/1535 [50:26<11:37,  2.42s/it] 81%|████████▏ | 1248/1535 [50:28<11:34,  2.42s/it] 81%|████████▏ | 1249/1535 [50:31<11:31,  2.42s/it] 81%|████████▏ | 1250/1535 [50:33<11:29,  2.42s/it] 81%|████████▏ | 1251/1535 [50:36<11:27,  2.42s/it] 82%|████████▏ | 1252/1535 [50:38<11:24,  2.42s/it] 82%|████████▏ | 1253/1535 [50:41<11:22,  2.42s/it] 82%|████████▏ | 1254/1535 [50:43<11:19,  2.42s/it] 82%|████████▏ | 1255/1535 [50:45<11:19,  2.43s/it] 82%|████████▏ | 1256/1535 [50:48<11:16,  2.42s/it] 82%|████████▏ | 1257/1535 [50:50<11:13,  2.42s/it] 82%|████████▏ | 1258/1535 [50:53<11:10,  2.42s/it] 82%|████████▏ | 1259/1535 [50:55<11:07,  2.42s/it] 82%|████████▏ | 1260/1535 [50:58<11:05,  2.42s/it]                                                   {'loss': 0.3806, 'grad_norm': 0.3949951231479645, 'learning_rate': 6.018957281066151e-05, 'epoch': 4.1}
 82%|████████▏ | 1260/1535 [50:58<11:05,  2.42s/it] 82%|████████▏ | 1261/1535 [51:00<11:03,  2.42s/it] 82%|████████▏ | 1262/1535 [51:02<11:00,  2.42s/it] 82%|████████▏ | 1263/1535 [51:05<10:57,  2.42s/it] 82%|████████▏ | 1264/1535 [51:07<10:57,  2.43s/it] 82%|████████▏ | 1265/1535 [51:10<10:54,  2.42s/it] 82%|████████▏ | 1266/1535 [51:12<10:51,  2.42s/it] 83%|████████▎ | 1267/1535 [51:14<10:48,  2.42s/it] 83%|████████▎ | 1268/1535 [51:17<10:46,  2.42s/it] 83%|████████▎ | 1269/1535 [51:19<10:43,  2.42s/it] 83%|████████▎ | 1270/1535 [51:22<10:42,  2.42s/it] 83%|████████▎ | 1271/1535 [51:24<10:39,  2.42s/it] 83%|████████▎ | 1272/1535 [51:27<10:36,  2.42s/it] 83%|████████▎ | 1273/1535 [51:29<10:34,  2.42s/it] 83%|████████▎ | 1274/1535 [51:31<10:31,  2.42s/it] 83%|████████▎ | 1275/1535 [51:34<10:28,  2.42s/it] 83%|████████▎ | 1276/1535 [51:36<10:26,  2.42s/it] 83%|████████▎ | 1277/1535 [51:39<10:23,  2.42s/it] 83%|████████▎ | 1278/1535 [51:41<10:21,  2.42s/it] 83%|████████▎ | 1279/1535 [51:44<10:19,  2.42s/it] 83%|████████▎ | 1280/1535 [51:46<10:17,  2.42s/it]                                                   {'loss': 0.4192, 'grad_norm': 0.35507866740226746, 'learning_rate': 5.2116787743835217e-05, 'epoch': 4.16}
 83%|████████▎ | 1280/1535 [51:46<10:17,  2.42s/it] 83%|████████▎ | 1281/1535 [51:48<10:15,  2.42s/it] 84%|████████▎ | 1282/1535 [51:51<10:12,  2.42s/it] 84%|████████▎ | 1283/1535 [51:53<10:09,  2.42s/it] 84%|████████▎ | 1284/1535 [51:56<10:10,  2.43s/it] 84%|████████▎ | 1285/1535 [51:58<10:06,  2.43s/it] 84%|████████▍ | 1286/1535 [52:00<10:03,  2.43s/it] 84%|████████▍ | 1287/1535 [52:03<10:00,  2.42s/it] 84%|████████▍ | 1288/1535 [52:05<09:58,  2.42s/it] 84%|████████▍ | 1289/1535 [52:08<09:55,  2.42s/it] 84%|████████▍ | 1290/1535 [52:10<09:52,  2.42s/it] 84%|████████▍ | 1291/1535 [52:13<09:50,  2.42s/it] 84%|████████▍ | 1292/1535 [52:15<09:47,  2.42s/it] 84%|████████▍ | 1293/1535 [52:17<09:45,  2.42s/it] 84%|████████▍ | 1294/1535 [52:20<09:42,  2.42s/it] 84%|████████▍ | 1295/1535 [52:22<09:40,  2.42s/it] 84%|████████▍ | 1296/1535 [52:25<09:37,  2.42s/it] 84%|████████▍ | 1297/1535 [52:27<09:35,  2.42s/it] 85%|████████▍ | 1298/1535 [52:30<09:33,  2.42s/it] 85%|████████▍ | 1299/1535 [52:32<09:32,  2.43s/it] 85%|████████▍ | 1300/1535 [52:34<09:29,  2.42s/it]                                                   {'loss': 0.4002, 'grad_norm': 0.4046410918235779, 'learning_rate': 4.456193999741731e-05, 'epoch': 4.23}
 85%|████████▍ | 1300/1535 [52:34<09:29,  2.42s/it] 85%|████████▍ | 1301/1535 [52:37<09:26,  2.42s/it] 85%|████████▍ | 1302/1535 [52:39<09:24,  2.42s/it] 85%|████████▍ | 1303/1535 [52:42<09:21,  2.42s/it] 85%|████████▍ | 1304/1535 [52:44<09:18,  2.42s/it] 85%|████████▌ | 1305/1535 [52:46<09:16,  2.42s/it] 85%|████████▌ | 1306/1535 [52:49<09:13,  2.42s/it] 85%|████████▌ | 1307/1535 [52:51<09:13,  2.43s/it] 85%|████████▌ | 1308/1535 [52:54<09:10,  2.43s/it] 85%|████████▌ | 1309/1535 [52:56<09:07,  2.42s/it] 85%|████████▌ | 1310/1535 [52:59<09:04,  2.42s/it] 85%|████████▌ | 1311/1535 [53:01<09:02,  2.42s/it] 85%|████████▌ | 1312/1535 [53:03<08:59,  2.42s/it] 86%|████████▌ | 1313/1535 [53:06<08:58,  2.43s/it] 86%|████████▌ | 1314/1535 [53:08<08:55,  2.42s/it] 86%|████████▌ | 1315/1535 [53:11<08:52,  2.42s/it] 86%|████████▌ | 1316/1535 [53:13<08:50,  2.42s/it] 86%|████████▌ | 1317/1535 [53:16<08:47,  2.42s/it] 86%|████████▌ | 1318/1535 [53:18<08:45,  2.42s/it] 86%|████████▌ | 1319/1535 [53:20<08:42,  2.42s/it] 86%|████████▌ | 1320/1535 [53:23<08:40,  2.42s/it]                                                   {'loss': 0.4263, 'grad_norm': 0.40331679582595825, 'learning_rate': 3.7544803545931927e-05, 'epoch': 4.29}
 86%|████████▌ | 1320/1535 [53:23<08:40,  2.42s/it] 86%|████████▌ | 1321/1535 [53:25<08:39,  2.43s/it] 86%|████████▌ | 1322/1535 [53:28<08:36,  2.42s/it] 86%|████████▌ | 1323/1535 [53:30<08:33,  2.42s/it] 86%|████████▋ | 1324/1535 [53:32<08:30,  2.42s/it] 86%|████████▋ | 1325/1535 [53:35<08:28,  2.42s/it] 86%|████████▋ | 1326/1535 [53:37<08:25,  2.42s/it] 86%|████████▋ | 1327/1535 [53:40<08:23,  2.42s/it] 87%|████████▋ | 1328/1535 [53:42<08:20,  2.42s/it] 87%|████████▋ | 1329/1535 [53:45<08:18,  2.42s/it] 87%|████████▋ | 1330/1535 [53:47<08:15,  2.42s/it] 87%|████████▋ | 1331/1535 [53:49<08:13,  2.42s/it] 87%|████████▋ | 1332/1535 [53:52<08:10,  2.42s/it] 87%|████████▋ | 1333/1535 [53:54<08:08,  2.42s/it] 87%|████████▋ | 1334/1535 [53:57<08:07,  2.43s/it] 87%|████████▋ | 1335/1535 [53:59<08:04,  2.42s/it] 87%|████████▋ | 1336/1535 [54:02<08:02,  2.42s/it] 87%|████████▋ | 1337/1535 [54:04<07:59,  2.42s/it] 87%|████████▋ | 1338/1535 [54:06<07:56,  2.42s/it] 87%|████████▋ | 1339/1535 [54:09<07:54,  2.42s/it] 87%|████████▋ | 1340/1535 [54:11<07:51,  2.42s/it]                                                   {'loss': 0.422, 'grad_norm': 0.3808134198188782, 'learning_rate': 3.1083744964335886e-05, 'epoch': 4.36}
 87%|████████▋ | 1340/1535 [54:11<07:51,  2.42s/it] 87%|████████▋ | 1341/1535 [54:14<07:49,  2.42s/it] 87%|████████▋ | 1342/1535 [54:16<07:48,  2.43s/it] 87%|████████▋ | 1343/1535 [54:18<07:45,  2.43s/it] 88%|████████▊ | 1344/1535 [54:21<07:42,  2.42s/it] 88%|████████▊ | 1345/1535 [54:23<07:40,  2.42s/it] 88%|████████▊ | 1346/1535 [54:26<07:37,  2.42s/it] 88%|████████▊ | 1347/1535 [54:28<07:34,  2.42s/it] 88%|████████▊ | 1348/1535 [54:31<07:32,  2.42s/it] 88%|████████▊ | 1349/1535 [54:33<07:30,  2.42s/it] 88%|████████▊ | 1350/1535 [54:35<07:28,  2.42s/it] 88%|████████▊ | 1351/1535 [54:38<07:25,  2.42s/it] 88%|████████▊ | 1352/1535 [54:40<07:22,  2.42s/it] 88%|████████▊ | 1353/1535 [54:43<07:20,  2.42s/it] 88%|████████▊ | 1354/1535 [54:45<07:17,  2.42s/it] 88%|████████▊ | 1355/1535 [54:48<07:15,  2.42s/it] 88%|████████▊ | 1356/1535 [54:50<07:12,  2.42s/it] 88%|████████▊ | 1357/1535 [54:52<07:11,  2.42s/it] 88%|████████▊ | 1358/1535 [54:55<07:08,  2.42s/it] 89%|████████▊ | 1359/1535 [54:57<07:05,  2.42s/it] 89%|████████▊ | 1360/1535 [55:00<07:03,  2.42s/it]                                                   {'loss': 0.4041, 'grad_norm': 0.36392152309417725, 'learning_rate': 2.5195675355550037e-05, 'epoch': 4.42}
 89%|████████▊ | 1360/1535 [55:00<07:03,  2.42s/it] 89%|████████▊ | 1361/1535 [55:02<07:01,  2.42s/it] 89%|████████▊ | 1362/1535 [55:04<06:59,  2.42s/it] 89%|████████▉ | 1363/1535 [55:07<06:56,  2.42s/it] 89%|████████▉ | 1364/1535 [55:09<06:54,  2.42s/it] 89%|████████▉ | 1365/1535 [55:12<06:51,  2.42s/it] 89%|████████▉ | 1366/1535 [55:14<06:48,  2.42s/it] 89%|████████▉ | 1367/1535 [55:17<06:46,  2.42s/it] 89%|████████▉ | 1368/1535 [55:19<06:44,  2.42s/it] 89%|████████▉ | 1369/1535 [55:21<06:41,  2.42s/it] 89%|████████▉ | 1370/1535 [55:24<06:39,  2.42s/it] 89%|████████▉ | 1371/1535 [55:26<06:36,  2.42s/it] 89%|████████▉ | 1372/1535 [55:29<06:34,  2.42s/it] 89%|████████▉ | 1373/1535 [55:31<06:31,  2.42s/it] 90%|████████▉ | 1374/1535 [55:34<06:29,  2.42s/it] 90%|████████▉ | 1375/1535 [55:36<06:26,  2.42s/it] 90%|████████▉ | 1376/1535 [55:38<06:24,  2.42s/it] 90%|████████▉ | 1377/1535 [55:41<06:24,  2.43s/it] 90%|████████▉ | 1378/1535 [55:43<06:21,  2.43s/it] 90%|████████▉ | 1379/1535 [55:46<06:18,  2.42s/it] 90%|████████▉ | 1380/1535 [55:48<06:15,  2.42s/it]                                                   {'loss': 0.4115, 'grad_norm': 0.3705468475818634, 'learning_rate': 1.9896006087526036e-05, 'epoch': 4.49}
 90%|████████▉ | 1380/1535 [55:48<06:15,  2.42s/it] 90%|████████▉ | 1381/1535 [55:50<06:13,  2.42s/it] 90%|█████████ | 1382/1535 [55:53<06:10,  2.42s/it] 90%|█████████ | 1383/1535 [55:55<06:07,  2.42s/it] 90%|█████████ | 1384/1535 [55:58<06:05,  2.42s/it] 90%|█████████ | 1385/1535 [56:00<06:02,  2.42s/it] 90%|█████████ | 1386/1535 [56:03<06:00,  2.42s/it] 90%|█████████ | 1387/1535 [56:05<05:58,  2.42s/it] 90%|█████████ | 1388/1535 [56:07<05:55,  2.42s/it] 90%|█████████ | 1389/1535 [56:10<05:53,  2.42s/it] 91%|█████████ | 1390/1535 [56:12<05:51,  2.43s/it] 91%|█████████ | 1391/1535 [56:15<05:49,  2.42s/it] 91%|█████████ | 1392/1535 [56:17<05:46,  2.42s/it] 91%|█████████ | 1393/1535 [56:20<05:43,  2.42s/it] 91%|█████████ | 1394/1535 [56:22<05:41,  2.42s/it] 91%|█████████ | 1395/1535 [56:24<05:38,  2.42s/it] 91%|█████████ | 1396/1535 [56:27<05:36,  2.42s/it] 91%|█████████ | 1397/1535 [56:29<05:33,  2.42s/it] 91%|█████████ | 1398/1535 [56:32<05:31,  2.42s/it] 91%|█████████ | 1399/1535 [56:34<05:28,  2.42s/it] 91%|█████████ | 1400/1535 [56:36<05:27,  2.43s/it]                                                   {'loss': 0.4197, 'grad_norm': 0.37341904640197754, 'learning_rate': 1.519860845570356e-05, 'epoch': 4.55}
 91%|█████████ | 1400/1535 [56:36<05:27,  2.43s/it] 91%|█████████▏| 1401/1535 [56:39<05:24,  2.42s/it] 91%|█████████▏| 1402/1535 [56:41<05:22,  2.42s/it] 91%|█████████▏| 1403/1535 [56:44<05:21,  2.43s/it] 91%|█████████▏| 1404/1535 [56:46<05:18,  2.43s/it] 92%|█████████▏| 1405/1535 [56:49<05:15,  2.43s/it] 92%|█████████▏| 1406/1535 [56:51<05:12,  2.42s/it] 92%|█████████▏| 1407/1535 [56:53<05:09,  2.42s/it] 92%|█████████▏| 1408/1535 [56:56<05:07,  2.42s/it] 92%|█████████▏| 1409/1535 [56:58<05:04,  2.42s/it] 92%|█████████▏| 1410/1535 [57:01<05:02,  2.42s/it] 92%|█████████▏| 1411/1535 [57:03<04:59,  2.42s/it] 92%|█████████▏| 1412/1535 [57:06<04:57,  2.42s/it] 92%|█████████▏| 1413/1535 [57:08<04:54,  2.42s/it] 92%|█████████▏| 1414/1535 [57:10<04:52,  2.42s/it] 92%|█████████▏| 1415/1535 [57:13<04:50,  2.42s/it] 92%|█████████▏| 1416/1535 [57:15<04:47,  2.42s/it] 92%|█████████▏| 1417/1535 [57:18<04:45,  2.42s/it] 92%|█████████▏| 1418/1535 [57:20<04:42,  2.42s/it] 92%|█████████▏| 1419/1535 [57:22<04:40,  2.42s/it] 93%|█████████▎| 1420/1535 [57:25<04:38,  2.42s/it]                                                   {'loss': 0.3975, 'grad_norm': 0.36578062176704407, 'learning_rate': 1.1115777376435292e-05, 'epoch': 4.62}
 93%|█████████▎| 1420/1535 [57:25<04:38,  2.42s/it] 93%|█████████▎| 1421/1535 [57:27<04:35,  2.42s/it] 93%|█████████▎| 1422/1535 [57:30<04:33,  2.42s/it] 93%|█████████▎| 1423/1535 [57:32<04:30,  2.42s/it] 93%|█████████▎| 1424/1535 [57:35<04:28,  2.42s/it] 93%|█████████▎| 1425/1535 [57:37<04:25,  2.42s/it] 93%|█████████▎| 1426/1535 [57:39<04:23,  2.42s/it] 93%|█████████▎| 1427/1535 [57:42<04:21,  2.42s/it] 93%|█████████▎| 1428/1535 [57:44<04:18,  2.42s/it] 93%|█████████▎| 1429/1535 [57:47<04:16,  2.42s/it] 93%|█████████▎| 1430/1535 [57:49<04:13,  2.42s/it] 93%|█████████▎| 1431/1535 [57:51<04:11,  2.42s/it] 93%|█████████▎| 1432/1535 [57:54<04:09,  2.42s/it] 93%|█████████▎| 1433/1535 [57:56<04:06,  2.42s/it] 93%|█████████▎| 1434/1535 [57:59<04:04,  2.42s/it] 93%|█████████▎| 1435/1535 [58:01<04:01,  2.42s/it] 94%|█████████▎| 1436/1535 [58:04<03:59,  2.42s/it] 94%|█████████▎| 1437/1535 [58:06<03:56,  2.42s/it] 94%|█████████▎| 1438/1535 [58:08<03:54,  2.42s/it] 94%|█████████▎| 1439/1535 [58:11<03:52,  2.42s/it] 94%|█████████▍| 1440/1535 [58:13<03:49,  2.42s/it]                                                   {'loss': 0.4019, 'grad_norm': 0.3417545557022095, 'learning_rate': 7.658199206410004e-06, 'epoch': 4.68}
 94%|█████████▍| 1440/1535 [58:13<03:49,  2.42s/it] 94%|█████████▍| 1441/1535 [58:16<03:47,  2.42s/it] 94%|█████████▍| 1442/1535 [58:18<03:44,  2.42s/it] 94%|█████████▍| 1443/1535 [58:21<03:42,  2.42s/it] 94%|█████████▍| 1444/1535 [58:23<03:40,  2.42s/it] 94%|█████████▍| 1445/1535 [58:25<03:37,  2.42s/it] 94%|█████████▍| 1446/1535 [58:28<03:36,  2.43s/it] 94%|█████████▍| 1447/1535 [58:30<03:33,  2.43s/it] 94%|█████████▍| 1448/1535 [58:33<03:30,  2.43s/it] 94%|█████████▍| 1449/1535 [58:35<03:28,  2.42s/it] 94%|█████████▍| 1450/1535 [58:37<03:25,  2.42s/it] 95%|█████████▍| 1451/1535 [58:40<03:23,  2.42s/it] 95%|█████████▍| 1452/1535 [58:42<03:20,  2.42s/it] 95%|█████████▍| 1453/1535 [58:45<03:18,  2.42s/it] 95%|█████████▍| 1454/1535 [58:47<03:15,  2.42s/it] 95%|█████████▍| 1455/1535 [58:50<03:13,  2.42s/it] 95%|█████████▍| 1456/1535 [58:52<03:11,  2.42s/it] 95%|█████████▍| 1457/1535 [58:54<03:08,  2.42s/it] 95%|█████████▍| 1458/1535 [58:57<03:06,  2.42s/it] 95%|█████████▌| 1459/1535 [58:59<03:03,  2.42s/it] 95%|█████████▌| 1460/1535 [59:02<03:01,  2.43s/it]                                                   {'loss': 0.4168, 'grad_norm': 0.39042428135871887, 'learning_rate': 4.834923772301048e-06, 'epoch': 4.75}
 95%|█████████▌| 1460/1535 [59:02<03:01,  2.43s/it] 95%|█████████▌| 1461/1535 [59:04<02:59,  2.42s/it] 95%|█████████▌| 1462/1535 [59:07<02:56,  2.42s/it] 95%|█████████▌| 1463/1535 [59:09<02:54,  2.42s/it] 95%|█████████▌| 1464/1535 [59:11<02:51,  2.42s/it] 95%|█████████▌| 1465/1535 [59:14<02:49,  2.42s/it] 96%|█████████▌| 1466/1535 [59:16<02:46,  2.42s/it] 96%|█████████▌| 1467/1535 [59:19<02:44,  2.42s/it] 96%|█████████▌| 1468/1535 [59:21<02:42,  2.42s/it] 96%|█████████▌| 1469/1535 [59:23<02:39,  2.42s/it] 96%|█████████▌| 1470/1535 [59:26<02:37,  2.42s/it] 96%|█████████▌| 1471/1535 [59:28<02:34,  2.42s/it] 96%|█████████▌| 1472/1535 [59:31<02:32,  2.42s/it] 96%|█████████▌| 1473/1535 [59:33<02:30,  2.43s/it] 96%|█████████▌| 1474/1535 [59:36<02:27,  2.42s/it] 96%|█████████▌| 1475/1535 [59:38<02:25,  2.42s/it] 96%|█████████▌| 1476/1535 [59:40<02:22,  2.42s/it] 96%|█████████▌| 1477/1535 [59:43<02:20,  2.42s/it] 96%|█████████▋| 1478/1535 [59:45<02:17,  2.42s/it] 96%|█████████▋| 1479/1535 [59:48<02:15,  2.42s/it] 96%|█████████▋| 1480/1535 [59:50<02:13,  2.42s/it]                                                   {'loss': 0.4127, 'grad_norm': 0.38263243436813354, 'learning_rate': 2.6533406838507022e-06, 'epoch': 4.81}
 96%|█████████▋| 1480/1535 [59:50<02:13,  2.42s/it] 96%|█████████▋| 1481/1535 [59:52<02:10,  2.42s/it] 97%|█████████▋| 1482/1535 [59:55<02:08,  2.42s/it] 97%|█████████▋| 1483/1535 [59:57<02:05,  2.42s/it] 97%|█████████▋| 1484/1535 [1:00:00<02:03,  2.42s/it] 97%|█████████▋| 1485/1535 [1:00:02<02:00,  2.42s/it] 97%|█████████▋| 1486/1535 [1:00:05<01:58,  2.42s/it] 97%|█████████▋| 1487/1535 [1:00:07<01:56,  2.42s/it] 97%|█████████▋| 1488/1535 [1:00:09<01:53,  2.42s/it] 97%|█████████▋| 1489/1535 [1:00:12<01:51,  2.42s/it] 97%|█████████▋| 1490/1535 [1:00:14<01:48,  2.42s/it] 97%|█████████▋| 1491/1535 [1:00:17<01:46,  2.42s/it] 97%|█████████▋| 1492/1535 [1:00:19<01:43,  2.42s/it] 97%|█████████▋| 1493/1535 [1:00:22<01:41,  2.42s/it] 97%|█████████▋| 1494/1535 [1:00:24<01:39,  2.42s/it] 97%|█████████▋| 1495/1535 [1:00:26<01:36,  2.42s/it] 97%|█████████▋| 1496/1535 [1:00:29<01:34,  2.42s/it] 98%|█████████▊| 1497/1535 [1:00:31<01:31,  2.42s/it] 98%|█████████▊| 1498/1535 [1:00:34<01:29,  2.42s/it] 98%|█████████▊| 1499/1535 [1:00:36<01:27,  2.42s/it] 98%|█████████▊| 1500/1535 [1:00:38<01:24,  2.42s/it]                                                     {'loss': 0.3969, 'grad_norm': 0.43129515647888184, 'learning_rate': 1.1191599923876806e-06, 'epoch': 4.88}
 98%|█████████▊| 1500/1535 [1:00:38<01:24,  2.42s/it] 98%|█████████▊| 1501/1535 [1:00:41<01:22,  2.42s/it] 98%|█████████▊| 1502/1535 [1:00:43<01:19,  2.42s/it] 98%|█████████▊| 1503/1535 [1:00:46<01:17,  2.42s/it] 98%|█████████▊| 1504/1535 [1:00:48<01:14,  2.42s/it] 98%|█████████▊| 1505/1535 [1:00:51<01:12,  2.42s/it] 98%|█████████▊| 1506/1535 [1:00:53<01:10,  2.42s/it] 98%|█████████▊| 1507/1535 [1:00:55<01:07,  2.42s/it] 98%|█████████▊| 1508/1535 [1:00:58<01:05,  2.42s/it] 98%|█████████▊| 1509/1535 [1:01:00<01:02,  2.42s/it] 98%|█████████▊| 1510/1535 [1:01:03<01:00,  2.42s/it] 98%|█████████▊| 1511/1535 [1:01:05<00:58,  2.42s/it] 99%|█████████▊| 1512/1535 [1:01:07<00:55,  2.42s/it] 99%|█████████▊| 1513/1535 [1:01:10<00:53,  2.42s/it] 99%|█████████▊| 1514/1535 [1:01:12<00:50,  2.42s/it] 99%|█████████▊| 1515/1535 [1:01:15<00:48,  2.42s/it] 99%|█████████▉| 1516/1535 [1:01:17<00:46,  2.43s/it] 99%|█████████▉| 1517/1535 [1:01:20<00:43,  2.43s/it] 99%|█████████▉| 1518/1535 [1:01:22<00:41,  2.42s/it] 99%|█████████▉| 1519/1535 [1:01:24<00:38,  2.42s/it] 99%|█████████▉| 1520/1535 [1:01:27<00:36,  2.42s/it]                                                     {'loss': 0.3787, 'grad_norm': 0.34912529587745667, 'learning_rate': 2.3639724540239217e-07, 'epoch': 4.94}
 99%|█████████▉| 1520/1535 [1:01:27<00:36,  2.42s/it] 99%|█████████▉| 1521/1535 [1:01:29<00:33,  2.42s/it] 99%|█████████▉| 1522/1535 [1:01:32<00:31,  2.42s/it] 99%|█████████▉| 1523/1535 [1:01:34<00:29,  2.42s/it] 99%|█████████▉| 1524/1535 [1:01:37<00:26,  2.42s/it] 99%|█████████▉| 1525/1535 [1:01:39<00:24,  2.42s/it] 99%|█████████▉| 1526/1535 [1:01:41<00:21,  2.42s/it] 99%|█████████▉| 1527/1535 [1:01:44<00:19,  2.42s/it]100%|█████████▉| 1528/1535 [1:01:46<00:16,  2.42s/it]100%|█████████▉| 1529/1535 [1:01:49<00:14,  2.43s/it]100%|█████████▉| 1530/1535 [1:01:51<00:12,  2.42s/it]100%|█████████▉| 1531/1535 [1:01:53<00:09,  2.42s/it]100%|█████████▉| 1532/1535 [1:01:56<00:07,  2.42s/it]100%|█████████▉| 1533/1535 [1:01:58<00:04,  2.42s/it]100%|█████████▉| 1534/1535 [1:02:01<00:02,  2.42s/it]100%|██████████| 1535/1535 [1:02:03<00:00,  2.42s/it][INFO|trainer.py:2231] 2024-05-25 07:20:33,115 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 3745.7111, 'train_samples_per_second': 3.282, 'train_steps_per_second': 0.41, 'train_loss': 0.6102952242674191, 'epoch': 4.99}
100%|██████████| 1535/1535 [1:02:03<00:00,  2.42s/it]100%|██████████| 1535/1535 [1:02:03<00:00,  2.43s/it]
***** train metrics *****
  epoch                    =       4.99
  train_loss               =     0.6103
  train_runtime            = 1:02:25.71
  train_samples_per_second =      3.282
  train_steps_per_second   =       0.41
[INFO|trainer.py:3203] 2024-05-25 07:20:33,122 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/topic_then_length
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-05-25 07:20:34,280 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 07:20:34,283 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 07:20:35,565 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 07:20:35,570 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 07:20:35,625 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/topic_then_length/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 07:20:35,626 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/topic_then_length/special_tokens_map.json
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:471] 2024-05-25 07:20:36,308 >> Configuration saved in /scratch/tathagato/adapter_experiments/topic_then_length/final_merged_model/config.json
[INFO|configuration_utils.py:697] 2024-05-25 07:20:36,310 >> Configuration saved in /scratch/tathagato/adapter_experiments/topic_then_length/final_merged_model/generation_config.json
[INFO|modeling_utils.py:2474] 2024-05-25 07:20:42,573 >> Model weights saved in /scratch/tathagato/adapter_experiments/topic_then_length/final_merged_model/model.safetensors
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.006 MB uploadedwandb: | 0.006 MB of 0.034 MB uploadedwandb: / 0.006 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:     train/grad_norm ▂▁▂▂▁▃▁▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▁▂▂
wandb: train/learning_rate ▁▂▃▄▅▆▆▇██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁
wandb:          train/loss █▇▇▇▇▇▆▆▆▅▅▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 1.567843226180649e+17
wandb:              train/epoch 4.99
wandb:        train/global_step 1535
wandb:          train/grad_norm 0.34913
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.3787
wandb:               train_loss 0.6103
wandb:            train_runtime 3745.7111
wandb: train_samples_per_second 3.282
wandb:   train_steps_per_second 0.41
wandb: 
wandb: 🚀 View run denim-snowflake-102 at: https://wandb.ai/ihub-drug-discovery/huggingface/runs/mzcv67nv
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240525_061813-mzcv67nv/logs
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-05-25 07:21:28 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 07:21:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 07:21:28 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1 distributed training: True, 16-bits training: True
2024-05-25 07:21:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0005,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch/tathagato/adapter_experiments/topic_then_extractiveness/runs/May25_07-21-28_gnode081,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=20,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/scratch/tathagato/adapter_experiments/topic_then_extractiveness,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch/tathagato/adapter_experiments/topic_then_extractiveness,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=400,
save_strategy=steps,
save_total_limit=400,
seed=0,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
2024-05-25 07:21:28 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'q_proj', 'o_proj', 'v_proj', 'k_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
2024-05-25 07:21:28 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1 distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:726] 2024-05-25 07:21:28,471 >> loading configuration file config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 07:21:28,537 >> Model config LlamaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": false,
  "vocab_size": 32000
}

[INFO|quantizer_bnb_4bit.py:247] 2024-05-25 07:21:29,272 >> The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' 
[WARNING|modeling_utils.py:3058] 2024-05-25 07:21:29,272 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 07:21:29,272 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:3283] 2024-05-25 07:21:29,273 >> loading weights file model.safetensors from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors
[WARNING|modeling_utils.py:3058] 2024-05-25 07:21:29,275 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[WARNING|modeling_utils.py:3058] 2024-05-25 07:21:29,275 >> `low_cpu_mem_usage` was None, now set to True since model is quantized.
[INFO|modeling_utils.py:1417] 2024-05-25 07:21:29,292 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:928] 2024-05-25 07:21:29,294 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

[INFO|modeling_utils.py:4024] 2024-05-25 07:21:32,314 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-25 07:21:32,315 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-05-25 07:21:32,931 >> loading configuration file generation_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-25 07:21:32,932 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 2048,
  "pad_token_id": 0
}

loading model from : /scratch/tathagato/adapter_experiments/topic/topic
[INFO|tokenization_utils_base.py:2084] 2024-05-25 07:21:33,346 >> loading file tokenizer.model from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-05-25 07:21:33,347 >> loading file tokenizer.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 07:21:33,347 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-05-25 07:21:33,347 >> loading file special_tokens_map.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-05-25 07:21:33,347 >> loading file tokenizer_config.json from cache at /scratch/tathagato/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json
loading model from : /scratch/tathagato/adapter_experiments/topic/topic
loading model from : /scratch/tathagato/adapter_experiments/topic/topic
loading model from : /scratch/tathagato/adapter_experiments/topic/topic
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
train dataset size 4278
test dataset size 554
4278
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
Spawning 10 processes
2024-05-25 07:21:36 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894
total model parameters : 4505600
train dataset size 4278
test dataset size 554
4278
train dataset size 4278
test dataset size 554
4278
train dataset size 4278
test dataset size 554
4278
Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:11:58,  1.01s/ examples]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:07, 524.36 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 0/4278 [00:00<?, ? examples/s]Applying chat template to train_sft (num_proc=10):  16%|█▌        | 682/4278 [00:01<00:05, 602.81 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:01<00:05, 603.69 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:01<00:02, 1035.01 examples/s]Applying chat template to train_sft (num_proc=10):  35%|███▌      | 1513/4278 [00:02<00:03, 841.93 examples/s] Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:19:26,  1.11s/ examples]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:07, 483.51 examples/s]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:22:42,  1.16s/ examples]Applying chat template to train_sft (num_proc=10):   0%|          | 1/4278 [00:01<1:26:02,  1.21s/ examples]Applying chat template to train_sft (num_proc=10):  10%|█         | 428/4278 [00:01<00:08, 442.01 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:02<00:03, 643.41 examples/s]Applying chat template to train_sft (num_proc=10):  16%|█▌        | 682/4278 [00:01<00:06, 541.15 examples/s]Applying chat template to train_sft (num_proc=10):  10%|█         | 429/4278 [00:01<00:11, 335.51 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:02<00:02, 1010.94 examples/s]Applying chat template to train_sft (num_proc=10):  16%|█▌        | 678/4278 [00:01<00:06, 548.85 examples/s]Applying chat template to train_sft (num_proc=10):  17%|█▋        | 718/4278 [00:01<00:07, 457.69 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:02<00:07, 440.47 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:02<00:03, 816.98 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:02<00:07, 453.88 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1284/4278 [00:02<00:03, 844.67 examples/s]Applying chat template to train_sft (num_proc=10):  35%|███▌      | 1517/4278 [00:02<00:03, 715.68 examples/s]Applying chat template to train_sft (num_proc=10):  20%|██        | 857/4278 [00:02<00:10, 341.05 examples/s]Applying chat template to train_sft (num_proc=10):  35%|███▌      | 1517/4278 [00:02<00:04, 688.56 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1713/4278 [00:03<00:04, 621.23 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1712/4278 [00:02<00:03, 825.12 examples/s]Applying chat template to train_sft (num_proc=10):  30%|███       | 1285/4278 [00:03<00:05, 500.92 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2140/4278 [00:03<00:02, 971.85 examples/s]Applying chat template to train_sft (num_proc=10):  40%|████      | 1712/4278 [00:03<00:03, 789.08 examples/s]Applying chat template to train_sft (num_proc=10):  45%|████▍     | 1914/4278 [00:03<00:02, 844.73 examples/s]Applying chat template to train_sft (num_proc=10):  55%|█████▍    | 2346/4278 [00:04<00:05, 331.95 examples/s] Applying chat template to train_sft (num_proc=10):  56%|█████▌    | 2377/4278 [00:03<00:02, 772.91 examples/s]Applying chat template to train_sft (num_proc=10):  45%|████▌     | 1943/4278 [00:03<00:03, 706.79 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2568/4278 [00:05<00:04, 418.54 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2568/4278 [00:03<00:01, 878.75 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2141/4278 [00:03<00:03, 660.70 examples/s]Applying chat template to train_sft (num_proc=10):  65%|██████▌   | 2782/4278 [00:05<00:02, 518.09 examples/s]Applying chat template to train_sft (num_proc=10):  50%|█████     | 2141/4278 [00:04<00:03, 667.54 examples/s]Applying chat template to train_sft (num_proc=10):  66%|██████▌   | 2810/4278 [00:04<00:01, 843.61 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2569/4278 [00:04<00:02, 799.87 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2996/4278 [00:04<00:01, 916.76 examples/s]Applying chat template to train_sft (num_proc=10):  60%|██████    | 2568/4278 [00:04<00:01, 910.52 examples/s]Applying chat template to train_sft (num_proc=10):  66%|██████▋   | 2844/4278 [00:04<00:01, 941.42 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:05<00:02, 427.28 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:04<00:01, 1004.16 examples/s]Applying chat template to train_sft (num_proc=10):  75%|███████▍  | 3208/4278 [00:04<00:01, 729.65 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3424/4278 [00:06<00:01, 693.85 examples/s]Applying chat template to train_sft (num_proc=10):  70%|███████   | 2997/4278 [00:04<00:01, 794.72 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3424/4278 [00:04<00:00, 888.32 examples/s]Applying chat template to train_sft (num_proc=10):  80%|████████  | 3424/4278 [00:04<00:00, 1118.01 examples/s]Applying chat template to train_sft (num_proc=10):  75%|███████▍  | 3199/4278 [00:05<00:01, 742.34 examples/s] Applying chat template to train_sft (num_proc=10):  85%|████████▍ | 3636/4278 [00:05<00:00, 1021.07 examples/s]Applying chat template to train_sft (num_proc=10):  84%|████████▍ | 3610/4278 [00:05<00:00, 692.28 examples/s]Applying chat template to train_sft (num_proc=10):  86%|████████▌ | 3680/4278 [00:05<00:00, 1161.52 examples/s]Applying chat template to train_sft (num_proc=10):  85%|████████▌ | 3646/4278 [00:06<00:01, 578.37 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3851/4278 [00:05<00:00, 895.33 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3852/4278 [00:05<00:00, 865.98 examples/s] Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 1220.13 examples/s]Applying chat template to train_sft (num_proc=10):  90%|█████████ | 3852/4278 [00:07<00:00, 574.15 examples/s]Applying chat template to train_sft (num_proc=10):  96%|█████████▌| 4106/4278 [00:05<00:00, 819.36 examples/s]Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:07<00:00, 879.95 examples/s]Applying chat template to train_sft (num_proc=10):  96%|█████████▌| 4117/4278 [00:05<00:00, 922.51 examples/s] Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:05<00:00, 728.50 examples/s] 
Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:06<00:00, 693.54 examples/s]
Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:07<00:00, 572.51 examples/s]
Concatenating 10 shards
2024-05-25 07:21:44 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Applying chat template to train_sft (num_proc=10): 100%|██████████| 4278/4278 [00:06<00:00, 673.74 examples/s]
Spawning 10 processes
2024-05-25 07:21:44 - INFO - datasets.arrow_dataset - Spawning 10 processes
Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 0/554 [00:00<?, ? examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<04:10,  2.20 examples/s]Applying chat template to test_sft (num_proc=10):  10%|█         | 57/554 [00:00<00:03, 134.49 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<05:17,  1.74 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<05:24,  1.70 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:01, 221.02 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 189.05 examples/s]Applying chat template to test_sft (num_proc=10):  10%|█         | 57/554 [00:00<00:04, 107.21 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:00<00:00, 358.12 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:00<00:01, 296.14 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 280/554 [00:01<00:00, 383.78 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 153.89 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 389/554 [00:01<00:00, 548.21 examples/s]Applying chat template to test_sft (num_proc=10):  51%|█████     | 280/554 [00:01<00:00, 336.76 examples/s]Applying chat template to test_sft (num_proc=10):   0%|          | 1/554 [00:00<06:51,  1.34 examples/s]Applying chat template to test_sft (num_proc=10):  41%|████      | 225/554 [00:01<00:01, 304.15 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 389/554 [00:01<00:00, 488.59 examples/s]Applying chat template to test_sft (num_proc=10):  10%|█         | 57/554 [00:00<00:05, 89.22 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 499/554 [00:01<00:00, 569.80 examples/s]Applying chat template to test_sft (num_proc=10):  20%|██        | 113/554 [00:00<00:02, 168.57 examples/s]Applying chat template to test_sft (num_proc=10):  60%|██████    | 335/554 [00:01<00:00, 396.34 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 499/554 [00:01<00:00, 489.70 examples/s]Applying chat template to test_sft (num_proc=10):  40%|████      | 224/554 [00:01<00:00, 345.73 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 390/554 [00:01<00:00, 391.76 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 347.38 examples/s]
Applying chat template to test_sft (num_proc=10):  80%|████████  | 445/554 [00:01<00:00, 413.43 examples/s]Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 325.94 examples/s]
Applying chat template to test_sft (num_proc=10):  60%|██████    | 334/554 [00:01<00:00, 396.45 examples/s]Concatenating 10 shards
2024-05-25 07:21:46 - INFO - datasets.arrow_dataset - Concatenating 10 shards
tokenizer padding side left
Applying chat template to test_sft (num_proc=10):  90%|█████████ | 500/554 [00:01<00:00, 436.80 examples/s]Applying chat template to test_sft (num_proc=10):  70%|███████   | 390/554 [00:01<00:00, 413.81 examples/s]tokenizer padding side left
Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 292.94 examples/s]
Applying chat template to test_sft (num_proc=10):  80%|████████  | 445/554 [00:01<00:00, 394.33 examples/s]Applying chat template to test_sft (num_proc=10):  90%|█████████ | 500/554 [00:01<00:00, 402.08 examples/s]tokenizer padding side left
Applying chat template to test_sft (num_proc=10): 100%|██████████| 554/554 [00:01<00:00, 281.97 examples/s]
Using custom data configuration default-37faee928f8259ce
2024-05-25 07:21:47 - INFO - datasets.builder - Using custom data configuration default-37faee928f8259ce
Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
2024-05-25 07:21:47 - INFO - datasets.info - Loading Dataset Infos from /home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/datasets/packaged_modules/generator
Overwrite dataset info from restored data version if exists.
2024-05-25 07:21:47 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0
2024-05-25 07:21:47 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0
Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0)
2024-05-25 07:21:47 - INFO - datasets.builder - Found cached dataset generator (/home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0)
Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0
2024-05-25 07:21:47 - INFO - datasets.info - Loading Dataset info from /home2/tathagato/.cache/huggingface/datasets/generator/default-37faee928f8259ce/0.0.0
tokenizer padding side left
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
2024-05-25 07:21:48 - WARNING - accelerate.utils.other - Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
[INFO|trainer.py:607] 2024-05-25 07:21:50,956 >> Using auto half precision backend
is  model parallelism  ParallelMode.DISTRIBUTED
is  model parallelism  ParallelMode.DISTRIBUTED
[INFO|trainer.py:1969] 2024-05-25 07:21:51,208 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-25 07:21:51,208 >>   Num examples = 2,462
[INFO|trainer.py:1971] 2024-05-25 07:21:51,208 >>   Num Epochs = 5
[INFO|trainer.py:1972] 2024-05-25 07:21:51,208 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-25 07:21:51,208 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-05-25 07:21:51,208 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-05-25 07:21:51,208 >>   Total optimization steps = 1,540
[INFO|trainer.py:1978] 2024-05-25 07:21:51,210 >>   Number of trainable parameters = 4,505,600
[INFO|integration_utils.py:723] 2024-05-25 07:21:51,275 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: roy3 (ihub-drug-discovery). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /home2/tathagato/summarization/MACSum/experiments/wandb/run-20240525_072157-d65347to
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-microwave-103
wandb: ⭐️ View project at https://wandb.ai/ihub-drug-discovery/huggingface
wandb: 🚀 View run at https://wandb.ai/ihub-drug-discovery/huggingface/runs/d65347to
  0%|          | 0/1540 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/1540 [00:02<1:03:24,  2.47s/it]  0%|          | 2/1540 [00:04<1:02:14,  2.43s/it]  0%|          | 3/1540 [00:07<1:01:52,  2.42s/it]  0%|          | 4/1540 [00:09<1:01:40,  2.41s/it]  0%|          | 5/1540 [00:12<1:01:36,  2.41s/it]  0%|          | 6/1540 [00:14<1:01:32,  2.41s/it]  0%|          | 7/1540 [00:16<1:01:31,  2.41s/it]  1%|          | 8/1540 [00:19<1:01:28,  2.41s/it]  1%|          | 9/1540 [00:21<1:01:33,  2.41s/it]  1%|          | 10/1540 [00:24<1:01:27,  2.41s/it]  1%|          | 11/1540 [00:26<1:01:22,  2.41s/it]  1%|          | 12/1540 [00:29<1:01:56,  2.43s/it]  1%|          | 13/1540 [00:31<1:02:16,  2.45s/it]  1%|          | 14/1540 [00:33<1:01:56,  2.44s/it]  1%|          | 15/1540 [00:36<1:01:41,  2.43s/it]  1%|          | 16/1540 [00:38<1:01:29,  2.42s/it]  1%|          | 17/1540 [00:41<1:01:21,  2.42s/it]  1%|          | 18/1540 [00:43<1:01:14,  2.41s/it]  1%|          | 19/1540 [00:45<1:01:09,  2.41s/it]  1%|▏         | 20/1540 [00:48<1:01:06,  2.41s/it]                                                   {'loss': 0.9627, 'grad_norm': 0.3349049687385559, 'learning_rate': 3.246753246753247e-05, 'epoch': 0.06}
  1%|▏         | 20/1540 [00:48<1:01:06,  2.41s/it]  1%|▏         | 21/1540 [00:50<1:01:12,  2.42s/it]  1%|▏         | 22/1540 [00:53<1:01:10,  2.42s/it]  1%|▏         | 23/1540 [00:55<1:01:05,  2.42s/it]  2%|▏         | 24/1540 [00:58<1:01:01,  2.42s/it]  2%|▏         | 25/1540 [01:00<1:01:00,  2.42s/it]  2%|▏         | 26/1540 [01:02<1:01:09,  2.42s/it]  2%|▏         | 27/1540 [01:05<1:01:02,  2.42s/it]  2%|▏         | 28/1540 [01:07<1:00:58,  2.42s/it]  2%|▏         | 29/1540 [01:10<1:01:08,  2.43s/it]  2%|▏         | 30/1540 [01:12<1:01:01,  2.42s/it]  2%|▏         | 31/1540 [01:15<1:00:56,  2.42s/it]  2%|▏         | 32/1540 [01:17<1:00:52,  2.42s/it]  2%|▏         | 33/1540 [01:19<1:00:48,  2.42s/it]  2%|▏         | 34/1540 [01:22<1:00:43,  2.42s/it]  2%|▏         | 35/1540 [01:24<1:00:39,  2.42s/it]  2%|▏         | 36/1540 [01:27<1:00:37,  2.42s/it]  2%|▏         | 37/1540 [01:29<1:00:35,  2.42s/it]  2%|▏         | 38/1540 [01:31<1:00:33,  2.42s/it]  3%|▎         | 39/1540 [01:34<1:00:32,  2.42s/it]  3%|▎         | 40/1540 [01:36<1:00:29,  2.42s/it]                                                   {'loss': 0.9265, 'grad_norm': 0.37487247586250305, 'learning_rate': 6.493506493506494e-05, 'epoch': 0.13}
  3%|▎         | 40/1540 [01:36<1:00:29,  2.42s/it]  3%|▎         | 41/1540 [01:39<1:00:27,  2.42s/it]  3%|▎         | 42/1540 [01:41<1:00:36,  2.43s/it]  3%|▎         | 43/1540 [01:44<1:00:29,  2.42s/it]  3%|▎         | 44/1540 [01:46<1:00:23,  2.42s/it]  3%|▎         | 45/1540 [01:48<1:00:20,  2.42s/it]  3%|▎         | 46/1540 [01:51<1:00:17,  2.42s/it]  3%|▎         | 47/1540 [01:53<1:00:14,  2.42s/it]  3%|▎         | 48/1540 [01:56<1:00:11,  2.42s/it]  3%|▎         | 49/1540 [01:58<1:00:08,  2.42s/it]  3%|▎         | 50/1540 [02:00<1:00:05,  2.42s/it]  3%|▎         | 51/1540 [02:03<1:00:03,  2.42s/it]  3%|▎         | 52/1540 [02:05<1:00:00,  2.42s/it]  3%|▎         | 53/1540 [02:08<59:58,  2.42s/it]    4%|▎         | 54/1540 [02:10<1:00:05,  2.43s/it]  4%|▎         | 55/1540 [02:13<59:59,  2.42s/it]    4%|▎         | 56/1540 [02:15<1:00:27,  2.44s/it]  4%|▎         | 57/1540 [02:18<1:00:14,  2.44s/it]  4%|▍         | 58/1540 [02:20<1:00:04,  2.43s/it]  4%|▍         | 59/1540 [02:22<59:57,  2.43s/it]    4%|▍         | 60/1540 [02:25<59:51,  2.43s/it]                                                 {'loss': 0.9137, 'grad_norm': 0.45463690161705017, 'learning_rate': 9.577922077922078e-05, 'epoch': 0.19}
  4%|▍         | 60/1540 [02:25<59:51,  2.43s/it]  4%|▍         | 61/1540 [02:27<59:47,  2.43s/it]  4%|▍         | 62/1540 [02:30<59:43,  2.42s/it]  4%|▍         | 63/1540 [02:32<59:39,  2.42s/it]  4%|▍         | 64/1540 [02:34<59:36,  2.42s/it]  4%|▍         | 65/1540 [02:37<59:33,  2.42s/it]  4%|▍         | 66/1540 [02:39<59:29,  2.42s/it]  4%|▍         | 67/1540 [02:42<59:26,  2.42s/it]  4%|▍         | 68/1540 [02:44<59:24,  2.42s/it]  4%|▍         | 69/1540 [02:47<59:21,  2.42s/it]  5%|▍         | 70/1540 [02:49<59:38,  2.43s/it]  5%|▍         | 71/1540 [02:51<59:31,  2.43s/it]  5%|▍         | 72/1540 [02:54<59:24,  2.43s/it]  5%|▍         | 73/1540 [02:56<59:19,  2.43s/it]  5%|▍         | 74/1540 [02:59<59:15,  2.43s/it]  5%|▍         | 75/1540 [03:01<59:15,  2.43s/it]  5%|▍         | 76/1540 [03:04<59:10,  2.43s/it]  5%|▌         | 77/1540 [03:06<59:07,  2.42s/it]  5%|▌         | 78/1540 [03:08<59:03,  2.42s/it]  5%|▌         | 79/1540 [03:11<58:59,  2.42s/it]  5%|▌         | 80/1540 [03:13<58:57,  2.42s/it]                                                 {'loss': 0.9388, 'grad_norm': 0.39316368103027344, 'learning_rate': 0.00012824675324675324, 'epoch': 0.26}
  5%|▌         | 80/1540 [03:13<58:57,  2.42s/it]  5%|▌         | 81/1540 [03:16<58:56,  2.42s/it]  5%|▌         | 82/1540 [03:18<58:52,  2.42s/it]  5%|▌         | 83/1540 [03:21<59:02,  2.43s/it]  5%|▌         | 84/1540 [03:23<58:55,  2.43s/it]  6%|▌         | 85/1540 [03:25<58:49,  2.43s/it]  6%|▌         | 86/1540 [03:28<58:47,  2.43s/it]  6%|▌         | 87/1540 [03:30<58:43,  2.43s/it]  6%|▌         | 88/1540 [03:33<58:40,  2.42s/it]  6%|▌         | 89/1540 [03:35<58:37,  2.42s/it]  6%|▌         | 90/1540 [03:38<58:33,  2.42s/it]  6%|▌         | 91/1540 [03:40<58:31,  2.42s/it]  6%|▌         | 92/1540 [03:42<58:28,  2.42s/it]  6%|▌         | 93/1540 [03:45<58:26,  2.42s/it]  6%|▌         | 94/1540 [03:47<58:25,  2.42s/it]  6%|▌         | 95/1540 [03:50<58:22,  2.42s/it]  6%|▌         | 96/1540 [03:52<58:19,  2.42s/it]  6%|▋         | 97/1540 [03:55<58:17,  2.42s/it]  6%|▋         | 98/1540 [03:57<58:24,  2.43s/it]  6%|▋         | 99/1540 [03:59<58:18,  2.43s/it]  6%|▋         | 100/1540 [04:02<58:13,  2.43s/it]                                                  {'loss': 0.9712, 'grad_norm': 0.315456360578537, 'learning_rate': 0.00016071428571428573, 'epoch': 0.32}
  6%|▋         | 100/1540 [04:02<58:13,  2.43s/it]  7%|▋         | 101/1540 [04:04<58:10,  2.43s/it]  7%|▋         | 102/1540 [04:07<58:08,  2.43s/it]  7%|▋         | 103/1540 [04:09<58:04,  2.43s/it]  7%|▋         | 104/1540 [04:11<58:00,  2.42s/it]  7%|▋         | 105/1540 [04:14<57:58,  2.42s/it]  7%|▋         | 106/1540 [04:16<57:55,  2.42s/it]  7%|▋         | 107/1540 [04:19<57:52,  2.42s/it]  7%|▋         | 108/1540 [04:21<57:49,  2.42s/it]  7%|▋         | 109/1540 [04:24<57:47,  2.42s/it]  7%|▋         | 110/1540 [04:26<57:44,  2.42s/it]  7%|▋         | 111/1540 [04:28<57:48,  2.43s/it]  7%|▋         | 112/1540 [04:31<57:44,  2.43s/it]  7%|▋         | 113/1540 [04:33<57:41,  2.43s/it]  7%|▋         | 114/1540 [04:36<57:38,  2.43s/it]  7%|▋         | 115/1540 [04:38<57:34,  2.42s/it]  8%|▊         | 116/1540 [04:41<57:31,  2.42s/it]  8%|▊         | 117/1540 [04:43<57:28,  2.42s/it]  8%|▊         | 118/1540 [04:45<57:25,  2.42s/it]  8%|▊         | 119/1540 [04:48<57:22,  2.42s/it]  8%|▊         | 120/1540 [04:50<57:19,  2.42s/it]                                                  {'loss': 0.913, 'grad_norm': 0.2901560366153717, 'learning_rate': 0.00019318181818181817, 'epoch': 0.39}
  8%|▊         | 120/1540 [04:50<57:19,  2.42s/it]  8%|▊         | 121/1540 [04:53<57:19,  2.42s/it]  8%|▊         | 122/1540 [04:55<57:16,  2.42s/it]  8%|▊         | 123/1540 [04:58<57:13,  2.42s/it]  8%|▊         | 124/1540 [05:00<57:10,  2.42s/it]  8%|▊         | 125/1540 [05:02<57:08,  2.42s/it]  8%|▊         | 126/1540 [05:05<57:41,  2.45s/it]  8%|▊         | 127/1540 [05:07<57:28,  2.44s/it]  8%|▊         | 128/1540 [05:10<57:18,  2.43s/it]  8%|▊         | 129/1540 [05:12<57:09,  2.43s/it]  8%|▊         | 130/1540 [05:15<57:03,  2.43s/it]  9%|▊         | 131/1540 [05:17<56:59,  2.43s/it]  9%|▊         | 132/1540 [05:19<56:54,  2.43s/it]  9%|▊         | 133/1540 [05:22<56:51,  2.42s/it]  9%|▊         | 134/1540 [05:24<56:47,  2.42s/it]  9%|▉         | 135/1540 [05:27<56:44,  2.42s/it]  9%|▉         | 136/1540 [05:29<56:41,  2.42s/it]  9%|▉         | 137/1540 [05:32<56:39,  2.42s/it]  9%|▉         | 138/1540 [05:34<56:36,  2.42s/it]  9%|▉         | 139/1540 [05:36<56:39,  2.43s/it]  9%|▉         | 140/1540 [05:39<56:34,  2.42s/it]                                                  {'loss': 0.8695, 'grad_norm': 0.3249622583389282, 'learning_rate': 0.00022564935064935067, 'epoch': 0.45}
  9%|▉         | 140/1540 [05:39<56:34,  2.42s/it]  9%|▉         | 141/1540 [05:41<56:31,  2.42s/it]  9%|▉         | 142/1540 [05:44<56:27,  2.42s/it]  9%|▉         | 143/1540 [05:46<56:24,  2.42s/it]  9%|▉         | 144/1540 [05:49<56:21,  2.42s/it]  9%|▉         | 145/1540 [05:51<56:18,  2.42s/it]  9%|▉         | 146/1540 [05:53<56:16,  2.42s/it] 10%|▉         | 147/1540 [05:56<56:13,  2.42s/it] 10%|▉         | 148/1540 [05:58<56:10,  2.42s/it] 10%|▉         | 149/1540 [06:01<56:08,  2.42s/it] 10%|▉         | 150/1540 [06:03<56:05,  2.42s/it] 10%|▉         | 151/1540 [06:05<56:03,  2.42s/it] 10%|▉         | 152/1540 [06:08<56:13,  2.43s/it] 10%|▉         | 153/1540 [06:10<56:07,  2.43s/it] 10%|█         | 154/1540 [06:13<56:01,  2.43s/it] 10%|█         | 155/1540 [06:15<55:57,  2.42s/it] 10%|█         | 156/1540 [06:18<55:54,  2.42s/it] 10%|█         | 157/1540 [06:20<55:51,  2.42s/it] 10%|█         | 158/1540 [06:22<55:48,  2.42s/it] 10%|█         | 159/1540 [06:25<55:46,  2.42s/it] 10%|█         | 160/1540 [06:27<55:42,  2.42s/it]                                                  {'loss': 0.9055, 'grad_norm': 0.3224767744541168, 'learning_rate': 0.00025811688311688314, 'epoch': 0.52}
 10%|█         | 160/1540 [06:27<55:42,  2.42s/it] 10%|█         | 161/1540 [06:30<55:41,  2.42s/it] 11%|█         | 162/1540 [06:32<55:38,  2.42s/it] 11%|█         | 163/1540 [06:35<55:35,  2.42s/it] 11%|█         | 164/1540 [06:37<55:32,  2.42s/it] 11%|█         | 165/1540 [06:39<55:30,  2.42s/it] 11%|█         | 166/1540 [06:42<55:27,  2.42s/it] 11%|█         | 167/1540 [06:44<55:24,  2.42s/it] 11%|█         | 168/1540 [06:47<55:31,  2.43s/it] 11%|█         | 169/1540 [06:49<55:26,  2.43s/it] 11%|█         | 170/1540 [06:52<55:22,  2.43s/it] 11%|█         | 171/1540 [06:54<55:18,  2.42s/it] 11%|█         | 172/1540 [06:56<55:14,  2.42s/it] 11%|█         | 173/1540 [06:59<55:11,  2.42s/it] 11%|█▏        | 174/1540 [07:01<55:08,  2.42s/it] 11%|█▏        | 175/1540 [07:04<55:05,  2.42s/it] 11%|█▏        | 176/1540 [07:06<55:02,  2.42s/it] 11%|█▏        | 177/1540 [07:08<55:00,  2.42s/it] 12%|█▏        | 178/1540 [07:11<54:58,  2.42s/it] 12%|█▏        | 179/1540 [07:13<54:55,  2.42s/it] 12%|█▏        | 180/1540 [07:16<54:53,  2.42s/it]                                                  {'loss': 0.8572, 'grad_norm': 0.2940329313278198, 'learning_rate': 0.0002905844155844156, 'epoch': 0.58}
 12%|█▏        | 180/1540 [07:16<54:53,  2.42s/it] 12%|█▏        | 181/1540 [07:18<54:52,  2.42s/it] 12%|█▏        | 182/1540 [07:21<54:49,  2.42s/it] 12%|█▏        | 183/1540 [07:23<54:47,  2.42s/it] 12%|█▏        | 184/1540 [07:25<54:51,  2.43s/it] 12%|█▏        | 185/1540 [07:28<54:46,  2.43s/it] 12%|█▏        | 186/1540 [07:30<54:41,  2.42s/it] 12%|█▏        | 187/1540 [07:33<54:38,  2.42s/it] 12%|█▏        | 188/1540 [07:35<54:34,  2.42s/it] 12%|█▏        | 189/1540 [07:38<54:31,  2.42s/it] 12%|█▏        | 190/1540 [07:40<54:28,  2.42s/it] 12%|█▏        | 191/1540 [07:42<54:26,  2.42s/it] 12%|█▏        | 192/1540 [07:45<54:23,  2.42s/it] 13%|█▎        | 193/1540 [07:47<54:21,  2.42s/it] 13%|█▎        | 194/1540 [07:50<54:18,  2.42s/it] 13%|█▎        | 195/1540 [07:52<54:44,  2.44s/it] 13%|█▎        | 196/1540 [07:55<54:32,  2.44s/it] 13%|█▎        | 197/1540 [07:57<54:25,  2.43s/it] 13%|█▎        | 198/1540 [07:59<54:18,  2.43s/it] 13%|█▎        | 199/1540 [08:02<54:13,  2.43s/it] 13%|█▎        | 200/1540 [08:04<54:08,  2.42s/it]                                                  {'loss': 0.8406, 'grad_norm': 0.3517863154411316, 'learning_rate': 0.000323051948051948, 'epoch': 0.65}
 13%|█▎        | 200/1540 [08:04<54:08,  2.42s/it] 13%|█▎        | 201/1540 [08:07<54:05,  2.42s/it] 13%|█▎        | 202/1540 [08:09<54:01,  2.42s/it] 13%|█▎        | 203/1540 [08:12<53:57,  2.42s/it] 13%|█▎        | 204/1540 [08:14<53:54,  2.42s/it] 13%|█▎        | 205/1540 [08:16<53:52,  2.42s/it] 13%|█▎        | 206/1540 [08:19<53:49,  2.42s/it] 13%|█▎        | 207/1540 [08:21<53:46,  2.42s/it] 14%|█▎        | 208/1540 [08:24<53:44,  2.42s/it] 14%|█▎        | 209/1540 [08:26<53:47,  2.42s/it] 14%|█▎        | 210/1540 [08:28<53:42,  2.42s/it] 14%|█▎        | 211/1540 [08:31<53:39,  2.42s/it] 14%|█▍        | 212/1540 [08:33<53:35,  2.42s/it] 14%|█▍        | 213/1540 [08:36<53:32,  2.42s/it] 14%|█▍        | 214/1540 [08:38<53:29,  2.42s/it] 14%|█▍        | 215/1540 [08:41<53:27,  2.42s/it] 14%|█▍        | 216/1540 [08:43<53:25,  2.42s/it] 14%|█▍        | 217/1540 [08:45<53:22,  2.42s/it] 14%|█▍        | 218/1540 [08:48<53:20,  2.42s/it] 14%|█▍        | 219/1540 [08:50<53:17,  2.42s/it] 14%|█▍        | 220/1540 [08:53<53:15,  2.42s/it]                                                  {'loss': 0.899, 'grad_norm': 0.4229690134525299, 'learning_rate': 0.00035551948051948054, 'epoch': 0.71}
 14%|█▍        | 220/1540 [08:53<53:15,  2.42s/it] 14%|█▍        | 221/1540 [08:55<53:13,  2.42s/it] 14%|█▍        | 222/1540 [08:58<53:14,  2.42s/it] 14%|█▍        | 223/1540 [09:00<53:10,  2.42s/it] 15%|█▍        | 224/1540 [09:02<53:07,  2.42s/it] 15%|█▍        | 225/1540 [09:05<53:03,  2.42s/it] 15%|█▍        | 226/1540 [09:07<53:00,  2.42s/it] 15%|█▍        | 227/1540 [09:10<52:58,  2.42s/it] 15%|█▍        | 228/1540 [09:12<52:56,  2.42s/it] 15%|█▍        | 229/1540 [09:14<52:53,  2.42s/it] 15%|█▍        | 230/1540 [09:17<52:50,  2.42s/it] 15%|█▌        | 231/1540 [09:19<52:48,  2.42s/it] 15%|█▌        | 232/1540 [09:22<52:45,  2.42s/it] 15%|█▌        | 233/1540 [09:24<52:43,  2.42s/it] 15%|█▌        | 234/1540 [09:27<52:41,  2.42s/it] 15%|█▌        | 235/1540 [09:29<52:38,  2.42s/it] 15%|█▌        | 236/1540 [09:31<52:36,  2.42s/it] 15%|█▌        | 237/1540 [09:34<52:45,  2.43s/it] 15%|█▌        | 238/1540 [09:36<52:38,  2.43s/it] 16%|█▌        | 239/1540 [09:39<52:33,  2.42s/it] 16%|█▌        | 240/1540 [09:41<52:29,  2.42s/it]                                                  {'loss': 0.8269, 'grad_norm': 0.3053847551345825, 'learning_rate': 0.000387987012987013, 'epoch': 0.78}
 16%|█▌        | 240/1540 [09:41<52:29,  2.42s/it] 16%|█▌        | 241/1540 [09:44<52:27,  2.42s/it] 16%|█▌        | 242/1540 [09:46<52:35,  2.43s/it] 16%|█▌        | 243/1540 [09:48<52:28,  2.43s/it] 16%|█▌        | 244/1540 [09:51<52:23,  2.43s/it] 16%|█▌        | 245/1540 [09:53<52:18,  2.42s/it] 16%|█▌        | 246/1540 [09:56<52:14,  2.42s/it] 16%|█▌        | 247/1540 [09:58<52:13,  2.42s/it] 16%|█▌        | 248/1540 [10:01<52:09,  2.42s/it] 16%|█▌        | 249/1540 [10:03<52:05,  2.42s/it] 16%|█▌        | 250/1540 [10:05<52:02,  2.42s/it] 16%|█▋        | 251/1540 [10:08<51:59,  2.42s/it] 16%|█▋        | 252/1540 [10:10<51:57,  2.42s/it] 16%|█▋        | 253/1540 [10:13<51:54,  2.42s/it] 16%|█▋        | 254/1540 [10:15<51:52,  2.42s/it] 17%|█▋        | 255/1540 [10:17<51:49,  2.42s/it] 17%|█▋        | 256/1540 [10:20<51:47,  2.42s/it] 17%|█▋        | 257/1540 [10:22<51:45,  2.42s/it] 17%|█▋        | 258/1540 [10:25<51:42,  2.42s/it] 17%|█▋        | 259/1540 [10:27<51:40,  2.42s/it] 17%|█▋        | 260/1540 [10:30<51:37,  2.42s/it]                                                  {'loss': 0.8515, 'grad_norm': 0.26355016231536865, 'learning_rate': 0.0004204545454545455, 'epoch': 0.84}
 17%|█▋        | 260/1540 [10:30<51:37,  2.42s/it] 17%|█▋        | 261/1540 [10:32<51:36,  2.42s/it] 17%|█▋        | 262/1540 [10:34<51:33,  2.42s/it] 17%|█▋        | 263/1540 [10:37<51:31,  2.42s/it] 17%|█▋        | 264/1540 [10:39<51:28,  2.42s/it] 17%|█▋        | 265/1540 [10:42<51:50,  2.44s/it] 17%|█▋        | 266/1540 [10:44<51:40,  2.43s/it] 17%|█▋        | 267/1540 [10:47<51:33,  2.43s/it] 17%|█▋        | 268/1540 [10:49<51:27,  2.43s/it] 17%|█▋        | 269/1540 [10:51<51:22,  2.43s/it] 18%|█▊        | 270/1540 [10:54<51:17,  2.42s/it] 18%|█▊        | 271/1540 [10:56<51:14,  2.42s/it] 18%|█▊        | 272/1540 [10:59<51:11,  2.42s/it] 18%|█▊        | 273/1540 [11:01<51:08,  2.42s/it] 18%|█▊        | 274/1540 [11:03<51:05,  2.42s/it] 18%|█▊        | 275/1540 [11:06<51:02,  2.42s/it] 18%|█▊        | 276/1540 [11:08<50:59,  2.42s/it] 18%|█▊        | 277/1540 [11:11<50:57,  2.42s/it] 18%|█▊        | 278/1540 [11:13<51:02,  2.43s/it] 18%|█▊        | 279/1540 [11:16<50:57,  2.42s/it] 18%|█▊        | 280/1540 [11:18<50:53,  2.42s/it]                                                  {'loss': 0.8241, 'grad_norm': 0.28401023149490356, 'learning_rate': 0.00045292207792207794, 'epoch': 0.91}
 18%|█▊        | 280/1540 [11:18<50:53,  2.42s/it] 18%|█▊        | 281/1540 [11:20<50:50,  2.42s/it] 18%|█▊        | 282/1540 [11:23<50:46,  2.42s/it] 18%|█▊        | 283/1540 [11:25<50:43,  2.42s/it] 18%|█▊        | 284/1540 [11:28<50:40,  2.42s/it] 19%|█▊        | 285/1540 [11:30<50:38,  2.42s/it] 19%|█▊        | 286/1540 [11:33<50:43,  2.43s/it] 19%|█▊        | 287/1540 [11:35<50:38,  2.42s/it] 19%|█▊        | 288/1540 [11:37<50:34,  2.42s/it] 19%|█▉        | 289/1540 [11:40<50:30,  2.42s/it] 19%|█▉        | 290/1540 [11:42<50:27,  2.42s/it] 19%|█▉        | 291/1540 [11:45<50:24,  2.42s/it] 19%|█▉        | 292/1540 [11:47<50:21,  2.42s/it] 19%|█▉        | 293/1540 [11:50<50:18,  2.42s/it] 19%|█▉        | 294/1540 [11:52<50:16,  2.42s/it] 19%|█▉        | 295/1540 [11:54<50:13,  2.42s/it] 19%|█▉        | 296/1540 [11:57<50:10,  2.42s/it] 19%|█▉        | 297/1540 [11:59<50:09,  2.42s/it] 19%|█▉        | 298/1540 [12:02<50:06,  2.42s/it] 19%|█▉        | 299/1540 [12:04<50:04,  2.42s/it] 19%|█▉        | 300/1540 [12:06<50:01,  2.42s/it]                                                  {'loss': 0.8575, 'grad_norm': 0.3434644341468811, 'learning_rate': 0.00048538961038961035, 'epoch': 0.97}
 19%|█▉        | 300/1540 [12:06<50:01,  2.42s/it] 20%|█▉        | 301/1540 [12:09<49:59,  2.42s/it] 20%|█▉        | 302/1540 [12:11<49:56,  2.42s/it] 20%|█▉        | 303/1540 [12:14<49:54,  2.42s/it] 20%|█▉        | 304/1540 [12:16<49:52,  2.42s/it] 20%|█▉        | 305/1540 [12:19<49:49,  2.42s/it] 20%|█▉        | 306/1540 [12:21<49:46,  2.42s/it] 20%|█▉        | 307/1540 [12:23<49:56,  2.43s/it] 20%|██        | 308/1540 [12:26<49:49,  2.43s/it] 20%|██        | 309/1540 [12:28<49:45,  2.42s/it] 20%|██        | 310/1540 [12:31<49:41,  2.42s/it] 20%|██        | 311/1540 [12:33<49:37,  2.42s/it] 20%|██        | 312/1540 [12:36<49:33,  2.42s/it] 20%|██        | 313/1540 [12:38<49:30,  2.42s/it] 20%|██        | 314/1540 [12:40<49:28,  2.42s/it] 20%|██        | 315/1540 [12:43<49:25,  2.42s/it] 21%|██        | 316/1540 [12:45<49:22,  2.42s/it] 21%|██        | 317/1540 [12:48<49:19,  2.42s/it] 21%|██        | 318/1540 [12:50<49:17,  2.42s/it] 21%|██        | 319/1540 [12:52<49:14,  2.42s/it] 21%|██        | 320/1540 [12:55<49:11,  2.42s/it]                                                  {'loss': 0.8292, 'grad_norm': 0.42198610305786133, 'learning_rate': 0.0004999016565957633, 'epoch': 1.04}
 21%|██        | 320/1540 [12:55<49:11,  2.42s/it] 21%|██        | 321/1540 [12:57<49:10,  2.42s/it] 21%|██        | 322/1540 [13:00<49:07,  2.42s/it] 21%|██        | 323/1540 [13:02<49:05,  2.42s/it] 21%|██        | 324/1540 [13:05<49:02,  2.42s/it] 21%|██        | 325/1540 [13:07<49:00,  2.42s/it] 21%|██        | 326/1540 [13:09<48:57,  2.42s/it] 21%|██        | 327/1540 [13:12<48:54,  2.42s/it] 21%|██▏       | 328/1540 [13:14<48:53,  2.42s/it] 21%|██▏       | 329/1540 [13:17<48:50,  2.42s/it] 21%|██▏       | 330/1540 [13:19<48:48,  2.42s/it] 21%|██▏       | 331/1540 [13:22<48:45,  2.42s/it] 22%|██▏       | 332/1540 [13:24<48:44,  2.42s/it] 22%|██▏       | 333/1540 [13:26<48:41,  2.42s/it] 22%|██▏       | 334/1540 [13:29<49:05,  2.44s/it] 22%|██▏       | 335/1540 [13:31<48:55,  2.44s/it] 22%|██▏       | 336/1540 [13:34<48:47,  2.43s/it] 22%|██▏       | 337/1540 [13:36<48:40,  2.43s/it] 22%|██▏       | 338/1540 [13:39<48:35,  2.43s/it] 22%|██▏       | 339/1540 [13:41<48:30,  2.42s/it] 22%|██▏       | 340/1540 [13:43<48:26,  2.42s/it]                                                  {'loss': 0.7857, 'grad_norm': 0.3553069531917572, 'learning_rate': 0.0004992192975102804, 'epoch': 1.1}
 22%|██▏       | 340/1540 [13:43<48:26,  2.42s/it] 22%|██▏       | 341/1540 [13:46<48:24,  2.42s/it] 22%|██▏       | 342/1540 [13:48<48:20,  2.42s/it] 22%|██▏       | 343/1540 [13:51<48:17,  2.42s/it] 22%|██▏       | 344/1540 [13:53<48:14,  2.42s/it] 22%|██▏       | 345/1540 [13:55<48:11,  2.42s/it] 22%|██▏       | 346/1540 [13:58<48:09,  2.42s/it] 23%|██▎       | 347/1540 [14:00<48:06,  2.42s/it] 23%|██▎       | 348/1540 [14:03<48:12,  2.43s/it] 23%|██▎       | 349/1540 [14:05<48:07,  2.42s/it] 23%|██▎       | 350/1540 [14:08<48:03,  2.42s/it] 23%|██▎       | 351/1540 [14:10<48:00,  2.42s/it] 23%|██▎       | 352/1540 [14:12<47:56,  2.42s/it] 23%|██▎       | 353/1540 [14:15<47:55,  2.42s/it] 23%|██▎       | 354/1540 [14:17<47:51,  2.42s/it] 23%|██▎       | 355/1540 [14:20<47:48,  2.42s/it] 23%|██▎       | 356/1540 [14:22<47:45,  2.42s/it] 23%|██▎       | 357/1540 [14:25<47:42,  2.42s/it] 23%|██▎       | 358/1540 [14:27<47:41,  2.42s/it] 23%|██▎       | 359/1540 [14:29<47:38,  2.42s/it] 23%|██▎       | 360/1540 [14:32<47:36,  2.42s/it]                                                  {'loss': 0.7297, 'grad_norm': 0.40408095717430115, 'learning_rate': 0.0004978888625516589, 'epoch': 1.17}
 23%|██▎       | 360/1540 [14:32<47:36,  2.42s/it] 23%|██▎       | 361/1540 [14:34<47:35,  2.42s/it] 24%|██▎       | 362/1540 [14:37<47:33,  2.42s/it] 24%|██▎       | 363/1540 [14:39<47:30,  2.42s/it] 24%|██▎       | 364/1540 [14:41<47:27,  2.42s/it] 24%|██▎       | 365/1540 [14:44<47:24,  2.42s/it] 24%|██▍       | 366/1540 [14:46<47:21,  2.42s/it] 24%|██▍       | 367/1540 [14:49<47:19,  2.42s/it] 24%|██▍       | 368/1540 [14:51<47:16,  2.42s/it] 24%|██▍       | 369/1540 [14:54<47:14,  2.42s/it] 24%|██▍       | 370/1540 [14:56<47:11,  2.42s/it] 24%|██▍       | 371/1540 [14:58<47:09,  2.42s/it] 24%|██▍       | 372/1540 [15:01<47:06,  2.42s/it] 24%|██▍       | 373/1540 [15:03<47:13,  2.43s/it] 24%|██▍       | 374/1540 [15:06<47:08,  2.43s/it] 24%|██▍       | 375/1540 [15:08<47:03,  2.42s/it] 24%|██▍       | 376/1540 [15:11<47:07,  2.43s/it] 24%|██▍       | 377/1540 [15:13<47:01,  2.43s/it] 25%|██▍       | 378/1540 [15:15<46:56,  2.42s/it] 25%|██▍       | 379/1540 [15:18<46:52,  2.42s/it] 25%|██▍       | 380/1540 [15:20<46:49,  2.42s/it]                                                  {'loss': 0.7709, 'grad_norm': 0.4290267825126648, 'learning_rate': 0.0004959138114150592, 'epoch': 1.23}
 25%|██▍       | 380/1540 [15:20<46:49,  2.42s/it] 25%|██▍       | 381/1540 [15:23<46:47,  2.42s/it] 25%|██▍       | 382/1540 [15:25<46:43,  2.42s/it] 25%|██▍       | 383/1540 [15:28<46:40,  2.42s/it] 25%|██▍       | 384/1540 [15:30<46:38,  2.42s/it] 25%|██▌       | 385/1540 [15:32<46:35,  2.42s/it] 25%|██▌       | 386/1540 [15:35<46:32,  2.42s/it] 25%|██▌       | 387/1540 [15:37<46:30,  2.42s/it] 25%|██▌       | 388/1540 [15:40<46:28,  2.42s/it] 25%|██▌       | 389/1540 [15:42<46:32,  2.43s/it] 25%|██▌       | 390/1540 [15:44<46:27,  2.42s/it] 25%|██▌       | 391/1540 [15:47<46:23,  2.42s/it] 25%|██▌       | 392/1540 [15:49<46:20,  2.42s/it] 26%|██▌       | 393/1540 [15:52<46:17,  2.42s/it] 26%|██▌       | 394/1540 [15:54<46:14,  2.42s/it] 26%|██▌       | 395/1540 [15:57<46:11,  2.42s/it] 26%|██▌       | 396/1540 [15:59<46:09,  2.42s/it] 26%|██▌       | 397/1540 [16:01<46:06,  2.42s/it] 26%|██▌       | 398/1540 [16:04<46:04,  2.42s/it] 26%|██▌       | 399/1540 [16:06<46:02,  2.42s/it] 26%|██▌       | 400/1540 [16:09<45:59,  2.42s/it]                                                  {'loss': 0.7286, 'grad_norm': 0.41215240955352783, 'learning_rate': 0.0004932992800711009, 'epoch': 1.3}
 26%|██▌       | 400/1540 [16:09<45:59,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 07:38:22,536 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-400
[INFO|configuration_utils.py:726] 2024-05-25 07:38:23,421 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 07:38:23,423 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 07:38:23,494 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 07:38:23,494 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-400/special_tokens_map.json
 26%|██▌       | 401/1540 [16:12<52:13,  2.75s/it] 26%|██▌       | 402/1540 [16:15<50:18,  2.65s/it] 26%|██▌       | 403/1540 [16:17<48:56,  2.58s/it] 26%|██▌       | 404/1540 [16:20<48:20,  2.55s/it] 26%|██▋       | 405/1540 [16:22<47:32,  2.51s/it] 26%|██▋       | 406/1540 [16:24<46:57,  2.48s/it] 26%|██▋       | 407/1540 [16:27<46:32,  2.47s/it] 26%|██▋       | 408/1540 [16:29<46:15,  2.45s/it] 27%|██▋       | 409/1540 [16:32<46:02,  2.44s/it] 27%|██▋       | 410/1540 [16:34<45:52,  2.44s/it] 27%|██▋       | 411/1540 [16:36<45:45,  2.43s/it] 27%|██▋       | 412/1540 [16:39<45:38,  2.43s/it] 27%|██▋       | 413/1540 [16:41<45:33,  2.43s/it] 27%|██▋       | 414/1540 [16:44<45:29,  2.42s/it] 27%|██▋       | 415/1540 [16:46<46:01,  2.45s/it] 27%|██▋       | 416/1540 [16:49<45:46,  2.44s/it] 27%|██▋       | 417/1540 [16:51<45:44,  2.44s/it] 27%|██▋       | 418/1540 [16:54<45:33,  2.44s/it] 27%|██▋       | 419/1540 [16:56<45:25,  2.43s/it] 27%|██▋       | 420/1540 [16:58<45:19,  2.43s/it]                                                  {'loss': 0.7794, 'grad_norm': 0.345782995223999, 'learning_rate': 0.0004900520674101607, 'epoch': 1.36}
 27%|██▋       | 420/1540 [16:58<45:19,  2.43s/it] 27%|██▋       | 421/1540 [17:01<45:15,  2.43s/it] 27%|██▋       | 422/1540 [17:03<45:10,  2.42s/it] 27%|██▋       | 423/1540 [17:06<45:07,  2.42s/it] 28%|██▊       | 424/1540 [17:08<45:03,  2.42s/it] 28%|██▊       | 425/1540 [17:10<45:00,  2.42s/it] 28%|██▊       | 426/1540 [17:13<44:57,  2.42s/it] 28%|██▊       | 427/1540 [17:15<44:54,  2.42s/it] 28%|██▊       | 428/1540 [17:18<44:51,  2.42s/it] 28%|██▊       | 429/1540 [17:20<44:48,  2.42s/it] 28%|██▊       | 430/1540 [17:23<44:46,  2.42s/it] 28%|██▊       | 431/1540 [17:25<44:43,  2.42s/it] 28%|██▊       | 432/1540 [17:27<44:41,  2.42s/it] 28%|██▊       | 433/1540 [17:30<44:39,  2.42s/it] 28%|██▊       | 434/1540 [17:32<44:37,  2.42s/it] 28%|██▊       | 435/1540 [17:35<44:34,  2.42s/it] 28%|██▊       | 436/1540 [17:37<44:31,  2.42s/it] 28%|██▊       | 437/1540 [17:40<44:32,  2.42s/it] 28%|██▊       | 438/1540 [17:42<44:29,  2.42s/it] 29%|██▊       | 439/1540 [17:44<44:26,  2.42s/it] 29%|██▊       | 440/1540 [17:47<44:24,  2.42s/it]                                                  {'loss': 0.7158, 'grad_norm': 0.4016563594341278, 'learning_rate': 0.0004861806175623745, 'epoch': 1.43}
 29%|██▊       | 440/1540 [17:47<44:24,  2.42s/it] 29%|██▊       | 441/1540 [17:49<44:21,  2.42s/it] 29%|██▊       | 442/1540 [17:52<44:18,  2.42s/it] 29%|██▉       | 443/1540 [17:54<44:20,  2.42s/it] 29%|██▉       | 444/1540 [17:56<44:15,  2.42s/it] 29%|██▉       | 445/1540 [17:59<44:12,  2.42s/it] 29%|██▉       | 446/1540 [18:01<44:18,  2.43s/it] 29%|██▉       | 447/1540 [18:04<44:13,  2.43s/it] 29%|██▉       | 448/1540 [18:06<44:08,  2.43s/it] 29%|██▉       | 449/1540 [18:09<44:04,  2.42s/it] 29%|██▉       | 450/1540 [18:11<44:00,  2.42s/it] 29%|██▉       | 451/1540 [18:13<43:57,  2.42s/it] 29%|██▉       | 452/1540 [18:16<43:55,  2.42s/it] 29%|██▉       | 453/1540 [18:18<43:53,  2.42s/it] 29%|██▉       | 454/1540 [18:21<43:50,  2.42s/it] 30%|██▉       | 455/1540 [18:23<43:47,  2.42s/it] 30%|██▉       | 456/1540 [18:26<43:44,  2.42s/it] 30%|██▉       | 457/1540 [18:28<43:41,  2.42s/it] 30%|██▉       | 458/1540 [18:30<43:39,  2.42s/it] 30%|██▉       | 459/1540 [18:33<43:41,  2.42s/it] 30%|██▉       | 460/1540 [18:35<43:37,  2.42s/it]                                                  {'loss': 0.7686, 'grad_norm': 0.43257206678390503, 'learning_rate': 0.0004816949979393171, 'epoch': 1.49}
 30%|██▉       | 460/1540 [18:35<43:37,  2.42s/it] 30%|██▉       | 461/1540 [18:38<43:34,  2.42s/it] 30%|███       | 462/1540 [18:40<43:30,  2.42s/it] 30%|███       | 463/1540 [18:43<43:28,  2.42s/it] 30%|███       | 464/1540 [18:45<43:25,  2.42s/it] 30%|███       | 465/1540 [18:47<43:22,  2.42s/it] 30%|███       | 466/1540 [18:50<43:19,  2.42s/it] 30%|███       | 467/1540 [18:52<43:17,  2.42s/it] 30%|███       | 468/1540 [18:55<43:14,  2.42s/it] 30%|███       | 469/1540 [18:57<43:12,  2.42s/it] 31%|███       | 470/1540 [18:59<43:09,  2.42s/it] 31%|███       | 471/1540 [19:02<43:07,  2.42s/it] 31%|███       | 472/1540 [19:04<43:04,  2.42s/it] 31%|███       | 473/1540 [19:07<43:23,  2.44s/it] 31%|███       | 474/1540 [19:09<43:14,  2.43s/it] 31%|███       | 475/1540 [19:12<43:07,  2.43s/it] 31%|███       | 476/1540 [19:14<43:01,  2.43s/it] 31%|███       | 477/1540 [19:16<42:56,  2.42s/it] 31%|███       | 478/1540 [19:19<42:52,  2.42s/it] 31%|███       | 479/1540 [19:21<42:49,  2.42s/it] 31%|███       | 480/1540 [19:24<42:46,  2.42s/it]                                                  {'loss': 0.7616, 'grad_norm': 0.4735579192638397, 'learning_rate': 0.00047660687305446235, 'epoch': 1.56}
 31%|███       | 480/1540 [19:24<42:46,  2.42s/it] 31%|███       | 481/1540 [19:26<42:44,  2.42s/it] 31%|███▏      | 482/1540 [19:29<42:41,  2.42s/it] 31%|███▏      | 483/1540 [19:31<42:38,  2.42s/it] 31%|███▏      | 484/1540 [19:33<42:36,  2.42s/it] 31%|███▏      | 485/1540 [19:36<42:33,  2.42s/it] 32%|███▏      | 486/1540 [19:38<42:30,  2.42s/it] 32%|███▏      | 487/1540 [19:41<42:32,  2.42s/it] 32%|███▏      | 488/1540 [19:43<42:28,  2.42s/it] 32%|███▏      | 489/1540 [19:46<42:25,  2.42s/it] 32%|███▏      | 490/1540 [19:48<42:22,  2.42s/it] 32%|███▏      | 491/1540 [19:50<42:19,  2.42s/it] 32%|███▏      | 492/1540 [19:53<42:16,  2.42s/it] 32%|███▏      | 493/1540 [19:55<42:14,  2.42s/it] 32%|███▏      | 494/1540 [19:58<42:13,  2.42s/it] 32%|███▏      | 495/1540 [20:00<42:10,  2.42s/it] 32%|███▏      | 496/1540 [20:02<42:07,  2.42s/it] 32%|███▏      | 497/1540 [20:05<42:04,  2.42s/it] 32%|███▏      | 498/1540 [20:07<42:02,  2.42s/it] 32%|███▏      | 499/1540 [20:10<41:59,  2.42s/it] 32%|███▏      | 500/1540 [20:12<41:56,  2.42s/it]                                                  {'loss': 0.7442, 'grad_norm': nan, 'learning_rate': 0.0004712271175980134, 'epoch': 1.62}
 32%|███▏      | 500/1540 [20:12<41:56,  2.42s/it] 33%|███▎      | 501/1540 [20:15<41:55,  2.42s/it] 33%|███▎      | 502/1540 [20:17<41:52,  2.42s/it] 33%|███▎      | 503/1540 [20:19<41:50,  2.42s/it] 33%|███▎      | 504/1540 [20:22<41:47,  2.42s/it] 33%|███▎      | 505/1540 [20:24<41:44,  2.42s/it] 33%|███▎      | 506/1540 [20:27<41:42,  2.42s/it] 33%|███▎      | 507/1540 [20:29<41:40,  2.42s/it] 33%|███▎      | 508/1540 [20:32<41:37,  2.42s/it] 33%|███▎      | 509/1540 [20:34<41:35,  2.42s/it] 33%|███▎      | 510/1540 [20:36<41:32,  2.42s/it] 33%|███▎      | 511/1540 [20:39<41:30,  2.42s/it] 33%|███▎      | 512/1540 [20:41<41:27,  2.42s/it] 33%|███▎      | 513/1540 [20:44<41:25,  2.42s/it] 33%|███▎      | 514/1540 [20:46<41:22,  2.42s/it] 33%|███▎      | 515/1540 [20:48<41:27,  2.43s/it] 34%|███▎      | 516/1540 [20:51<41:22,  2.42s/it] 34%|███▎      | 517/1540 [20:53<41:22,  2.43s/it] 34%|███▎      | 518/1540 [20:56<41:17,  2.42s/it] 34%|███▎      | 519/1540 [20:58<41:13,  2.42s/it] 34%|███▍      | 520/1540 [21:01<41:10,  2.42s/it]                                                  {'loss': 0.6686, 'grad_norm': 0.3766549825668335, 'learning_rate': 0.0004650035600519251, 'epoch': 1.69}
 34%|███▍      | 520/1540 [21:01<41:10,  2.42s/it] 34%|███▍      | 521/1540 [21:03<41:07,  2.42s/it] 34%|███▍      | 522/1540 [21:05<41:04,  2.42s/it] 34%|███▍      | 523/1540 [21:08<41:01,  2.42s/it] 34%|███▍      | 524/1540 [21:10<40:59,  2.42s/it] 34%|███▍      | 525/1540 [21:13<40:56,  2.42s/it] 34%|███▍      | 526/1540 [21:15<40:53,  2.42s/it] 34%|███▍      | 527/1540 [21:18<40:51,  2.42s/it] 34%|███▍      | 528/1540 [21:20<40:50,  2.42s/it] 34%|███▍      | 529/1540 [21:22<40:47,  2.42s/it] 34%|███▍      | 530/1540 [21:25<40:44,  2.42s/it] 34%|███▍      | 531/1540 [21:27<40:41,  2.42s/it] 35%|███▍      | 532/1540 [21:30<40:39,  2.42s/it] 35%|███▍      | 533/1540 [21:32<40:36,  2.42s/it] 35%|███▍      | 534/1540 [21:34<40:34,  2.42s/it] 35%|███▍      | 535/1540 [21:37<40:32,  2.42s/it] 35%|███▍      | 536/1540 [21:39<40:30,  2.42s/it] 35%|███▍      | 537/1540 [21:42<40:27,  2.42s/it] 35%|███▍      | 538/1540 [21:44<40:25,  2.42s/it] 35%|███▌      | 539/1540 [21:47<40:22,  2.42s/it] 35%|███▌      | 540/1540 [21:49<40:20,  2.42s/it]                                                  {'loss': 0.7021, 'grad_norm': 0.4114067554473877, 'learning_rate': 0.0004582209020617679, 'epoch': 1.75}
 35%|███▌      | 540/1540 [21:49<40:20,  2.42s/it] 35%|███▌      | 541/1540 [21:51<40:18,  2.42s/it] 35%|███▌      | 542/1540 [21:54<40:15,  2.42s/it] 35%|███▌      | 543/1540 [21:56<40:31,  2.44s/it] 35%|███▌      | 544/1540 [21:59<40:23,  2.43s/it] 35%|███▌      | 545/1540 [22:01<40:17,  2.43s/it] 35%|███▌      | 546/1540 [22:04<40:12,  2.43s/it] 36%|███▌      | 547/1540 [22:06<40:07,  2.42s/it] 36%|███▌      | 548/1540 [22:08<40:03,  2.42s/it] 36%|███▌      | 549/1540 [22:11<40:00,  2.42s/it] 36%|███▌      | 550/1540 [22:13<39:57,  2.42s/it] 36%|███▌      | 551/1540 [22:16<39:54,  2.42s/it] 36%|███▌      | 552/1540 [22:18<39:51,  2.42s/it] 36%|███▌      | 553/1540 [22:21<39:48,  2.42s/it] 36%|███▌      | 554/1540 [22:23<39:46,  2.42s/it] 36%|███▌      | 555/1540 [22:25<39:43,  2.42s/it] 36%|███▌      | 556/1540 [22:28<39:44,  2.42s/it] 36%|███▌      | 557/1540 [22:30<39:41,  2.42s/it] 36%|███▌      | 558/1540 [22:33<39:37,  2.42s/it] 36%|███▋      | 559/1540 [22:35<39:35,  2.42s/it] 36%|███▋      | 560/1540 [22:37<39:32,  2.42s/it]                                                  {'loss': 0.6596, 'grad_norm': 0.4850960075855255, 'learning_rate': 0.0004508967814149967, 'epoch': 1.82}
 36%|███▋      | 560/1540 [22:37<39:32,  2.42s/it] 36%|███▋      | 561/1540 [22:40<39:30,  2.42s/it] 36%|███▋      | 562/1540 [22:42<39:27,  2.42s/it] 37%|███▋      | 563/1540 [22:45<39:25,  2.42s/it] 37%|███▋      | 564/1540 [22:47<39:22,  2.42s/it] 37%|███▋      | 565/1540 [22:50<39:19,  2.42s/it] 37%|███▋      | 566/1540 [22:52<39:17,  2.42s/it] 37%|███▋      | 567/1540 [22:54<39:15,  2.42s/it] 37%|███▋      | 568/1540 [22:57<39:13,  2.42s/it] 37%|███▋      | 569/1540 [22:59<39:11,  2.42s/it] 37%|███▋      | 570/1540 [23:02<39:08,  2.42s/it] 37%|███▋      | 571/1540 [23:04<39:05,  2.42s/it] 37%|███▋      | 572/1540 [23:07<39:03,  2.42s/it] 37%|███▋      | 573/1540 [23:09<39:00,  2.42s/it] 37%|███▋      | 574/1540 [23:11<38:57,  2.42s/it] 37%|███▋      | 575/1540 [23:14<38:55,  2.42s/it] 37%|███▋      | 576/1540 [23:16<38:53,  2.42s/it] 37%|███▋      | 577/1540 [23:19<38:50,  2.42s/it] 38%|███▊      | 578/1540 [23:21<38:47,  2.42s/it] 38%|███▊      | 579/1540 [23:23<38:45,  2.42s/it] 38%|███▊      | 580/1540 [23:26<38:43,  2.42s/it]                                                  {'loss': 0.6934, 'grad_norm': 0.4100537598133087, 'learning_rate': 0.0004430502439316204, 'epoch': 1.88}
 38%|███▊      | 580/1540 [23:26<38:43,  2.42s/it] 38%|███▊      | 581/1540 [23:28<38:41,  2.42s/it] 38%|███▊      | 582/1540 [23:31<38:39,  2.42s/it] 38%|███▊      | 583/1540 [23:33<38:36,  2.42s/it] 38%|███▊      | 584/1540 [23:36<38:33,  2.42s/it] 38%|███▊      | 585/1540 [23:38<38:31,  2.42s/it] 38%|███▊      | 586/1540 [23:40<38:29,  2.42s/it] 38%|███▊      | 587/1540 [23:43<38:27,  2.42s/it] 38%|███▊      | 588/1540 [23:45<38:24,  2.42s/it] 38%|███▊      | 589/1540 [23:48<38:22,  2.42s/it] 38%|███▊      | 590/1540 [23:50<38:19,  2.42s/it] 38%|███▊      | 591/1540 [23:52<38:16,  2.42s/it] 38%|███▊      | 592/1540 [23:55<38:14,  2.42s/it] 39%|███▊      | 593/1540 [23:57<38:11,  2.42s/it] 39%|███▊      | 594/1540 [24:00<38:09,  2.42s/it] 39%|███▊      | 595/1540 [24:02<38:07,  2.42s/it] 39%|███▊      | 596/1540 [24:05<38:04,  2.42s/it] 39%|███▉      | 597/1540 [24:07<38:02,  2.42s/it] 39%|███▉      | 598/1540 [24:09<38:01,  2.42s/it] 39%|███▉      | 599/1540 [24:12<37:58,  2.42s/it] 39%|███▉      | 600/1540 [24:14<37:55,  2.42s/it]                                                  {'loss': 0.7308, 'grad_norm': 0.4114418923854828, 'learning_rate': 0.000434701693936992, 'epoch': 1.95}
 39%|███▉      | 600/1540 [24:14<37:55,  2.42s/it] 39%|███▉      | 601/1540 [24:17<37:53,  2.42s/it] 39%|███▉      | 602/1540 [24:19<37:50,  2.42s/it] 39%|███▉      | 603/1540 [24:22<37:47,  2.42s/it] 39%|███▉      | 604/1540 [24:24<37:48,  2.42s/it] 39%|███▉      | 605/1540 [24:26<37:45,  2.42s/it] 39%|███▉      | 606/1540 [24:29<37:42,  2.42s/it] 39%|███▉      | 607/1540 [24:31<37:38,  2.42s/it] 39%|███▉      | 608/1540 [24:34<37:35,  2.42s/it] 40%|███▉      | 609/1540 [24:36<37:33,  2.42s/it] 40%|███▉      | 610/1540 [24:38<37:30,  2.42s/it] 40%|███▉      | 611/1540 [24:41<37:28,  2.42s/it] 40%|███▉      | 612/1540 [24:43<37:47,  2.44s/it] 40%|███▉      | 613/1540 [24:46<37:38,  2.44s/it] 40%|███▉      | 614/1540 [24:48<37:31,  2.43s/it] 40%|███▉      | 615/1540 [24:51<37:25,  2.43s/it] 40%|████      | 616/1540 [24:53<37:20,  2.42s/it] 40%|████      | 617/1540 [24:56<37:17,  2.42s/it] 40%|████      | 618/1540 [24:58<37:13,  2.42s/it] 40%|████      | 619/1540 [25:00<37:10,  2.42s/it] 40%|████      | 620/1540 [25:03<37:07,  2.42s/it]                                                  {'loss': 0.6242, 'grad_norm': 0.4817900061607361, 'learning_rate': 0.00042587284120190896, 'epoch': 2.01}
 40%|████      | 620/1540 [25:03<37:07,  2.42s/it] 40%|████      | 621/1540 [25:05<37:05,  2.42s/it] 40%|████      | 622/1540 [25:08<37:02,  2.42s/it] 40%|████      | 623/1540 [25:10<36:59,  2.42s/it] 41%|████      | 624/1540 [25:12<36:57,  2.42s/it] 41%|████      | 625/1540 [25:15<36:54,  2.42s/it] 41%|████      | 626/1540 [25:17<36:54,  2.42s/it] 41%|████      | 627/1540 [25:20<36:51,  2.42s/it] 41%|████      | 628/1540 [25:22<36:48,  2.42s/it] 41%|████      | 629/1540 [25:25<36:45,  2.42s/it] 41%|████      | 630/1540 [25:27<36:43,  2.42s/it] 41%|████      | 631/1540 [25:29<36:40,  2.42s/it] 41%|████      | 632/1540 [25:32<36:37,  2.42s/it] 41%|████      | 633/1540 [25:34<36:35,  2.42s/it] 41%|████      | 634/1540 [25:37<36:32,  2.42s/it] 41%|████      | 635/1540 [25:39<36:30,  2.42s/it] 41%|████▏     | 636/1540 [25:42<36:27,  2.42s/it] 41%|████▏     | 637/1540 [25:44<36:25,  2.42s/it] 41%|████▏     | 638/1540 [25:46<36:23,  2.42s/it] 41%|████▏     | 639/1540 [25:49<36:23,  2.42s/it] 42%|████▏     | 640/1540 [25:51<36:20,  2.42s/it]                                                  {'loss': 0.6025, 'grad_norm': 0.38645270466804504, 'learning_rate': 0.000416586644488001, 'epoch': 2.08}
 42%|████▏     | 640/1540 [25:51<36:20,  2.42s/it] 42%|████▏     | 641/1540 [25:54<36:17,  2.42s/it] 42%|████▏     | 642/1540 [25:56<36:14,  2.42s/it] 42%|████▏     | 643/1540 [25:58<36:11,  2.42s/it] 42%|████▏     | 644/1540 [26:01<36:08,  2.42s/it] 42%|████▏     | 645/1540 [26:03<36:05,  2.42s/it] 42%|████▏     | 646/1540 [26:06<36:03,  2.42s/it] 42%|████▏     | 647/1540 [26:08<36:00,  2.42s/it] 42%|████▏     | 648/1540 [26:11<35:58,  2.42s/it] 42%|████▏     | 649/1540 [26:13<35:55,  2.42s/it] 42%|████▏     | 650/1540 [26:15<35:53,  2.42s/it] 42%|████▏     | 651/1540 [26:18<35:51,  2.42s/it] 42%|████▏     | 652/1540 [26:20<35:49,  2.42s/it] 42%|████▏     | 653/1540 [26:23<35:46,  2.42s/it] 42%|████▏     | 654/1540 [26:25<35:44,  2.42s/it] 43%|████▎     | 655/1540 [26:27<35:41,  2.42s/it] 43%|████▎     | 656/1540 [26:30<35:39,  2.42s/it] 43%|████▎     | 657/1540 [26:32<35:36,  2.42s/it] 43%|████▎     | 658/1540 [26:35<35:34,  2.42s/it] 43%|████▎     | 659/1540 [26:37<35:31,  2.42s/it] 43%|████▎     | 660/1540 [26:40<35:29,  2.42s/it]                                                  {'loss': 0.6377, 'grad_norm': 0.4365008473396301, 'learning_rate': 0.000406867251845213, 'epoch': 2.14}
 43%|████▎     | 660/1540 [26:40<35:29,  2.42s/it] 43%|████▎     | 661/1540 [26:42<35:27,  2.42s/it] 43%|████▎     | 662/1540 [26:44<35:25,  2.42s/it] 43%|████▎     | 663/1540 [26:47<35:22,  2.42s/it] 43%|████▎     | 664/1540 [26:49<35:20,  2.42s/it] 43%|████▎     | 665/1540 [26:52<35:17,  2.42s/it] 43%|████▎     | 666/1540 [26:54<35:15,  2.42s/it] 43%|████▎     | 667/1540 [26:57<35:16,  2.42s/it] 43%|████▎     | 668/1540 [26:59<35:12,  2.42s/it] 43%|████▎     | 669/1540 [27:01<35:09,  2.42s/it] 44%|████▎     | 670/1540 [27:04<35:06,  2.42s/it] 44%|████▎     | 671/1540 [27:06<35:03,  2.42s/it] 44%|████▎     | 672/1540 [27:09<35:01,  2.42s/it] 44%|████▎     | 673/1540 [27:11<34:58,  2.42s/it] 44%|████▍     | 674/1540 [27:13<34:56,  2.42s/it] 44%|████▍     | 675/1540 [27:16<34:53,  2.42s/it] 44%|████▍     | 676/1540 [27:18<34:51,  2.42s/it] 44%|████▍     | 677/1540 [27:21<34:48,  2.42s/it] 44%|████▍     | 678/1540 [27:23<34:45,  2.42s/it] 44%|████▍     | 679/1540 [27:26<34:44,  2.42s/it] 44%|████▍     | 680/1540 [27:28<34:41,  2.42s/it]                                                  {'loss': 0.5872, 'grad_norm': 0.3745628893375397, 'learning_rate': 0.0003967399378166333, 'epoch': 2.21}
 44%|████▍     | 680/1540 [27:28<34:41,  2.42s/it] 44%|████▍     | 681/1540 [27:30<34:39,  2.42s/it] 44%|████▍     | 682/1540 [27:33<34:57,  2.44s/it] 44%|████▍     | 683/1540 [27:35<34:48,  2.44s/it] 44%|████▍     | 684/1540 [27:38<34:41,  2.43s/it] 44%|████▍     | 685/1540 [27:40<34:36,  2.43s/it] 45%|████▍     | 686/1540 [27:43<34:31,  2.43s/it] 45%|████▍     | 687/1540 [27:45<34:27,  2.42s/it] 45%|████▍     | 688/1540 [27:47<34:24,  2.42s/it] 45%|████▍     | 689/1540 [27:50<34:20,  2.42s/it] 45%|████▍     | 690/1540 [27:52<34:17,  2.42s/it] 45%|████▍     | 691/1540 [27:55<34:21,  2.43s/it] 45%|████▍     | 692/1540 [27:57<34:16,  2.43s/it] 45%|████▌     | 693/1540 [28:00<34:12,  2.42s/it] 45%|████▌     | 694/1540 [28:02<34:09,  2.42s/it] 45%|████▌     | 695/1540 [28:04<34:06,  2.42s/it] 45%|████▌     | 696/1540 [28:07<34:03,  2.42s/it] 45%|████▌     | 697/1540 [28:09<34:00,  2.42s/it] 45%|████▌     | 698/1540 [28:12<33:57,  2.42s/it] 45%|████▌     | 699/1540 [28:14<33:55,  2.42s/it] 45%|████▌     | 700/1540 [28:17<33:54,  2.42s/it]                                                  {'loss': 0.5611, 'grad_norm': 0.45378556847572327, 'learning_rate': 0.00038623103771396195, 'epoch': 2.27}
 45%|████▌     | 700/1540 [28:17<33:54,  2.42s/it] 46%|████▌     | 701/1540 [28:19<33:52,  2.42s/it] 46%|████▌     | 702/1540 [28:21<33:49,  2.42s/it] 46%|████▌     | 703/1540 [28:24<33:46,  2.42s/it] 46%|████▌     | 704/1540 [28:26<33:43,  2.42s/it] 46%|████▌     | 705/1540 [28:29<33:41,  2.42s/it] 46%|████▌     | 706/1540 [28:31<33:38,  2.42s/it] 46%|████▌     | 707/1540 [28:33<33:36,  2.42s/it] 46%|████▌     | 708/1540 [28:36<33:33,  2.42s/it] 46%|████▌     | 709/1540 [28:38<33:31,  2.42s/it] 46%|████▌     | 710/1540 [28:41<33:28,  2.42s/it] 46%|████▌     | 711/1540 [28:43<33:26,  2.42s/it] 46%|████▌     | 712/1540 [28:46<33:23,  2.42s/it] 46%|████▋     | 713/1540 [28:48<33:21,  2.42s/it] 46%|████▋     | 714/1540 [28:50<33:19,  2.42s/it] 46%|████▋     | 715/1540 [28:53<33:16,  2.42s/it] 46%|████▋     | 716/1540 [28:55<33:14,  2.42s/it] 47%|████▋     | 717/1540 [28:58<33:11,  2.42s/it] 47%|████▋     | 718/1540 [29:00<33:08,  2.42s/it] 47%|████▋     | 719/1540 [29:03<33:06,  2.42s/it] 47%|████▋     | 720/1540 [29:05<33:04,  2.42s/it]                                                  {'loss': 0.5538, 'grad_norm': 0.4519694447517395, 'learning_rate': 0.00037536787913453106, 'epoch': 2.34}
 47%|████▋     | 720/1540 [29:05<33:04,  2.42s/it] 47%|████▋     | 721/1540 [29:07<33:02,  2.42s/it] 47%|████▋     | 722/1540 [29:10<32:59,  2.42s/it] 47%|████▋     | 723/1540 [29:12<32:56,  2.42s/it] 47%|████▋     | 724/1540 [29:15<32:59,  2.43s/it] 47%|████▋     | 725/1540 [29:17<32:55,  2.42s/it] 47%|████▋     | 726/1540 [29:19<32:52,  2.42s/it] 47%|████▋     | 727/1540 [29:22<32:48,  2.42s/it] 47%|████▋     | 728/1540 [29:24<32:46,  2.42s/it] 47%|████▋     | 729/1540 [29:27<32:43,  2.42s/it] 47%|████▋     | 730/1540 [29:29<32:40,  2.42s/it] 47%|████▋     | 731/1540 [29:32<32:38,  2.42s/it] 48%|████▊     | 732/1540 [29:34<32:36,  2.42s/it] 48%|████▊     | 733/1540 [29:36<32:33,  2.42s/it] 48%|████▊     | 734/1540 [29:39<32:30,  2.42s/it] 48%|████▊     | 735/1540 [29:41<32:28,  2.42s/it] 48%|████▊     | 736/1540 [29:44<32:25,  2.42s/it] 48%|████▊     | 737/1540 [29:46<32:25,  2.42s/it] 48%|████▊     | 738/1540 [29:49<32:22,  2.42s/it] 48%|████▊     | 739/1540 [29:51<32:19,  2.42s/it] 48%|████▊     | 740/1540 [29:53<32:16,  2.42s/it]                                                  {'loss': 0.5662, 'grad_norm': 0.47021204233169556, 'learning_rate': 0.0003641787108979617, 'epoch': 2.4}
 48%|████▊     | 740/1540 [29:53<32:16,  2.42s/it] 48%|████▊     | 741/1540 [29:56<32:14,  2.42s/it] 48%|████▊     | 742/1540 [29:58<32:11,  2.42s/it] 48%|████▊     | 743/1540 [30:01<32:08,  2.42s/it] 48%|████▊     | 744/1540 [30:03<32:06,  2.42s/it] 48%|████▊     | 745/1540 [30:05<32:03,  2.42s/it] 48%|████▊     | 746/1540 [30:08<32:01,  2.42s/it] 49%|████▊     | 747/1540 [30:10<31:58,  2.42s/it] 49%|████▊     | 748/1540 [30:13<31:56,  2.42s/it] 49%|████▊     | 749/1540 [30:15<31:54,  2.42s/it] 49%|████▊     | 750/1540 [30:18<31:51,  2.42s/it] 49%|████▉     | 751/1540 [30:20<32:06,  2.44s/it] 49%|████▉     | 752/1540 [30:22<31:58,  2.44s/it] 49%|████▉     | 753/1540 [30:25<31:52,  2.43s/it] 49%|████▉     | 754/1540 [30:27<31:47,  2.43s/it] 49%|████▉     | 755/1540 [30:30<31:43,  2.42s/it] 49%|████▉     | 756/1540 [30:32<31:39,  2.42s/it] 49%|████▉     | 757/1540 [30:35<31:36,  2.42s/it] 49%|████▉     | 758/1540 [30:37<31:33,  2.42s/it] 49%|████▉     | 759/1540 [30:39<31:30,  2.42s/it] 49%|████▉     | 760/1540 [30:42<31:27,  2.42s/it]                                                  {'loss': 0.584, 'grad_norm': 0.41844773292541504, 'learning_rate': 0.00035269262958725125, 'epoch': 2.47}
 49%|████▉     | 760/1540 [30:42<31:27,  2.42s/it] 49%|████▉     | 761/1540 [30:44<31:25,  2.42s/it] 49%|████▉     | 762/1540 [30:47<31:23,  2.42s/it] 50%|████▉     | 763/1540 [30:49<31:20,  2.42s/it] 50%|████▉     | 764/1540 [30:51<31:18,  2.42s/it] 50%|████▉     | 765/1540 [30:54<31:15,  2.42s/it] 50%|████▉     | 766/1540 [30:56<31:13,  2.42s/it] 50%|████▉     | 767/1540 [30:59<31:10,  2.42s/it] 50%|████▉     | 768/1540 [31:01<31:08,  2.42s/it] 50%|████▉     | 769/1540 [31:04<31:05,  2.42s/it] 50%|█████     | 770/1540 [31:06<31:03,  2.42s/it] 50%|█████     | 771/1540 [31:08<31:00,  2.42s/it] 50%|█████     | 772/1540 [31:11<30:58,  2.42s/it] 50%|█████     | 773/1540 [31:13<30:55,  2.42s/it] 50%|█████     | 774/1540 [31:16<30:53,  2.42s/it] 50%|█████     | 775/1540 [31:18<30:50,  2.42s/it] 50%|█████     | 776/1540 [31:21<30:48,  2.42s/it] 50%|█████     | 777/1540 [31:23<30:46,  2.42s/it] 51%|█████     | 778/1540 [31:25<30:46,  2.42s/it] 51%|█████     | 779/1540 [31:28<30:42,  2.42s/it] 51%|█████     | 780/1540 [31:30<30:39,  2.42s/it]                                                  {'loss': 0.5634, 'grad_norm': 0.40611985325813293, 'learning_rate': 0.00034093950388531787, 'epoch': 2.53}
 51%|█████     | 780/1540 [31:30<30:39,  2.42s/it] 51%|█████     | 781/1540 [31:33<30:37,  2.42s/it] 51%|█████     | 782/1540 [31:35<30:35,  2.42s/it] 51%|█████     | 783/1540 [31:37<30:32,  2.42s/it] 51%|█████     | 784/1540 [31:40<30:29,  2.42s/it] 51%|█████     | 785/1540 [31:42<30:27,  2.42s/it] 51%|█████     | 786/1540 [31:45<30:24,  2.42s/it] 51%|█████     | 787/1540 [31:47<30:22,  2.42s/it] 51%|█████     | 788/1540 [31:50<30:19,  2.42s/it] 51%|█████     | 789/1540 [31:52<30:17,  2.42s/it] 51%|█████▏    | 790/1540 [31:54<30:14,  2.42s/it] 51%|█████▏    | 791/1540 [31:57<30:12,  2.42s/it] 51%|█████▏    | 792/1540 [31:59<30:10,  2.42s/it] 51%|█████▏    | 793/1540 [32:02<30:12,  2.43s/it] 52%|█████▏    | 794/1540 [32:04<30:08,  2.42s/it] 52%|█████▏    | 795/1540 [32:07<30:04,  2.42s/it] 52%|█████▏    | 796/1540 [32:09<30:01,  2.42s/it] 52%|█████▏    | 797/1540 [32:11<29:58,  2.42s/it] 52%|█████▏    | 798/1540 [32:14<29:56,  2.42s/it] 52%|█████▏    | 799/1540 [32:16<29:53,  2.42s/it] 52%|█████▏    | 800/1540 [32:19<29:50,  2.42s/it]                                                  {'loss': 0.5555, 'grad_norm': 0.4489195644855499, 'learning_rate': 0.00032894989690375627, 'epoch': 2.6}
 52%|█████▏    | 800/1540 [32:19<29:50,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 07:54:32,494 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-800
[INFO|configuration_utils.py:726] 2024-05-25 07:54:33,141 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 07:54:33,143 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 07:54:34,036 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 07:54:34,039 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 07:54:34,084 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 07:54:34,085 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-800/special_tokens_map.json
 52%|█████▏    | 801/1540 [32:23<36:20,  2.95s/it] 52%|█████▏    | 802/1540 [32:25<34:18,  2.79s/it] 52%|█████▏    | 803/1540 [32:28<32:53,  2.68s/it] 52%|█████▏    | 804/1540 [32:30<31:52,  2.60s/it] 52%|█████▏    | 805/1540 [32:32<31:10,  2.54s/it] 52%|█████▏    | 806/1540 [32:35<30:42,  2.51s/it] 52%|█████▏    | 807/1540 [32:37<30:19,  2.48s/it] 52%|█████▏    | 808/1540 [32:40<30:02,  2.46s/it] 53%|█████▎    | 809/1540 [32:42<29:50,  2.45s/it] 53%|█████▎    | 810/1540 [32:45<29:40,  2.44s/it] 53%|█████▎    | 811/1540 [32:47<29:33,  2.43s/it] 53%|█████▎    | 812/1540 [32:49<29:27,  2.43s/it] 53%|█████▎    | 813/1540 [32:52<29:22,  2.42s/it] 53%|█████▎    | 814/1540 [32:54<29:18,  2.42s/it] 53%|█████▎    | 815/1540 [32:57<29:15,  2.42s/it] 53%|█████▎    | 816/1540 [32:59<29:11,  2.42s/it] 53%|█████▎    | 817/1540 [33:02<29:08,  2.42s/it] 53%|█████▎    | 818/1540 [33:04<29:06,  2.42s/it] 53%|█████▎    | 819/1540 [33:06<29:03,  2.42s/it] 53%|█████▎    | 820/1540 [33:09<29:01,  2.42s/it]                                                  {'loss': 0.5482, 'grad_norm': 0.4105282425880432, 'learning_rate': 0.0003167549867057854, 'epoch': 2.66}
 53%|█████▎    | 820/1540 [33:09<29:01,  2.42s/it] 53%|█████▎    | 821/1540 [33:11<29:19,  2.45s/it] 53%|█████▎    | 822/1540 [33:14<29:10,  2.44s/it] 53%|█████▎    | 823/1540 [33:16<29:03,  2.43s/it] 54%|█████▎    | 824/1540 [33:19<28:58,  2.43s/it] 54%|█████▎    | 825/1540 [33:21<28:53,  2.42s/it] 54%|█████▎    | 826/1540 [33:23<28:49,  2.42s/it] 54%|█████▎    | 827/1540 [33:26<28:45,  2.42s/it] 54%|█████▍    | 828/1540 [33:28<28:42,  2.42s/it] 54%|█████▍    | 829/1540 [33:31<28:39,  2.42s/it] 54%|█████▍    | 830/1540 [33:33<28:36,  2.42s/it] 54%|█████▍    | 831/1540 [33:35<28:33,  2.42s/it] 54%|█████▍    | 832/1540 [33:38<28:31,  2.42s/it] 54%|█████▍    | 833/1540 [33:40<28:28,  2.42s/it] 54%|█████▍    | 834/1540 [33:43<28:25,  2.42s/it] 54%|█████▍    | 835/1540 [33:45<28:23,  2.42s/it] 54%|█████▍    | 836/1540 [33:48<28:21,  2.42s/it] 54%|█████▍    | 837/1540 [33:50<28:18,  2.42s/it] 54%|█████▍    | 838/1540 [33:52<28:16,  2.42s/it] 54%|█████▍    | 839/1540 [33:55<28:15,  2.42s/it] 55%|█████▍    | 840/1540 [33:57<28:13,  2.42s/it]                                                  {'loss': 0.5384, 'grad_norm': 0.46498122811317444, 'learning_rate': 0.00030438648523006085, 'epoch': 2.73}
 55%|█████▍    | 840/1540 [33:57<28:13,  2.42s/it] 55%|█████▍    | 841/1540 [34:00<28:11,  2.42s/it] 55%|█████▍    | 842/1540 [34:02<28:09,  2.42s/it] 55%|█████▍    | 843/1540 [34:04<28:06,  2.42s/it] 55%|█████▍    | 844/1540 [34:07<28:04,  2.42s/it] 55%|█████▍    | 845/1540 [34:09<28:01,  2.42s/it] 55%|█████▍    | 846/1540 [34:12<27:59,  2.42s/it] 55%|█████▌    | 847/1540 [34:14<27:58,  2.42s/it] 55%|█████▌    | 848/1540 [34:17<27:55,  2.42s/it] 55%|█████▌    | 849/1540 [34:19<27:53,  2.42s/it] 55%|█████▌    | 850/1540 [34:21<27:50,  2.42s/it] 55%|█████▌    | 851/1540 [34:24<27:47,  2.42s/it] 55%|█████▌    | 852/1540 [34:26<27:45,  2.42s/it] 55%|█████▌    | 853/1540 [34:29<27:42,  2.42s/it] 55%|█████▌    | 854/1540 [34:31<27:40,  2.42s/it] 56%|█████▌    | 855/1540 [34:34<27:37,  2.42s/it] 56%|█████▌    | 856/1540 [34:36<27:35,  2.42s/it] 56%|█████▌    | 857/1540 [34:38<27:32,  2.42s/it] 56%|█████▌    | 858/1540 [34:41<27:30,  2.42s/it] 56%|█████▌    | 859/1540 [34:43<27:27,  2.42s/it] 56%|█████▌    | 860/1540 [34:46<27:25,  2.42s/it]                                                  {'loss': 0.5397, 'grad_norm': 0.4631447494029999, 'learning_rate': 0.0002918765558261841, 'epoch': 2.79}
 56%|█████▌    | 860/1540 [34:46<27:25,  2.42s/it] 56%|█████▌    | 861/1540 [34:48<27:23,  2.42s/it] 56%|█████▌    | 862/1540 [34:50<27:20,  2.42s/it] 56%|█████▌    | 863/1540 [34:53<27:22,  2.43s/it] 56%|█████▌    | 864/1540 [34:55<27:21,  2.43s/it] 56%|█████▌    | 865/1540 [34:58<27:16,  2.43s/it] 56%|█████▌    | 866/1540 [35:00<27:13,  2.42s/it] 56%|█████▋    | 867/1540 [35:03<27:10,  2.42s/it] 56%|█████▋    | 868/1540 [35:05<27:07,  2.42s/it] 56%|█████▋    | 869/1540 [35:07<27:04,  2.42s/it] 56%|█████▋    | 870/1540 [35:10<27:01,  2.42s/it] 57%|█████▋    | 871/1540 [35:12<26:59,  2.42s/it] 57%|█████▋    | 872/1540 [35:15<26:56,  2.42s/it] 57%|█████▋    | 873/1540 [35:17<26:53,  2.42s/it] 57%|█████▋    | 874/1540 [35:20<26:51,  2.42s/it] 57%|█████▋    | 875/1540 [35:22<26:49,  2.42s/it] 57%|█████▋    | 876/1540 [35:24<26:47,  2.42s/it] 57%|█████▋    | 877/1540 [35:27<26:44,  2.42s/it] 57%|█████▋    | 878/1540 [35:29<26:41,  2.42s/it] 57%|█████▋    | 879/1540 [35:32<26:39,  2.42s/it] 57%|█████▋    | 880/1540 [35:34<26:37,  2.42s/it]                                                  {'loss': 0.5697, 'grad_norm': 0.46023181080818176, 'learning_rate': 0.00027925772961635294, 'epoch': 2.86}
 57%|█████▋    | 880/1540 [35:34<26:37,  2.42s/it] 57%|█████▋    | 881/1540 [35:36<26:35,  2.42s/it] 57%|█████▋    | 882/1540 [35:39<26:32,  2.42s/it] 57%|█████▋    | 883/1540 [35:41<26:30,  2.42s/it] 57%|█████▋    | 884/1540 [35:44<26:27,  2.42s/it] 57%|█████▋    | 885/1540 [35:46<26:25,  2.42s/it] 58%|█████▊    | 886/1540 [35:49<26:22,  2.42s/it] 58%|█████▊    | 887/1540 [35:51<26:20,  2.42s/it] 58%|█████▊    | 888/1540 [35:53<26:17,  2.42s/it] 58%|█████▊    | 889/1540 [35:56<26:15,  2.42s/it] 58%|█████▊    | 890/1540 [35:58<26:28,  2.44s/it] 58%|█████▊    | 891/1540 [36:01<26:21,  2.44s/it] 58%|█████▊    | 892/1540 [36:03<26:15,  2.43s/it] 58%|█████▊    | 893/1540 [36:06<26:10,  2.43s/it] 58%|█████▊    | 894/1540 [36:08<26:06,  2.43s/it] 58%|█████▊    | 895/1540 [36:10<26:03,  2.42s/it] 58%|█████▊    | 896/1540 [36:13<26:00,  2.42s/it] 58%|█████▊    | 897/1540 [36:15<25:57,  2.42s/it] 58%|█████▊    | 898/1540 [36:18<25:54,  2.42s/it] 58%|█████▊    | 899/1540 [36:20<25:51,  2.42s/it] 58%|█████▊    | 900/1540 [36:23<25:48,  2.42s/it]                                                  {'loss': 0.5232, 'grad_norm': 0.36580970883369446, 'learning_rate': 0.000266562820900646, 'epoch': 2.92}
 58%|█████▊    | 900/1540 [36:23<25:48,  2.42s/it] 59%|█████▊    | 901/1540 [36:25<25:46,  2.42s/it] 59%|█████▊    | 902/1540 [36:27<25:44,  2.42s/it] 59%|█████▊    | 903/1540 [36:30<25:41,  2.42s/it] 59%|█████▊    | 904/1540 [36:32<25:39,  2.42s/it] 59%|█████▉    | 905/1540 [36:35<25:36,  2.42s/it] 59%|█████▉    | 906/1540 [36:37<25:34,  2.42s/it] 59%|█████▉    | 907/1540 [36:39<25:31,  2.42s/it] 59%|█████▉    | 908/1540 [36:42<25:29,  2.42s/it] 59%|█████▉    | 909/1540 [36:44<25:26,  2.42s/it] 59%|█████▉    | 910/1540 [36:47<25:24,  2.42s/it] 59%|█████▉    | 911/1540 [36:49<25:21,  2.42s/it] 59%|█████▉    | 912/1540 [36:52<25:19,  2.42s/it] 59%|█████▉    | 913/1540 [36:54<25:17,  2.42s/it] 59%|█████▉    | 914/1540 [36:56<25:15,  2.42s/it] 59%|█████▉    | 915/1540 [36:59<25:12,  2.42s/it] 59%|█████▉    | 916/1540 [37:01<25:10,  2.42s/it] 60%|█████▉    | 917/1540 [37:04<25:10,  2.42s/it] 60%|█████▉    | 918/1540 [37:06<25:06,  2.42s/it] 60%|█████▉    | 919/1540 [37:08<25:03,  2.42s/it] 60%|█████▉    | 920/1540 [37:11<25:00,  2.42s/it]                                                  {'loss': 0.5268, 'grad_norm': 0.43213218450546265, 'learning_rate': 0.00025382484182592563, 'epoch': 2.99}
 60%|█████▉    | 920/1540 [37:11<25:00,  2.42s/it] 60%|█████▉    | 921/1540 [37:13<24:58,  2.42s/it] 60%|█████▉    | 922/1540 [37:16<24:56,  2.42s/it] 60%|█████▉    | 923/1540 [37:18<24:53,  2.42s/it] 60%|██████    | 924/1540 [37:21<24:50,  2.42s/it] 60%|██████    | 925/1540 [37:23<24:48,  2.42s/it] 60%|██████    | 926/1540 [37:25<24:45,  2.42s/it] 60%|██████    | 927/1540 [37:28<24:43,  2.42s/it] 60%|██████    | 928/1540 [37:30<24:41,  2.42s/it] 60%|██████    | 929/1540 [37:33<24:38,  2.42s/it] 60%|██████    | 930/1540 [37:35<24:35,  2.42s/it] 60%|██████    | 931/1540 [37:38<24:33,  2.42s/it] 61%|██████    | 932/1540 [37:40<24:31,  2.42s/it] 61%|██████    | 933/1540 [37:42<24:29,  2.42s/it] 61%|██████    | 934/1540 [37:45<24:26,  2.42s/it] 61%|██████    | 935/1540 [37:47<24:23,  2.42s/it] 61%|██████    | 936/1540 [37:50<24:21,  2.42s/it] 61%|██████    | 937/1540 [37:52<24:19,  2.42s/it] 61%|██████    | 938/1540 [37:54<24:16,  2.42s/it] 61%|██████    | 939/1540 [37:57<24:14,  2.42s/it] 61%|██████    | 940/1540 [37:59<24:12,  2.42s/it]                                                  {'loss': 0.4629, 'grad_norm': 0.40692535042762756, 'learning_rate': 0.0002410769165402549, 'epoch': 3.05}
 61%|██████    | 940/1540 [37:59<24:12,  2.42s/it] 61%|██████    | 941/1540 [38:02<24:10,  2.42s/it] 61%|██████    | 942/1540 [38:04<24:07,  2.42s/it] 61%|██████    | 943/1540 [38:07<24:04,  2.42s/it] 61%|██████▏   | 944/1540 [38:09<24:02,  2.42s/it] 61%|██████▏   | 945/1540 [38:11<23:59,  2.42s/it] 61%|██████▏   | 946/1540 [38:14<23:57,  2.42s/it] 61%|██████▏   | 947/1540 [38:16<23:54,  2.42s/it] 62%|██████▏   | 948/1540 [38:19<23:52,  2.42s/it] 62%|██████▏   | 949/1540 [38:21<23:50,  2.42s/it] 62%|██████▏   | 950/1540 [38:24<23:47,  2.42s/it] 62%|██████▏   | 951/1540 [38:26<23:49,  2.43s/it] 62%|██████▏   | 952/1540 [38:28<23:46,  2.43s/it] 62%|██████▏   | 953/1540 [38:31<23:42,  2.42s/it] 62%|██████▏   | 954/1540 [38:33<23:39,  2.42s/it] 62%|██████▏   | 955/1540 [38:36<23:36,  2.42s/it] 62%|██████▏   | 956/1540 [38:38<23:33,  2.42s/it] 62%|██████▏   | 957/1540 [38:40<23:31,  2.42s/it] 62%|██████▏   | 958/1540 [38:43<23:28,  2.42s/it] 62%|██████▏   | 959/1540 [38:45<23:26,  2.42s/it] 62%|██████▏   | 960/1540 [38:48<23:34,  2.44s/it]                                                  {'loss': 0.4661, 'grad_norm': 0.4512575566768646, 'learning_rate': 0.00022835219505606353, 'epoch': 3.12}
 62%|██████▏   | 960/1540 [38:48<23:34,  2.44s/it] 62%|██████▏   | 961/1540 [38:50<23:29,  2.43s/it] 62%|██████▏   | 962/1540 [38:53<23:24,  2.43s/it] 63%|██████▎   | 963/1540 [38:55<23:19,  2.43s/it] 63%|██████▎   | 964/1540 [38:57<23:16,  2.42s/it] 63%|██████▎   | 965/1540 [39:00<23:13,  2.42s/it] 63%|██████▎   | 966/1540 [39:02<23:10,  2.42s/it] 63%|██████▎   | 967/1540 [39:05<23:07,  2.42s/it] 63%|██████▎   | 968/1540 [39:07<23:04,  2.42s/it] 63%|██████▎   | 969/1540 [39:10<23:02,  2.42s/it] 63%|██████▎   | 970/1540 [39:12<22:59,  2.42s/it] 63%|██████▎   | 971/1540 [39:14<22:57,  2.42s/it] 63%|██████▎   | 972/1540 [39:17<22:54,  2.42s/it] 63%|██████▎   | 973/1540 [39:19<22:52,  2.42s/it] 63%|██████▎   | 974/1540 [39:22<22:49,  2.42s/it] 63%|██████▎   | 975/1540 [39:24<22:47,  2.42s/it] 63%|██████▎   | 976/1540 [39:27<22:44,  2.42s/it] 63%|██████▎   | 977/1540 [39:29<22:42,  2.42s/it] 64%|██████▎   | 978/1540 [39:31<22:40,  2.42s/it] 64%|██████▎   | 979/1540 [39:34<22:38,  2.42s/it] 64%|██████▎   | 980/1540 [39:36<22:35,  2.42s/it]                                                  {'loss': 0.4767, 'grad_norm': 0.4526023268699646, 'learning_rate': 0.00021568376704605635, 'epoch': 3.18}
 64%|██████▎   | 980/1540 [39:36<22:35,  2.42s/it] 64%|██████▎   | 981/1540 [39:39<22:33,  2.42s/it] 64%|██████▍   | 982/1540 [39:41<22:30,  2.42s/it] 64%|██████▍   | 983/1540 [39:43<22:27,  2.42s/it] 64%|██████▍   | 984/1540 [39:46<22:25,  2.42s/it] 64%|██████▍   | 985/1540 [39:48<22:23,  2.42s/it] 64%|██████▍   | 986/1540 [39:51<22:23,  2.42s/it] 64%|██████▍   | 987/1540 [39:53<22:19,  2.42s/it] 64%|██████▍   | 988/1540 [39:56<22:16,  2.42s/it] 64%|██████▍   | 989/1540 [39:58<22:13,  2.42s/it] 64%|██████▍   | 990/1540 [40:00<22:11,  2.42s/it] 64%|██████▍   | 991/1540 [40:03<22:08,  2.42s/it] 64%|██████▍   | 992/1540 [40:05<22:06,  2.42s/it] 64%|██████▍   | 993/1540 [40:08<22:03,  2.42s/it] 65%|██████▍   | 994/1540 [40:10<22:01,  2.42s/it] 65%|██████▍   | 995/1540 [40:13<21:58,  2.42s/it] 65%|██████▍   | 996/1540 [40:15<21:56,  2.42s/it] 65%|██████▍   | 997/1540 [40:17<21:53,  2.42s/it] 65%|██████▍   | 998/1540 [40:20<21:51,  2.42s/it] 65%|██████▍   | 999/1540 [40:22<21:48,  2.42s/it] 65%|██████▍   | 1000/1540 [40:25<21:46,  2.42s/it]                                                   {'loss': 0.4616, 'grad_norm': 0.4819333255290985, 'learning_rate': 0.0002031045757960297, 'epoch': 3.25}
 65%|██████▍   | 1000/1540 [40:25<21:46,  2.42s/it] 65%|██████▌   | 1001/1540 [40:27<21:44,  2.42s/it] 65%|██████▌   | 1002/1540 [40:29<21:42,  2.42s/it] 65%|██████▌   | 1003/1540 [40:32<21:39,  2.42s/it] 65%|██████▌   | 1004/1540 [40:34<21:37,  2.42s/it] 65%|██████▌   | 1005/1540 [40:37<21:34,  2.42s/it] 65%|██████▌   | 1006/1540 [40:39<21:32,  2.42s/it] 65%|██████▌   | 1007/1540 [40:42<21:29,  2.42s/it] 65%|██████▌   | 1008/1540 [40:44<21:27,  2.42s/it] 66%|██████▌   | 1009/1540 [40:46<21:24,  2.42s/it] 66%|██████▌   | 1010/1540 [40:49<21:22,  2.42s/it] 66%|██████▌   | 1011/1540 [40:51<21:20,  2.42s/it] 66%|██████▌   | 1012/1540 [40:54<21:17,  2.42s/it] 66%|██████▌   | 1013/1540 [40:56<21:14,  2.42s/it] 66%|██████▌   | 1014/1540 [40:58<21:12,  2.42s/it] 66%|██████▌   | 1015/1540 [41:01<21:10,  2.42s/it] 66%|██████▌   | 1016/1540 [41:03<21:07,  2.42s/it] 66%|██████▌   | 1017/1540 [41:06<21:05,  2.42s/it] 66%|██████▌   | 1018/1540 [41:08<21:03,  2.42s/it] 66%|██████▌   | 1019/1540 [41:11<21:01,  2.42s/it] 66%|██████▌   | 1020/1540 [41:13<20:58,  2.42s/it]                                                   {'loss': 0.4691, 'grad_norm': 0.4204781949520111, 'learning_rate': 0.0001906473325383568, 'epoch': 3.31}
 66%|██████▌   | 1020/1540 [41:13<20:58,  2.42s/it] 66%|██████▋   | 1021/1540 [41:15<20:56,  2.42s/it] 66%|██████▋   | 1022/1540 [41:18<20:53,  2.42s/it] 66%|██████▋   | 1023/1540 [41:20<20:51,  2.42s/it] 66%|██████▋   | 1024/1540 [41:23<20:48,  2.42s/it] 67%|██████▋   | 1025/1540 [41:25<20:46,  2.42s/it] 67%|██████▋   | 1026/1540 [41:28<20:43,  2.42s/it] 67%|██████▋   | 1027/1540 [41:30<20:41,  2.42s/it] 67%|██████▋   | 1028/1540 [41:32<20:39,  2.42s/it] 67%|██████▋   | 1029/1540 [41:35<20:46,  2.44s/it] 67%|██████▋   | 1030/1540 [41:37<20:40,  2.43s/it] 67%|██████▋   | 1031/1540 [41:40<20:36,  2.43s/it] 67%|██████▋   | 1032/1540 [41:42<20:32,  2.43s/it] 67%|██████▋   | 1033/1540 [41:45<20:29,  2.42s/it] 67%|██████▋   | 1034/1540 [41:47<20:26,  2.42s/it] 67%|██████▋   | 1035/1540 [41:49<20:23,  2.42s/it] 67%|██████▋   | 1036/1540 [41:52<20:20,  2.42s/it] 67%|██████▋   | 1037/1540 [41:54<20:17,  2.42s/it] 67%|██████▋   | 1038/1540 [41:57<20:17,  2.43s/it] 67%|██████▋   | 1039/1540 [41:59<20:14,  2.42s/it] 68%|██████▊   | 1040/1540 [42:01<20:11,  2.42s/it]                                                   {'loss': 0.4918, 'grad_norm': 0.448316752910614, 'learning_rate': 0.00017834443138890977, 'epoch': 3.38}
 68%|██████▊   | 1040/1540 [42:01<20:11,  2.42s/it] 68%|██████▊   | 1041/1540 [42:04<20:08,  2.42s/it] 68%|██████▊   | 1042/1540 [42:06<20:06,  2.42s/it] 68%|██████▊   | 1043/1540 [42:09<20:05,  2.42s/it] 68%|██████▊   | 1044/1540 [42:11<20:01,  2.42s/it] 68%|██████▊   | 1045/1540 [42:14<19:59,  2.42s/it] 68%|██████▊   | 1046/1540 [42:16<19:56,  2.42s/it] 68%|██████▊   | 1047/1540 [42:18<19:53,  2.42s/it] 68%|██████▊   | 1048/1540 [42:21<19:50,  2.42s/it] 68%|██████▊   | 1049/1540 [42:23<19:48,  2.42s/it] 68%|██████▊   | 1050/1540 [42:26<19:45,  2.42s/it] 68%|██████▊   | 1051/1540 [42:28<19:43,  2.42s/it] 68%|██████▊   | 1052/1540 [42:31<19:41,  2.42s/it] 68%|██████▊   | 1053/1540 [42:33<19:38,  2.42s/it] 68%|██████▊   | 1054/1540 [42:35<19:36,  2.42s/it] 69%|██████▊   | 1055/1540 [42:38<19:33,  2.42s/it] 69%|██████▊   | 1056/1540 [42:40<19:33,  2.42s/it] 69%|██████▊   | 1057/1540 [42:43<19:30,  2.42s/it] 69%|██████▊   | 1058/1540 [42:45<19:27,  2.42s/it] 69%|██████▉   | 1059/1540 [42:48<19:24,  2.42s/it] 69%|██████▉   | 1060/1540 [42:50<19:21,  2.42s/it]                                                   {'loss': 0.461, 'grad_norm': 0.5095953941345215, 'learning_rate': 0.00016622786510861942, 'epoch': 3.44}
 69%|██████▉   | 1060/1540 [42:50<19:21,  2.42s/it] 69%|██████▉   | 1061/1540 [42:52<19:19,  2.42s/it] 69%|██████▉   | 1062/1540 [42:55<19:16,  2.42s/it] 69%|██████▉   | 1063/1540 [42:57<19:14,  2.42s/it] 69%|██████▉   | 1064/1540 [43:00<19:12,  2.42s/it] 69%|██████▉   | 1065/1540 [43:02<19:09,  2.42s/it] 69%|██████▉   | 1066/1540 [43:04<19:06,  2.42s/it] 69%|██████▉   | 1067/1540 [43:07<19:04,  2.42s/it] 69%|██████▉   | 1068/1540 [43:09<19:02,  2.42s/it] 69%|██████▉   | 1069/1540 [43:12<18:59,  2.42s/it] 69%|██████▉   | 1070/1540 [43:14<18:57,  2.42s/it] 70%|██████▉   | 1071/1540 [43:17<18:54,  2.42s/it] 70%|██████▉   | 1072/1540 [43:19<18:52,  2.42s/it] 70%|██████▉   | 1073/1540 [43:21<18:49,  2.42s/it] 70%|██████▉   | 1074/1540 [43:24<18:47,  2.42s/it] 70%|██████▉   | 1075/1540 [43:26<18:45,  2.42s/it] 70%|██████▉   | 1076/1540 [43:29<18:42,  2.42s/it] 70%|██████▉   | 1077/1540 [43:31<18:40,  2.42s/it] 70%|███████   | 1078/1540 [43:33<18:37,  2.42s/it] 70%|███████   | 1079/1540 [43:36<18:35,  2.42s/it] 70%|███████   | 1080/1540 [43:38<18:32,  2.42s/it]                                                   {'loss': 0.4617, 'grad_norm': 0.44007888436317444, 'learning_rate': 0.00015432914190872756, 'epoch': 3.51}
 70%|███████   | 1080/1540 [43:38<18:32,  2.42s/it] 70%|███████   | 1081/1540 [43:41<18:30,  2.42s/it] 70%|███████   | 1082/1540 [43:43<18:28,  2.42s/it] 70%|███████   | 1083/1540 [43:46<18:25,  2.42s/it] 70%|███████   | 1084/1540 [43:48<18:23,  2.42s/it] 70%|███████   | 1085/1540 [43:50<18:21,  2.42s/it] 71%|███████   | 1086/1540 [43:53<18:18,  2.42s/it] 71%|███████   | 1087/1540 [43:55<18:16,  2.42s/it] 71%|███████   | 1088/1540 [43:58<18:13,  2.42s/it] 71%|███████   | 1089/1540 [44:00<18:11,  2.42s/it] 71%|███████   | 1090/1540 [44:03<18:08,  2.42s/it] 71%|███████   | 1091/1540 [44:05<18:06,  2.42s/it] 71%|███████   | 1092/1540 [44:07<18:03,  2.42s/it] 71%|███████   | 1093/1540 [44:10<18:01,  2.42s/it] 71%|███████   | 1094/1540 [44:12<17:59,  2.42s/it] 71%|███████   | 1095/1540 [44:15<17:56,  2.42s/it] 71%|███████   | 1096/1540 [44:17<17:54,  2.42s/it] 71%|███████   | 1097/1540 [44:19<17:51,  2.42s/it] 71%|███████▏  | 1098/1540 [44:22<17:49,  2.42s/it] 71%|███████▏  | 1099/1540 [44:24<17:55,  2.44s/it] 71%|███████▏  | 1100/1540 [44:27<17:50,  2.43s/it]                                                   {'loss': 0.4292, 'grad_norm': 0.45520439743995667, 'learning_rate': 0.00014267920351607508, 'epoch': 3.57}
 71%|███████▏  | 1100/1540 [44:27<17:50,  2.43s/it] 71%|███████▏  | 1101/1540 [44:29<17:46,  2.43s/it] 72%|███████▏  | 1102/1540 [44:32<17:42,  2.43s/it] 72%|███████▏  | 1103/1540 [44:34<17:39,  2.42s/it] 72%|███████▏  | 1104/1540 [44:36<17:36,  2.42s/it] 72%|███████▏  | 1105/1540 [44:39<17:33,  2.42s/it] 72%|███████▏  | 1106/1540 [44:41<17:30,  2.42s/it] 72%|███████▏  | 1107/1540 [44:44<17:28,  2.42s/it] 72%|███████▏  | 1108/1540 [44:46<17:25,  2.42s/it] 72%|███████▏  | 1109/1540 [44:49<17:23,  2.42s/it] 72%|███████▏  | 1110/1540 [44:51<17:22,  2.42s/it] 72%|███████▏  | 1111/1540 [44:53<17:19,  2.42s/it] 72%|███████▏  | 1112/1540 [44:56<17:18,  2.43s/it] 72%|███████▏  | 1113/1540 [44:58<17:15,  2.42s/it] 72%|███████▏  | 1114/1540 [45:01<17:12,  2.42s/it] 72%|███████▏  | 1115/1540 [45:03<17:09,  2.42s/it] 72%|███████▏  | 1116/1540 [45:06<17:06,  2.42s/it] 73%|███████▎  | 1117/1540 [45:08<17:03,  2.42s/it] 73%|███████▎  | 1118/1540 [45:10<17:01,  2.42s/it] 73%|███████▎  | 1119/1540 [45:13<16:58,  2.42s/it] 73%|███████▎  | 1120/1540 [45:15<16:56,  2.42s/it]                                                   {'loss': 0.4357, 'grad_norm': 0.44856923818588257, 'learning_rate': 0.0001313083447114886, 'epoch': 3.64}
 73%|███████▎  | 1120/1540 [45:15<16:56,  2.42s/it] 73%|███████▎  | 1121/1540 [45:18<16:54,  2.42s/it] 73%|███████▎  | 1122/1540 [45:20<16:51,  2.42s/it] 73%|███████▎  | 1123/1540 [45:22<16:49,  2.42s/it] 73%|███████▎  | 1124/1540 [45:25<16:46,  2.42s/it] 73%|███████▎  | 1125/1540 [45:27<16:47,  2.43s/it] 73%|███████▎  | 1126/1540 [45:30<16:43,  2.42s/it] 73%|███████▎  | 1127/1540 [45:32<16:40,  2.42s/it] 73%|███████▎  | 1128/1540 [45:35<16:37,  2.42s/it] 73%|███████▎  | 1129/1540 [45:37<16:35,  2.42s/it] 73%|███████▎  | 1130/1540 [45:39<16:32,  2.42s/it] 73%|███████▎  | 1131/1540 [45:42<16:30,  2.42s/it] 74%|███████▎  | 1132/1540 [45:44<16:27,  2.42s/it] 74%|███████▎  | 1133/1540 [45:47<16:24,  2.42s/it] 74%|███████▎  | 1134/1540 [45:49<16:22,  2.42s/it] 74%|███████▎  | 1135/1540 [45:52<16:19,  2.42s/it] 74%|███████▍  | 1136/1540 [45:54<16:17,  2.42s/it] 74%|███████▍  | 1137/1540 [45:56<16:15,  2.42s/it] 74%|███████▍  | 1138/1540 [45:59<16:12,  2.42s/it] 74%|███████▍  | 1139/1540 [46:01<16:10,  2.42s/it] 74%|███████▍  | 1140/1540 [46:04<16:08,  2.42s/it]                                                   {'loss': 0.4405, 'grad_norm': 0.46541497111320496, 'learning_rate': 0.00012024613455050157, 'epoch': 3.7}
 74%|███████▍  | 1140/1540 [46:04<16:08,  2.42s/it] 74%|███████▍  | 1141/1540 [46:06<16:06,  2.42s/it] 74%|███████▍  | 1142/1540 [46:08<16:03,  2.42s/it] 74%|███████▍  | 1143/1540 [46:11<16:00,  2.42s/it] 74%|███████▍  | 1144/1540 [46:13<15:58,  2.42s/it] 74%|███████▍  | 1145/1540 [46:16<15:55,  2.42s/it] 74%|███████▍  | 1146/1540 [46:18<15:53,  2.42s/it] 74%|███████▍  | 1147/1540 [46:21<15:50,  2.42s/it] 75%|███████▍  | 1148/1540 [46:23<15:48,  2.42s/it] 75%|███████▍  | 1149/1540 [46:25<15:45,  2.42s/it] 75%|███████▍  | 1150/1540 [46:28<15:43,  2.42s/it] 75%|███████▍  | 1151/1540 [46:30<15:41,  2.42s/it] 75%|███████▍  | 1152/1540 [46:33<15:38,  2.42s/it] 75%|███████▍  | 1153/1540 [46:35<15:36,  2.42s/it] 75%|███████▍  | 1154/1540 [46:37<15:34,  2.42s/it] 75%|███████▌  | 1155/1540 [46:40<15:31,  2.42s/it] 75%|███████▌  | 1156/1540 [46:42<15:29,  2.42s/it] 75%|███████▌  | 1157/1540 [46:45<15:26,  2.42s/it] 75%|███████▌  | 1158/1540 [46:47<15:24,  2.42s/it] 75%|███████▌  | 1159/1540 [46:50<15:22,  2.42s/it] 75%|███████▌  | 1160/1540 [46:52<15:19,  2.42s/it]                                                   {'loss': 0.4785, 'grad_norm': 0.4408126473426819, 'learning_rate': 0.00010952133947126928, 'epoch': 3.77}
 75%|███████▌  | 1160/1540 [46:52<15:19,  2.42s/it] 75%|███████▌  | 1161/1540 [46:54<15:17,  2.42s/it] 75%|███████▌  | 1162/1540 [46:57<15:14,  2.42s/it] 76%|███████▌  | 1163/1540 [46:59<15:12,  2.42s/it] 76%|███████▌  | 1164/1540 [47:02<15:09,  2.42s/it] 76%|███████▌  | 1165/1540 [47:04<15:07,  2.42s/it] 76%|███████▌  | 1166/1540 [47:07<15:04,  2.42s/it] 76%|███████▌  | 1167/1540 [47:09<15:02,  2.42s/it] 76%|███████▌  | 1168/1540 [47:11<15:07,  2.44s/it] 76%|███████▌  | 1169/1540 [47:14<15:02,  2.43s/it] 76%|███████▌  | 1170/1540 [47:16<14:58,  2.43s/it] 76%|███████▌  | 1171/1540 [47:19<14:55,  2.43s/it] 76%|███████▌  | 1172/1540 [47:21<14:52,  2.42s/it] 76%|███████▌  | 1173/1540 [47:24<14:49,  2.42s/it] 76%|███████▌  | 1174/1540 [47:26<14:46,  2.42s/it] 76%|███████▋  | 1175/1540 [47:28<14:43,  2.42s/it] 76%|███████▋  | 1176/1540 [47:31<14:41,  2.42s/it] 76%|███████▋  | 1177/1540 [47:33<14:38,  2.42s/it] 76%|███████▋  | 1178/1540 [47:36<14:36,  2.42s/it] 77%|███████▋  | 1179/1540 [47:38<14:33,  2.42s/it] 77%|███████▋  | 1180/1540 [47:40<14:31,  2.42s/it]                                                   {'loss': 0.4499, 'grad_norm': 0.3866991698741913, 'learning_rate': 9.916184848962892e-05, 'epoch': 3.83}
 77%|███████▋  | 1180/1540 [47:40<14:31,  2.42s/it] 77%|███████▋  | 1181/1540 [47:43<14:28,  2.42s/it] 77%|███████▋  | 1182/1540 [47:45<14:26,  2.42s/it] 77%|███████▋  | 1183/1540 [47:48<14:24,  2.42s/it] 77%|███████▋  | 1184/1540 [47:50<14:21,  2.42s/it] 77%|███████▋  | 1185/1540 [47:53<14:19,  2.42s/it] 77%|███████▋  | 1186/1540 [47:55<14:16,  2.42s/it] 77%|███████▋  | 1187/1540 [47:57<14:14,  2.42s/it] 77%|███████▋  | 1188/1540 [48:00<14:11,  2.42s/it] 77%|███████▋  | 1189/1540 [48:02<14:09,  2.42s/it] 77%|███████▋  | 1190/1540 [48:05<14:07,  2.42s/it] 77%|███████▋  | 1191/1540 [48:07<14:04,  2.42s/it] 77%|███████▋  | 1192/1540 [48:10<14:02,  2.42s/it] 77%|███████▋  | 1193/1540 [48:12<13:59,  2.42s/it] 78%|███████▊  | 1194/1540 [48:14<13:57,  2.42s/it] 78%|███████▊  | 1195/1540 [48:17<13:57,  2.43s/it] 78%|███████▊  | 1196/1540 [48:19<13:54,  2.43s/it] 78%|███████▊  | 1197/1540 [48:22<13:51,  2.42s/it] 78%|███████▊  | 1198/1540 [48:24<13:48,  2.42s/it] 78%|███████▊  | 1199/1540 [48:26<13:45,  2.42s/it] 78%|███████▊  | 1200/1540 [48:29<13:43,  2.42s/it]                                                   {'loss': 0.4196, 'grad_norm': 0.43729981780052185, 'learning_rate': 8.919460067583032e-05, 'epoch': 3.9}
 78%|███████▊  | 1200/1540 [48:29<13:43,  2.42s/it][INFO|trainer.py:3203] 2024-05-25 08:10:42,751 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-1200
[INFO|configuration_utils.py:726] 2024-05-25 08:10:43,823 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 08:10:43,825 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 08:10:44,353 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 08:10:44,355 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 08:10:44,398 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 08:10:44,398 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/checkpoint-1200/special_tokens_map.json
 78%|███████▊  | 1201/1540 [48:33<16:40,  2.95s/it] 78%|███████▊  | 1202/1540 [48:36<15:43,  2.79s/it] 78%|███████▊  | 1203/1540 [48:38<15:02,  2.68s/it] 78%|███████▊  | 1204/1540 [48:40<14:33,  2.60s/it] 78%|███████▊  | 1205/1540 [48:43<14:12,  2.54s/it] 78%|███████▊  | 1206/1540 [48:45<13:56,  2.51s/it] 78%|███████▊  | 1207/1540 [48:48<13:45,  2.48s/it] 78%|███████▊  | 1208/1540 [48:50<13:36,  2.46s/it] 79%|███████▊  | 1209/1540 [48:52<13:29,  2.45s/it] 79%|███████▊  | 1210/1540 [48:55<13:25,  2.44s/it] 79%|███████▊  | 1211/1540 [48:57<13:20,  2.43s/it] 79%|███████▊  | 1212/1540 [49:00<13:17,  2.43s/it] 79%|███████▉  | 1213/1540 [49:02<13:13,  2.43s/it] 79%|███████▉  | 1214/1540 [49:05<13:10,  2.42s/it] 79%|███████▉  | 1215/1540 [49:07<13:06,  2.42s/it] 79%|███████▉  | 1216/1540 [49:09<13:04,  2.42s/it] 79%|███████▉  | 1217/1540 [49:12<13:01,  2.42s/it] 79%|███████▉  | 1218/1540 [49:14<12:58,  2.42s/it] 79%|███████▉  | 1219/1540 [49:17<12:56,  2.42s/it] 79%|███████▉  | 1220/1540 [49:19<12:53,  2.42s/it]                                                   {'loss': 0.433, 'grad_norm': 0.4047577679157257, 'learning_rate': 7.96455151015272e-05, 'epoch': 3.96}
 79%|███████▉  | 1220/1540 [49:19<12:53,  2.42s/it] 79%|███████▉  | 1221/1540 [49:21<12:51,  2.42s/it] 79%|███████▉  | 1222/1540 [49:24<12:49,  2.42s/it] 79%|███████▉  | 1223/1540 [49:26<12:46,  2.42s/it] 79%|███████▉  | 1224/1540 [49:29<12:44,  2.42s/it] 80%|███████▉  | 1225/1540 [49:31<12:41,  2.42s/it] 80%|███████▉  | 1226/1540 [49:34<12:39,  2.42s/it] 80%|███████▉  | 1227/1540 [49:36<12:36,  2.42s/it] 80%|███████▉  | 1228/1540 [49:38<12:34,  2.42s/it] 80%|███████▉  | 1229/1540 [49:41<12:31,  2.42s/it] 80%|███████▉  | 1230/1540 [49:43<12:29,  2.42s/it] 80%|███████▉  | 1231/1540 [49:46<12:27,  2.42s/it] 80%|████████  | 1232/1540 [49:48<12:24,  2.42s/it] 80%|████████  | 1233/1540 [49:50<12:22,  2.42s/it] 80%|████████  | 1234/1540 [49:53<12:20,  2.42s/it] 80%|████████  | 1235/1540 [49:55<12:17,  2.42s/it] 80%|████████  | 1236/1540 [49:58<12:15,  2.42s/it] 80%|████████  | 1237/1540 [50:00<12:13,  2.42s/it] 80%|████████  | 1238/1540 [50:03<12:15,  2.44s/it] 80%|████████  | 1239/1540 [50:05<12:11,  2.43s/it] 81%|████████  | 1240/1540 [50:07<12:08,  2.43s/it]                                                   {'loss': 0.4138, 'grad_norm': 0.36105766892433167, 'learning_rate': 7.053942343919881e-05, 'epoch': 4.03}
 81%|████████  | 1240/1540 [50:07<12:08,  2.43s/it] 81%|████████  | 1241/1540 [50:10<12:05,  2.43s/it] 81%|████████  | 1242/1540 [50:12<12:02,  2.42s/it] 81%|████████  | 1243/1540 [50:15<11:59,  2.42s/it] 81%|████████  | 1244/1540 [50:17<11:56,  2.42s/it] 81%|████████  | 1245/1540 [50:20<11:54,  2.42s/it] 81%|████████  | 1246/1540 [50:22<11:51,  2.42s/it] 81%|████████  | 1247/1540 [50:24<11:49,  2.42s/it] 81%|████████  | 1248/1540 [50:27<11:46,  2.42s/it] 81%|████████  | 1249/1540 [50:29<11:44,  2.42s/it] 81%|████████  | 1250/1540 [50:32<11:41,  2.42s/it] 81%|████████  | 1251/1540 [50:34<11:39,  2.42s/it] 81%|████████▏ | 1252/1540 [50:36<11:37,  2.42s/it] 81%|████████▏ | 1253/1540 [50:39<11:34,  2.42s/it] 81%|████████▏ | 1254/1540 [50:41<11:32,  2.42s/it] 81%|████████▏ | 1255/1540 [50:44<11:29,  2.42s/it] 82%|████████▏ | 1256/1540 [50:46<11:27,  2.42s/it] 82%|████████▏ | 1257/1540 [50:49<11:24,  2.42s/it] 82%|████████▏ | 1258/1540 [50:51<11:22,  2.42s/it] 82%|████████▏ | 1259/1540 [50:53<11:20,  2.42s/it] 82%|████████▏ | 1260/1540 [50:56<11:17,  2.42s/it]                                                   {'loss': 0.4246, 'grad_norm': 0.39792585372924805, 'learning_rate': 6.190000538926915e-05, 'epoch': 4.09}
 82%|████████▏ | 1260/1540 [50:56<11:17,  2.42s/it] 82%|████████▏ | 1261/1540 [50:58<11:15,  2.42s/it] 82%|████████▏ | 1262/1540 [51:01<11:12,  2.42s/it] 82%|████████▏ | 1263/1540 [51:03<11:10,  2.42s/it] 82%|████████▏ | 1264/1540 [51:06<11:09,  2.43s/it] 82%|████████▏ | 1265/1540 [51:08<11:06,  2.42s/it] 82%|████████▏ | 1266/1540 [51:10<11:03,  2.42s/it] 82%|████████▏ | 1267/1540 [51:13<11:01,  2.42s/it] 82%|████████▏ | 1268/1540 [51:15<10:58,  2.42s/it] 82%|████████▏ | 1269/1540 [51:18<10:56,  2.42s/it] 82%|████████▏ | 1270/1540 [51:20<10:53,  2.42s/it] 83%|████████▎ | 1271/1540 [51:22<10:51,  2.42s/it] 83%|████████▎ | 1272/1540 [51:25<10:48,  2.42s/it] 83%|████████▎ | 1273/1540 [51:27<10:46,  2.42s/it] 83%|████████▎ | 1274/1540 [51:30<10:43,  2.42s/it] 83%|████████▎ | 1275/1540 [51:32<10:41,  2.42s/it] 83%|████████▎ | 1276/1540 [51:35<10:38,  2.42s/it] 83%|████████▎ | 1277/1540 [51:37<10:36,  2.42s/it] 83%|████████▎ | 1278/1540 [51:39<10:34,  2.42s/it] 83%|████████▎ | 1279/1540 [51:42<10:31,  2.42s/it] 83%|████████▎ | 1280/1540 [51:44<10:29,  2.42s/it]                                                   {'loss': 0.4351, 'grad_norm': 0.37512290477752686, 'learning_rate': 5.3749727102843066e-05, 'epoch': 4.16}
 83%|████████▎ | 1280/1540 [51:44<10:29,  2.42s/it] 83%|████████▎ | 1281/1540 [51:47<10:27,  2.42s/it] 83%|████████▎ | 1282/1540 [51:49<10:24,  2.42s/it] 83%|████████▎ | 1283/1540 [51:52<10:22,  2.42s/it] 83%|████████▎ | 1284/1540 [51:54<10:19,  2.42s/it] 83%|████████▎ | 1285/1540 [51:56<10:17,  2.42s/it] 84%|████████▎ | 1286/1540 [51:59<10:14,  2.42s/it] 84%|████████▎ | 1287/1540 [52:01<10:12,  2.42s/it] 84%|████████▎ | 1288/1540 [52:04<10:09,  2.42s/it] 84%|████████▎ | 1289/1540 [52:06<10:07,  2.42s/it] 84%|████████▍ | 1290/1540 [52:08<10:04,  2.42s/it] 84%|████████▍ | 1291/1540 [52:11<10:02,  2.42s/it] 84%|████████▍ | 1292/1540 [52:13<10:00,  2.42s/it] 84%|████████▍ | 1293/1540 [52:16<09:57,  2.42s/it] 84%|████████▍ | 1294/1540 [52:18<09:55,  2.42s/it] 84%|████████▍ | 1295/1540 [52:21<09:52,  2.42s/it] 84%|████████▍ | 1296/1540 [52:23<09:50,  2.42s/it] 84%|████████▍ | 1297/1540 [52:25<09:47,  2.42s/it] 84%|████████▍ | 1298/1540 [52:28<09:45,  2.42s/it] 84%|████████▍ | 1299/1540 [52:30<09:44,  2.43s/it] 84%|████████▍ | 1300/1540 [52:33<09:41,  2.42s/it]                                                   {'loss': 0.3929, 'grad_norm': 0.3961869180202484, 'learning_rate': 4.610978276018496e-05, 'epoch': 4.22}
 84%|████████▍ | 1300/1540 [52:33<09:41,  2.42s/it] 84%|████████▍ | 1301/1540 [52:35<09:39,  2.42s/it] 85%|████████▍ | 1302/1540 [52:38<09:36,  2.42s/it] 85%|████████▍ | 1303/1540 [52:40<09:33,  2.42s/it] 85%|████████▍ | 1304/1540 [52:42<09:31,  2.42s/it] 85%|████████▍ | 1305/1540 [52:45<09:28,  2.42s/it] 85%|████████▍ | 1306/1540 [52:47<09:26,  2.42s/it] 85%|████████▍ | 1307/1540 [52:50<09:29,  2.44s/it] 85%|████████▍ | 1308/1540 [52:52<09:25,  2.44s/it] 85%|████████▌ | 1309/1540 [52:55<09:21,  2.43s/it] 85%|████████▌ | 1310/1540 [52:57<09:18,  2.43s/it] 85%|████████▌ | 1311/1540 [52:59<09:15,  2.43s/it] 85%|████████▌ | 1312/1540 [53:02<09:12,  2.42s/it] 85%|████████▌ | 1313/1540 [53:04<09:10,  2.43s/it] 85%|████████▌ | 1314/1540 [53:07<09:07,  2.42s/it] 85%|████████▌ | 1315/1540 [53:09<09:05,  2.42s/it] 85%|████████▌ | 1316/1540 [53:12<09:02,  2.42s/it] 86%|████████▌ | 1317/1540 [53:14<08:59,  2.42s/it] 86%|████████▌ | 1318/1540 [53:16<08:57,  2.42s/it] 86%|████████▌ | 1319/1540 [53:19<08:54,  2.42s/it] 86%|████████▌ | 1320/1540 [53:21<08:52,  2.42s/it]                                                   {'loss': 0.4156, 'grad_norm': 0.3343645930290222, 'learning_rate': 3.900003945686123e-05, 'epoch': 4.29}
 86%|████████▌ | 1320/1540 [53:21<08:52,  2.42s/it] 86%|████████▌ | 1321/1540 [53:24<08:50,  2.42s/it] 86%|████████▌ | 1322/1540 [53:26<08:47,  2.42s/it] 86%|████████▌ | 1323/1540 [53:28<08:45,  2.42s/it] 86%|████████▌ | 1324/1540 [53:31<08:42,  2.42s/it] 86%|████████▌ | 1325/1540 [53:33<08:40,  2.42s/it] 86%|████████▌ | 1326/1540 [53:36<08:37,  2.42s/it] 86%|████████▌ | 1327/1540 [53:38<08:35,  2.42s/it] 86%|████████▌ | 1328/1540 [53:41<08:33,  2.42s/it] 86%|████████▋ | 1329/1540 [53:43<08:30,  2.42s/it] 86%|████████▋ | 1330/1540 [53:45<08:28,  2.42s/it] 86%|████████▋ | 1331/1540 [53:48<08:25,  2.42s/it] 86%|████████▋ | 1332/1540 [53:50<08:23,  2.42s/it] 87%|████████▋ | 1333/1540 [53:53<08:20,  2.42s/it] 87%|████████▋ | 1334/1540 [53:55<08:18,  2.42s/it] 87%|████████▋ | 1335/1540 [53:57<08:15,  2.42s/it] 87%|████████▋ | 1336/1540 [54:00<08:13,  2.42s/it] 87%|████████▋ | 1337/1540 [54:02<08:11,  2.42s/it] 87%|████████▋ | 1338/1540 [54:05<08:08,  2.42s/it] 87%|████████▋ | 1339/1540 [54:07<08:06,  2.42s/it] 87%|████████▋ | 1340/1540 [54:10<08:03,  2.42s/it]                                                   {'loss': 0.3981, 'grad_norm': 0.35415753722190857, 'learning_rate': 3.243898554086536e-05, 'epoch': 4.35}
 87%|████████▋ | 1340/1540 [54:10<08:03,  2.42s/it] 87%|████████▋ | 1341/1540 [54:12<08:01,  2.42s/it] 87%|████████▋ | 1342/1540 [54:14<07:59,  2.42s/it] 87%|████████▋ | 1343/1540 [54:17<07:56,  2.42s/it] 87%|████████▋ | 1344/1540 [54:19<07:54,  2.42s/it] 87%|████████▋ | 1345/1540 [54:22<07:51,  2.42s/it] 87%|████████▋ | 1346/1540 [54:24<07:49,  2.42s/it] 87%|████████▋ | 1347/1540 [54:27<07:46,  2.42s/it] 88%|████████▊ | 1348/1540 [54:29<07:44,  2.42s/it] 88%|████████▊ | 1349/1540 [54:31<07:43,  2.43s/it] 88%|████████▊ | 1350/1540 [54:34<07:40,  2.42s/it] 88%|████████▊ | 1351/1540 [54:36<07:37,  2.42s/it] 88%|████████▊ | 1352/1540 [54:39<07:35,  2.42s/it] 88%|████████▊ | 1353/1540 [54:41<07:32,  2.42s/it] 88%|████████▊ | 1354/1540 [54:43<07:30,  2.42s/it] 88%|████████▊ | 1355/1540 [54:46<07:27,  2.42s/it] 88%|████████▊ | 1356/1540 [54:48<07:25,  2.42s/it] 88%|████████▊ | 1357/1540 [54:51<07:22,  2.42s/it] 88%|████████▊ | 1358/1540 [54:53<07:20,  2.42s/it] 88%|████████▊ | 1359/1540 [54:56<07:18,  2.42s/it] 88%|████████▊ | 1360/1540 [54:58<07:15,  2.42s/it]                                                   {'loss': 0.4064, 'grad_norm': 0.34814226627349854, 'learning_rate': 2.6443682535072176e-05, 'epoch': 4.42}
 88%|████████▊ | 1360/1540 [54:58<07:15,  2.42s/it] 88%|████████▊ | 1361/1540 [55:00<07:13,  2.42s/it] 88%|████████▊ | 1362/1540 [55:03<07:10,  2.42s/it] 89%|████████▊ | 1363/1540 [55:05<07:08,  2.42s/it] 89%|████████▊ | 1364/1540 [55:08<07:05,  2.42s/it] 89%|████████▊ | 1365/1540 [55:10<07:03,  2.42s/it] 89%|████████▊ | 1366/1540 [55:13<07:00,  2.42s/it] 89%|████████▉ | 1367/1540 [55:15<06:58,  2.42s/it] 89%|████████▉ | 1368/1540 [55:17<06:56,  2.42s/it] 89%|████████▉ | 1369/1540 [55:20<06:53,  2.42s/it] 89%|████████▉ | 1370/1540 [55:22<06:51,  2.42s/it] 89%|████████▉ | 1371/1540 [55:25<06:48,  2.42s/it] 89%|████████▉ | 1372/1540 [55:27<06:46,  2.42s/it] 89%|████████▉ | 1373/1540 [55:29<06:44,  2.42s/it] 89%|████████▉ | 1374/1540 [55:32<06:41,  2.42s/it] 89%|████████▉ | 1375/1540 [55:34<06:39,  2.42s/it] 89%|████████▉ | 1376/1540 [55:37<06:36,  2.42s/it] 89%|████████▉ | 1377/1540 [55:39<06:37,  2.44s/it] 89%|████████▉ | 1378/1540 [55:42<06:34,  2.43s/it] 90%|████████▉ | 1379/1540 [55:44<06:31,  2.43s/it] 90%|████████▉ | 1380/1540 [55:46<06:28,  2.43s/it]                                                   {'loss': 0.3708, 'grad_norm': 0.44205042719841003, 'learning_rate': 2.1029720770042116e-05, 'epoch': 4.48}
 90%|████████▉ | 1380/1540 [55:46<06:28,  2.43s/it] 90%|████████▉ | 1381/1540 [55:49<06:25,  2.43s/it] 90%|████████▉ | 1382/1540 [55:51<06:22,  2.42s/it] 90%|████████▉ | 1383/1540 [55:54<06:20,  2.42s/it] 90%|████████▉ | 1384/1540 [55:56<06:17,  2.42s/it] 90%|████████▉ | 1385/1540 [55:59<06:15,  2.42s/it] 90%|█████████ | 1386/1540 [56:01<06:12,  2.42s/it] 90%|█████████ | 1387/1540 [56:03<06:10,  2.42s/it] 90%|█████████ | 1388/1540 [56:06<06:07,  2.42s/it] 90%|█████████ | 1389/1540 [56:08<06:05,  2.42s/it] 90%|█████████ | 1390/1540 [56:11<06:03,  2.42s/it] 90%|█████████ | 1391/1540 [56:13<06:00,  2.42s/it] 90%|█████████ | 1392/1540 [56:15<05:58,  2.42s/it] 90%|█████████ | 1393/1540 [56:18<05:55,  2.42s/it] 91%|█████████ | 1394/1540 [56:20<05:53,  2.42s/it] 91%|█████████ | 1395/1540 [56:23<05:50,  2.42s/it] 91%|█████████ | 1396/1540 [56:25<05:48,  2.42s/it] 91%|█████████ | 1397/1540 [56:28<05:46,  2.42s/it] 91%|█████████ | 1398/1540 [56:30<05:43,  2.42s/it] 91%|█████████ | 1399/1540 [56:32<05:41,  2.42s/it] 91%|█████████ | 1400/1540 [56:35<05:38,  2.42s/it]                                                   {'loss': 0.3954, 'grad_norm': 0.34614846110343933, 'learning_rate': 1.6211178842549468e-05, 'epoch': 4.55}
 91%|█████████ | 1400/1540 [56:35<05:38,  2.42s/it] 91%|█████████ | 1401/1540 [56:37<05:36,  2.42s/it] 91%|█████████ | 1402/1540 [56:40<05:34,  2.42s/it] 91%|█████████ | 1403/1540 [56:42<05:31,  2.42s/it] 91%|█████████ | 1404/1540 [56:45<05:29,  2.42s/it] 91%|█████████ | 1405/1540 [56:47<05:26,  2.42s/it] 91%|█████████▏| 1406/1540 [56:49<05:24,  2.42s/it] 91%|█████████▏| 1407/1540 [56:52<05:21,  2.42s/it] 91%|█████████▏| 1408/1540 [56:54<05:19,  2.42s/it] 91%|█████████▏| 1409/1540 [56:57<05:17,  2.42s/it] 92%|█████████▏| 1410/1540 [56:59<05:14,  2.42s/it] 92%|█████████▏| 1411/1540 [57:01<05:12,  2.42s/it] 92%|█████████▏| 1412/1540 [57:04<05:09,  2.42s/it] 92%|█████████▏| 1413/1540 [57:06<05:07,  2.42s/it] 92%|█████████▏| 1414/1540 [57:09<05:04,  2.42s/it] 92%|█████████▏| 1415/1540 [57:11<05:02,  2.42s/it] 92%|█████████▏| 1416/1540 [57:14<05:00,  2.42s/it] 92%|█████████▏| 1417/1540 [57:16<04:57,  2.42s/it] 92%|█████████▏| 1418/1540 [57:18<04:55,  2.42s/it] 92%|█████████▏| 1419/1540 [57:21<04:53,  2.43s/it] 92%|█████████▏| 1420/1540 [57:23<04:51,  2.43s/it]                                                   {'loss': 0.4167, 'grad_norm': 0.35199904441833496, 'learning_rate': 1.2000587005259606e-05, 'epoch': 4.61}
 92%|█████████▏| 1420/1540 [57:23<04:51,  2.43s/it] 92%|█████████▏| 1421/1540 [57:26<04:48,  2.42s/it] 92%|█████████▏| 1422/1540 [57:28<04:46,  2.42s/it] 92%|█████████▏| 1423/1540 [57:31<04:43,  2.42s/it] 92%|█████████▏| 1424/1540 [57:33<04:40,  2.42s/it] 93%|█████████▎| 1425/1540 [57:35<04:38,  2.42s/it] 93%|█████████▎| 1426/1540 [57:38<04:36,  2.42s/it] 93%|█████████▎| 1427/1540 [57:40<04:33,  2.42s/it] 93%|█████████▎| 1428/1540 [57:43<04:31,  2.42s/it] 93%|█████████▎| 1429/1540 [57:45<04:28,  2.42s/it] 93%|█████████▎| 1430/1540 [57:47<04:26,  2.42s/it] 93%|█████████▎| 1431/1540 [57:50<04:23,  2.42s/it] 93%|█████████▎| 1432/1540 [57:52<04:21,  2.42s/it] 93%|█████████▎| 1433/1540 [57:55<04:18,  2.42s/it] 93%|█████████▎| 1434/1540 [57:57<04:16,  2.42s/it] 93%|█████████▎| 1435/1540 [58:00<04:14,  2.42s/it] 93%|█████████▎| 1436/1540 [58:02<04:11,  2.42s/it] 93%|█████████▎| 1437/1540 [58:04<04:09,  2.42s/it] 93%|█████████▎| 1438/1540 [58:07<04:06,  2.42s/it] 93%|█████████▎| 1439/1540 [58:09<04:04,  2.42s/it] 94%|█████████▎| 1440/1540 [58:12<04:01,  2.42s/it]                                                   {'loss': 0.4217, 'grad_norm': 0.3452942371368408, 'learning_rate': 8.408894582757953e-06, 'epoch': 4.68}
 94%|█████████▎| 1440/1540 [58:12<04:01,  2.42s/it] 94%|█████████▎| 1441/1540 [58:14<03:59,  2.42s/it] 94%|█████████▎| 1442/1540 [58:17<03:57,  2.42s/it] 94%|█████████▎| 1443/1540 [58:19<03:54,  2.42s/it] 94%|█████████▍| 1444/1540 [58:21<03:52,  2.42s/it] 94%|█████████▍| 1445/1540 [58:24<03:49,  2.42s/it] 94%|█████████▍| 1446/1540 [58:26<03:49,  2.44s/it] 94%|█████████▍| 1447/1540 [58:29<03:46,  2.44s/it] 94%|█████████▍| 1448/1540 [58:31<03:43,  2.43s/it] 94%|█████████▍| 1449/1540 [58:34<03:40,  2.43s/it] 94%|█████████▍| 1450/1540 [58:36<03:38,  2.43s/it] 94%|█████████▍| 1451/1540 [58:38<03:35,  2.42s/it] 94%|█████████▍| 1452/1540 [58:41<03:33,  2.42s/it] 94%|█████████▍| 1453/1540 [58:43<03:30,  2.42s/it] 94%|█████████▍| 1454/1540 [58:46<03:28,  2.42s/it] 94%|█████████▍| 1455/1540 [58:48<03:25,  2.42s/it] 95%|█████████▍| 1456/1540 [58:50<03:23,  2.42s/it] 95%|█████████▍| 1457/1540 [58:53<03:20,  2.42s/it] 95%|█████████▍| 1458/1540 [58:55<03:18,  2.42s/it] 95%|█████████▍| 1459/1540 [58:58<03:15,  2.42s/it] 95%|█████████▍| 1460/1540 [59:00<03:13,  2.42s/it]                                                   {'loss': 0.4109, 'grad_norm': 0.38909912109375, 'learning_rate': 5.445441498662052e-06, 'epoch': 4.74}
 95%|█████████▍| 1460/1540 [59:00<03:13,  2.42s/it] 95%|█████████▍| 1461/1540 [59:03<03:11,  2.42s/it] 95%|█████████▍| 1462/1540 [59:05<03:08,  2.42s/it] 95%|█████████▌| 1463/1540 [59:07<03:06,  2.42s/it] 95%|█████████▌| 1464/1540 [59:10<03:04,  2.42s/it] 95%|█████████▌| 1465/1540 [59:12<03:01,  2.42s/it] 95%|█████████▌| 1466/1540 [59:15<02:59,  2.42s/it] 95%|█████████▌| 1467/1540 [59:17<02:56,  2.42s/it] 95%|█████████▌| 1468/1540 [59:20<02:54,  2.42s/it] 95%|█████████▌| 1469/1540 [59:22<02:51,  2.42s/it] 95%|█████████▌| 1470/1540 [59:24<02:49,  2.42s/it] 96%|█████████▌| 1471/1540 [59:27<02:46,  2.42s/it] 96%|█████████▌| 1472/1540 [59:29<02:44,  2.42s/it] 96%|█████████▌| 1473/1540 [59:32<02:42,  2.42s/it] 96%|█████████▌| 1474/1540 [59:34<02:39,  2.42s/it] 96%|█████████▌| 1475/1540 [59:36<02:37,  2.42s/it] 96%|█████████▌| 1476/1540 [59:39<02:34,  2.42s/it] 96%|█████████▌| 1477/1540 [59:41<02:32,  2.42s/it] 96%|█████████▌| 1478/1540 [59:44<02:30,  2.42s/it] 96%|█████████▌| 1479/1540 [59:46<02:27,  2.42s/it] 96%|█████████▌| 1480/1540 [59:49<02:25,  2.42s/it]                                                   {'loss': 0.4202, 'grad_norm': 0.3741728365421295, 'learning_rate': 3.1179339878588397e-06, 'epoch': 4.81}
 96%|█████████▌| 1480/1540 [59:49<02:25,  2.42s/it] 96%|█████████▌| 1481/1540 [59:51<02:22,  2.42s/it] 96%|█████████▌| 1482/1540 [59:53<02:20,  2.42s/it] 96%|█████████▋| 1483/1540 [59:56<02:17,  2.42s/it] 96%|█████████▋| 1484/1540 [59:58<02:15,  2.42s/it] 96%|█████████▋| 1485/1540 [1:00:01<02:13,  2.42s/it] 96%|█████████▋| 1486/1540 [1:00:03<02:10,  2.42s/it] 97%|█████████▋| 1487/1540 [1:00:06<02:08,  2.42s/it] 97%|█████████▋| 1488/1540 [1:00:08<02:05,  2.42s/it] 97%|█████████▋| 1489/1540 [1:00:10<02:03,  2.42s/it] 97%|█████████▋| 1490/1540 [1:00:13<02:00,  2.42s/it] 97%|█████████▋| 1491/1540 [1:00:15<01:58,  2.42s/it] 97%|█████████▋| 1492/1540 [1:00:18<01:56,  2.42s/it] 97%|█████████▋| 1493/1540 [1:00:20<01:53,  2.42s/it] 97%|█████████▋| 1494/1540 [1:00:22<01:51,  2.42s/it] 97%|█████████▋| 1495/1540 [1:00:25<01:48,  2.42s/it] 97%|█████████▋| 1496/1540 [1:00:27<01:46,  2.42s/it] 97%|█████████▋| 1497/1540 [1:00:30<01:44,  2.42s/it] 97%|█████████▋| 1498/1540 [1:00:32<01:41,  2.42s/it] 97%|█████████▋| 1499/1540 [1:00:35<01:39,  2.42s/it] 97%|█████████▋| 1500/1540 [1:00:37<01:36,  2.42s/it]                                                     {'loss': 0.3878, 'grad_norm': 0.41138914227485657, 'learning_rate': 1.4324245570256633e-06, 'epoch': 4.87}
 97%|█████████▋| 1500/1540 [1:00:37<01:36,  2.42s/it] 97%|█████████▋| 1501/1540 [1:00:39<01:34,  2.42s/it] 98%|█████████▊| 1502/1540 [1:00:42<01:31,  2.42s/it] 98%|█████████▊| 1503/1540 [1:00:44<01:29,  2.42s/it] 98%|█████████▊| 1504/1540 [1:00:47<01:27,  2.42s/it] 98%|█████████▊| 1505/1540 [1:00:49<01:24,  2.42s/it] 98%|█████████▊| 1506/1540 [1:00:52<01:22,  2.42s/it] 98%|█████████▊| 1507/1540 [1:00:54<01:19,  2.42s/it] 98%|█████████▊| 1508/1540 [1:00:56<01:17,  2.42s/it] 98%|█████████▊| 1509/1540 [1:00:59<01:14,  2.42s/it] 98%|█████████▊| 1510/1540 [1:01:01<01:12,  2.42s/it] 98%|█████████▊| 1511/1540 [1:01:04<01:10,  2.42s/it] 98%|█████████▊| 1512/1540 [1:01:06<01:07,  2.42s/it] 98%|█████████▊| 1513/1540 [1:01:08<01:05,  2.42s/it] 98%|█████████▊| 1514/1540 [1:01:11<01:02,  2.42s/it] 98%|█████████▊| 1515/1540 [1:01:13<01:00,  2.42s/it] 98%|█████████▊| 1516/1540 [1:01:16<00:58,  2.44s/it] 99%|█████████▊| 1517/1540 [1:01:18<00:55,  2.43s/it] 99%|█████████▊| 1518/1540 [1:01:21<00:53,  2.43s/it] 99%|█████████▊| 1519/1540 [1:01:23<00:50,  2.43s/it] 99%|█████████▊| 1520/1540 [1:01:25<00:48,  2.42s/it]                                                     {'loss': 0.4096, 'grad_norm': 0.37005406618118286, 'learning_rate': 3.932962455458489e-07, 'epoch': 4.94}
 99%|█████████▊| 1520/1540 [1:01:25<00:48,  2.42s/it] 99%|█████████▉| 1521/1540 [1:01:28<00:46,  2.42s/it] 99%|█████████▉| 1522/1540 [1:01:30<00:43,  2.42s/it] 99%|█████████▉| 1523/1540 [1:01:33<00:41,  2.42s/it] 99%|█████████▉| 1524/1540 [1:01:35<00:38,  2.42s/it] 99%|█████████▉| 1525/1540 [1:01:38<00:36,  2.42s/it] 99%|█████████▉| 1526/1540 [1:01:40<00:33,  2.42s/it] 99%|█████████▉| 1527/1540 [1:01:42<00:31,  2.42s/it] 99%|█████████▉| 1528/1540 [1:01:45<00:29,  2.42s/it] 99%|█████████▉| 1529/1540 [1:01:47<00:26,  2.42s/it] 99%|█████████▉| 1530/1540 [1:01:50<00:24,  2.42s/it] 99%|█████████▉| 1531/1540 [1:01:52<00:21,  2.42s/it] 99%|█████████▉| 1532/1540 [1:01:54<00:19,  2.42s/it]100%|█████████▉| 1533/1540 [1:01:57<00:16,  2.42s/it]100%|█████████▉| 1534/1540 [1:01:59<00:14,  2.42s/it]100%|█████████▉| 1535/1540 [1:02:02<00:12,  2.42s/it]100%|█████████▉| 1536/1540 [1:02:04<00:09,  2.42s/it]100%|█████████▉| 1537/1540 [1:02:07<00:07,  2.42s/it]100%|█████████▉| 1538/1540 [1:02:09<00:04,  2.42s/it]100%|█████████▉| 1539/1540 [1:02:11<00:02,  2.42s/it]100%|██████████| 1540/1540 [1:02:14<00:00,  2.42s/it]                                                     {'loss': 0.3907, 'grad_norm': 0.3646155297756195, 'learning_rate': 3.2512277473584205e-09, 'epoch': 5.0}
100%|██████████| 1540/1540 [1:02:14<00:00,  2.42s/it][INFO|trainer.py:2231] 2024-05-25 08:24:27,683 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 3756.4729, 'train_samples_per_second': 3.277, 'train_steps_per_second': 0.41, 'train_loss': 0.6085033330050382, 'epoch': 5.0}
100%|██████████| 1540/1540 [1:02:14<00:00,  2.42s/it]100%|██████████| 1540/1540 [1:02:14<00:00,  2.42s/it]
***** train metrics *****
  epoch                    =        5.0
  train_loss               =     0.6085
  train_runtime            = 1:02:36.47
  train_samples_per_second =      3.277
  train_steps_per_second   =       0.41
[INFO|trainer.py:3203] 2024-05-25 08:24:27,689 >> Saving model checkpoint to /scratch/tathagato/adapter_experiments/topic_then_extractiveness
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-05-25 08:24:29,157 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 08:24:29,160 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|configuration_utils.py:726] 2024-05-25 08:24:30,042 >> loading configuration file config.json from cache at /home2/tathagato/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json
[INFO|configuration_utils.py:789] 2024-05-25 08:24:30,044 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.39.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-05-25 08:24:30,098 >> tokenizer config file saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-25 08:24:30,099 >> Special tokens file saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/special_tokens_map.json
/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
[INFO|configuration_utils.py:471] 2024-05-25 08:24:31,054 >> Configuration saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/final_merged_model/config.json
[INFO|configuration_utils.py:697] 2024-05-25 08:24:31,054 >> Configuration saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/final_merged_model/generation_config.json
[INFO|modeling_utils.py:2474] 2024-05-25 08:24:36,644 >> Model weights saved in /scratch/tathagato/adapter_experiments/topic_then_extractiveness/final_merged_model/model.safetensors
wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.006 MB of 0.006 MB uploadedwandb: | 0.006 MB of 0.006 MB uploadedwandb: / 0.006 MB of 0.034 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb: \ 0.038 MB of 0.038 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:     train/grad_norm ▃▄▄▁▂▃▂▁▅▅▅▅▇▄▇▅▄▄▆▅▅▅▇▄▅▆▅█▆▇▄▅▅▄▃▆▃▄▅▄
wandb: train/learning_rate ▁▂▃▄▅▆▆▇██████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:          train/loss ███▇▇▇▆▆▆▅▅▅▆▅▄▅▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 1.5729502080466944e+17
wandb:              train/epoch 5.0
wandb:        train/global_step 1540
wandb:          train/grad_norm 0.36462
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.3907
wandb:               train_loss 0.6085
wandb:            train_runtime 3756.4729
wandb: train_samples_per_second 3.277
wandb:   train_steps_per_second 0.41
wandb: 
wandb: 🚀 View run sunny-microwave-103 at: https://wandb.ai/ihub-drug-discovery/huggingface/runs/d65347to
wandb: ⭐️ View project at: https://wandb.ai/ihub-drug-discovery/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240525_072157-d65347to/logs
Completed
