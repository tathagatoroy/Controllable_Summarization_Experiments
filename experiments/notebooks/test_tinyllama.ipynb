{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "cache_dir = \"/scratch/tathagato\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading quantized model\n"
     ]
    }
   ],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cuda:0',\n",
    "    cache_dir = cache_dir,\n",
    "    attn_implementation = \"eager\",\n",
    "    quantization_config = nf4_config, \n",
    ")\n",
    "\n",
    "print(\"loading quantized model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path,cache_dir = cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate \n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize= False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "#add a function here\n",
    "\n",
    "outputs = tokenizer(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=2048,\n",
    "                return_overflowing_tokens=False,\n",
    "                return_length=False,\n",
    "            )\n",
    "print(len(outputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(outputs[\"input_ids\"], skip_special_tokens=True))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always help the user \n",
      "<|user|>\n",
      "Write a summary of the text. The summary should be normal in length.. The input text is given below \n",
      "(CNN)Beer and ice cream. It doesn't exactly spring to mind when you think of classic food pairings -- old friends such as bacon and eggs or steak and cabernet. But Colorado's New Belgium Brewery and the folks at Ben & Jerry's are teaming up on a beer inspired by ice cream -- salted caramel brownie ice cream, to be precise. \"At this time I can confirm that Ben & Jerry's and New Belgium are collaborating to raise awareness around issues we are passionate about, and that the results will be delicious,\" New Belgium's Director of Sustainability, Jenn Vervier, said in a statement. Both companies have a history of social activism, and the new project will be no different, they say. Their release doesn't say what the campaign will be all about, but Ben & Jerry's Senior Global Marketing Manager Jay Curley promises it will be \"impactful.\" \"We're big fans of New Belgium Brewery, their values, and their fun culture, and of course their beer,\" he said. \"We're excited for the campaign we've developed together.\" The companies will announce the details later this year, and the beer is set to hit shelves in the fall. New Belgium and Ben & Jerry's are both what are called \"B Corporations,\" a certification issued by the private non-profit B Labs to companies that meet its social, environmental, accountability and transparency standards. New Belgium supports sustainable agriculture, climate change and other initiatives, while Ben & Jerry's -- now a subsidiary of global conglomerate Unilever -- stays true to its hippie roots with support for environmental initiatives, fair trade efforts, marriage equality and more. Last month, Ben & Jerry's cofounder Ben Cohen said he'd be open to the idea of a marijuana-infused ice cream someday, news that set pot fans ablaze. But sadly for beer fans, there's no talk of a beer-flavored ice cream. Not yet anyway. \n",
      "<|assistant|>\n",
      "Last month, Ben & Jerry's news set marijuana fans ablaze, cofounder Ben Cohen expressed that someday Ben & Jerry's could launch a marijuana-infused ice cream, staying true to its hippie roots. \n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "example = [1, 529, 29989, 5205, 29989, 29958, 13, 3492, 526, 263, 19780, 13563, 7451, 1058, 2337, 1371, 278, 1404, 2, 29871, 13, 29966, 29989, 1792, 29989, 29958, 13, 6113, 263, 15837, 310, 278, 1426, 29889, 450, 15837, 881, 367, 4226, 297, 3309, 636, 450, 1881, 1426, 338, 2183, 2400, 29871, 13, 29898, 29907, 10262, 29897, 3629, 261, 322, 14890, 907, 314, 29889, 739, 1838, 29915, 29873, 3721, 6709, 304, 3458, 746, 366, 1348, 310, 22037, 9687, 5101, 886, 1192, 2030, 7875, 1316, 408, 9922, 535, 322, 29808, 470, 1886, 557, 322, 7776, 824, 300, 29889, 1205, 21137, 29915, 29879, 1570, 9923, 1974, 5826, 556, 29891, 322, 278, 900, 2039, 472, 4111, 669, 23052, 29915, 29879, 526, 3815, 292, 701, 373, 263, 367, 261, 20603, 491, 14890, 907, 314, 1192, 15795, 287, 1559, 314, 295, 3347, 2786, 14890, 907, 314, 29892, 304, 367, 18378, 29889, 376, 4178, 445, 931, 306, 508, 9659, 393, 4111, 669, 23052, 29915, 29879, 322, 1570, 9923, 1974, 526, 11465, 1218, 304, 12020, 3773, 8326, 404, 2820, 5626, 591, 526, 15935, 403, 1048, 29892, 322, 393, 278, 2582, 674, 367, 628, 14803, 1699, 1570, 9923, 1974, 29915, 29879, 15944, 310, 317, 504, 475, 3097, 29892, 23774, 1798, 7214, 29892, 1497, 297, 263, 3229, 29889, 9134, 14582, 505, 263, 4955, 310, 5264, 5039, 1608, 29892, 322, 278, 716, 2060, 674, 367, 694, 1422, 29892, 896, 1827, 29889, 11275, 6507, 1838, 29915, 29873, 1827, 825, 278, 11531, 674, 367, 599, 1048, 29892, 541, 4111, 669, 23052, 29915, 29879, 24260, 12002, 4485, 15133, 15629, 19556, 10837, 2330, 27584, 372, 674, 367, 376, 6574, 627, 1319, 1213, 376, 4806, 29915, 276, 4802, 24909, 310, 1570, 9923, 1974, 5826, 556, 29891, 29892, 1009, 1819, 29892, 322, 1009, 2090, 9257, 29892, 322, 310, 3236, 1009, 367, 261, 1699, 540, 1497, 29889, 376, 4806, 29915, 276, 24173, 363, 278, 11531, 591, 29915, 345, 8906, 4208, 1213, 450, 14582, 674, 7475, 346, 278, 4902, 2678, 445, 1629, 29892, 322, 278, 367, 261, 338, 731, 304, 7124, 528, 295, 1960, 297, 278, 6416, 29889, 1570, 9923, 1974, 322, 4111, 669, 23052, 29915, 29879, 526, 1716, 825, 526, 2000, 376, 29933, 12767, 800, 1699, 263, 2284, 2450, 16610, 491, 278, 2024, 1661, 29899, 771, 9202, 350, 365, 6897, 304, 14582, 393, 5870, 967, 5264, 29892, 29380, 29892, 3633, 3097, 322, 1301, 862, 3819, 20801, 29889, 1570, 9923, 1974, 11286, 15075, 475, 519, 18032, 545, 29892, 23622, 1735, 322, 916, 14511, 5056, 29892, 1550, 4111, 669, 23052, 29915, 29879, 1192, 1286, 263, 11684, 8819, 653, 310, 5534, 378, 3820, 12392, 403, 853, 488, 369, 1192, 27111, 1565, 304, 967, 7251, 407, 347, 16778, 411, 2304, 363, 29380, 14511, 5056, 29892, 6534, 11302, 14231, 29892, 13718, 17193, 322, 901, 29889, 9208, 4098, 29892, 4111, 669, 23052, 29915, 29879, 274, 974, 618, 261, 4111, 315, 14899, 1497, 540, 29915, 29881, 367, 1722, 304, 278, 2969, 310, 263, 1766, 26323, 1648, 29899, 7192, 3880, 14890, 907, 314, 1047, 287, 388, 29892, 9763, 393, 731, 3104, 24909, 633, 433, 911, 29889, 1205, 14610, 368, 363, 367, 261, 24909, 29892, 727, 29915, 29879, 694, 5193, 310, 263, 367, 261, 29899, 29888, 4112, 4395, 14890, 907, 314, 29889, 2216, 3447, 8763, 29889, 2, 29871, 13, 29966, 29989, 465, 22137, 29989, 29958, 13, 8897, 4098, 29892, 4111, 669, 23052, 29915, 29879, 9763, 731, 1766, 26323, 1648, 24909, 633, 433, 911, 29892, 274, 974, 618, 261, 4111, 315, 14899, 13384, 393, 1047, 287, 388, 4111, 669, 23052, 29915, 29879, 1033, 6826, 263, 1766, 26323, 1648, 29899, 7192, 3880, 14890, 907, 314, 29892, 7952, 292, 1565, 304, 967, 7251, 407, 347, 16778, 29889, 2, 29871, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(tokenizer.decode(example, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}\n",
      "[1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "#print the special tokens\n",
    "print(tokenizer.special_tokens_map)\n",
    "#print all of the token id for the special tokens\n",
    "print(tokenizer.all_special_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting?</s>\n",
      "\n",
      "tensor([    1,   529, 29989,  5205, 29989, 29958,    13,  3492,   526,   263,\n",
      "        19780, 13563,  7451,  1058,  2337, 10049, 29879,   297,   278,  3114,\n",
      "          310,   263, 21625,   403,     2, 29871,    13, 29966, 29989,  1792,\n",
      "        29989, 29958,    13,  5328,  1784,  1081,   293,   459,  2153,   508,\n",
      "          263,  5199, 17545,   297,   697, 16246, 29973,     2, 29871,    13])\n",
      "<s> 1\n",
      "<|system|> 0\n",
      "You 3492\n",
      "are 598\n",
      "a 29874\n",
      "friendly 0\n",
      "chatbot 0\n",
      "who 15970\n",
      "always 21936\n",
      "responds 0\n",
      "in 262\n",
      "the 1552\n",
      "style 3293\n",
      "of 974\n",
      "a 29874\n",
      "pirate</s> 0\n",
      "<|user|> 0\n",
      "How 5328\n",
      "many 13011\n",
      "helicopters 0\n",
      "can 3068\n",
      "a 29874\n",
      "human 26029\n",
      "eat 0\n",
      "in 262\n",
      "one 650\n",
      "sitting?</s> 0\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize= False)\n",
    "print(text)\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "print(tokens['input_ids'][0])\n",
    "\n",
    "#decode with tokenizer with special tokens intact\n",
    "decoded = tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=False)\n",
    "\n",
    "for token in decoded.split():\n",
    "    print(token, tokenizer.convert_tokens_to_ids(token))\n",
    "#print special tokens\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.decode([13], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁<', '|', 'system', '|', '>', '<0x0A>', 'You', '▁are', '▁a', '▁friendly', '▁chat', 'bot', '▁who', '▁always', '▁respond', 's', '▁in', '▁the', '▁style', '▁of', '▁a', '▁pir', 'ate', '</s>', '▁', '<0x0A>', '<', '|', 'user', '|', '>', '<0x0A>', 'How', '▁many', '▁hel', 'ic', 'op', 'ters', '▁can', '▁a', '▁human', '▁eat', '▁in', '▁one', '▁sitting', '?', '</s>', '▁', '<0x0A>']\n",
      "[(['<s>'], tensor(1)), (['▁<'], tensor(529)), (['|'], tensor(29989)), (['system'], tensor(5205)), (['|'], tensor(29989)), (['>'], tensor(29958)), (['<0x0A>'], tensor(13)), (['You'], tensor(3492)), (['▁are'], tensor(526)), (['▁a'], tensor(263)), (['▁friendly'], tensor(19780)), (['▁chat'], tensor(13563)), (['bot'], tensor(7451)), (['▁who'], tensor(1058)), (['▁always'], tensor(2337)), (['▁respond'], tensor(10049)), (['s'], tensor(29879)), (['▁in'], tensor(297)), (['▁the'], tensor(278)), (['▁style'], tensor(3114)), (['▁of'], tensor(310)), (['▁a'], tensor(263)), (['▁pir'], tensor(21625)), (['ate'], tensor(403)), (['</s>'], tensor(2)), (['▁'], tensor(29871)), (['<0x0A>'], tensor(13)), (['<'], tensor(29966)), (['|'], tensor(29989)), (['user'], tensor(1792)), (['|'], tensor(29989)), (['>'], tensor(29958)), (['<0x0A>'], tensor(13)), (['How'], tensor(5328)), (['▁many'], tensor(1784)), (['▁hel'], tensor(1081)), (['ic'], tensor(293)), (['op'], tensor(459)), (['ters'], tensor(2153)), (['▁can'], tensor(508)), (['▁a'], tensor(263)), (['▁human'], tensor(5199)), (['▁eat'], tensor(17545)), (['▁in'], tensor(297)), (['▁one'], tensor(697)), (['▁sitting'], tensor(16246)), (['?'], tensor(29973)), (['</s>'], tensor(2)), (['▁'], tensor(29871)), (['<0x0A>'], tensor(13))]\n"
     ]
    }
   ],
   "source": [
    "#get the token to txt mapping from the tokenizer\n",
    "print(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]))\n",
    "token_token_id_pair = [(tokenizer.convert_ids_to_tokens([tokens[\"input_ids\"][0][i]]), tokens[\"input_ids\"][0][i]) for i in range(len(tokens[\"input_ids\"][0]))]\n",
    "print(token_token_id_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   529, 29989,  5205, 29989, 29958,    13,  3492,   526,   263,\n",
      "         19780, 13563,  7451,  1058,  2337, 10049, 29879,   297,   278,  3114,\n",
      "           310,   263, 21625,   403,     2, 29871,    13, 29966, 29989,  1792,\n",
      "         29989, 29958,    13,  5328,  1784,  1081,   293,   459,  2153,   508,\n",
      "           263,  5199, 17545,   297,   697, 16246, 29973,     2, 29871,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [50] at entry 0 and [10] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m input_id_1 \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m input_id_2 \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_id_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_id_2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [50] at entry 0 and [10] at entry 1"
     ]
    }
   ],
   "source": [
    "#create a batched input where the first 10 words of the token is copied and batched\n",
    "input_id_1 = tokens[\"input_ids\"][0]\n",
    "input_id_2 = tokens[\"input_ids\"][0][:10]\n",
    "input_ids = torch.stack([input_id_1, input_id_2])\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits'])\n"
     ]
    }
   ],
   "source": [
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 32000])\n"
     ]
    }
   ],
   "source": [
    "print(output['logits'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode the output\n",
    "decoded_output = tokenizer.decode(output['logits'][0].argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT|system|>\n",
      "</s> can a successful andbot that can gres to a same of a humanate. \n",
      "<|user|>\n",
      "Can can pirmopters does the pir carry in one sitting?</s> \n",
      "<\n"
     ]
    }
   ],
   "source": [
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
