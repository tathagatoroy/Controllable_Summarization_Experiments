{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home2/tathagato/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home2/tathagato/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home2/tathagato/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl \n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas \n",
    "from pyrouge import Rouge155\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import tempfile\n",
    "import shutil\n",
    "import tqdm\n",
    "import time\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize, ngrams\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "st_words = stopwords.words('english')\n",
    "import pprint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_score(candidates , references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rouge3', 'rougeL'], use_stemmer=True)\n",
    "    arguments = [(candidate, reference, scorer) for candidate,reference in zip(candidates, references)]\n",
    "    with Pool(multiprocessing.cpu_count() - 2) as pool:\n",
    "        results = pool.starmap(scores, arguments)\n",
    "    # do agregation\n",
    "    aggregrated_score = {}\n",
    "    aggregrated_score['rouge1'] = {'precision' : 0, 'recall' : 0, 'f1' : 0}\n",
    "    aggregrated_score['rouge2'] = {'precision' : 0, 'recall' : 0, 'f1' : 0}\n",
    "    aggregrated_score['rouge3'] = {'precision' : 0, 'recall' : 0, 'f1' : 0}\n",
    "    aggregrated_score['rougeL'] = {'precision' : 0, 'recall' : 0, 'f1' : 0}\n",
    "\n",
    "    for result in results:\n",
    "        for key in result.keys():\n",
    "            aggregrated_score[key]['precision'] += result[key]['precision']\n",
    "            aggregrated_score[key]['recall'] += result[key]['recall']\n",
    "            aggregrated_score[key]['f1'] += result[key]['f1']\n",
    "    for key in aggregrated_score.keys():\n",
    "        aggregrated_score[key]['precision'] /= len(results)\n",
    "        aggregrated_score[key]['recall'] /= len(results)\n",
    "        aggregrated_score[key]['f1'] /= len(results)\n",
    "    output = {'aggregrated_score' : aggregrated_score, 'results' : results}\n",
    "    return output\n",
    "    \n",
    "\n",
    "def scores(summary, reference, scorer):\n",
    "    score = scorer.score(summary, reference)\n",
    "    res = {}\n",
    "    res['rouge1'] = {'precision' : score['rouge1'].precision, 'recall' : score['rouge1'].recall, 'f1' : score['rouge1'].fmeasure}\n",
    "    res['rouge2'] = {'precision' : score['rouge2'].precision, 'recall' : score['rouge2'].recall, 'f1' : score['rouge2'].fmeasure}\n",
    "    res['rouge3'] = {'precision' : score['rouge3'].precision, 'recall' : score['rouge3'].recall, 'f1' : score['rouge3'].fmeasure}\n",
    "    res['rougeL'] = {'precision' : score['rougeL'].precision, 'recall' : score['rougeL'].recall, 'f1' : score['rougeL'].fmeasure}\n",
    "    return res\n",
    "def get_summary_length(summary):\n",
    "    return len(word_tokenize(summary.lower()))\n",
    "\n",
    "def get_compression_ratio(article, summary):\n",
    "    return float(len(word_tokenize(summary.lower()))) / float(len(word_tokenize(article.lower())))\n",
    "\n",
    "def get_length_results(candidates, references, articles, control_values):\n",
    "    rouge_results = get_rouge_score(candidates, references)\n",
    "    summary_lengths = [get_summary_length(summary) for summary in candidates]\n",
    "    compression_ratios = [get_compression_ratio(article, summary) for article, summary in zip(articles, candidates)]\n",
    "    groundtruth_lengths = [get_summary_length(reference) for reference in references]\n",
    "    groundtruth_compression_ratios = [get_compression_ratio(article, reference) for article, reference in zip(articles, references)]\n",
    "    rouge_1s = [result['rouge1']['f1'] for result in rouge_results['results']]\n",
    "    rouge_2s = [result['rouge2']['f1'] for result in rouge_results['results']]\n",
    "    rouge_3s = [result['rouge3']['f1'] for result in rouge_results['results']]\n",
    "    rouge_Ls = [result['rougeL']['f1'] for result in rouge_results['results']]\n",
    "\n",
    "    final_results = {}\n",
    "    final_results['rouge_raw'] = rouge_results\n",
    "    final_results['overall'] = {'summary_length' : np.mean(summary_lengths), 'gold_summary_length' : np.mean(groundtruth_lengths), 'compression_ratio' : np.mean(compression_ratios),'gold_compression_ratio' : np.mean(groundtruth_compression_ratios), 'rouge1' : np.mean(rouge_1s), 'rouge2' : np.mean(rouge_2s), 'rouge3' : np.mean(rouge_3s), 'rougeL' : np.mean(rouge_Ls), 'number' : len(candidates)}\n",
    "    short_indexes = [index for index, control_value in enumerate(control_values) if control_value == 'short']\n",
    "    short_candidates = [candidates[index] for index in short_indexes]\n",
    "    short_references = [references[index] for index in short_indexes]\n",
    "    short_length = [summary_lengths[index] for index in short_indexes]\n",
    "    short_compression = [compression_ratios[index] for index in short_indexes]\n",
    "    short_rouge1 = [rouge_1s[index] for index in short_indexes]\n",
    "    short_rouge2 = [rouge_2s[index] for index in short_indexes]\n",
    "    short_rouge3 = [rouge_3s[index] for index in short_indexes]\n",
    "    short_rougeL = [rouge_Ls[index] for index in short_indexes]\n",
    "    short_gold_length = [groundtruth_lengths[index] for index in short_indexes]\n",
    "    short_gold_compression = [groundtruth_compression_ratios[index] for index in short_indexes]\n",
    "    final_results['short'] = {'summary_length' : np.mean(short_length), 'gold_summary_length' : np.mean(short_gold_length), 'compression_ratio' : np.mean(short_compression), 'gold_compression_ratio' : np.mean(short_gold_compression), 'rouge1' : np.mean(short_rouge1), 'rouge2' : np.mean(short_rouge2), 'rouge3' : np.mean(short_rouge3), 'rougeL' : np.mean(short_rougeL), 'number' : len(short_candidates)}\n",
    "    \n",
    "    normal_indexes = [index for index, control_value in enumerate(control_values) if control_value == 'normal']\n",
    "    normal_candidates = [candidates[index] for index in normal_indexes]\n",
    "    normal_references = [references[index] for index in normal_indexes]\n",
    "    normal_length = [summary_lengths[index] for index in normal_indexes]\n",
    "    normal_compression = [compression_ratios[index] for index in normal_indexes]\n",
    "    normal_rouge1 = [rouge_1s[index] for index in normal_indexes]\n",
    "    normal_rouge2 = [rouge_2s[index] for index in normal_indexes]\n",
    "    normal_rouge3 = [rouge_3s[index] for index in normal_indexes]\n",
    "    normal_rougeL = [rouge_Ls[index] for index in normal_indexes]\n",
    "    normal_gold_length = [groundtruth_lengths[index] for index in normal_indexes]\n",
    "    normal_gold_compression = [groundtruth_compression_ratios[index] for index in normal_indexes]\n",
    "    final_results['normal'] = {'summary_length' : np.mean(normal_length), 'gold_summary_length' : np.mean(normal_gold_length), 'compression_ratio' : np.mean(normal_compression), 'gold_compression_ratio' : np.mean(normal_gold_compression), 'rouge1' : np.mean(normal_rouge1), 'rouge2' : np.mean(normal_rouge2), 'rouge3' : np.mean(normal_rouge3), 'rougeL' : np.mean(normal_rougeL), 'number' : len(normal_candidates)}\n",
    "    #get indexes of control values where it is long\n",
    "    long_indexes = [index for index, control_value in enumerate(control_values) if control_value == 'long']\n",
    "    long_candidates = [candidates[index] for index in long_indexes]\n",
    "    long_references = [references[index] for index in long_indexes]\n",
    "    long_length = [summary_lengths[index] for index in long_indexes]\n",
    "    long_compression = [compression_ratios[index] for index in long_indexes]\n",
    "    long_rouge1 = [rouge_1s[index] for index in long_indexes]\n",
    "    long_rouge2 = [rouge_2s[index] for index in long_indexes]\n",
    "    long_rouge3 = [rouge_3s[index] for index in long_indexes]\n",
    "    long_rougeL = [rouge_Ls[index] for index in long_indexes]\n",
    "    long_gold_length = [groundtruth_lengths[index] for index in long_indexes]\n",
    "    long_gold_compression = [groundtruth_compression_ratios[index] for index in long_indexes]\n",
    "    final_results['long'] = {'summary_length' : np.mean(long_length), 'gold_summary_length' : np.mean(long_gold_length), 'compression_ratio' : np.mean(long_compression), 'gold_compression_ratio' : np.mean(long_gold_compression), 'rouge1' : np.mean(long_rouge1), 'rouge2' : np.mean(long_rouge2), 'rouge3' : np.mean(long_rouge3), 'rougeL' : np.mean(long_rougeL), 'number' : len(long_candidates)}\n",
    "    print(\"\\n\\nlength evaluation\")\n",
    "\n",
    "    for key in final_results.keys():\n",
    "        if key == 'rouge_raw':\n",
    "            continue\n",
    "        print(f\"--------------{key}----------------\")\n",
    "        res = final_results[key]\n",
    "        for sub_key in res.keys():\n",
    "            print(f\"{sub_key} : {res[sub_key]}\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    return final_results\n",
    "\n",
    "\n",
    "def get_fragment_density(article, summary):\n",
    "    \"\"\"\n",
    "    Calculates the fragment density of a summary on an article.\n",
    "\n",
    "    Density is defined as the average squared length of extracted fragments.\n",
    "\n",
    "    Args:\n",
    "        article (str): The article text.\n",
    "        summary (str): The summary text.\n",
    "\n",
    "    Returns:\n",
    "        float: The fragment density of the summary on the article.\n",
    "    \"\"\"\n",
    "\n",
    "    frags, article_tokens, summary_tokens = get_extractive_fragments(article, summary)\n",
    "    density = float(sum([len(f)**2 for f in frags])) / float(len(summary_tokens))\n",
    "    return density\n",
    "\n",
    "def get_overlap(inp, out, ngram = 2):\n",
    "    grams_inp = set(ngrams(word_tokenize(inp.lower()), ngram))\n",
    "    grams_out = set(ngrams(word_tokenize(out.lower()), ngram))\n",
    "\n",
    "    total = len(grams_out)\n",
    "    common = len(grams_inp.intersection(grams_out))\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return float(common) / float(total)\n",
    "def get_extractive_fragments(article, summary):\n",
    "    \"\"\"\n",
    "    Extracts fragments from an article that match sequences of words in a summary.\n",
    "\n",
    "    Args:\n",
    "        article (str): The article text.\n",
    "        summary (str): The summary text.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist represents a sequence of word indexes\n",
    "            in the article that match a sequence in the summary.\n",
    "        list: The tokenized article.\n",
    "        list: The tokenized summary.\n",
    "    \"\"\"\n",
    "\n",
    "    article_tokens = word_tokenize(article.lower())\n",
    "    summary_tokens = word_tokenize(summary.lower())\n",
    "\n",
    "    F = []  # List to store the extracted fragments\n",
    "    i, j = 0, 0  # Indexes for iterating over article and summary tokens, respectively\n",
    "\n",
    "    while i < len(summary_tokens):\n",
    "        f = []  # List to store the current fragment\n",
    "        while j < len(article_tokens):\n",
    "            if summary_tokens[i] == article_tokens[j]:\n",
    "                i_, j_ = i, j  # Store starting indexes of potential fragment\n",
    "                #print(len(summary_tokens), len(article_tokens), i, j, i_, j_, summary_tokens[i_], article_tokens[j_])\n",
    "                while (i_ < len(summary_tokens) and j_ < len(article_tokens)) and summary_tokens[i_] == article_tokens[j_]:\n",
    "                    i_, j_ = i_ + 1, j_ + 1  # Update indexes while words match\n",
    "                if len(f) < (i_ - i):  # Update fragment if a longer match is found\n",
    "                    f = list(range(i, i_))\n",
    "                j = j_  # Set j to the next position after the matched sequence\n",
    "            else:\n",
    "                j += 1  # Move to the next article token if no match found\n",
    "        i += max(len(f), 1)  # Update i by the length of the extracted fragment or 1\n",
    "        j = 1  # Reset j for the next iteration\n",
    "\n",
    "        F.append(f)  # Append the extracted fragment to the list\n",
    "\n",
    "    return F, article_tokens, summary_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_extractive_coverage(article, summary):\n",
    "    \"\"\"\n",
    "    Calculates the extractive coverage of a summary on an article.\n",
    "\n",
    "    Coverage is defined as the ratio of words in the summary covered by fragments\n",
    "    extracted from the article.\n",
    "\n",
    "    Args:\n",
    "        article (str): The article text.\n",
    "        summary (str): The summary text.\n",
    "\n",
    "    Returns:\n",
    "        float: The extractive coverage of the summary on the article.\n",
    "    \"\"\"\n",
    "\n",
    "    frags, article_tokens, summary_tokens = get_extractive_fragments(article, summary)\n",
    "    coverage = float(sum([len(f) for f in frags])) / float(len(summary_tokens))\n",
    "    return coverage\n",
    "def get_extractiveness_results(candidates, references, articles, control_values):\n",
    "    rouge_results = get_rouge_score(candidates, references)\n",
    "    article_referenced_rouge_results = get_rouge_score(articles, references)\n",
    "    gold_article_referenced_rouge_results = get_rouge_score(articles, candidates)\n",
    "    fragment_densities = [get_fragment_density(article, summary) for article, summary in zip(articles, candidates)]\n",
    "    gold_fragment_densities = [get_fragment_density(article, reference) for article, reference in zip(articles, references)]\n",
    "    extractive_coverages = [get_extractive_coverage(article, summary) for article, summary in zip(articles, candidates)]\n",
    "    gold_extractive_coverages = [get_extractive_coverage(article, reference) for article, reference in zip(articles, references)]\n",
    "    overlaps = [get_overlap(article, summary) for article, summary in zip(articles, candidates)]\n",
    "    gold_overlaps = [get_overlap(article, reference) for article, reference in zip(articles, references)]\n",
    "    \n",
    "    f_value = [(rouge_2_precision + rouge_3_precision) / 2 for rouge_2_precision, rouge_3_precision in zip([result['rouge2']['precision'] for result in article_referenced_rouge_results['results']], [result['rouge3']['precision'] for result in article_referenced_rouge_results['results']])]\n",
    "    gold_f_value = [(rouge_2_precision + rouge_3_precision) / 2 for rouge_2_precision, rouge_3_precision in zip([result['rouge2']['precision'] for result in gold_article_referenced_rouge_results['results']], [result['rouge3']['precision'] for result in gold_article_referenced_rouge_results['results']])]\n",
    "\n",
    "    rouge_1s = [result['rouge1']['f1'] for result in rouge_results['results']]\n",
    "    rouge_2s = [result['rouge2']['f1'] for result in rouge_results['results']]\n",
    "    rouge_3s = [result['rouge3']['f1'] for result in rouge_results['results']]\n",
    "    rouge_Ls = [result['rougeL']['f1'] for result in rouge_results['results']]\n",
    "\n",
    "    final_results = {}\n",
    "    final_results['rouge_raw'] = rouge_results\n",
    "    final_results['overall'] = {'fragment_density' : np.mean(fragment_densities), 'gold_fragment_density' : np.mean(gold_fragment_densities), 'coverage' : np.mean(extractive_coverages), 'gold_coverage' : np.mean(gold_extractive_coverages), 'overlap' : np.mean(overlaps), 'gold_overlap' : np.mean(gold_overlaps), 'f_value' : np.mean(f_value), 'gold_f_value' : np.mean(gold_f_value), 'rouge1' : np.mean(rouge_1s), 'rouge2' : np.mean(rouge_2s), 'rouge3' : np.mean(rouge_3s), 'rougeL' : np.mean(rouge_Ls), 'number' : len(candidates)}\n",
    "    normal_indexes = [index for index, control_value in enumerate(control_values) if control_value == 'normal']\n",
    "    normal_fragment_densities = [fragment_densities[index] for index in normal_indexes]\n",
    "    normal_extractive_coverages = [extractive_coverages[index] for index in normal_indexes]\n",
    "    normal_overlaps = [overlaps[index] for index in normal_indexes]\n",
    "    normal_f_value = [f_value[index] for index in normal_indexes]\n",
    "    normal_rouge1 = [rouge_1s[index] for index in normal_indexes]\n",
    "    normal_rouge2 = [rouge_2s[index] for index in normal_indexes]\n",
    "    normal_rouge3 = [rouge_3s[index] for index in normal_indexes]\n",
    "    normal_rougeL = [rouge_Ls[index] for index in normal_indexes]\n",
    "    gold_normal_fragment_densities = [gold_fragment_densities[index] for index in normal_indexes]\n",
    "    gold_normal_extractive_coverages = [gold_extractive_coverages[index] for index in normal_indexes]\n",
    "    gold_normal_overlaps = [gold_overlaps[index] for index in normal_indexes]\n",
    "    gold_normal_f_value = [gold_f_value[index] for index in normal_indexes]\n",
    "\n",
    "    final_results['normal'] = {'fragment_density' : np.mean(normal_fragment_densities), 'gold_fragment_density' : np.mean(gold_normal_fragment_densities), 'coverage' : np.mean(normal_extractive_coverages), 'gold_coverage' : np.mean(gold_normal_extractive_coverages), 'overlap' : np.mean(normal_overlaps), 'gold_overlap' : np.mean(gold_normal_overlaps), 'f_value' : np.mean(normal_f_value), 'gold_f_value' : np.mean(gold_normal_f_value), 'rouge1' : np.mean(normal_rouge1), 'rouge2' : np.mean(normal_rouge2), 'rouge3' : np.mean(normal_rouge3), 'rougeL' : np.mean(normal_rougeL), 'number' : len(normal_indexes)}\n",
    "    high_indexes = [index for index, control_value in enumerate(control_values) if control_value == 'high']\n",
    "    high_fragment_densities = [fragment_densities[index] for index in high_indexes]\n",
    "    high_extractive_coverages = [extractive_coverages[index] for index in high_indexes]\n",
    "    high_overlaps = [overlaps[index] for index in high_indexes]\n",
    "    high_f_value = [f_value[index] for index in high_indexes]\n",
    "    high_rouge1 = [rouge_1s[index] for index in high_indexes]\n",
    "    high_rouge2 = [rouge_2s[index] for index in high_indexes]\n",
    "    high_rouge3 = [rouge_3s[index] for index in high_indexes]\n",
    "    high_rougeL = [rouge_Ls[index] for index in high_indexes]\n",
    "    gold_high_fragment_densities = [gold_fragment_densities[index] for index in high_indexes]\n",
    "    gold_high_extractive_coverages = [gold_extractive_coverages[index] for index in high_indexes]\n",
    "    gold_high_overlaps = [gold_overlaps[index] for index in high_indexes]\n",
    "    gold_high_f_value = [gold_f_value[index] for index in high_indexes]\n",
    "    final_results['high'] = {'fragment_density' : np.mean(high_fragment_densities), 'gold_fragment_density' : np.mean(gold_high_fragment_densities), 'coverage' : np.mean(high_extractive_coverages), 'gold_coverage' : np.mean(gold_high_extractive_coverages), 'overlap' : np.mean(high_overlaps), 'gold_overlap' : np.mean(gold_high_overlaps), 'f_value' : np.mean(high_f_value), 'gold_f_value' : np.mean(gold_high_f_value), 'rouge1' : np.mean(high_rouge1), 'rouge2' : np.mean(high_rouge2), 'rouge3' : np.mean(high_rouge3), 'rougeL' : np.mean(high_rougeL), 'number' : len(high_indexes)}\n",
    "\n",
    "    fully_indexes = [index for index, control_value in enumerate(control_values) if control_value == 'fully']\n",
    "    fully_fragment_densities = [fragment_densities[index] for index in fully_indexes]\n",
    "    fully_extractive_coverages = [extractive_coverages[index] for index in fully_indexes]\n",
    "    fully_overlaps = [overlaps[index] for index in fully_indexes]\n",
    "    fully_f_value = [f_value[index] for index in fully_indexes]\n",
    "    fully_rouge1 = [rouge_1s[index] for index in fully_indexes]\n",
    "    fully_rouge2 = [rouge_2s[index] for index in fully_indexes]\n",
    "    fully_rouge3 = [rouge_3s[index] for index in fully_indexes]\n",
    "    fully_rougeL = [rouge_Ls[index] for index in fully_indexes]\n",
    "    gold_fully_fragment_densities = [gold_fragment_densities[index] for index in fully_indexes]\n",
    "    gold_fully_extractive_coverages = [gold_extractive_coverages[index] for index in fully_indexes]\n",
    "    gold_fully_overlaps = [gold_overlaps[index] for index in fully_indexes]\n",
    "    gold_fully_f_value = [gold_f_value[index] for index in fully_indexes]\n",
    "    final_results['fully'] = {'fragment_density' : np.mean(fully_fragment_densities), 'gold_fragment_density' : np.mean(gold_fully_fragment_densities), 'coverage' : np.mean(fully_extractive_coverages), 'gold_coverage' : np.mean(gold_fully_extractive_coverages), 'overlap' : np.mean(fully_overlaps), 'gold_overlap' : np.mean(gold_fully_overlaps), 'f_value' : np.mean(fully_f_value), 'gold_f_value' : np.mean(gold_fully_f_value), 'rouge1' : np.mean(fully_rouge1), 'rouge2' : np.mean(fully_rouge2), 'rouge3' : np.mean(fully_rouge3), 'rougeL' : np.mean(fully_rougeL), 'number' : len(fully_indexes)}\n",
    "\n",
    "    print(\"extractiveness evaluation\")\n",
    "    for key in final_results.keys():\n",
    "        if key == 'rouge_raw':\n",
    "            continue\n",
    "        print(f\"--------------{key}----------------\")\n",
    "        res = final_results[key]\n",
    "        for sub_key in res.keys():\n",
    "            print(f\"{sub_key} : {res[sub_key]}\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    return final_results\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def get_model_and_attributes(filename):\n",
    "    all_attributes = ['length', 'extractiveness' , 'topic', 'specificity']\n",
    "    basename = os.path.basename(filename)\n",
    "    name, ext = os.path.splitext(basename)\n",
    "    model = name.split(\"_\")[0]\n",
    "    splits = name.split(\"_\")\n",
    "    attributes = [attribute for attribute in splits if attribute in all_attributes]\n",
    "    return model, attributes\n",
    "\n",
    "def load_pickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_results(file, candidates, references, articles, attribute, control_values):\n",
    "    print(f\"{file} is being evaluated on {attribute}\")\n",
    "    if attribute == 'length':\n",
    "        return get_length_results(candidates, references, articles, control_values)\n",
    "    elif attribute == 'extractiveness':\n",
    "        return get_extractiveness_results(candidates, references, articles, control_values)\n",
    "    else:\n",
    "        print(\"not implemented for this attribute\", attribute)\n",
    "        None \n",
    "\n",
    "def evaluate(file, supported_evaluation = ['length','extractiveness']):\n",
    "    model, attributes = get_model_and_attributes(file)\n",
    "    print(f\"evaluating {model} with attributes {attributes}\")\n",
    "    data = load_pickle(file)\n",
    "\n",
    "\n",
    "\n",
    "    if len(attributes) == 1:\n",
    "        candidates = [item['predicted_summary'] for item in data.values()]\n",
    "        references = [item['output'] for item in data.values()]\n",
    "        articles = [item['input'] for item in data.values()]\n",
    "        control_values = [item['control_value'][0] for item in data.values()]\n",
    "        assert len(candidates) == len(references) == len(articles) == len(control_values)\n",
    "        print(\"length of data is \", len(candidates))\n",
    "        if attributes[0] in supported_evaluation:\n",
    "            outputs = get_results(file, candidates, references, articles, attributes[0], control_values),  f\"Error in length mismatch candidates {len(candidates)} references {len(references)} articles {len(articles)} control_values {len(control_values)}\"\n",
    "            return {attributes[0] : outputs}\n",
    "        else:\n",
    "            print(f\"{attributes[0]} is not supported for evaluation\")\n",
    "            print(f\"skipping {attributes[0]} for {os.path.basename(file)}\")\n",
    "            return {attributes[0] : None}\n",
    "    else:\n",
    "        #filter given none of the control values are ''\n",
    "        #print(data.values())\n",
    "        candidates = [item['predicted_summary'] for item in data.values() if item['control_value'][0] != '' and item['control_value'][1] != '']\n",
    "        references = [item['output'] for item in data.values() if item['control_value'][0] != '' and item['control_value'][1] != '']\n",
    "        articles = [item['input'] for item in data.values() if item['control_value'][0] != '' and item['control_value'][1] != '']\n",
    "        first_control_values = []\n",
    "        second_control_values = []\n",
    "        for item in data.values():\n",
    "            for index, attribute in enumerate(item['control_attribute']):\n",
    "                if attribute == attributes[0]:\n",
    "                    first_control_values.append(item['control_value'][index])\n",
    "                if attribute == attributes[1]:\n",
    "                    second_control_values.append(item['control_value'][index])\n",
    "        assert len(candidates) == len(references) == len(articles) == len(first_control_values) == len(second_control_values), f\"Error in length mismatch candidates {len(candidates)} references {len(references)} articles {len(articles)} control_values {len(first_control_values)}, {len(second_control_values)}\"\n",
    "        print(\"length of data is \", len(candidates))\n",
    "        if attributes[0] not in supported_evaluation:\n",
    "            print(f\"{attributes[0]} is not supported for evaluation\")\n",
    "            print(f\"skipping {attributes[0]} for {os.path.basename(file)}\")\n",
    "            first_outputs = None\n",
    "        else:\n",
    "            first_outputs = get_results(file, candidates, references, articles, attributes[0], first_control_values)\n",
    "        if attributes[1] not in supported_evaluation:\n",
    "            print(f\"{attributes[1]} is not supported for evaluation\")\n",
    "            print(f\"skipping {attributes[1]} for {os.path.basename(file)}\")\n",
    "            second_outputs = None\n",
    "        else:\n",
    "            second_outputs = get_results(file, candidates, references, articles, attributes[1], second_control_values)\n",
    "        return {attributes[0] : first_outputs, attributes[1] : second_outputs}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def zero_shot_evaluation(zero_shot_directory):\n",
    "    files = [os.path.join(zero_shot_directory, file) for file in os.listdir(zero_shot_directory) if file.endswith(\".pkl\")]\n",
    "    results = {}\n",
    "    for file in files:\n",
    "        model_results = evaluate(file)\n",
    "        results[file] = model_results\n",
    "    #save results\n",
    "    output_dir = \"/scratch/tathagato/naacl/compiled_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    output_file = os.path.join(output_dir, \"zero_shot_results.pkl\")\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pkl.dump(results, f)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mistral with attributes ['extractiveness']\n",
      "length of data is  547\n",
      "/scratch/tathagato/naacl/zero_shot/mistral_extractiveness.pkl is being evaluated on extractiveness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/tathagato/miniconda3/envs/roy/lib/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extractiveness evaluation\n",
      "--------------overall----------------\n",
      "fragment_density : 4.792034612402258\n",
      "gold_fragment_density : 3.5740066422781958\n",
      "coverage : 0.8666580095075271\n",
      "gold_coverage : 0.8769379470940913\n",
      "overlap : 0.5141632289605639\n",
      "gold_overlap : 0.4790941100760585\n",
      "f_value : 0.39449032393162653\n",
      "gold_f_value : 0.44421539489139666\n",
      "rouge1 : 0.22753121351994113\n",
      "rouge2 : 0.07977617969258015\n",
      "rouge3 : 0.035638649025248936\n",
      "rougeL : 0.1395743182877806\n",
      "number : 547\n",
      "-------------------------------------------------\n",
      "--------------normal----------------\n",
      "fragment_density : 4.75563122955075\n",
      "gold_fragment_density : 3.0470808464199073\n",
      "coverage : 0.8650705490847009\n",
      "gold_coverage : 0.866959576381\n",
      "overlap : 0.5106737666573987\n",
      "gold_overlap : 0.44346948972372485\n",
      "f_value : 0.3500997596328743\n",
      "gold_f_value : 0.44128871385747576\n",
      "rouge1 : 0.23731405986819326\n",
      "rouge2 : 0.08140401945924802\n",
      "rouge3 : 0.03522534588073913\n",
      "rougeL : 0.14318989800071977\n",
      "number : 467\n",
      "-------------------------------------------------\n",
      "--------------high----------------\n",
      "fragment_density : 5.161856983363319\n",
      "gold_fragment_density : 5.795913412523072\n",
      "coverage : 0.8844328118455643\n",
      "gold_coverage : 0.9251531914859265\n",
      "overlap : 0.5409798999259361\n",
      "gold_overlap : 0.6154424686586898\n",
      "f_value : 0.5490566696774377\n",
      "gold_f_value : 0.46996998870358725\n",
      "rouge1 : 0.175664298375226\n",
      "rouge2 : 0.0704414310735077\n",
      "rouge3 : 0.03657972679440199\n",
      "rougeL : 0.1179847562353359\n",
      "number : 43\n",
      "-------------------------------------------------\n",
      "--------------fully----------------\n",
      "fragment_density : 4.821710770248963\n",
      "gold_fragment_density : 7.64244868404282\n",
      "coverage : 0.8660371315865592\n",
      "gold_coverage : 0.9468472339634081\n",
      "overlap : 0.5270405815029218\n",
      "gold_overlap : 0.770275685359483\n",
      "f_value : 0.7751400715112862\n",
      "gold_f_value : 0.4512238410783391\n",
      "rouge1 : 0.16433359477910295\n",
      "rouge2 : 0.07007869373545118\n",
      "rouge3 : 0.03976151995531637\n",
      "rougeL : 0.1190304113232544\n",
      "number : 37\n",
      "-------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "evaluating llama with attributes ['extractiveness', 'specificity']\n",
      "length of data is  547\n",
      "/scratch/tathagato/naacl/zero_shot/llama_extractiveness_and_specificity.pkl is being evaluated on extractiveness\n",
      "extractiveness evaluation\n",
      "--------------overall----------------\n",
      "fragment_density : 3.200199879680703\n",
      "gold_fragment_density : 3.5740066422781958\n",
      "coverage : 0.9014329439259938\n",
      "gold_coverage : 0.8769379470940913\n",
      "overlap : 0.5050742784939468\n",
      "gold_overlap : 0.4790941100760585\n",
      "f_value : 0.39449032393162653\n",
      "gold_f_value : 0.3935447733737069\n",
      "rouge1 : 0.30272027718421596\n",
      "rouge2 : 0.09527703970804517\n",
      "rouge3 : 0.03977233701654131\n",
      "rougeL : 0.18855604933777156\n",
      "number : 547\n",
      "-------------------------------------------------\n",
      "--------------normal----------------\n",
      "fragment_density : 3.0099243481493443\n",
      "gold_fragment_density : 3.0470808464199073\n",
      "coverage : 0.8988141482026891\n",
      "gold_coverage : 0.866959576381\n",
      "overlap : 0.4974804823954432\n",
      "gold_overlap : 0.44346948972372485\n",
      "f_value : 0.3500997596328743\n",
      "gold_f_value : 0.3864833426448362\n",
      "rouge1 : 0.3144294155971922\n",
      "rouge2 : 0.09818711851902291\n",
      "rouge3 : 0.04062122505840623\n",
      "rougeL : 0.19388224916228083\n",
      "number : 467\n",
      "-------------------------------------------------\n",
      "--------------high----------------\n",
      "fragment_density : 3.4313057593149274\n",
      "gold_fragment_density : 5.795913412523072\n",
      "coverage : 0.9178778795019177\n",
      "gold_coverage : 0.9251531914859265\n",
      "overlap : 0.5398573267426028\n",
      "gold_overlap : 0.6154424686586898\n",
      "f_value : 0.5490566696774377\n",
      "gold_f_value : 0.42342673042455303\n",
      "rouge1 : 0.2448771658280607\n",
      "rouge2 : 0.07136991264502113\n",
      "rouge3 : 0.026824267209875137\n",
      "rougeL : 0.15760381920078745\n",
      "number : 43\n",
      "-------------------------------------------------\n",
      "--------------fully----------------\n",
      "fragment_density : 5.333203133758346\n",
      "gold_fragment_density : 7.64244868404282\n",
      "coverage : 0.9153747107643344\n",
      "gold_coverage : 0.9468472339634081\n",
      "overlap : 0.5604967569617569\n",
      "gold_overlap : 0.770275685359483\n",
      "f_value : 0.7751400715112862\n",
      "gold_f_value : 0.4479438003249576\n",
      "rouge1 : 0.22215503798029082\n",
      "rouge2 : 0.08633108454543538\n",
      "rouge3 : 0.0441057501553448\n",
      "rougeL : 0.157302281171406\n",
      "number : 37\n",
      "-------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "specificity is not supported for evaluation\n",
      "skipping specificity for llama_extractiveness_and_specificity.pkl\n",
      "evaluating mistral with attributes ['topic', 'extractiveness']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Error in length mismatch candidates 266 references 266 articles 266 control_values 547, 547",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m zero_shot_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratch/tathagato/naacl/zero_shot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mzero_shot_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzero_shot_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 365\u001b[0m, in \u001b[0;36mzero_shot_evaluation\u001b[0;34m(zero_shot_directory)\u001b[0m\n\u001b[1;32m    363\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m--> 365\u001b[0m     model_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m     results[file] \u001b[38;5;241m=\u001b[39m model_results\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m#save results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 342\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(file, supported_evaluation)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attribute \u001b[38;5;241m==\u001b[39m attributes[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    341\u001b[0m             second_control_values\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol_value\u001b[39m\u001b[38;5;124m'\u001b[39m][index])\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(candidates) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(references) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(first_control_values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(second_control_values), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in length mismatch candidates \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m references \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m articles \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(articles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m control_values \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(first_control_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(second_control_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of data is \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(candidates))\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attributes[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_evaluation:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Error in length mismatch candidates 266 references 266 articles 266 control_values 547, 547"
     ]
    }
   ],
   "source": [
    "zero_shot_directory = \"/scratch/tathagato/naacl/zero_shot\"\n",
    "results = zero_shot_evaluation(zero_shot_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal']\n",
      "['extractiveness']\n",
      "The 2014 Khumbu Icefall on Everest caused 16 Nepali deaths last year, resulting in at the end of the Nepal season. Mountaineers return to Everest after the deadly 2014 season. To avoid the deadly area, changes were made to the routes. Climbing permits have increased this year.\n",
      "\n",
      "1. Mount Everest climbing season has resumed for 2023 after a fatal season in 2022.\n",
      "2. In 2014, an avalanche caused by a piece of glacial ice falling killed 16 Nepalis, which was the deadliest incident on Mount Everest.\n",
      "3. To reduce risks, the route through Khumbu Icefall, where the 16 were killed, has been changed to a longer, but safer, central route.\n",
      "4. Nepal has issued 347 permits for climbing Mount Everest this year, with 125 of them from the shortened 2022 season.\n",
      "5. The route change means climbers will have to cross more crevasses and use more ladders.\n",
      "6. A local Nepalese committee selected the central route for safety reasons.\n",
      "7. Some climbers have decided to use the northern route from China instead due to safety concerns.\n",
      "8. The Chinese side of Everest has seen an increase in popularity with 320 people registered to climb the northern route this year, 136 more than last year.\n",
      "9. Nepali companies are concerned about climbers diverting to the Chinese side, as Nepal depends heavily on tourism dollars.\n",
      "10. Many Sherpas, who lead expeditions, are returning to Everest this year despite family concerns and memories of the 2022 tragedy.\n",
      "11. Mountaineer Jon Reiter is among those returning this year, citing reasons similar to those he couldn't find words for last year.\n",
      "(CNN)Mountaineers have returned to Mount Everest for this year's climbing season, resuming the quest to summit the world's highest peak after a deadly season last year. In 2014, the Nepal climbing season ended after a piece of glacial ice fell, unleashing an avalanche that killed 16 Nepalis who had just finished their morning prayers. The April 18 accident was the single deadliest incident to ever occur on Mount Everest. The deaths launched fierce debates about the enormous risks faced by the Sherpas and the dangers of climbing Everest. In order to reduce risks, the route through Khumbu Icefall, the notoriously treacherous path where the 16 were killed, has been changed to one that takes longer but is expected to be safer. \"They're going in the icefall and, as we found out on April 18, it's the most dangerous place,\" said Conrad Anker, a veteran climber who has been to Everest three times. \"They're exposed to the tumbling ice, hanging seracs above it. It's very, very dangerous. It's the most dangerous place I've been in the mountains.\" At this point in the season, climbing teams have not yet entered Khumbu Icefall, which is essentially a frozen river rapid with jagged pieces breaking off and moving. Nepal has issued 347 permits this year to climb Mount Everest, with 125 of them from the previously shortened season, according to the Nepal Ministry of Tourism. It's a slight increase from the 334 who were given permission last year. The local Nepalese committee that determines the path up Everest announced in February that a different route had been selected. The climbers will now take a central route through the Khumbu Icefall, avoiding the area where the deaths occurred. The committee comprised of Sherpas voted to return to the central route for safety reasons. \"There will be little risk of avalanche than in the right or left,\" said Yangji Doma Sherpa, the spokeswoman for the Sagarmatha Pollution Control Committee. The central route had been used in the 1990s, but was abandoned in favor of a quicker route, she said. The new path means climbers will have to cross more crevasses, and use more vertical and horizontal ladders. The committee issued a recommendation that the weight of workers' gear be limited to avoid overloading the ladders. \"I think it will be an hour longer on the icefall,\" said Alan Arnette, who is blogging from Everest base camp this season. \"I don't think it will be game changer.\" But one company, Alpenglow Expeditions, said it would stop climbing from the Nepal side, where the climbers have to go through the icefall, in favor of the northern route from China. \"We've seen it get progressively more dangerous over the last few years,\" said  Adrian Ballinger, the company's founder and CEO. \"We believe the risk is too great for our workers.\" According to the China Tibet Mountaineering Association, 320 people have been registered to climb the northern route to Everest this year. That's 136 more than last year. The Chinese side of Everest has typically been less popular than its Nepal counterpart, because of concerns of government closures. Some Everest observers say the northern route has harsher weather and more rocky terrain, but it also doesn't have an icefall. The increasing popularity of the northern route has caused concern amongst Nepali companies that climbers will divert to the Chinese side. \"I can already see the shift with mountaineers I speak to,\" said Dawa Steven Sherpa, who is based in Nepal. \"More people are going to go to Tibet than Nepal. Nepal needs the tourism far more than China does. China has incredible wealth of resources and Nepal does not.\" Leading expeditions is how Sherpas feed their families and send their children to school. Nepal depends heavily on tourism dollars. Many of the guides had to bury their friends after the accident last year, and while they may be ready to return to the summit, their families are not. Many of them are \"leaving behind nervous, stressed-out wives and children,\" whose memories of what happened last year are fresh, said Dawa Sherpa, managing director of Asian Trekking. \"They do say they don't want to put them through that again,\" he said. \"They're not fearful for their own lives, it's what they're putting their family through.\" Several mountaineers are also returning this year. One of them is Jon Reiter, who spoke to CNN last year after the tragedy. When the icy avalanche thundered down, Reiter was shoved behind an ice block by his Sherpa guide. Reiter, who is making his way to base camp this year, could not be reached directly. But he explained why he's heading back to Everest this year on his blog. \"I can't quite find the words to tell you why, or what really pulls me back to the mountains,\" he wrote. \"When we were in the midst of last year's events it was hard to see the big picture. It was hard to remember that people die in the mountains but that it's more rare than not. \"It was hard for me to remember that I'm not choosing between my life at home and dying in the mountains. I like to think it's similar to surviving a plane crash or a major pile up on the freeway.\" CNN's Sugam Pokharel contributed to this report.\n",
      "dict_keys(['input', 'predicted_summary', 'reference', 'generated_text', 'control_value', 'control_attribute', 'input_ids', 'output', 'prompt'])\n"
     ]
    }
   ],
   "source": [
    "print(data[0]['control_value'])\n",
    "print(data[0]['control_attribute'])\n",
    "print(data[0]['output'])\n",
    "print()\n",
    "print(data[0]['predicted_summary'])\n",
    "print(data[0]['input'])\n",
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
